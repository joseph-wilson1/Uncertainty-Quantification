{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s4531973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\s4531973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import lsqr, LinearOperator, cg#, minres\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor, v2\n",
    "from torch.optim import lr_scheduler\n",
    "from functorch import make_functional, vmap, vjp, jvp, jacrev\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data/MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data/MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "N_TRAIN = 1000\n",
    "N_TEST = 100\n",
    "N_OUTPUT = 10\n",
    "\n",
    "training_data = torch.utils.data.Subset(training_data,range(N_TRAIN))\n",
    "test_data = torch.utils.data.Subset(test_data,range(N_TEST))\n",
    "    \n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters p = 1301\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 3, 5)\n",
    "        self.fc1 = nn.Linear(3 * 4 * 4, 10)\n",
    "        # self.fc2 = nn.Linear(120,84)\n",
    "        # self.fc3 = nn.Linear(84,10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,len(x.shape)-3)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "cnn = CNN()\n",
    "print(\"Number of parameters p = {}\".format(sum(p.numel() for p in cnn.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303186  [   50/ 1000]\n",
      "loss: 2.288059  [  550/ 1000]\n",
      "Test Accuracy: 8.0%, Avg loss: 2.243985 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.247828  [   50/ 1000]\n",
      "loss: 2.178858  [  550/ 1000]\n",
      "Test Accuracy: 52.0%, Avg loss: 1.636130 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.730775  [   50/ 1000]\n",
      "loss: 1.503000  [  550/ 1000]\n",
      "Test Accuracy: 73.0%, Avg loss: 0.807350 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.952932  [   50/ 1000]\n",
      "loss: 1.180496  [  550/ 1000]\n",
      "Test Accuracy: 80.0%, Avg loss: 0.616059 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.744069  [   50/ 1000]\n",
      "loss: 0.914102  [  550/ 1000]\n",
      "Test Accuracy: 84.0%, Avg loss: 0.531699 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.600971  [   50/ 1000]\n",
      "loss: 0.749042  [  550/ 1000]\n",
      "Test Accuracy: 85.0%, Avg loss: 0.476241 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.503416  [   50/ 1000]\n",
      "loss: 0.677698  [  550/ 1000]\n",
      "Test Accuracy: 87.0%, Avg loss: 0.434928 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.431369  [   50/ 1000]\n",
      "loss: 0.630729  [  550/ 1000]\n",
      "Test Accuracy: 86.0%, Avg loss: 0.385735 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.368116  [   50/ 1000]\n",
      "loss: 0.588956  [  550/ 1000]\n",
      "Test Accuracy: 89.0%, Avg loss: 0.339777 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.310955  [   50/ 1000]\n",
      "loss: 0.561292  [  550/ 1000]\n",
      "Test Accuracy: 91.0%, Avg loss: 0.298794 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.273035  [   50/ 1000]\n",
      "loss: 0.529323  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.263808 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.244981  [   50/ 1000]\n",
      "loss: 0.493273  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.236026 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.222825  [   50/ 1000]\n",
      "loss: 0.459595  [  550/ 1000]\n",
      "Test Accuracy: 93.0%, Avg loss: 0.214063 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.204643  [   50/ 1000]\n",
      "loss: 0.426813  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.197181 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.189157  [   50/ 1000]\n",
      "loss: 0.399420  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.181427 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.173752  [   50/ 1000]\n",
      "loss: 0.367143  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.168895 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.161364  [   50/ 1000]\n",
      "loss: 0.336912  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.160393 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.150212  [   50/ 1000]\n",
      "loss: 0.311049  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.149264 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.140810  [   50/ 1000]\n",
      "loss: 0.285589  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.139101 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.131164  [   50/ 1000]\n",
      "loss: 0.263962  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.131976 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.122649  [   50/ 1000]\n",
      "loss: 0.245907  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.124031 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.113184  [   50/ 1000]\n",
      "loss: 0.232361  [  550/ 1000]\n",
      "Test Accuracy: 97.0%, Avg loss: 0.118514 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.105360  [   50/ 1000]\n",
      "loss: 0.221686  [  550/ 1000]\n",
      "Test Accuracy: 97.0%, Avg loss: 0.114614 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.099295  [   50/ 1000]\n",
      "loss: 0.208059  [  550/ 1000]\n",
      "Test Accuracy: 96.0%, Avg loss: 0.110831 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.093213  [   50/ 1000]\n",
      "loss: 0.197528  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.112192 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.091776  [   50/ 1000]\n",
      "loss: 0.192080  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.110449 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.086182  [   50/ 1000]\n",
      "loss: 0.184733  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.108429 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.081152  [   50/ 1000]\n",
      "loss: 0.180308  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.108958 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.075423  [   50/ 1000]\n",
      "loss: 0.173409  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.111174 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.072286  [   50/ 1000]\n",
      "loss: 0.173117  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.109855 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.068159  [   50/ 1000]\n",
      "loss: 0.166765  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.114384 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.066624  [   50/ 1000]\n",
      "loss: 0.162198  [  550/ 1000]\n",
      "Test Accuracy: 95.0%, Avg loss: 0.116522 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.063331  [   50/ 1000]\n",
      "loss: 0.156869  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.119889 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.059403  [   50/ 1000]\n",
      "loss: 0.156885  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.118272 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.058855  [   50/ 1000]\n",
      "loss: 0.148958  [  550/ 1000]\n",
      "Test Accuracy: 93.0%, Avg loss: 0.124393 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.056812  [   50/ 1000]\n",
      "loss: 0.148083  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.130481 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.054912  [   50/ 1000]\n",
      "loss: 0.146087  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.135547 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.049873  [   50/ 1000]\n",
      "loss: 0.141679  [  550/ 1000]\n",
      "Test Accuracy: 92.0%, Avg loss: 0.132996 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.048696  [   50/ 1000]\n",
      "loss: 0.136074  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.139626 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.045969  [   50/ 1000]\n",
      "loss: 0.138473  [  550/ 1000]\n",
      "Test Accuracy: 93.0%, Avg loss: 0.140899 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.040890  [   50/ 1000]\n",
      "loss: 0.133419  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.146174 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.041874  [   50/ 1000]\n",
      "loss: 0.127278  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.153922 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.040927  [   50/ 1000]\n",
      "loss: 0.121971  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.155883 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.040867  [   50/ 1000]\n",
      "loss: 0.121552  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.159392 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.036854  [   50/ 1000]\n",
      "loss: 0.111659  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.161603 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.031559  [   50/ 1000]\n",
      "loss: 0.107868  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.170593 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.029705  [   50/ 1000]\n",
      "loss: 0.101494  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.163479 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.026266  [   50/ 1000]\n",
      "loss: 0.094084  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.173161 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.026267  [   50/ 1000]\n",
      "loss: 0.089205  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.172492 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.024626  [   50/ 1000]\n",
      "loss: 0.083581  [  550/ 1000]\n",
      "Test Accuracy: 94.0%, Avg loss: 0.173256 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, cnn, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, cnn, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find NTK\n",
    "def flatten_extend_gradient(parameters):\n",
    "    flat_list = []\n",
    "    for parameter in parameters:\n",
    "        flat_list.extend(parameter.grad.detach().numpy().flatten())\n",
    "    return flat_list\n",
    "\n",
    "def gradient_model(model,xi,c):\n",
    "    ## model needs to have parameters with requires_grad=true\n",
    "    optimizer.zero_grad()\n",
    "    model(xi)[c].backward()\n",
    "    grad_vec = np.array(flatten_extend_gradient(list(model.parameters())))\n",
    "    return grad_vec\n",
    "\n",
    "def ntk_single(x1,x2,model,c):\n",
    "    j1 = gradient_model(model,x1,c)\n",
    "    j2 = gradient_model(model,x2,c)\n",
    "    j = j1 @ j2.transpose()\n",
    "    return j\n",
    "\n",
    "def ntk_matrix(X1,X2,model,c):\n",
    "# x1, x2 must become torch variables\n",
    "    n1 = len(X1)\n",
    "    n2 = len(X2)\n",
    "    Kappa = np.empty((n1,n2))\n",
    "    for i1,x1 in enumerate(X1):\n",
    "        if type(x1) is tuple:\n",
    "            x1,_ = x1\n",
    "        for i2,x2 in enumerate(X2):\n",
    "            x2 = x2.reshape((1,28,-1))\n",
    "            if type(x2) is tuple:\n",
    "                x2,_ = x2\n",
    "            Kappa[i1,i2] = ntk_single(x1,x2,model,c)\n",
    "            break\n",
    "    return Kappa\n",
    "\n",
    "def MVP_JTX(v,model,X_training,c):\n",
    "    p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    mvp = np.zeros((p,1))\n",
    "    for i,(xi,_) in enumerate(X_training):\n",
    "        g = gradient_model(model,xi,c).reshape((p,1))\n",
    "        mvp += v[i]*g\n",
    "    return mvp\n",
    "\n",
    "def MVP_JX(v,model,X_training,c):\n",
    "    p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n = N_TRAIN\n",
    "    mvp = np.zeros((n,1))\n",
    "    v = v.reshape((p,1))\n",
    "    for i,(xi,_) in enumerate(X_training):\n",
    "        g = gradient_model(model,xi,c).reshape((p,1))\n",
    "        mvp[i,0] = g.transpose() @ v\n",
    "    return mvp\n",
    "\n",
    "def MVP_JJT(v,model,X_training,c):\n",
    "    x1 = MVP_JTX(v,model,X_training,c)\n",
    "    x2 = MVP_JX(x1,model,X_training,c)\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solvers as solv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(xi,yi) in enumerate(test_data):\n",
    "    for c in range(N_OUTPUT):\n",
    "        kappa_xX = ntk_matrix(training_data,xi,cnn,c)\n",
    "        mvp = lambda v : MVP_JJT(v=v,model=cnn,X_training=training_data, c = c)\n",
    "        A = LinearOperator((N_TRAIN,N_TRAIN), matvec=mvp)\n",
    "        b = kappa_xX\n",
    "        x = solv.CR(A,b,rtol=1e-7,maxit=50)\n",
    "        break\n",
    "    break\n",
    "kappa_xx = ntk_single(xi,xi,cnn,c)\n",
    "uq = kappa_xx - b.transpose() @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      "  1   |  1.00e+00  |  1.00e+00 \n",
      "  2   |  3.48e-01  |  9.04e-02 \n",
      "  3   |  1.21e-01  |  1.72e-02 \n",
      "  4   |  5.46e-02  |  6.07e-03 \n",
      "  5   |  3.19e-02  |  1.56e-03 \n",
      "  6   |  2.20e-02  |  1.40e-03 \n",
      "  7   |  1.47e-02  |  5.11e-04 \n",
      "  8   |  1.03e-02  |  4.18e-04 \n",
      "  9   |  7.06e-03  |  9.59e-05 \n",
      " 10   |  5.71e-03  |  1.40e-04 \n",
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      " 11   |  4.85e-03  |  9.21e-05 \n",
      " 12   |  3.96e-03  |  4.13e-04 \n",
      " 13   |  3.92e-03  |  2.40e-04 \n",
      " 14   |  3.45e-03  |  3.62e-05 \n",
      " 15   |  2.62e-03  |  2.81e-05 \n",
      " 16   |  2.15e-03  |  1.71e-05 \n",
      " 17   |  1.85e-03  |  1.26e-05 \n",
      " 18   |  1.58e-03  |  1.09e-05 \n",
      " 19   |  1.47e-03  |  1.19e-04 \n",
      " 20   |  1.32e-03  |  9.42e-06 \n",
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      " 21   |  1.16e-03  |  3.20e-05 \n",
      " 22   |  1.15e-03  |  9.59e-05 \n",
      " 23   |  1.05e-03  |  6.78e-06 \n",
      " 24   |  9.72e-04  |  5.04e-05 \n",
      " 25   |  8.95e-04  |  5.07e-06 \n",
      " 26   |  7.73e-04  |  2.77e-06 \n",
      " 27   |  6.68e-04  |  1.48e-05 \n",
      " 28   |  6.53e-04  |  5.75e-06 \n",
      " 29   |  5.93e-04  |  2.46e-06 \n",
      " 30   |  5.42e-04  |  6.11e-06 \n",
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      " 31   |  5.38e-04  |  6.26e-05 \n",
      " 32   |  4.67e-04  |  5.12e-06 \n",
      " 33   |  4.58e-04  |  4.31e-06 \n",
      " 34   |  4.08e-04  |  1.87e-05 \n",
      " 35   |  4.01e-04  |  1.63e-06 \n",
      " 36   |  3.68e-04  |  1.94e-06 \n",
      " 37   |  3.59e-04  |  4.44e-06 \n",
      " 38   |  3.42e-04  |  1.28e-06 \n",
      " 39   |  3.22e-04  |  5.68e-06 \n",
      " 40   |  3.08e-04  |  7.22e-06 \n",
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      " 41   |  3.08e-04  |  9.79e-06 \n",
      " 42   |  2.82e-04  |  6.99e-06 \n",
      " 43   |  2.78e-04  |  1.15e-06 \n",
      " 44   |  2.59e-04  |  3.39e-07 \n",
      " 45   |  2.38e-04  |  1.39e-06 \n",
      " 46   |  2.35e-04  |  6.24e-07 \n",
      " 47   |  2.20e-04  |  2.03e-06 \n",
      " 48   |  2.19e-04  |  2.27e-06 \n",
      " 49   |  2.17e-04  |  4.50e-06 \n",
      " 50   |  2.06e-04  |  3.79e-05 \n",
      "-------------------------------------\n",
      " ite  |  |rk|/|b|  | |Ark|/|Ab|\n",
      "-------------------------------------\n",
      " 51   |  2.02e-04  |  5.27e-07 \n"
     ]
    }
   ],
   "source": [
    "for i,(xi,yi) in enumerate(test_data):\n",
    "    for c in range(N_OUTPUT):\n",
    "        kappa_xX = ntk_matrix(training_data,xi,cnn,c)\n",
    "        mvp = lambda v : MVP_JJT(v=v,model=cnn,X_training=training_data, c = c)\n",
    "        A = LinearOperator((N_TRAIN,N_TRAIN), matvec=mvp)\n",
    "        b = kappa_xX\n",
    "        x = solv.CR(A,b,rtol=1e-7,maxit=50)\n",
    "        break\n",
    "    break\n",
    "kappa_xx = ntk_single(xi,xi,cnn,c)\n",
    "uq = kappa_xx - b.transpose() @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.50427673]]\n",
      "tensor([  0.2595,   1.5719,  15.7995,  18.2848, -17.5986,  -6.8794, -18.9326,\n",
      "         27.6804,  -3.4832,  -4.5646], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(uq)\n",
    "print(cnn(xi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
