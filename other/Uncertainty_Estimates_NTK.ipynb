{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Regression Problem from Deep Ensemble Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s4531973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\s4531973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor, v2\n",
    "from torch.optim import lr_scheduler\n",
    "from functorch import make_functional, vmap, vjp, jvp, jacrev\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_TRAIN_POINTS = 20\n",
    "eps = np.random.normal(loc = 0, scale = 3, size = (1,NUM_TRAIN_POINTS))\n",
    "training_data_x = np.random.uniform(low=-4,high=4,size = (1,NUM_TRAIN_POINTS))\n",
    "# training_data_x = np.linspace(-4,4,NUM_TRAIN_POINTS)\n",
    "training_data_y = np.power(training_data_x,3) + eps\n",
    "# training_data_y = training_data_x + eps\n",
    "\n",
    "train_x = training_data_x\n",
    "train_y = training_data_y\n",
    "\n",
    "training_data_y_std = training_data_y.std()\n",
    "training_data_y_mean = training_data_y.mean()\n",
    "\n",
    "# training_data_x = training_data_x - training_data_x.mean()\n",
    "# training_data_x /= training_data_x.std()\n",
    "# training_data_y = training_data_y - training_data_y.mean()\n",
    "# training_data_y /= training_data_y.std()\n",
    "\n",
    "plt.scatter(training_data_x,training_data_y)\n",
    "\n",
    "NUM_TEST_POINTS = 400\n",
    "\n",
    "test_data_x = np.linspace(-6,6,NUM_TEST_POINTS)\n",
    "test_data_y = np.power(test_data_x,3)\n",
    "# test_data_y = test_data_x\n",
    "test_data_x_1 = 1\n",
    "\n",
    "# test_data_x = test_data_x - test_data_x.mean()\n",
    "# test_data_x /= test_data_x.std()\n",
    "\n",
    "X_training = torch.from_numpy(training_data_x.reshape(-1,1)).float()\n",
    "y = torch.from_numpy(training_data_y.reshape(-1,1)).float()\n",
    "x_test = torch.from_numpy(test_data_x.reshape(-1,1)).float()\n",
    "y_test = torch.from_numpy(test_data_y.reshape(-1,1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up MSE Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "tensor(1363.3627)\n",
      "tensor(223.6316)\n",
      "tensor(1500.4470)\n",
      "tensor(90.3881)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 8094.511230\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "tensor(1358.5349)\n",
      "tensor(222.2868)\n",
      "tensor(1490.5916)\n",
      "tensor(88.5374)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 8052.939453\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "tensor(1348.1661)\n",
      "tensor(219.1350)\n",
      "tensor(1469.2301)\n",
      "tensor(84.2903)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 7957.653320\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "tensor(1331.7484)\n",
      "tensor(214.4434)\n",
      "tensor(1429.6024)\n",
      "tensor(75.2397)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 7755.307129\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "tensor(1324.6500)\n",
      "tensor(209.8935)\n",
      "tensor(1377.4503)\n",
      "tensor(58.2238)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 7376.029297\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "tensor(1310.0624)\n",
      "tensor(206.7736)\n",
      "tensor(1353.5330)\n",
      "tensor(30.9936)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 6763.893066\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "tensor(1360.8151)\n",
      "tensor(203.4924)\n",
      "tensor(1412.7489)\n",
      "tensor(6.0388)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 5895.328125\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "tensor(1365.5326)\n",
      "tensor(217.8157)\n",
      "tensor(1509.1984)\n",
      "tensor(49.1365)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 4741.375488\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "tensor(1131.4479)\n",
      "tensor(231.1908)\n",
      "tensor(1434.3613)\n",
      "tensor(94.3742)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 3425.070557\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "tensor(588.5424)\n",
      "tensor(291.6821)\n",
      "tensor(1119.5310)\n",
      "tensor(133.4321)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 2344.305176\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "tensor(732.1425)\n",
      "tensor(507.5664)\n",
      "tensor(1350.2576)\n",
      "tensor(151.5965)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 2242.171631\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "tensor(1895.1482)\n",
      "tensor(777.5228)\n",
      "tensor(2316.6748)\n",
      "tensor(122.3718)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 3229.490723\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "tensor(2547.5264)\n",
      "tensor(840.8002)\n",
      "tensor(2788.1985)\n",
      "tensor(33.1333)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 4001.582031\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "tensor(2276.7727)\n",
      "tensor(759.9395)\n",
      "tensor(2453.5312)\n",
      "tensor(78.8311)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 3657.067627\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "tensor(1521.0801)\n",
      "tensor(716.3829)\n",
      "tensor(1829.5563)\n",
      "tensor(159.1993)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 2838.083496\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "tensor(768.3229)\n",
      "tensor(692.4096)\n",
      "tensor(1480.9637)\n",
      "tensor(184.3757)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 2329.774658\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "tensor(663.8129)\n",
      "tensor(681.5153)\n",
      "tensor(1455.1223)\n",
      "tensor(166.6266)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 2212.950928\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "tensor(939.9876)\n",
      "tensor(575.2530)\n",
      "tensor(1471.1846)\n",
      "tensor(130.1772)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 2226.251953\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "tensor(1099.8287)\n",
      "tensor(474.3712)\n",
      "tensor(1419.1951)\n",
      "tensor(93.4494)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 2224.801514\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "tensor(1141.6174)\n",
      "tensor(394.3043)\n",
      "tensor(1327.0541)\n",
      "tensor(64.5441)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 2189.495117\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "tensor(1123.1512)\n",
      "tensor(338.0650)\n",
      "tensor(1229.3627)\n",
      "tensor(43.8767)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 2141.675537\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "tensor(1075.3097)\n",
      "tensor(291.3611)\n",
      "tensor(1123.6588)\n",
      "tensor(26.3677)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 2086.169678\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "tensor(984.0765)\n",
      "tensor(239.4615)\n",
      "tensor(975.2669)\n",
      "tensor(5.1132)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 2008.343872\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "tensor(816.9220)\n",
      "tensor(195.0516)\n",
      "tensor(762.7429)\n",
      "tensor(25.7290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 1900.239502\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "tensor(595.9645)\n",
      "tensor(248.8336)\n",
      "tensor(646.2122)\n",
      "tensor(67.5707)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 1805.245850\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "tensor(642.8955)\n",
      "tensor(420.1646)\n",
      "tensor(989.8976)\n",
      "tensor(113.6820)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 1824.950439\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "tensor(1012.1206)\n",
      "tensor(617.0488)\n",
      "tensor(1541.4648)\n",
      "tensor(146.8861)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 2000.302002\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "tensor(1236.3527)\n",
      "tensor(717.1172)\n",
      "tensor(1816.8281)\n",
      "tensor(143.9453)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 2128.864258\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "tensor(1209.0419)\n",
      "tensor(577.6177)\n",
      "tensor(1498.8448)\n",
      "tensor(89.9084)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 1943.449585\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "tensor(764.8326)\n",
      "tensor(363.4463)\n",
      "tensor(721.7357)\n",
      "tensor(3.4737)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 1595.562012\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "tensor(459.0847)\n",
      "tensor(431.7267)\n",
      "tensor(790.4269)\n",
      "tensor(94.9785)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 1529.053467\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "tensor(849.8109)\n",
      "tensor(548.3135)\n",
      "tensor(1365.3992)\n",
      "tensor(141.5731)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 1701.876709\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "tensor(962.6151)\n",
      "tensor(428.7879)\n",
      "tensor(1433.1678)\n",
      "tensor(127.6817)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 1711.272827\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "tensor(776.1713)\n",
      "tensor(200.7320)\n",
      "tensor(1026.8157)\n",
      "tensor(68.9891)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 1472.959229\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "tensor(504.1816)\n",
      "tensor(173.5610)\n",
      "tensor(537.2851)\n",
      "tensor(0.6359)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 1260.500000\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "tensor(316.9801)\n",
      "tensor(312.3355)\n",
      "tensor(513.5774)\n",
      "tensor(54.0460)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 1194.003540\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "tensor(370.5817)\n",
      "tensor(407.7760)\n",
      "tensor(739.3258)\n",
      "tensor(80.9866)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 1202.205200\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "tensor(459.9351)\n",
      "tensor(433.8815)\n",
      "tensor(833.5982)\n",
      "tensor(85.5367)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 1201.588867\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "tensor(484.6719)\n",
      "tensor(416.7044)\n",
      "tensor(810.5480)\n",
      "tensor(78.2720)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 1176.900146\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "tensor(475.7470)\n",
      "tensor(386.8561)\n",
      "tensor(740.8124)\n",
      "tensor(67.9531)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 1145.223022\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "tensor(450.2566)\n",
      "tensor(356.4986)\n",
      "tensor(664.3985)\n",
      "tensor(58.3272)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 1117.399780\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "tensor(395.6016)\n",
      "tensor(329.6569)\n",
      "tensor(583.5018)\n",
      "tensor(48.8002)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 1091.633667\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "tensor(329.7151)\n",
      "tensor(291.7621)\n",
      "tensor(470.4812)\n",
      "tensor(35.8695)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 1060.763306\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "tensor(250.9617)\n",
      "tensor(235.1236)\n",
      "tensor(296.3067)\n",
      "tensor(15.2995)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 1021.991760\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "tensor(150.9373)\n",
      "tensor(180.5033)\n",
      "tensor(174.8514)\n",
      "tensor(14.9937)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 991.150696\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "tensor(293.2016)\n",
      "tensor(216.9549)\n",
      "tensor(444.1412)\n",
      "tensor(51.9979)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 998.024475\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "tensor(481.2290)\n",
      "tensor(303.1931)\n",
      "tensor(769.9406)\n",
      "tensor(86.0665)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 1043.696411\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "tensor(594.2321)\n",
      "tensor(355.9966)\n",
      "tensor(945.1386)\n",
      "tensor(102.3820)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 1067.644165\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "tensor(557.5847)\n",
      "tensor(304.2575)\n",
      "tensor(848.1618)\n",
      "tensor(88.7482)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 993.937073\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "tensor(374.2483)\n",
      "tensor(168.6540)\n",
      "tensor(489.5010)\n",
      "tensor(45.9313)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 854.806946\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "tensor(239.4786)\n",
      "tensor(122.8841)\n",
      "tensor(252.3569)\n",
      "tensor(9.2912)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 771.766541\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "tensor(423.0730)\n",
      "tensor(258.0462)\n",
      "tensor(599.5966)\n",
      "tensor(51.6447)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 781.638489\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "tensor(526.2469)\n",
      "tensor(315.6097)\n",
      "tensor(753.7889)\n",
      "tensor(62.2327)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 782.360107\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "tensor(461.0250)\n",
      "tensor(263.2393)\n",
      "tensor(611.9747)\n",
      "tensor(41.7638)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 711.929504\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "tensor(291.3895)\n",
      "tensor(192.2957)\n",
      "tensor(331.7004)\n",
      "tensor(7.0382)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 627.850403\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "tensor(150.6086)\n",
      "tensor(179.8033)\n",
      "tensor(205.9871)\n",
      "tensor(23.6513)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 585.442688\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "tensor(190.0271)\n",
      "tensor(199.5675)\n",
      "tensor(313.3424)\n",
      "tensor(41.4471)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 574.741394\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "tensor(246.4092)\n",
      "tensor(202.5966)\n",
      "tensor(382.4033)\n",
      "tensor(47.0310)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 570.192688\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "tensor(266.6931)\n",
      "tensor(191.1440)\n",
      "tensor(391.3970)\n",
      "tensor(45.4699)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 562.760010\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "tensor(266.4435)\n",
      "tensor(172.7041)\n",
      "tensor(372.0089)\n",
      "tensor(41.4378)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 554.270447\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "tensor(256.4281)\n",
      "tensor(156.5879)\n",
      "tensor(345.3094)\n",
      "tensor(37.2508)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 546.747009\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "tensor(241.6210)\n",
      "tensor(140.9477)\n",
      "tensor(314.6033)\n",
      "tensor(32.8851)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 539.539246\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "tensor(218.1402)\n",
      "tensor(121.1878)\n",
      "tensor(269.8390)\n",
      "tensor(26.7590)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 530.341797\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "tensor(179.1786)\n",
      "tensor(93.1397)\n",
      "tensor(200.6539)\n",
      "tensor(16.8244)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 517.664124\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "tensor(131.9441)\n",
      "tensor(73.8491)\n",
      "tensor(141.9934)\n",
      "tensor(1.9456)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 504.348724\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "tensor(156.8530)\n",
      "tensor(110.7412)\n",
      "tensor(223.7468)\n",
      "tensor(16.3407)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 497.825165\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "tensor(259.4539)\n",
      "tensor(174.2151)\n",
      "tensor(374.5706)\n",
      "tensor(32.7727)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 500.999512\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "tensor(340.1165)\n",
      "tensor(211.5536)\n",
      "tensor(459.8690)\n",
      "tensor(39.5889)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 499.850983\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "tensor(317.6382)\n",
      "tensor(191.5684)\n",
      "tensor(397.2228)\n",
      "tensor(31.0778)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 474.248444\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "tensor(176.5610)\n",
      "tensor(130.3350)\n",
      "tensor(194.4526)\n",
      "tensor(9.4915)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 434.826416\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "tensor(139.5688)\n",
      "tensor(119.1138)\n",
      "tensor(149.7930)\n",
      "tensor(14.2108)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 416.515076\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "tensor(291.0369)\n",
      "tensor(142.8184)\n",
      "tensor(320.8530)\n",
      "tensor(27.3789)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 420.455872\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "tensor(342.0110)\n",
      "tensor(130.5657)\n",
      "tensor(351.3085)\n",
      "tensor(24.5015)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 414.673065\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "tensor(268.4471)\n",
      "tensor(77.3312)\n",
      "tensor(244.5174)\n",
      "tensor(9.7160)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 391.635345\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "tensor(130.5411)\n",
      "tensor(54.9698)\n",
      "tensor(116.2873)\n",
      "tensor(7.7041)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 372.359863\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "tensor(79.2186)\n",
      "tensor(91.2776)\n",
      "tensor(157.0119)\n",
      "tensor(20.3774)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 367.218262\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "tensor(147.4705)\n",
      "tensor(116.2913)\n",
      "tensor(223.2500)\n",
      "tensor(25.7941)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 367.471161\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "tensor(181.0519)\n",
      "tensor(121.7275)\n",
      "tensor(237.9315)\n",
      "tensor(25.6761)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 365.823639\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "tensor(152.9193)\n",
      "tensor(113.7166)\n",
      "tensor(219.3801)\n",
      "tensor(23.0049)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 362.264862\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "tensor(135.6932)\n",
      "tensor(103.0495)\n",
      "tensor(191.8650)\n",
      "tensor(19.9958)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 358.931091\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "tensor(116.7450)\n",
      "tensor(93.7196)\n",
      "tensor(166.1950)\n",
      "tensor(17.4302)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 356.391907\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "tensor(96.7577)\n",
      "tensor(84.9512)\n",
      "tensor(140.8810)\n",
      "tensor(15.0075)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 354.261261\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "tensor(69.7390)\n",
      "tensor(73.9536)\n",
      "tensor(107.2545)\n",
      "tensor(11.8309)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 351.893829\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "tensor(38.5902)\n",
      "tensor(60.1463)\n",
      "tensor(59.4355)\n",
      "tensor(6.9396)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 349.269165\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "tensor(72.4688)\n",
      "tensor(50.7262)\n",
      "tensor(56.8271)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 347.715118\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "tensor(151.9693)\n",
      "tensor(62.2339)\n",
      "tensor(140.4257)\n",
      "tensor(8.1983)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 349.103638\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "tensor(218.9543)\n",
      "tensor(86.3592)\n",
      "tensor(214.8413)\n",
      "tensor(15.0649)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 352.062897\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "tensor(228.5458)\n",
      "tensor(94.3856)\n",
      "tensor(228.2925)\n",
      "tensor(17.2158)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 350.308807\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "tensor(153.0239)\n",
      "tensor(74.8158)\n",
      "tensor(152.2025)\n",
      "tensor(12.4862)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 341.086060\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "tensor(25.6736)\n",
      "tensor(41.7219)\n",
      "tensor(28.9129)\n",
      "tensor(2.1959)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 333.663788\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "tensor(144.7520)\n",
      "tensor(57.4025)\n",
      "tensor(151.7825)\n",
      "tensor(8.3578)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 336.021973\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "tensor(210.8385)\n",
      "tensor(75.0456)\n",
      "tensor(218.2466)\n",
      "tensor(13.1262)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 339.214325\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "tensor(169.9952)\n",
      "tensor(58.2233)\n",
      "tensor(174.7721)\n",
      "tensor(9.7576)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 334.005524\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "tensor(75.1053)\n",
      "tensor(34.2577)\n",
      "tensor(67.8006)\n",
      "tensor(0.9743)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 327.150360\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "tensor(75.7924)\n",
      "tensor(57.0341)\n",
      "tensor(68.2543)\n",
      "tensor(8.0214)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 325.672699\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "tensor(121.4440)\n",
      "tensor(79.9760)\n",
      "tensor(128.5387)\n",
      "tensor(13.5417)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 326.535553\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "tensor(126.7260)\n",
      "tensor(84.1711)\n",
      "tensor(140.9785)\n",
      "tensor(14.8364)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 325.887115\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "tensor(103.9655)\n",
      "tensor(77.5205)\n",
      "tensor(121.8948)\n",
      "tensor(13.3315)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 323.889587\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "tensor(77.1944)\n",
      "tensor(66.9924)\n",
      "tensor(93.8503)\n",
      "tensor(10.8825)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 322.016083\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "tensor(56.4395)\n",
      "tensor(57.9696)\n",
      "tensor(69.8938)\n",
      "tensor(8.6499)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 320.809784\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "tensor(43.4786)\n",
      "tensor(51.4460)\n",
      "tensor(52.7136)\n",
      "tensor(6.9256)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 320.113770\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "tensor(35.5415)\n",
      "tensor(46.0725)\n",
      "tensor(39.0778)\n",
      "tensor(5.3975)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 319.636658\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "tensor(32.4794)\n",
      "tensor(40.0924)\n",
      "tensor(26.5583)\n",
      "tensor(3.4897)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 319.191437\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "tensor(41.3404)\n",
      "tensor(33.5533)\n",
      "tensor(29.3372)\n",
      "tensor(0.6829)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 318.786743\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "tensor(63.6104)\n",
      "tensor(31.4503)\n",
      "tensor(58.2902)\n",
      "tensor(3.1275)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 318.614746\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "tensor(87.7276)\n",
      "tensor(38.5215)\n",
      "tensor(92.6162)\n",
      "tensor(7.2640)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 318.686249\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "tensor(95.5169)\n",
      "tensor(45.2668)\n",
      "tensor(111.7393)\n",
      "tensor(10.1957)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 318.397888\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "tensor(75.2530)\n",
      "tensor(42.4224)\n",
      "tensor(97.3926)\n",
      "tensor(10.1053)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 316.818909\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "tensor(27.8069)\n",
      "tensor(27.0255)\n",
      "tensor(48.3951)\n",
      "tensor(6.3638)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 314.667969\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "tensor(49.3348)\n",
      "tensor(21.6940)\n",
      "tensor(40.0584)\n",
      "tensor(0.6087)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 314.000610\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "tensor(93.5347)\n",
      "tensor(35.2223)\n",
      "tensor(85.4693)\n",
      "tensor(3.7143)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 314.599823\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "tensor(97.6421)\n",
      "tensor(35.6016)\n",
      "tensor(89.0701)\n",
      "tensor(3.8242)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 314.038513\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "tensor(64.4830)\n",
      "tensor(22.8874)\n",
      "tensor(53.3573)\n",
      "tensor(0.2392)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 312.149628\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "tensor(26.9170)\n",
      "tensor(26.4930)\n",
      "tensor(40.8102)\n",
      "tensor(5.8073)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 310.853821\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "tensor(34.1872)\n",
      "tensor(40.9749)\n",
      "tensor(71.6364)\n",
      "tensor(9.8795)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 310.465088\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "tensor(43.0244)\n",
      "tensor(45.7963)\n",
      "tensor(83.6101)\n",
      "tensor(11.1165)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 309.953217\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "tensor(36.2440)\n",
      "tensor(41.0420)\n",
      "tensor(74.5307)\n",
      "tensor(10.0612)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 309.050842\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "tensor(23.4288)\n",
      "tensor(32.6736)\n",
      "tensor(57.1037)\n",
      "tensor(8.0459)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 308.185028\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "tensor(13.8759)\n",
      "tensor(25.1574)\n",
      "tensor(41.8560)\n",
      "tensor(6.1277)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 307.613861\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "tensor(12.9134)\n",
      "tensor(20.4823)\n",
      "tensor(32.6656)\n",
      "tensor(4.7359)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 307.300262\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "tensor(15.7623)\n",
      "tensor(18.0435)\n",
      "tensor(28.2362)\n",
      "tensor(3.8166)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 307.127411\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "tensor(18.9438)\n",
      "tensor(16.6854)\n",
      "tensor(26.1162)\n",
      "tensor(3.1012)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 307.004425\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "tensor(22.7759)\n",
      "tensor(15.8957)\n",
      "tensor(25.3619)\n",
      "tensor(2.3096)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 306.871704\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "tensor(27.6898)\n",
      "tensor(16.1420)\n",
      "tensor(26.7727)\n",
      "tensor(1.2808)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 306.692963\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "tensor(36.6915)\n",
      "tensor(21.0595)\n",
      "tensor(30.7247)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 306.439972\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "tensor(35.9265)\n",
      "tensor(21.6997)\n",
      "tensor(34.0656)\n",
      "tensor(0.9799)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 306.063416\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "tensor(31.6409)\n",
      "tensor(23.2542)\n",
      "tensor(31.5243)\n",
      "tensor(1.3666)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 305.515320\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "tensor(21.1281)\n",
      "tensor(22.4322)\n",
      "tensor(20.6599)\n",
      "tensor(0.7310)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 304.836914\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "tensor(19.6137)\n",
      "tensor(21.3981)\n",
      "tensor(14.3865)\n",
      "tensor(0.6988)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 304.213257\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "tensor(33.0892)\n",
      "tensor(22.6352)\n",
      "tensor(26.6798)\n",
      "tensor(2.0119)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 303.724426\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "tensor(40.2545)\n",
      "tensor(23.9394)\n",
      "tensor(31.5477)\n",
      "tensor(2.1673)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 303.162689\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "tensor(38.1802)\n",
      "tensor(26.9188)\n",
      "tensor(22.9931)\n",
      "tensor(0.8513)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 302.362488\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "tensor(31.3934)\n",
      "tensor(30.4391)\n",
      "tensor(16.3319)\n",
      "tensor(1.2033)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 301.474213\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "tensor(29.6479)\n",
      "tensor(33.2270)\n",
      "tensor(25.1564)\n",
      "tensor(2.8291)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 300.659607\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "tensor(27.7521)\n",
      "tensor(33.2483)\n",
      "tensor(29.9470)\n",
      "tensor(3.3303)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 299.855499\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "tensor(26.7264)\n",
      "tensor(31.1994)\n",
      "tensor(26.6362)\n",
      "tensor(2.7888)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 299.081177\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "tensor(25.4509)\n",
      "tensor(28.1287)\n",
      "tensor(20.0243)\n",
      "tensor(1.7605)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 298.415619\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "tensor(25.2473)\n",
      "tensor(25.7830)\n",
      "tensor(15.3664)\n",
      "tensor(0.7709)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 297.918823\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "tensor(25.6378)\n",
      "tensor(24.5105)\n",
      "tensor(14.1713)\n",
      "tensor(0.0684)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 297.581421\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "tensor(25.9492)\n",
      "tensor(23.9135)\n",
      "tensor(14.4942)\n",
      "tensor(0.3419)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 297.363739\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "tensor(26.0486)\n",
      "tensor(23.6167)\n",
      "tensor(14.9293)\n",
      "tensor(0.5629)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 297.220123\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "tensor(25.9984)\n",
      "tensor(23.4167)\n",
      "tensor(15.2472)\n",
      "tensor(0.7000)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 297.100861\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "tensor(25.7693)\n",
      "tensor(23.2049)\n",
      "tensor(15.4920)\n",
      "tensor(0.8139)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 296.953308\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "tensor(27.3436)\n",
      "tensor(25.2100)\n",
      "tensor(15.6130)\n",
      "tensor(0.9096)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 296.726990\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "tensor(26.5464)\n",
      "tensor(24.9004)\n",
      "tensor(15.5601)\n",
      "tensor(0.9491)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 296.382324\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "tensor(22.8526)\n",
      "tensor(22.0335)\n",
      "tensor(15.5553)\n",
      "tensor(0.8941)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 295.894440\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "tensor(30.4062)\n",
      "tensor(22.0594)\n",
      "tensor(16.2883)\n",
      "tensor(0.7757)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 295.287109\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "tensor(31.2680)\n",
      "tensor(21.5189)\n",
      "tensor(17.9662)\n",
      "tensor(0.7447)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 294.561157\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "tensor(28.8220)\n",
      "tensor(18.8136)\n",
      "tensor(19.7326)\n",
      "tensor(0.9567)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 293.724762\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "tensor(16.9451)\n",
      "tensor(14.2804)\n",
      "tensor(20.8771)\n",
      "tensor(1.4756)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 292.905182\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "tensor(15.3417)\n",
      "tensor(14.4286)\n",
      "tensor(21.9989)\n",
      "tensor(2.1408)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 292.307220\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "tensor(26.4896)\n",
      "tensor(17.7603)\n",
      "tensor(23.5461)\n",
      "tensor(2.6971)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 291.704865\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "tensor(26.1056)\n",
      "tensor(18.3716)\n",
      "tensor(24.3967)\n",
      "tensor(2.9404)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 291.107025\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "tensor(24.9186)\n",
      "tensor(17.7843)\n",
      "tensor(23.6023)\n",
      "tensor(2.7932)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 290.470581\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "tensor(11.6815)\n",
      "tensor(14.7157)\n",
      "tensor(21.8848)\n",
      "tensor(2.3985)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 289.899414\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "tensor(12.6391)\n",
      "tensor(14.3046)\n",
      "tensor(20.3478)\n",
      "tensor(1.9384)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 289.434357\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "tensor(13.2647)\n",
      "tensor(14.3056)\n",
      "tensor(19.3689)\n",
      "tensor(1.5731)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 289.064117\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "tensor(13.4243)\n",
      "tensor(14.4893)\n",
      "tensor(18.7576)\n",
      "tensor(1.3550)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 288.788025\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "tensor(20.3227)\n",
      "tensor(19.2294)\n",
      "tensor(18.3375)\n",
      "tensor(1.2553)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 288.599731\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "tensor(13.0976)\n",
      "tensor(14.8074)\n",
      "tensor(18.0469)\n",
      "tensor(1.2229)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 288.471832\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "tensor(12.9254)\n",
      "tensor(14.8927)\n",
      "tensor(17.8579)\n",
      "tensor(1.2199)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 288.390167\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "tensor(12.7767)\n",
      "tensor(14.9560)\n",
      "tensor(17.7105)\n",
      "tensor(1.2295)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 288.323029\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "tensor(12.6090)\n",
      "tensor(15.0254)\n",
      "tensor(17.5478)\n",
      "tensor(1.2520)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 288.241119\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "tensor(12.4267)\n",
      "tensor(15.1224)\n",
      "tensor(17.3448)\n",
      "tensor(1.2955)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 288.116669\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "tensor(12.3550)\n",
      "tensor(15.2624)\n",
      "tensor(17.1292)\n",
      "tensor(1.3604)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 287.926971\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "tensor(12.6025)\n",
      "tensor(15.4511)\n",
      "tensor(16.9320)\n",
      "tensor(1.4205)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 287.656525\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "tensor(19.4753)\n",
      "tensor(20.1206)\n",
      "tensor(16.6318)\n",
      "tensor(1.4117)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 287.302643\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "tensor(13.7183)\n",
      "tensor(15.9912)\n",
      "tensor(16.0479)\n",
      "tensor(1.2527)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 286.855347\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "tensor(13.9136)\n",
      "tensor(16.3880)\n",
      "tensor(15.1758)\n",
      "tensor(0.9207)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 286.334076\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "tensor(23.1225)\n",
      "tensor(17.3910)\n",
      "tensor(14.6617)\n",
      "tensor(0.5052)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 285.771088\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "tensor(30.5125)\n",
      "tensor(18.0576)\n",
      "tensor(14.5565)\n",
      "tensor(0.2100)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 285.190033\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "tensor(24.8217)\n",
      "tensor(17.6775)\n",
      "tensor(14.1414)\n",
      "tensor(0.1761)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 284.557648\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "tensor(16.6591)\n",
      "tensor(17.0519)\n",
      "tensor(13.6105)\n",
      "tensor(0.3270)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 283.941467\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "tensor(17.1952)\n",
      "tensor(16.9615)\n",
      "tensor(13.4993)\n",
      "tensor(0.4936)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 283.379974\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "tensor(16.2428)\n",
      "tensor(16.9322)\n",
      "tensor(13.9154)\n",
      "tensor(0.5954)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 282.876312\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "tensor(20.9402)\n",
      "tensor(21.3773)\n",
      "tensor(14.4172)\n",
      "tensor(0.6272)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 282.464508\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "tensor(20.6402)\n",
      "tensor(21.4089)\n",
      "tensor(14.7968)\n",
      "tensor(0.6256)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 282.112610\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "tensor(13.2621)\n",
      "tensor(16.6854)\n",
      "tensor(14.9586)\n",
      "tensor(0.6186)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 281.854980\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "tensor(13.0239)\n",
      "tensor(16.5108)\n",
      "tensor(14.9339)\n",
      "tensor(0.6219)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 281.679169\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "tensor(12.9026)\n",
      "tensor(16.3417)\n",
      "tensor(14.8665)\n",
      "tensor(0.6326)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 281.564056\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "tensor(22.0941)\n",
      "tensor(16.7230)\n",
      "tensor(14.8103)\n",
      "tensor(0.6458)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 281.490051\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "tensor(22.0515)\n",
      "tensor(16.6146)\n",
      "tensor(14.7463)\n",
      "tensor(0.6639)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 281.428192\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "tensor(12.9812)\n",
      "tensor(15.8889)\n",
      "tensor(14.6683)\n",
      "tensor(0.6936)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 281.353119\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "tensor(13.2178)\n",
      "tensor(15.6219)\n",
      "tensor(14.6198)\n",
      "tensor(0.7424)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 281.240509\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "tensor(24.3064)\n",
      "tensor(16.2194)\n",
      "tensor(14.6869)\n",
      "tensor(0.8178)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 281.071320\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "tensor(24.4904)\n",
      "tensor(15.9405)\n",
      "tensor(14.9230)\n",
      "tensor(0.9325)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 280.830963\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "tensor(14.9268)\n",
      "tensor(14.4310)\n",
      "tensor(15.3020)\n",
      "tensor(1.0701)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 280.506470\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "tensor(14.1547)\n",
      "tensor(14.2406)\n",
      "tensor(15.6058)\n",
      "tensor(1.1683)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 280.103851\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "tensor(12.3327)\n",
      "tensor(14.3283)\n",
      "tensor(15.8992)\n",
      "tensor(1.1958)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 279.627411\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "tensor(27.8751)\n",
      "tensor(21.2883)\n",
      "tensor(16.5408)\n",
      "tensor(1.1780)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 279.122711\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "tensor(11.6706)\n",
      "tensor(14.8319)\n",
      "tensor(17.0850)\n",
      "tensor(1.1858)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 278.544800\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "tensor(11.3638)\n",
      "tensor(14.5733)\n",
      "tensor(16.9930)\n",
      "tensor(1.2311)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 277.974304\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "tensor(11.2122)\n",
      "tensor(14.0173)\n",
      "tensor(16.5971)\n",
      "tensor(1.2846)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 277.414185\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "tensor(12.1271)\n",
      "tensor(13.5665)\n",
      "tensor(16.4579)\n",
      "tensor(1.3021)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 276.892975\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "tensor(22.6838)\n",
      "tensor(14.8262)\n",
      "tensor(16.3901)\n",
      "tensor(1.2689)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 276.444031\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "tensor(12.7613)\n",
      "tensor(13.4406)\n",
      "tensor(16.1710)\n",
      "tensor(1.2191)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 276.040985\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "tensor(54.2447)\n",
      "tensor(21.8299)\n",
      "tensor(15.9362)\n",
      "tensor(1.1738)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 275.831421\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "tensor(50.5855)\n",
      "tensor(21.2534)\n",
      "tensor(15.6279)\n",
      "tensor(1.2408)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 275.716858\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "tensor(49.1135)\n",
      "tensor(21.2669)\n",
      "tensor(15.4142)\n",
      "tensor(1.2955)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 275.556793\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "tensor(48.2624)\n",
      "tensor(21.3136)\n",
      "tensor(15.3076)\n",
      "tensor(1.3191)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 275.407990\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "tensor(17.1002)\n",
      "tensor(13.9749)\n",
      "tensor(15.2449)\n",
      "tensor(1.3228)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 275.296143\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "tensor(24.2177)\n",
      "tensor(15.3143)\n",
      "tensor(15.1051)\n",
      "tensor(1.3053)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 275.264862\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "tensor(24.2109)\n",
      "tensor(15.3878)\n",
      "tensor(14.8091)\n",
      "tensor(1.2484)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 275.218536\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "tensor(17.0069)\n",
      "tensor(14.4777)\n",
      "tensor(14.2435)\n",
      "tensor(1.1164)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 275.145172\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "tensor(21.8111)\n",
      "tensor(15.6441)\n",
      "tensor(13.5362)\n",
      "tensor(0.8567)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 275.038208\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "tensor(21.0192)\n",
      "tensor(16.3569)\n",
      "tensor(13.5484)\n",
      "tensor(0.4624)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 274.872864\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "tensor(14.4545)\n",
      "tensor(17.4396)\n",
      "tensor(14.9525)\n",
      "tensor(0.0167)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 274.650604\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "tensor(15.2853)\n",
      "tensor(18.5997)\n",
      "tensor(16.5030)\n",
      "tensor(0.3134)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 274.369385\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "tensor(23.0305)\n",
      "tensor(23.7314)\n",
      "tensor(15.9596)\n",
      "tensor(0.3390)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 274.033813\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "tensor(52.1319)\n",
      "tensor(23.2058)\n",
      "tensor(13.6831)\n",
      "tensor(0.0035)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 273.697388\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "tensor(25.8460)\n",
      "tensor(16.7361)\n",
      "tensor(12.1674)\n",
      "tensor(0.6799)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 273.262390\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "tensor(29.6416)\n",
      "tensor(16.7847)\n",
      "tensor(13.8450)\n",
      "tensor(1.0336)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 272.881866\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "tensor(44.9464)\n",
      "tensor(20.1250)\n",
      "tensor(12.6680)\n",
      "tensor(0.8464)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 272.549561\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "tensor(21.5329)\n",
      "tensor(16.7812)\n",
      "tensor(11.5812)\n",
      "tensor(0.4982)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 272.137085\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "tensor(22.0050)\n",
      "tensor(17.7936)\n",
      "tensor(11.9477)\n",
      "tensor(0.0267)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 271.830475\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "tensor(15.9842)\n",
      "tensor(18.6799)\n",
      "tensor(13.8136)\n",
      "tensor(0.2840)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 271.555695\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "tensor(48.4980)\n",
      "tensor(17.1278)\n",
      "tensor(14.8360)\n",
      "tensor(0.3407)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 271.398102\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "tensor(46.6870)\n",
      "tensor(17.3052)\n",
      "tensor(13.5563)\n",
      "tensor(0.1603)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 271.186279\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "tensor(16.1244)\n",
      "tensor(17.4749)\n",
      "tensor(12.1240)\n",
      "tensor(0.0959)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 271.087189\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "tensor(17.2789)\n",
      "tensor(17.0570)\n",
      "tensor(11.6537)\n",
      "tensor(0.2607)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 271.025085\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "tensor(17.9199)\n",
      "tensor(16.8262)\n",
      "tensor(11.5481)\n",
      "tensor(0.3554)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 270.980286\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "tensor(23.5293)\n",
      "tensor(16.9236)\n",
      "tensor(11.5477)\n",
      "tensor(0.4188)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 270.945496\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "tensor(27.0861)\n",
      "tensor(17.0463)\n",
      "tensor(11.5937)\n",
      "tensor(0.4795)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 270.900726\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "tensor(27.5011)\n",
      "tensor(16.9859)\n",
      "tensor(11.6906)\n",
      "tensor(0.5474)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 270.820984\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "tensor(18.7532)\n",
      "tensor(16.1743)\n",
      "tensor(11.8446)\n",
      "tensor(0.6130)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 270.688782\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "tensor(43.9798)\n",
      "tensor(18.7059)\n",
      "tensor(12.1312)\n",
      "tensor(0.6468)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 270.567352\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "tensor(43.9462)\n",
      "tensor(18.8893)\n",
      "tensor(12.3120)\n",
      "tensor(0.7299)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 270.371155\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "tensor(22.6399)\n",
      "tensor(15.9533)\n",
      "tensor(12.4555)\n",
      "tensor(0.8615)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 270.087982\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "tensor(18.3681)\n",
      "tensor(15.4155)\n",
      "tensor(12.4623)\n",
      "tensor(0.8382)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 269.799927\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "tensor(16.5626)\n",
      "tensor(15.6956)\n",
      "tensor(12.4355)\n",
      "tensor(0.6174)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 269.456818\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "tensor(21.7955)\n",
      "tensor(16.4164)\n",
      "tensor(13.5496)\n",
      "tensor(0.3714)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 269.100586\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "tensor(27.0030)\n",
      "tensor(16.6312)\n",
      "tensor(14.4153)\n",
      "tensor(0.3383)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 268.703735\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "tensor(53.6414)\n",
      "tensor(18.7395)\n",
      "tensor(13.9017)\n",
      "tensor(0.6431)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 268.491791\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "tensor(55.4317)\n",
      "tensor(27.0201)\n",
      "tensor(14.0689)\n",
      "tensor(1.3155)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 268.285980\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "tensor(34.1178)\n",
      "tensor(15.2454)\n",
      "tensor(16.7353)\n",
      "tensor(1.8355)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 268.085327\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "tensor(34.6030)\n",
      "tensor(15.8806)\n",
      "tensor(16.6903)\n",
      "tensor(1.7760)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 267.935699\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "tensor(34.4356)\n",
      "tensor(16.3501)\n",
      "tensor(14.9599)\n",
      "tensor(1.3409)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 267.757416\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "tensor(33.6749)\n",
      "tensor(16.5318)\n",
      "tensor(14.0511)\n",
      "tensor(0.8516)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 267.582275\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "tensor(30.5202)\n",
      "tensor(16.8808)\n",
      "tensor(14.0616)\n",
      "tensor(0.4866)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 267.437164\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "tensor(20.5335)\n",
      "tensor(17.9484)\n",
      "tensor(14.3402)\n",
      "tensor(0.2812)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 267.389893\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "tensor(20.3091)\n",
      "tensor(18.1555)\n",
      "tensor(14.3426)\n",
      "tensor(0.1540)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 267.352814\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "tensor(20.0292)\n",
      "tensor(18.2555)\n",
      "tensor(14.1838)\n",
      "tensor(0.0625)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 267.316223\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "tensor(19.6285)\n",
      "tensor(18.2882)\n",
      "tensor(13.8436)\n",
      "tensor(0.0286)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 267.265045\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "tensor(24.6970)\n",
      "tensor(18.0324)\n",
      "tensor(13.2774)\n",
      "tensor(0.1275)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 267.184204\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "tensor(47.9268)\n",
      "tensor(16.7967)\n",
      "tensor(12.6454)\n",
      "tensor(0.1922)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 267.078186\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "tensor(45.6751)\n",
      "tensor(17.8232)\n",
      "tensor(12.2187)\n",
      "tensor(0.0419)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 267.031067\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "tensor(40.9438)\n",
      "tensor(19.3794)\n",
      "tensor(14.7288)\n",
      "tensor(0.3447)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 266.883118\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "tensor(45.5449)\n",
      "tensor(24.1781)\n",
      "tensor(18.2918)\n",
      "tensor(0.8390)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 266.772644\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "tensor(31.8595)\n",
      "tensor(16.8416)\n",
      "tensor(14.9534)\n",
      "tensor(0.9354)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 266.531311\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "tensor(31.9519)\n",
      "tensor(16.9159)\n",
      "tensor(12.3103)\n",
      "tensor(0.4950)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 266.277405\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "tensor(35.3953)\n",
      "tensor(18.7792)\n",
      "tensor(21.6087)\n",
      "tensor(0.0204)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 266.070923\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "tensor(60.1378)\n",
      "tensor(16.8630)\n",
      "tensor(25.6152)\n",
      "tensor(0.1707)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 265.870667\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "tensor(23.0147)\n",
      "tensor(18.3702)\n",
      "tensor(15.0544)\n",
      "tensor(0.1863)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 265.447083\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "tensor(25.2633)\n",
      "tensor(16.8352)\n",
      "tensor(12.0696)\n",
      "tensor(0.5069)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 265.218567\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "tensor(48.6407)\n",
      "tensor(20.5542)\n",
      "tensor(16.3322)\n",
      "tensor(0.5598)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 265.113495\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "tensor(40.2016)\n",
      "tensor(20.2476)\n",
      "tensor(16.3180)\n",
      "tensor(0.6213)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 264.863373\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "tensor(29.0265)\n",
      "tensor(16.7709)\n",
      "tensor(13.8133)\n",
      "tensor(0.6189)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 264.730347\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "tensor(25.2605)\n",
      "tensor(17.1282)\n",
      "tensor(11.3625)\n",
      "tensor(0.4746)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 264.613464\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "tensor(22.5744)\n",
      "tensor(17.4216)\n",
      "tensor(11.3367)\n",
      "tensor(0.3215)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 264.524750\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "tensor(34.2997)\n",
      "tensor(17.1659)\n",
      "tensor(12.2683)\n",
      "tensor(0.2175)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 264.480408\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "tensor(34.2354)\n",
      "tensor(17.2357)\n",
      "tensor(13.0753)\n",
      "tensor(0.1751)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 264.440277\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "tensor(23.0876)\n",
      "tensor(18.0085)\n",
      "tensor(13.6879)\n",
      "tensor(0.1640)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 264.401245\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "tensor(23.2874)\n",
      "tensor(18.0749)\n",
      "tensor(14.1815)\n",
      "tensor(0.1602)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 264.358093\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "tensor(18.3259)\n",
      "tensor(18.1840)\n",
      "tensor(14.4447)\n",
      "tensor(0.1685)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 264.289978\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "tensor(50.8534)\n",
      "tensor(16.5372)\n",
      "tensor(14.2906)\n",
      "tensor(0.2190)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 264.282257\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "tensor(43.5286)\n",
      "tensor(17.7447)\n",
      "tensor(12.8658)\n",
      "tensor(0.4582)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 264.227570\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "tensor(39.9013)\n",
      "tensor(19.0402)\n",
      "tensor(11.9885)\n",
      "tensor(0.7983)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 264.050629\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "tensor(39.2873)\n",
      "tensor(22.8291)\n",
      "tensor(15.6233)\n",
      "tensor(1.1074)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 263.880737\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "tensor(50.7559)\n",
      "tensor(25.6577)\n",
      "tensor(15.8191)\n",
      "tensor(0.9576)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 263.763214\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "tensor(24.7095)\n",
      "tensor(17.2626)\n",
      "tensor(11.1431)\n",
      "tensor(0.4428)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 263.444611\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "tensor(24.8171)\n",
      "tensor(19.2219)\n",
      "tensor(16.0068)\n",
      "tensor(0.2707)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 263.218201\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "tensor(54.3216)\n",
      "tensor(23.2053)\n",
      "tensor(20.8690)\n",
      "tensor(0.6673)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 262.989777\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "tensor(49.8199)\n",
      "tensor(16.2158)\n",
      "tensor(16.9615)\n",
      "tensor(0.2367)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 262.832184\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "tensor(38.4507)\n",
      "tensor(18.2152)\n",
      "tensor(11.4974)\n",
      "tensor(0.5693)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 262.561951\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "tensor(44.3415)\n",
      "tensor(16.8250)\n",
      "tensor(17.7701)\n",
      "tensor(1.1182)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 262.305145\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "tensor(36.9581)\n",
      "tensor(17.3051)\n",
      "tensor(17.8042)\n",
      "tensor(1.1619)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 262.174744\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "tensor(29.8268)\n",
      "tensor(16.8277)\n",
      "tensor(13.3443)\n",
      "tensor(0.7689)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 261.990540\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "tensor(39.9677)\n",
      "tensor(17.5703)\n",
      "tensor(11.0882)\n",
      "tensor(0.3046)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 261.857208\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "tensor(41.8963)\n",
      "tensor(16.7558)\n",
      "tensor(11.7866)\n",
      "tensor(0.0287)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 261.760590\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "tensor(17.9591)\n",
      "tensor(18.2664)\n",
      "tensor(12.3457)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 261.704865\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "tensor(20.2202)\n",
      "tensor(18.3295)\n",
      "tensor(12.6966)\n",
      "tensor(0.1395)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 261.676880\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "tensor(29.9514)\n",
      "tensor(17.9273)\n",
      "tensor(12.8892)\n",
      "tensor(0.1658)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 261.654327\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "tensor(29.8916)\n",
      "tensor(17.9098)\n",
      "tensor(13.0424)\n",
      "tensor(0.1574)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 261.622559\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "tensor(20.6758)\n",
      "tensor(18.2290)\n",
      "tensor(13.1458)\n",
      "tensor(0.0917)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 261.555725\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "tensor(42.1968)\n",
      "tensor(16.7645)\n",
      "tensor(13.0106)\n",
      "tensor(0.0495)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 261.498383\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "tensor(49.5638)\n",
      "tensor(16.9353)\n",
      "tensor(11.9527)\n",
      "tensor(0.3458)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 261.405640\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "tensor(34.4735)\n",
      "tensor(23.4000)\n",
      "tensor(12.5202)\n",
      "tensor(0.8748)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 261.314209\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "tensor(30.2301)\n",
      "tensor(17.1125)\n",
      "tensor(14.2108)\n",
      "tensor(1.1488)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 261.180756\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "tensor(39.7228)\n",
      "tensor(26.0309)\n",
      "tensor(13.0529)\n",
      "tensor(0.9135)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 260.970093\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "tensor(33.8376)\n",
      "tensor(16.7959)\n",
      "tensor(11.3669)\n",
      "tensor(0.1910)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 260.663116\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "tensor(46.7192)\n",
      "tensor(16.1987)\n",
      "tensor(15.5256)\n",
      "tensor(0.3157)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 260.564728\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "tensor(45.4051)\n",
      "tensor(16.1320)\n",
      "tensor(14.3601)\n",
      "tensor(0.2570)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 260.315216\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "tensor(32.0944)\n",
      "tensor(16.7609)\n",
      "tensor(11.0646)\n",
      "tensor(0.2551)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 260.017365\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "tensor(28.5620)\n",
      "tensor(16.8619)\n",
      "tensor(12.4933)\n",
      "tensor(0.6700)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 259.839600\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "tensor(26.0271)\n",
      "tensor(16.7309)\n",
      "tensor(12.0340)\n",
      "tensor(0.6079)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 259.634583\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "tensor(20.9693)\n",
      "tensor(17.2414)\n",
      "tensor(11.2205)\n",
      "tensor(0.2370)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 259.413605\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "tensor(28.4641)\n",
      "tensor(17.6569)\n",
      "tensor(12.5091)\n",
      "tensor(0.1306)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 259.239929\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "tensor(49.5588)\n",
      "tensor(15.6480)\n",
      "tensor(14.2431)\n",
      "tensor(0.2416)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 259.217346\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "tensor(46.1194)\n",
      "tensor(16.0688)\n",
      "tensor(14.1523)\n",
      "tensor(0.1404)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 259.152405\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "tensor(44.6227)\n",
      "tensor(16.3081)\n",
      "tensor(13.1845)\n",
      "tensor(0.0063)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 259.073517\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "tensor(43.0384)\n",
      "tensor(16.5750)\n",
      "tensor(12.2862)\n",
      "tensor(0.1093)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 258.998749\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "tensor(20.9302)\n",
      "tensor(17.0583)\n",
      "tensor(11.6237)\n",
      "tensor(0.2203)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 258.942566\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "tensor(20.4114)\n",
      "tensor(16.7198)\n",
      "tensor(11.3111)\n",
      "tensor(0.3442)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 258.924103\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "tensor(22.5317)\n",
      "tensor(16.3754)\n",
      "tensor(11.4624)\n",
      "tensor(0.4785)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 258.890503\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "tensor(34.4097)\n",
      "tensor(15.9896)\n",
      "tensor(12.0900)\n",
      "tensor(0.5839)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 258.841217\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "tensor(36.0726)\n",
      "tensor(16.2426)\n",
      "tensor(12.2971)\n",
      "tensor(0.6382)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 258.759521\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "tensor(26.7965)\n",
      "tensor(16.6045)\n",
      "tensor(11.6945)\n",
      "tensor(0.5917)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 258.610474\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "tensor(48.2340)\n",
      "tensor(24.5053)\n",
      "tensor(12.1336)\n",
      "tensor(0.3618)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 258.454041\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "tensor(25.3913)\n",
      "tensor(17.1580)\n",
      "tensor(12.8091)\n",
      "tensor(0.2135)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 258.218719\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "tensor(51.5941)\n",
      "tensor(26.8143)\n",
      "tensor(12.5668)\n",
      "tensor(0.0853)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 258.030060\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "tensor(31.8409)\n",
      "tensor(16.5029)\n",
      "tensor(11.1744)\n",
      "tensor(0.2454)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 257.791443\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "tensor(53.7079)\n",
      "tensor(20.3301)\n",
      "tensor(11.3790)\n",
      "tensor(0.4065)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 257.597534\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "tensor(25.9195)\n",
      "tensor(16.8777)\n",
      "tensor(11.6823)\n",
      "tensor(0.4663)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 257.308105\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "tensor(37.7452)\n",
      "tensor(16.7688)\n",
      "tensor(11.5963)\n",
      "tensor(0.2271)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 257.100739\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "tensor(38.7119)\n",
      "tensor(16.4325)\n",
      "tensor(11.1997)\n",
      "tensor(0.0405)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 256.885040\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "tensor(23.0915)\n",
      "tensor(17.3867)\n",
      "tensor(11.0329)\n",
      "tensor(0.0047)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 256.756409\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "tensor(39.9191)\n",
      "tensor(20.0480)\n",
      "tensor(11.1463)\n",
      "tensor(0.0908)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 256.640259\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "tensor(21.8393)\n",
      "tensor(17.5760)\n",
      "tensor(11.2754)\n",
      "tensor(0.1708)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 256.504120\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "tensor(22.8773)\n",
      "tensor(17.4877)\n",
      "tensor(11.4298)\n",
      "tensor(0.2189)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 256.403900\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "tensor(32.1356)\n",
      "tensor(17.2933)\n",
      "tensor(11.5923)\n",
      "tensor(0.2269)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 256.345703\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "tensor(32.0729)\n",
      "tensor(17.1913)\n",
      "tensor(11.7045)\n",
      "tensor(0.1962)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 256.308258\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "tensor(31.7464)\n",
      "tensor(17.0913)\n",
      "tensor(11.8349)\n",
      "tensor(0.1462)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 256.269562\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "tensor(22.1151)\n",
      "tensor(17.3173)\n",
      "tensor(12.0900)\n",
      "tensor(0.0599)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 256.213989\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "tensor(43.2155)\n",
      "tensor(16.2711)\n",
      "tensor(12.6775)\n",
      "tensor(0.0995)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 256.229462\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "tensor(42.4870)\n",
      "tensor(16.9694)\n",
      "tensor(13.0252)\n",
      "tensor(0.3830)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 256.207977\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "tensor(40.2664)\n",
      "tensor(18.1386)\n",
      "tensor(13.0778)\n",
      "tensor(0.7691)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 256.102264\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "tensor(36.2996)\n",
      "tensor(19.3668)\n",
      "tensor(14.1690)\n",
      "tensor(1.1217)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 255.885651\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "tensor(40.3275)\n",
      "tensor(15.6993)\n",
      "tensor(16.7439)\n",
      "tensor(1.2530)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 255.848328\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "tensor(37.9119)\n",
      "tensor(15.9243)\n",
      "tensor(14.0633)\n",
      "tensor(0.8063)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 255.676697\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "tensor(34.2020)\n",
      "tensor(25.3170)\n",
      "tensor(11.6131)\n",
      "tensor(0.0573)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 255.483429\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "tensor(40.9841)\n",
      "tensor(29.8167)\n",
      "tensor(19.4777)\n",
      "tensor(0.9054)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 255.331848\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "tensor(49.7926)\n",
      "tensor(24.9287)\n",
      "tensor(22.1980)\n",
      "tensor(1.0969)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 255.061295\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "tensor(19.9693)\n",
      "tensor(18.8494)\n",
      "tensor(14.4006)\n",
      "tensor(0.5069)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 254.686890\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "tensor(36.4375)\n",
      "tensor(17.2425)\n",
      "tensor(11.8095)\n",
      "tensor(0.2980)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 254.387192\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "tensor(44.4338)\n",
      "tensor(20.1643)\n",
      "tensor(17.3970)\n",
      "tensor(1.0006)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 254.246017\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "tensor(43.4509)\n",
      "tensor(16.4985)\n",
      "tensor(19.0085)\n",
      "tensor(1.3395)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 254.104965\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "tensor(29.8883)\n",
      "tensor(16.1075)\n",
      "tensor(15.3844)\n",
      "tensor(1.1902)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 253.947495\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "tensor(24.1295)\n",
      "tensor(16.0904)\n",
      "tensor(12.7741)\n",
      "tensor(0.7766)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 253.824539\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "tensor(40.2812)\n",
      "tensor(16.7353)\n",
      "tensor(13.0176)\n",
      "tensor(0.3882)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 253.763336\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "tensor(42.5179)\n",
      "tensor(16.1492)\n",
      "tensor(13.5201)\n",
      "tensor(0.1639)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 253.716736\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "tensor(26.8961)\n",
      "tensor(17.0831)\n",
      "tensor(13.4504)\n",
      "tensor(0.0515)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 253.672089\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "tensor(26.7037)\n",
      "tensor(17.1010)\n",
      "tensor(13.2322)\n",
      "tensor(0.0220)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 253.643890\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "tensor(17.4917)\n",
      "tensor(17.5491)\n",
      "tensor(12.8184)\n",
      "tensor(0.0894)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 253.607407\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "tensor(17.3268)\n",
      "tensor(17.4262)\n",
      "tensor(12.2990)\n",
      "tensor(0.1664)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 253.553955\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "tensor(44.4356)\n",
      "tensor(15.7965)\n",
      "tensor(11.9833)\n",
      "tensor(0.2206)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 253.536209\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "tensor(43.7915)\n",
      "tensor(16.3666)\n",
      "tensor(11.7695)\n",
      "tensor(0.0875)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 253.443588\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "tensor(36.3411)\n",
      "tensor(16.1649)\n",
      "tensor(12.5037)\n",
      "tensor(0.3113)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 253.335648\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "tensor(38.6765)\n",
      "tensor(17.7830)\n",
      "tensor(13.0466)\n",
      "tensor(0.7135)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 253.179565\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "tensor(26.6775)\n",
      "tensor(16.5111)\n",
      "tensor(12.6943)\n",
      "tensor(0.7391)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 252.984650\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "tensor(19.5418)\n",
      "tensor(17.1847)\n",
      "tensor(13.0684)\n",
      "tensor(0.2923)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 252.718628\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "tensor(44.6623)\n",
      "tensor(15.5366)\n",
      "tensor(15.8020)\n",
      "tensor(0.3044)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 252.596115\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "tensor(40.8054)\n",
      "tensor(15.4680)\n",
      "tensor(13.1702)\n",
      "tensor(0.3895)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 252.308701\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "tensor(67.7612)\n",
      "tensor(35.7047)\n",
      "tensor(12.9181)\n",
      "tensor(0.0421)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 252.108643\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "tensor(37.4586)\n",
      "tensor(16.1712)\n",
      "tensor(13.3460)\n",
      "tensor(0.5635)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 251.839401\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "tensor(26.3444)\n",
      "tensor(16.5290)\n",
      "tensor(13.0347)\n",
      "tensor(0.7260)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 251.650192\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "tensor(22.5139)\n",
      "tensor(17.3758)\n",
      "tensor(14.1740)\n",
      "tensor(0.4298)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 251.474197\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "tensor(40.6606)\n",
      "tensor(15.8895)\n",
      "tensor(15.5529)\n",
      "tensor(0.0076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 251.382050\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "tensor(40.0618)\n",
      "tensor(15.5600)\n",
      "tensor(14.5951)\n",
      "tensor(0.2334)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 251.272766\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "tensor(37.6930)\n",
      "tensor(15.4776)\n",
      "tensor(12.8448)\n",
      "tensor(0.3170)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 251.150620\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "tensor(23.8191)\n",
      "tensor(17.4090)\n",
      "tensor(11.7364)\n",
      "tensor(0.3036)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 251.098099\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "tensor(25.8775)\n",
      "tensor(17.0267)\n",
      "tensor(11.3769)\n",
      "tensor(0.2804)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 251.074753\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "tensor(26.1179)\n",
      "tensor(16.9166)\n",
      "tensor(11.2817)\n",
      "tensor(0.2548)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 251.049545\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "tensor(32.6527)\n",
      "tensor(16.9149)\n",
      "tensor(11.3247)\n",
      "tensor(0.2171)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 251.018509\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "tensor(32.1215)\n",
      "tensor(16.8014)\n",
      "tensor(11.3574)\n",
      "tensor(0.1324)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 250.976318\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "tensor(58.5508)\n",
      "tensor(20.2500)\n",
      "tensor(11.1967)\n",
      "tensor(0.0071)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 250.887802\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "tensor(34.2159)\n",
      "tensor(16.7667)\n",
      "tensor(11.4131)\n",
      "tensor(0.3253)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 250.787582\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "tensor(28.0454)\n",
      "tensor(16.5145)\n",
      "tensor(12.4969)\n",
      "tensor(0.6618)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 250.701721\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "tensor(27.6247)\n",
      "tensor(16.5769)\n",
      "tensor(12.7265)\n",
      "tensor(0.7058)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 250.544525\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "tensor(20.4577)\n",
      "tensor(16.5356)\n",
      "tensor(11.8803)\n",
      "tensor(0.3517)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 250.325851\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "tensor(38.7236)\n",
      "tensor(15.7263)\n",
      "tensor(13.1602)\n",
      "tensor(0.2377)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 250.179016\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "tensor(44.4904)\n",
      "tensor(15.5779)\n",
      "tensor(13.0533)\n",
      "tensor(0.4222)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 249.972855\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "tensor(31.4616)\n",
      "tensor(16.1274)\n",
      "tensor(11.5384)\n",
      "tensor(0.0948)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 249.717880\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "tensor(25.4534)\n",
      "tensor(16.2257)\n",
      "tensor(12.1355)\n",
      "tensor(0.6405)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 249.488770\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "tensor(26.5245)\n",
      "tensor(16.6474)\n",
      "tensor(12.9483)\n",
      "tensor(0.7463)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 249.298325\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "tensor(37.9789)\n",
      "tensor(26.7955)\n",
      "tensor(12.6295)\n",
      "tensor(0.4745)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 249.075760\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "tensor(56.4679)\n",
      "tensor(32.2667)\n",
      "tensor(12.7543)\n",
      "tensor(0.0910)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 248.902420\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "tensor(54.2511)\n",
      "tensor(19.2337)\n",
      "tensor(13.0063)\n",
      "tensor(0.1480)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 248.768539\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "tensor(42.5858)\n",
      "tensor(15.5621)\n",
      "tensor(11.9344)\n",
      "tensor(0.1377)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 248.684921\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "tensor(41.4147)\n",
      "tensor(15.9146)\n",
      "tensor(11.3776)\n",
      "tensor(0.0059)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 248.589539\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "tensor(31.8339)\n",
      "tensor(15.9852)\n",
      "tensor(11.3235)\n",
      "tensor(0.1288)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 248.502960\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "tensor(23.0361)\n",
      "tensor(16.0565)\n",
      "tensor(11.3542)\n",
      "tensor(0.2204)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 248.475525\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "tensor(22.2075)\n",
      "tensor(16.0070)\n",
      "tensor(11.3675)\n",
      "tensor(0.2688)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 248.455170\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "tensor(21.8785)\n",
      "tensor(16.0405)\n",
      "tensor(11.3364)\n",
      "tensor(0.2895)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 248.423080\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "tensor(23.5781)\n",
      "tensor(16.2113)\n",
      "tensor(11.3164)\n",
      "tensor(0.2668)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 248.367294\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "tensor(36.5733)\n",
      "tensor(16.1286)\n",
      "tensor(11.6973)\n",
      "tensor(0.1921)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 248.334946\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "tensor(40.2022)\n",
      "tensor(15.7125)\n",
      "tensor(12.1401)\n",
      "tensor(0.1137)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 248.246292\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "tensor(30.6621)\n",
      "tensor(16.0679)\n",
      "tensor(11.5580)\n",
      "tensor(0.1112)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 248.106750\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "tensor(31.0242)\n",
      "tensor(16.0550)\n",
      "tensor(11.4247)\n",
      "tensor(0.1501)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 247.957321\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "tensor(22.8994)\n",
      "tensor(16.6970)\n",
      "tensor(11.7651)\n",
      "tensor(0.1750)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 247.742401\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "tensor(36.2401)\n",
      "tensor(15.7371)\n",
      "tensor(11.9548)\n",
      "tensor(0.0386)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 247.587723\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "tensor(34.5751)\n",
      "tensor(15.7950)\n",
      "tensor(11.0791)\n",
      "tensor(0.0186)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 247.338486\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "tensor(27.7449)\n",
      "tensor(16.4974)\n",
      "tensor(12.2526)\n",
      "tensor(0.0303)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 247.113861\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "tensor(25.4817)\n",
      "tensor(16.8253)\n",
      "tensor(11.9879)\n",
      "tensor(0.1331)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 246.879883\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "tensor(30.7740)\n",
      "tensor(17.0477)\n",
      "tensor(11.6673)\n",
      "tensor(0.3967)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 246.674805\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "tensor(45.9553)\n",
      "tensor(19.6856)\n",
      "tensor(14.4260)\n",
      "tensor(0.3864)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 246.466843\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "tensor(45.4556)\n",
      "tensor(14.3137)\n",
      "tensor(15.8878)\n",
      "tensor(0.0790)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 246.287018\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "tensor(38.5438)\n",
      "tensor(16.3248)\n",
      "tensor(14.6805)\n",
      "tensor(0.3799)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 246.161896\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "tensor(23.2220)\n",
      "tensor(16.3809)\n",
      "tensor(13.1564)\n",
      "tensor(0.6462)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 246.018539\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "tensor(23.8355)\n",
      "tensor(16.0138)\n",
      "tensor(12.6651)\n",
      "tensor(0.6765)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 245.959717\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "tensor(36.8026)\n",
      "tensor(17.7716)\n",
      "tensor(12.2393)\n",
      "tensor(0.5865)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 245.912247\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "tensor(31.9887)\n",
      "tensor(17.0373)\n",
      "tensor(11.8818)\n",
      "tensor(0.4759)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 245.875000\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "tensor(32.0190)\n",
      "tensor(16.8212)\n",
      "tensor(11.6636)\n",
      "tensor(0.3717)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 245.844727\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "tensor(25.5702)\n",
      "tensor(16.1188)\n",
      "tensor(11.5201)\n",
      "tensor(0.2416)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 245.819260\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "tensor(25.9075)\n",
      "tensor(16.1894)\n",
      "tensor(11.4466)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 245.778809\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "tensor(25.1899)\n",
      "tensor(16.4117)\n",
      "tensor(11.9304)\n",
      "tensor(0.3046)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 245.696640\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "tensor(43.7055)\n",
      "tensor(14.8160)\n",
      "tensor(13.3280)\n",
      "tensor(0.6441)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 245.616760\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "tensor(42.0945)\n",
      "tensor(14.9853)\n",
      "tensor(13.3698)\n",
      "tensor(0.6352)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 245.490128\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "tensor(27.2398)\n",
      "tensor(16.8229)\n",
      "tensor(11.9717)\n",
      "tensor(0.1158)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 245.348907\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "tensor(23.3451)\n",
      "tensor(17.0869)\n",
      "tensor(14.5294)\n",
      "tensor(0.5514)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 245.211624\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "tensor(70.8496)\n",
      "tensor(35.4475)\n",
      "tensor(15.6391)\n",
      "tensor(0.7770)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 245.029755\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "tensor(39.1840)\n",
      "tensor(27.7860)\n",
      "tensor(12.3762)\n",
      "tensor(0.4601)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 244.749008\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "tensor(34.7849)\n",
      "tensor(14.9848)\n",
      "tensor(11.4725)\n",
      "tensor(0.2847)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 244.489426\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "tensor(53.3301)\n",
      "tensor(19.6078)\n",
      "tensor(13.2809)\n",
      "tensor(0.6600)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 244.295914\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "tensor(37.8082)\n",
      "tensor(17.0307)\n",
      "tensor(15.0375)\n",
      "tensor(0.4756)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 244.074341\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "tensor(34.2014)\n",
      "tensor(16.3164)\n",
      "tensor(12.1287)\n",
      "tensor(0.1386)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 243.875946\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "tensor(24.0287)\n",
      "tensor(16.6266)\n",
      "tensor(12.1568)\n",
      "tensor(0.0922)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 243.705078\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "tensor(39.0588)\n",
      "tensor(15.1849)\n",
      "tensor(15.4030)\n",
      "tensor(0.1216)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 243.619614\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "tensor(39.1464)\n",
      "tensor(15.1706)\n",
      "tensor(15.4567)\n",
      "tensor(0.1225)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 243.515579\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "tensor(22.2899)\n",
      "tensor(17.0510)\n",
      "tensor(13.7102)\n",
      "tensor(0.1059)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 243.398773\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "tensor(21.8929)\n",
      "tensor(16.7723)\n",
      "tensor(12.5174)\n",
      "tensor(0.0543)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 243.346375\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "tensor(21.6438)\n",
      "tensor(16.6116)\n",
      "tensor(11.8478)\n",
      "tensor(0.0066)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 243.305298\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "tensor(20.3794)\n",
      "tensor(16.6443)\n",
      "tensor(11.4693)\n",
      "tensor(0.0711)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 243.271027\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "tensor(38.6691)\n",
      "tensor(14.7317)\n",
      "tensor(11.2883)\n",
      "tensor(0.1561)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 243.247345\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "tensor(40.8601)\n",
      "tensor(14.5058)\n",
      "tensor(11.3147)\n",
      "tensor(0.2195)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 243.217896\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "tensor(33.3700)\n",
      "tensor(15.8827)\n",
      "tensor(11.6325)\n",
      "tensor(0.2086)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 243.144577\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "tensor(24.3194)\n",
      "tensor(15.8294)\n",
      "tensor(11.7579)\n",
      "tensor(0.1229)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 243.058380\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "tensor(33.6331)\n",
      "tensor(15.4910)\n",
      "tensor(11.4329)\n",
      "tensor(0.0628)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 242.935638\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "tensor(22.6978)\n",
      "tensor(15.7759)\n",
      "tensor(11.2761)\n",
      "tensor(0.0799)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 242.758148\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "tensor(20.2924)\n",
      "tensor(16.0485)\n",
      "tensor(11.5184)\n",
      "tensor(0.0876)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 242.550568\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "tensor(50.6151)\n",
      "tensor(17.9583)\n",
      "tensor(12.9274)\n",
      "tensor(0.0141)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 242.392639\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "tensor(39.0749)\n",
      "tensor(15.5747)\n",
      "tensor(11.5223)\n",
      "tensor(0.1337)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 242.093338\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "tensor(37.1231)\n",
      "tensor(17.2936)\n",
      "tensor(12.6601)\n",
      "tensor(0.5596)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 241.936371\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "tensor(33.8040)\n",
      "tensor(15.4148)\n",
      "tensor(12.7987)\n",
      "tensor(0.5267)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 241.747589\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "tensor(24.4015)\n",
      "tensor(16.0344)\n",
      "tensor(11.4955)\n",
      "tensor(0.2295)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 241.518646\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "tensor(21.7222)\n",
      "tensor(16.8774)\n",
      "tensor(12.9763)\n",
      "tensor(0.1614)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 241.317947\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "tensor(17.5048)\n",
      "tensor(17.7779)\n",
      "tensor(14.2424)\n",
      "tensor(0.4078)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 241.124954\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "tensor(36.7100)\n",
      "tensor(14.8579)\n",
      "tensor(14.0681)\n",
      "tensor(0.4449)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 240.975433\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "tensor(34.2094)\n",
      "tensor(15.1103)\n",
      "tensor(12.3576)\n",
      "tensor(0.2585)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 240.869217\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "tensor(38.7921)\n",
      "tensor(15.4771)\n",
      "tensor(11.5370)\n",
      "tensor(0.0426)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 240.781891\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "tensor(30.3817)\n",
      "tensor(15.4951)\n",
      "tensor(11.5678)\n",
      "tensor(0.1568)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 240.752319\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "tensor(30.8283)\n",
      "tensor(15.3601)\n",
      "tensor(11.7047)\n",
      "tensor(0.2699)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 240.720367\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "tensor(22.8083)\n",
      "tensor(15.5341)\n",
      "tensor(11.7456)\n",
      "tensor(0.3417)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 240.687973\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "tensor(23.3853)\n",
      "tensor(15.7076)\n",
      "tensor(11.7283)\n",
      "tensor(0.3863)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 240.655762\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "tensor(22.6846)\n",
      "tensor(15.8033)\n",
      "tensor(11.7047)\n",
      "tensor(0.3884)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 240.600677\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "tensor(33.2320)\n",
      "tensor(16.1418)\n",
      "tensor(11.9899)\n",
      "tensor(0.3073)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 240.523438\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "tensor(34.9775)\n",
      "tensor(15.7650)\n",
      "tensor(12.4773)\n",
      "tensor(0.1976)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 240.446152\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "tensor(28.3767)\n",
      "tensor(15.8071)\n",
      "tensor(11.9514)\n",
      "tensor(0.0848)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 240.308945\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "tensor(22.1310)\n",
      "tensor(16.3738)\n",
      "tensor(11.7401)\n",
      "tensor(0.0054)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 240.160614\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "tensor(34.1991)\n",
      "tensor(14.7012)\n",
      "tensor(11.4421)\n",
      "tensor(0.1786)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 239.948792\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "tensor(32.6262)\n",
      "tensor(16.0757)\n",
      "tensor(11.3706)\n",
      "tensor(0.1419)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 239.763550\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "tensor(47.7375)\n",
      "tensor(20.6597)\n",
      "tensor(11.1389)\n",
      "tensor(0.0564)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 239.538681\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "tensor(30.4405)\n",
      "tensor(15.1630)\n",
      "tensor(10.9843)\n",
      "tensor(0.1405)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 239.285187\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "tensor(35.5708)\n",
      "tensor(14.6453)\n",
      "tensor(10.9909)\n",
      "tensor(0.1925)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 239.035492\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "tensor(58.8063)\n",
      "tensor(23.2412)\n",
      "tensor(11.2749)\n",
      "tensor(0.0386)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 238.879791\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "tensor(24.4345)\n",
      "tensor(16.9272)\n",
      "tensor(11.0563)\n",
      "tensor(0.0712)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 238.695663\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "tensor(22.3776)\n",
      "tensor(17.0863)\n",
      "tensor(11.0250)\n",
      "tensor(0.3053)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 238.507050\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "tensor(29.6593)\n",
      "tensor(16.6963)\n",
      "tensor(11.7778)\n",
      "tensor(0.5245)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 238.356735\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "tensor(39.7600)\n",
      "tensor(13.3964)\n",
      "tensor(12.4805)\n",
      "tensor(0.5402)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 238.261368\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "tensor(33.5492)\n",
      "tensor(14.3440)\n",
      "tensor(12.2411)\n",
      "tensor(0.3833)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 238.205597\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "tensor(33.6640)\n",
      "tensor(14.3818)\n",
      "tensor(11.7515)\n",
      "tensor(0.2422)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 238.150040\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "tensor(32.6571)\n",
      "tensor(14.6864)\n",
      "tensor(11.3602)\n",
      "tensor(0.1373)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 238.101349\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "tensor(22.9817)\n",
      "tensor(16.3184)\n",
      "tensor(11.1335)\n",
      "tensor(0.0429)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 238.076996\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "tensor(21.6861)\n",
      "tensor(16.4330)\n",
      "tensor(11.0933)\n",
      "tensor(0.0438)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 238.052277\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "tensor(22.6242)\n",
      "tensor(16.2507)\n",
      "tensor(11.2010)\n",
      "tensor(0.1132)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 238.015259\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "tensor(30.7169)\n",
      "tensor(15.7659)\n",
      "tensor(11.2531)\n",
      "tensor(0.1199)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 237.963898\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "tensor(29.3454)\n",
      "tensor(15.7806)\n",
      "tensor(11.1215)\n",
      "tensor(0.0631)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 237.857620\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "tensor(38.5722)\n",
      "tensor(14.4216)\n",
      "tensor(12.0098)\n",
      "tensor(0.0335)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 237.741333\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "tensor(34.1472)\n",
      "tensor(15.3068)\n",
      "tensor(13.2832)\n",
      "tensor(0.0535)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 237.599625\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "tensor(19.9701)\n",
      "tensor(16.5657)\n",
      "tensor(12.0592)\n",
      "tensor(0.1779)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 237.377609\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "tensor(22.0337)\n",
      "tensor(16.2030)\n",
      "tensor(11.1521)\n",
      "tensor(0.0549)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 237.196106\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "tensor(32.8451)\n",
      "tensor(17.4054)\n",
      "tensor(11.5774)\n",
      "tensor(0.2712)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 236.955063\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "tensor(32.9334)\n",
      "tensor(14.7810)\n",
      "tensor(12.4721)\n",
      "tensor(0.4321)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 236.767990\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "tensor(51.9943)\n",
      "tensor(19.9046)\n",
      "tensor(12.0580)\n",
      "tensor(0.1927)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 236.516068\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "tensor(34.1697)\n",
      "tensor(15.5238)\n",
      "tensor(13.2249)\n",
      "tensor(0.2729)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 236.240967\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "tensor(30.4521)\n",
      "tensor(15.2036)\n",
      "tensor(12.3652)\n",
      "tensor(0.5055)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 236.044724\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "tensor(22.8849)\n",
      "tensor(16.4515)\n",
      "tensor(13.7511)\n",
      "tensor(0.4187)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 235.872910\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "tensor(36.7290)\n",
      "tensor(15.3802)\n",
      "tensor(15.6194)\n",
      "tensor(0.1432)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 235.773758\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "tensor(36.5577)\n",
      "tensor(14.7153)\n",
      "tensor(14.5353)\n",
      "tensor(0.0476)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 235.662170\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "tensor(17.8556)\n",
      "tensor(17.0022)\n",
      "tensor(12.5129)\n",
      "tensor(0.1359)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 235.561584\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "tensor(19.5163)\n",
      "tensor(16.5552)\n",
      "tensor(11.6413)\n",
      "tensor(0.1981)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 235.517105\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "tensor(19.6868)\n",
      "tensor(16.4250)\n",
      "tensor(11.3653)\n",
      "tensor(0.2417)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 235.483490\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "tensor(18.5040)\n",
      "tensor(16.4924)\n",
      "tensor(11.3229)\n",
      "tensor(0.2769)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 235.452164\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "tensor(36.3293)\n",
      "tensor(14.4639)\n",
      "tensor(11.4253)\n",
      "tensor(0.3141)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 235.423141\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "tensor(37.3399)\n",
      "tensor(14.4243)\n",
      "tensor(11.4618)\n",
      "tensor(0.2892)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 235.390488\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "tensor(31.1166)\n",
      "tensor(15.6677)\n",
      "tensor(11.3944)\n",
      "tensor(0.1465)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 235.323578\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "tensor(24.3356)\n",
      "tensor(15.7302)\n",
      "tensor(11.2833)\n",
      "tensor(0.0766)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 235.227905\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "tensor(28.6629)\n",
      "tensor(15.7646)\n",
      "tensor(11.5175)\n",
      "tensor(0.2239)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 235.101013\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "tensor(21.5509)\n",
      "tensor(16.1950)\n",
      "tensor(11.9109)\n",
      "tensor(0.2577)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 234.945999\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "tensor(30.3470)\n",
      "tensor(14.9794)\n",
      "tensor(12.0441)\n",
      "tensor(0.0611)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 234.735275\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "tensor(58.8159)\n",
      "tensor(22.8916)\n",
      "tensor(11.5861)\n",
      "tensor(0.3581)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 234.557327\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "tensor(32.6318)\n",
      "tensor(15.9584)\n",
      "tensor(12.4102)\n",
      "tensor(0.4640)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 234.346466\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "tensor(52.1410)\n",
      "tensor(20.6150)\n",
      "tensor(12.1001)\n",
      "tensor(0.3148)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 234.087646\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "tensor(30.2977)\n",
      "tensor(15.6159)\n",
      "tensor(11.2404)\n",
      "tensor(0.0480)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 233.921585\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "tensor(29.2343)\n",
      "tensor(15.8072)\n",
      "tensor(12.1493)\n",
      "tensor(0.2837)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 233.730103\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "tensor(29.1623)\n",
      "tensor(15.7356)\n",
      "tensor(11.9024)\n",
      "tensor(0.2731)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 233.488449\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "tensor(24.1211)\n",
      "tensor(15.7885)\n",
      "tensor(11.4001)\n",
      "tensor(0.1173)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 233.306168\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "tensor(23.5151)\n",
      "tensor(15.8894)\n",
      "tensor(11.1185)\n",
      "tensor(0.1722)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 233.185684\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "tensor(29.1360)\n",
      "tensor(16.2132)\n",
      "tensor(11.3436)\n",
      "tensor(0.4336)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 233.081879\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "tensor(28.0543)\n",
      "tensor(16.4032)\n",
      "tensor(11.7098)\n",
      "tensor(0.5495)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 233.000443\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "tensor(20.8950)\n",
      "tensor(16.6663)\n",
      "tensor(12.0666)\n",
      "tensor(0.5672)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 232.941864\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "tensor(18.5774)\n",
      "tensor(17.0807)\n",
      "tensor(12.3137)\n",
      "tensor(0.5582)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 232.901306\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "tensor(18.4384)\n",
      "tensor(17.0771)\n",
      "tensor(12.4805)\n",
      "tensor(0.5368)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 232.868225\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "tensor(31.6962)\n",
      "tensor(13.9378)\n",
      "tensor(12.6101)\n",
      "tensor(0.4909)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 232.840988\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "tensor(36.2935)\n",
      "tensor(13.7154)\n",
      "tensor(12.2875)\n",
      "tensor(0.3634)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 232.809662\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "tensor(35.3890)\n",
      "tensor(14.1932)\n",
      "tensor(11.6514)\n",
      "tensor(0.0769)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 232.742157\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "tensor(22.6904)\n",
      "tensor(15.7370)\n",
      "tensor(12.1456)\n",
      "tensor(0.3733)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 232.629471\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "tensor(25.4735)\n",
      "tensor(15.9196)\n",
      "tensor(13.4731)\n",
      "tensor(0.6607)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 232.519714\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "tensor(28.2138)\n",
      "tensor(16.8472)\n",
      "tensor(12.6930)\n",
      "tensor(0.5184)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 232.363815\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "tensor(36.0052)\n",
      "tensor(17.5925)\n",
      "tensor(11.4776)\n",
      "tensor(0.0496)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 232.129349\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "tensor(48.7151)\n",
      "tensor(17.5596)\n",
      "tensor(12.3300)\n",
      "tensor(0.6112)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 231.955109\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "tensor(29.9336)\n",
      "tensor(18.3975)\n",
      "tensor(12.3488)\n",
      "tensor(0.6844)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 231.713791\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "tensor(26.3932)\n",
      "tensor(16.4134)\n",
      "tensor(11.9123)\n",
      "tensor(0.4973)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 231.463150\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "tensor(35.4343)\n",
      "tensor(15.5305)\n",
      "tensor(13.0179)\n",
      "tensor(0.0594)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 231.261490\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "tensor(27.1590)\n",
      "tensor(16.8062)\n",
      "tensor(12.6198)\n",
      "tensor(0.3489)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 231.018768\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "tensor(28.2231)\n",
      "tensor(15.9339)\n",
      "tensor(11.9243)\n",
      "tensor(0.3172)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 230.825058\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "tensor(26.5639)\n",
      "tensor(15.7349)\n",
      "tensor(12.0530)\n",
      "tensor(0.1266)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 230.681122\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "tensor(24.2493)\n",
      "tensor(15.4508)\n",
      "tensor(11.9604)\n",
      "tensor(0.2084)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 230.528534\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "tensor(29.6849)\n",
      "tensor(15.9848)\n",
      "tensor(11.9860)\n",
      "tensor(0.4847)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 230.416809\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "tensor(35.6560)\n",
      "tensor(13.5887)\n",
      "tensor(12.0385)\n",
      "tensor(0.5803)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 230.363754\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "tensor(34.5883)\n",
      "tensor(13.8387)\n",
      "tensor(12.0220)\n",
      "tensor(0.5443)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 230.314270\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "tensor(28.8110)\n",
      "tensor(14.1663)\n",
      "tensor(11.9267)\n",
      "tensor(0.4685)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 230.271545\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "tensor(28.7761)\n",
      "tensor(14.2173)\n",
      "tensor(11.7848)\n",
      "tensor(0.3867)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 230.236832\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "tensor(18.3238)\n",
      "tensor(16.5612)\n",
      "tensor(11.5695)\n",
      "tensor(0.2694)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 230.203857\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "tensor(19.1653)\n",
      "tensor(16.4049)\n",
      "tensor(11.5304)\n",
      "tensor(0.1192)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 230.168686\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "tensor(21.8613)\n",
      "tensor(16.0016)\n",
      "tensor(11.7100)\n",
      "tensor(0.0295)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 230.111008\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "tensor(28.5568)\n",
      "tensor(15.1758)\n",
      "tensor(11.8014)\n",
      "tensor(0.1073)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 230.017792\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "tensor(24.5079)\n",
      "tensor(16.0899)\n",
      "tensor(11.9573)\n",
      "tensor(0.1199)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 229.869766\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "tensor(34.8817)\n",
      "tensor(14.1142)\n",
      "tensor(11.6510)\n",
      "tensor(0.0499)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 229.705933\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "tensor(28.1693)\n",
      "tensor(14.7276)\n",
      "tensor(11.2499)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 229.512009\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "tensor(23.1788)\n",
      "tensor(15.5805)\n",
      "tensor(11.5233)\n",
      "tensor(0.1001)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 229.284836\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "tensor(46.7530)\n",
      "tensor(20.3229)\n",
      "tensor(11.5363)\n",
      "tensor(0.3390)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 229.081223\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "tensor(63.0020)\n",
      "tensor(22.5094)\n",
      "tensor(11.9552)\n",
      "tensor(0.5636)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 228.854446\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "tensor(36.0152)\n",
      "tensor(16.8438)\n",
      "tensor(12.5725)\n",
      "tensor(0.3357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 228.591843\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "tensor(33.4197)\n",
      "tensor(14.5781)\n",
      "tensor(11.3856)\n",
      "tensor(0.0780)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 228.362427\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "tensor(23.0128)\n",
      "tensor(15.7166)\n",
      "tensor(12.1234)\n",
      "tensor(0.2281)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 228.166031\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "tensor(27.9614)\n",
      "tensor(16.7969)\n",
      "tensor(13.0265)\n",
      "tensor(0.1882)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 228.018433\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "tensor(30.3520)\n",
      "tensor(14.4309)\n",
      "tensor(12.7191)\n",
      "tensor(0.0315)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 227.891129\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "tensor(28.1831)\n",
      "tensor(14.2238)\n",
      "tensor(11.6448)\n",
      "tensor(0.2031)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 227.775513\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "tensor(19.3534)\n",
      "tensor(16.2309)\n",
      "tensor(11.1812)\n",
      "tensor(0.2994)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 227.697586\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "tensor(19.7962)\n",
      "tensor(16.1648)\n",
      "tensor(11.2752)\n",
      "tensor(0.3761)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 227.653992\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "tensor(20.7570)\n",
      "tensor(15.8967)\n",
      "tensor(11.4240)\n",
      "tensor(0.4316)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 227.620453\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "tensor(20.3844)\n",
      "tensor(15.8890)\n",
      "tensor(11.5560)\n",
      "tensor(0.4740)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 227.589310\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "tensor(34.0836)\n",
      "tensor(13.3939)\n",
      "tensor(11.6982)\n",
      "tensor(0.5134)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 227.558655\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "tensor(33.2728)\n",
      "tensor(13.7698)\n",
      "tensor(11.6296)\n",
      "tensor(0.4853)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 227.521851\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "tensor(33.8840)\n",
      "tensor(13.3943)\n",
      "tensor(11.3475)\n",
      "tensor(0.3439)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 227.443924\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "tensor(21.3021)\n",
      "tensor(15.8422)\n",
      "tensor(11.5578)\n",
      "tensor(0.0116)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 227.382248\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "tensor(23.3481)\n",
      "tensor(15.9359)\n",
      "tensor(12.6708)\n",
      "tensor(0.2259)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 227.276657\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "tensor(22.7075)\n",
      "tensor(16.0457)\n",
      "tensor(12.7252)\n",
      "tensor(0.1799)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 227.098831\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "tensor(18.2406)\n",
      "tensor(16.4038)\n",
      "tensor(11.7424)\n",
      "tensor(0.2092)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 226.866074\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "tensor(45.8924)\n",
      "tensor(15.8609)\n",
      "tensor(12.9780)\n",
      "tensor(0.7038)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 226.709473\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "tensor(51.4264)\n",
      "tensor(19.4847)\n",
      "tensor(12.3729)\n",
      "tensor(0.5515)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 226.505875\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "tensor(28.2945)\n",
      "tensor(15.7562)\n",
      "tensor(14.8499)\n",
      "tensor(0.0367)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 226.207764\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "tensor(36.2073)\n",
      "tensor(15.5825)\n",
      "tensor(15.8589)\n",
      "tensor(0.3240)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 226.011795\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "tensor(23.9620)\n",
      "tensor(15.6854)\n",
      "tensor(12.2094)\n",
      "tensor(0.2566)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 225.806000\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "tensor(22.7581)\n",
      "tensor(16.8112)\n",
      "tensor(14.6187)\n",
      "tensor(0.2166)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 225.609863\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "tensor(40.7739)\n",
      "tensor(20.3608)\n",
      "tensor(16.9274)\n",
      "tensor(0.6767)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 225.449081\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "tensor(31.4863)\n",
      "tensor(13.8115)\n",
      "tensor(15.3420)\n",
      "tensor(0.8729)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 225.245667\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "tensor(33.3837)\n",
      "tensor(13.3078)\n",
      "tensor(12.7889)\n",
      "tensor(0.7748)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 225.161484\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "tensor(27.5856)\n",
      "tensor(15.9884)\n",
      "tensor(11.8054)\n",
      "tensor(0.5553)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 225.093201\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "tensor(28.6959)\n",
      "tensor(15.6095)\n",
      "tensor(11.7086)\n",
      "tensor(0.3887)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 225.043747\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "tensor(28.8986)\n",
      "tensor(15.4392)\n",
      "tensor(11.6155)\n",
      "tensor(0.2838)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 225.000626\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "tensor(24.9513)\n",
      "tensor(14.3694)\n",
      "tensor(11.4455)\n",
      "tensor(0.2041)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 224.968689\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "tensor(24.9578)\n",
      "tensor(14.5302)\n",
      "tensor(11.3411)\n",
      "tensor(0.1163)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 224.942032\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "tensor(22.0626)\n",
      "tensor(15.4353)\n",
      "tensor(11.3721)\n",
      "tensor(0.0162)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 224.894684\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "tensor(27.2721)\n",
      "tensor(14.4585)\n",
      "tensor(11.6014)\n",
      "tensor(0.0447)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 224.839127\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "tensor(22.4262)\n",
      "tensor(15.6845)\n",
      "tensor(11.9208)\n",
      "tensor(0.0702)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 224.759140\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "tensor(21.2496)\n",
      "tensor(15.8684)\n",
      "tensor(11.8346)\n",
      "tensor(0.0938)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 224.620041\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "tensor(75.3304)\n",
      "tensor(24.9359)\n",
      "tensor(11.8640)\n",
      "tensor(0.4275)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 224.503052\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "tensor(42.5459)\n",
      "tensor(18.3649)\n",
      "tensor(13.4410)\n",
      "tensor(0.4798)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 224.296082\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "tensor(34.1643)\n",
      "tensor(14.8947)\n",
      "tensor(15.4362)\n",
      "tensor(0.4045)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 224.112930\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "tensor(24.4447)\n",
      "tensor(14.5663)\n",
      "tensor(12.4600)\n",
      "tensor(0.1329)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 223.855896\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "tensor(29.6107)\n",
      "tensor(17.0730)\n",
      "tensor(11.8831)\n",
      "tensor(0.0093)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 223.691238\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "tensor(39.9085)\n",
      "tensor(19.2472)\n",
      "tensor(13.6858)\n",
      "tensor(0.2783)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 223.491653\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "tensor(20.8000)\n",
      "tensor(16.7527)\n",
      "tensor(13.0511)\n",
      "tensor(0.6734)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 223.246094\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "tensor(27.0950)\n",
      "tensor(12.6012)\n",
      "tensor(12.8045)\n",
      "tensor(0.9099)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 222.999832\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "tensor(31.8319)\n",
      "tensor(13.1173)\n",
      "tensor(12.9327)\n",
      "tensor(0.7592)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 222.836395\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "tensor(32.4873)\n",
      "tensor(15.7095)\n",
      "tensor(12.7838)\n",
      "tensor(0.3817)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 222.733810\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "tensor(23.7786)\n",
      "tensor(15.3876)\n",
      "tensor(11.8916)\n",
      "tensor(0.1390)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 222.635452\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "tensor(21.9675)\n",
      "tensor(15.5616)\n",
      "tensor(11.3982)\n",
      "tensor(0.0887)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 222.553177\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "tensor(25.3172)\n",
      "tensor(13.8909)\n",
      "tensor(11.4124)\n",
      "tensor(0.1158)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 222.518082\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "tensor(25.9028)\n",
      "tensor(13.7808)\n",
      "tensor(11.5353)\n",
      "tensor(0.1460)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 222.490295\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "tensor(26.1991)\n",
      "tensor(13.6979)\n",
      "tensor(11.5666)\n",
      "tensor(0.1776)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 222.460297\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "tensor(42.6976)\n",
      "tensor(17.0527)\n",
      "tensor(11.4839)\n",
      "tensor(0.2199)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 222.416687\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "tensor(27.2108)\n",
      "tensor(15.3775)\n",
      "tensor(11.2597)\n",
      "tensor(0.2600)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 222.382706\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "tensor(39.5388)\n",
      "tensor(17.6743)\n",
      "tensor(11.1916)\n",
      "tensor(0.3151)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 222.345779\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "tensor(35.6856)\n",
      "tensor(16.9996)\n",
      "tensor(11.2126)\n",
      "tensor(0.3811)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 222.254623\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "tensor(19.7005)\n",
      "tensor(15.9555)\n",
      "tensor(11.3500)\n",
      "tensor(0.4204)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 222.119644\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "tensor(27.6763)\n",
      "tensor(12.7803)\n",
      "tensor(11.9232)\n",
      "tensor(0.4994)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 221.966522\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "tensor(30.3238)\n",
      "tensor(14.1609)\n",
      "tensor(11.3735)\n",
      "tensor(0.3876)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 221.789780\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "tensor(23.6404)\n",
      "tensor(15.2418)\n",
      "tensor(12.1612)\n",
      "tensor(0.1511)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 221.571625\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "tensor(22.5985)\n",
      "tensor(15.2592)\n",
      "tensor(12.0504)\n",
      "tensor(0.2168)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 221.328629\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "tensor(26.7914)\n",
      "tensor(12.9730)\n",
      "tensor(11.8194)\n",
      "tensor(0.5022)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 221.091965\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "tensor(33.2404)\n",
      "tensor(12.3922)\n",
      "tensor(12.0776)\n",
      "tensor(0.4756)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 220.893204\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "tensor(25.9466)\n",
      "tensor(15.0418)\n",
      "tensor(11.5380)\n",
      "tensor(0.0588)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 220.686386\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "tensor(26.3004)\n",
      "tensor(15.0115)\n",
      "tensor(12.3822)\n",
      "tensor(0.1540)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 220.480057\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "tensor(20.9335)\n",
      "tensor(16.3566)\n",
      "tensor(12.8505)\n",
      "tensor(0.0854)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 220.303009\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "tensor(26.4466)\n",
      "tensor(13.7592)\n",
      "tensor(12.5087)\n",
      "tensor(0.2229)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 220.153839\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "tensor(47.4657)\n",
      "tensor(17.9110)\n",
      "tensor(11.7998)\n",
      "tensor(0.4348)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 220.055527\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "tensor(43.0093)\n",
      "tensor(19.9230)\n",
      "tensor(11.3825)\n",
      "tensor(0.5126)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 219.994629\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "tensor(20.4790)\n",
      "tensor(15.5486)\n",
      "tensor(11.6745)\n",
      "tensor(0.5511)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 219.942078\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "tensor(20.8356)\n",
      "tensor(15.4216)\n",
      "tensor(11.9542)\n",
      "tensor(0.5718)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 219.911255\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "tensor(56.3616)\n",
      "tensor(20.5306)\n",
      "tensor(12.1021)\n",
      "tensor(0.5818)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 219.893951\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "tensor(23.5135)\n",
      "tensor(13.2794)\n",
      "tensor(12.3475)\n",
      "tensor(0.5552)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 219.861969\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "tensor(23.5253)\n",
      "tensor(13.2003)\n",
      "tensor(12.6109)\n",
      "tensor(0.4898)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 219.812164\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "tensor(30.9902)\n",
      "tensor(16.4080)\n",
      "tensor(12.7703)\n",
      "tensor(0.3555)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 219.749161\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "tensor(29.6201)\n",
      "tensor(14.9939)\n",
      "tensor(12.2617)\n",
      "tensor(0.2267)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 219.671402\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "tensor(34.8207)\n",
      "tensor(14.6262)\n",
      "tensor(11.3545)\n",
      "tensor(0.1357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 219.543518\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "tensor(25.1122)\n",
      "tensor(15.1101)\n",
      "tensor(13.3714)\n",
      "tensor(0.0023)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 219.389008\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "tensor(45.3763)\n",
      "tensor(17.1215)\n",
      "tensor(16.2086)\n",
      "tensor(0.0291)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 219.214264\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "tensor(21.2739)\n",
      "tensor(16.4424)\n",
      "tensor(12.1073)\n",
      "tensor(0.1821)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 218.990662\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "tensor(22.3882)\n",
      "tensor(15.9066)\n",
      "tensor(12.2469)\n",
      "tensor(0.7036)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 218.752182\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "tensor(45.5019)\n",
      "tensor(16.7984)\n",
      "tensor(15.1930)\n",
      "tensor(1.1577)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 218.604187\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "tensor(24.4074)\n",
      "tensor(12.0514)\n",
      "tensor(13.1983)\n",
      "tensor(0.8431)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 218.345230\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "tensor(36.7082)\n",
      "tensor(16.3110)\n",
      "tensor(11.5437)\n",
      "tensor(0.2631)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 218.111160\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "tensor(52.0642)\n",
      "tensor(20.4778)\n",
      "tensor(12.5652)\n",
      "tensor(0.1224)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 217.925156\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "tensor(48.0349)\n",
      "tensor(18.6595)\n",
      "tensor(12.3722)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 217.786942\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "tensor(22.1985)\n",
      "tensor(15.2938)\n",
      "tensor(11.4832)\n",
      "tensor(0.1600)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 217.628311\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "tensor(28.4716)\n",
      "tensor(11.5871)\n",
      "tensor(11.4643)\n",
      "tensor(0.4672)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 217.521957\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "tensor(29.1126)\n",
      "tensor(13.9641)\n",
      "tensor(11.7494)\n",
      "tensor(0.5844)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 217.468109\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "tensor(23.4257)\n",
      "tensor(12.9991)\n",
      "tensor(11.7114)\n",
      "tensor(0.6281)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 217.419250\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "tensor(18.2955)\n",
      "tensor(16.4343)\n",
      "tensor(11.5896)\n",
      "tensor(0.6350)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 217.383667\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "tensor(18.7623)\n",
      "tensor(16.1291)\n",
      "tensor(11.5495)\n",
      "tensor(0.6370)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 217.359451\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "tensor(25.6485)\n",
      "tensor(14.9330)\n",
      "tensor(11.5442)\n",
      "tensor(0.6351)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 217.331558\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "tensor(28.8886)\n",
      "tensor(11.8070)\n",
      "tensor(11.4825)\n",
      "tensor(0.5880)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 217.297165\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "tensor(23.3428)\n",
      "tensor(12.6451)\n",
      "tensor(11.2409)\n",
      "tensor(0.4287)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 217.231918\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "tensor(31.3336)\n",
      "tensor(16.8818)\n",
      "tensor(11.3150)\n",
      "tensor(0.1931)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 217.142807\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "tensor(22.1079)\n",
      "tensor(15.1057)\n",
      "tensor(11.6545)\n",
      "tensor(0.0616)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 217.004425\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "tensor(53.2126)\n",
      "tensor(19.0469)\n",
      "tensor(11.3208)\n",
      "tensor(0.1689)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 216.887665\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "tensor(22.2843)\n",
      "tensor(13.4661)\n",
      "tensor(11.4032)\n",
      "tensor(0.1734)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 216.681564\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "tensor(21.9715)\n",
      "tensor(14.8895)\n",
      "tensor(11.7401)\n",
      "tensor(0.2526)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 216.458786\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "tensor(39.0189)\n",
      "tensor(17.7411)\n",
      "tensor(11.5423)\n",
      "tensor(0.5694)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 216.256897\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "tensor(26.1753)\n",
      "tensor(12.0671)\n",
      "tensor(12.5181)\n",
      "tensor(0.6592)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 216.034927\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "tensor(23.2972)\n",
      "tensor(13.1486)\n",
      "tensor(11.6368)\n",
      "tensor(0.4655)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 215.805527\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "tensor(21.8901)\n",
      "tensor(14.9694)\n",
      "tensor(11.4018)\n",
      "tensor(0.2356)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 215.576935\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "tensor(44.6420)\n",
      "tensor(18.5446)\n",
      "tensor(11.6074)\n",
      "tensor(0.2858)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 215.397781\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "tensor(50.8029)\n",
      "tensor(18.9730)\n",
      "tensor(11.2342)\n",
      "tensor(0.3940)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 215.243759\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "tensor(32.5600)\n",
      "tensor(13.7218)\n",
      "tensor(11.4348)\n",
      "tensor(0.4271)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 215.090744\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "tensor(53.2953)\n",
      "tensor(21.5613)\n",
      "tensor(11.4283)\n",
      "tensor(0.3753)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 215.021927\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "tensor(45.2255)\n",
      "tensor(19.3992)\n",
      "tensor(11.4899)\n",
      "tensor(0.3901)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 214.947922\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "tensor(21.1264)\n",
      "tensor(12.5605)\n",
      "tensor(11.4908)\n",
      "tensor(0.4283)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 214.892487\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "tensor(21.1318)\n",
      "tensor(12.3449)\n",
      "tensor(11.4759)\n",
      "tensor(0.4505)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 214.865524\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "tensor(21.4409)\n",
      "tensor(14.8970)\n",
      "tensor(11.4577)\n",
      "tensor(0.4664)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 214.842255\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "tensor(21.1542)\n",
      "tensor(12.0266)\n",
      "tensor(11.3765)\n",
      "tensor(0.5023)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 214.810287\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "tensor(26.0174)\n",
      "tensor(14.5292)\n",
      "tensor(11.3006)\n",
      "tensor(0.5345)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 214.775024\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "tensor(26.5873)\n",
      "tensor(12.0866)\n",
      "tensor(11.3116)\n",
      "tensor(0.5609)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 214.713806\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "tensor(23.0176)\n",
      "tensor(15.5244)\n",
      "tensor(11.9477)\n",
      "tensor(0.5059)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 214.625092\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "tensor(26.2484)\n",
      "tensor(12.8291)\n",
      "tensor(14.0826)\n",
      "tensor(0.4203)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 214.512695\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "tensor(21.1433)\n",
      "tensor(16.0460)\n",
      "tensor(13.6250)\n",
      "tensor(0.3069)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 214.372116\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "tensor(19.6921)\n",
      "tensor(15.5531)\n",
      "tensor(11.4822)\n",
      "tensor(0.3866)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 214.168091\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "tensor(21.6050)\n",
      "tensor(12.2254)\n",
      "tensor(11.8370)\n",
      "tensor(0.6327)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 213.969055\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "tensor(29.5036)\n",
      "tensor(12.3815)\n",
      "tensor(13.8730)\n",
      "tensor(0.6946)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 213.752991\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "tensor(35.5072)\n",
      "tensor(15.6608)\n",
      "tensor(12.3306)\n",
      "tensor(0.3736)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 213.486862\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "tensor(42.6885)\n",
      "tensor(19.0924)\n",
      "tensor(12.1735)\n",
      "tensor(0.0350)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 213.326385\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "tensor(41.5247)\n",
      "tensor(18.0306)\n",
      "tensor(11.4311)\n",
      "tensor(0.1913)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 213.117203\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "tensor(54.1533)\n",
      "tensor(21.9211)\n",
      "tensor(11.6763)\n",
      "tensor(0.5469)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 212.927765\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "tensor(43.8650)\n",
      "tensor(19.7462)\n",
      "tensor(12.0440)\n",
      "tensor(0.8304)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 212.727264\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "tensor(56.6633)\n",
      "tensor(19.1867)\n",
      "tensor(12.1639)\n",
      "tensor(0.8883)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 212.639236\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "tensor(48.0563)\n",
      "tensor(17.2997)\n",
      "tensor(11.9102)\n",
      "tensor(0.6819)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 212.572205\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "tensor(20.8989)\n",
      "tensor(12.5882)\n",
      "tensor(12.2090)\n",
      "tensor(0.4466)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 212.504074\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "tensor(21.2269)\n",
      "tensor(12.8782)\n",
      "tensor(12.4832)\n",
      "tensor(0.3175)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 212.465256\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "tensor(21.2656)\n",
      "tensor(13.0268)\n",
      "tensor(12.5039)\n",
      "tensor(0.2631)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 212.433655\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "tensor(21.1276)\n",
      "tensor(13.0327)\n",
      "tensor(12.3543)\n",
      "tensor(0.2461)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 212.402740\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "tensor(20.4641)\n",
      "tensor(12.7521)\n",
      "tensor(12.0164)\n",
      "tensor(0.2557)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 212.363953\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "tensor(19.9767)\n",
      "tensor(12.4627)\n",
      "tensor(11.4974)\n",
      "tensor(0.3105)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 212.302017\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "tensor(21.3190)\n",
      "tensor(14.9836)\n",
      "tensor(11.1647)\n",
      "tensor(0.4152)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 212.254654\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "tensor(23.4423)\n",
      "tensor(14.6196)\n",
      "tensor(12.1523)\n",
      "tensor(0.6476)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 212.205795\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "tensor(23.5685)\n",
      "tensor(15.5543)\n",
      "tensor(15.2277)\n",
      "tensor(0.8908)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 212.113266\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "tensor(21.2771)\n",
      "tensor(17.8341)\n",
      "tensor(17.0218)\n",
      "tensor(0.9672)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 211.963638\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "tensor(19.3479)\n",
      "tensor(17.6400)\n",
      "tensor(15.2174)\n",
      "tensor(0.7958)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 211.741043\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "tensor(23.6752)\n",
      "tensor(16.5838)\n",
      "tensor(11.3623)\n",
      "tensor(0.5570)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 211.481796\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "tensor(38.5702)\n",
      "tensor(16.9844)\n",
      "tensor(12.6868)\n",
      "tensor(0.3756)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 211.290497\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "tensor(41.3915)\n",
      "tensor(17.8219)\n",
      "tensor(16.4558)\n",
      "tensor(0.1723)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 211.113770\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "tensor(36.5567)\n",
      "tensor(18.7342)\n",
      "tensor(14.9519)\n",
      "tensor(0.1018)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 210.903870\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "tensor(26.9090)\n",
      "tensor(15.8305)\n",
      "tensor(12.6646)\n",
      "tensor(0.1013)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 210.671890\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "tensor(47.9551)\n",
      "tensor(18.5183)\n",
      "tensor(11.3308)\n",
      "tensor(0.1654)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 210.453400\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "tensor(31.7112)\n",
      "tensor(16.3639)\n",
      "tensor(11.0941)\n",
      "tensor(0.4310)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 210.280930\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "tensor(76.9510)\n",
      "tensor(27.7544)\n",
      "tensor(11.2722)\n",
      "tensor(0.7069)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 210.210526\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "tensor(21.0616)\n",
      "tensor(14.9749)\n",
      "tensor(11.7860)\n",
      "tensor(0.8275)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 210.073135\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "tensor(26.4415)\n",
      "tensor(15.0856)\n",
      "tensor(12.2295)\n",
      "tensor(0.8964)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 210.012955\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "tensor(25.8088)\n",
      "tensor(15.1057)\n",
      "tensor(12.1015)\n",
      "tensor(0.9087)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 209.959000\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "tensor(24.7559)\n",
      "tensor(15.5747)\n",
      "tensor(11.8717)\n",
      "tensor(0.8942)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 209.916031\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "tensor(25.0803)\n",
      "tensor(11.0245)\n",
      "tensor(11.7273)\n",
      "tensor(0.8648)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 209.879211\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "tensor(20.9268)\n",
      "tensor(11.5984)\n",
      "tensor(11.6784)\n",
      "tensor(0.7814)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 209.851852\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "tensor(21.9163)\n",
      "tensor(11.5237)\n",
      "tensor(11.9673)\n",
      "tensor(0.6306)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 209.821274\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "tensor(22.3429)\n",
      "tensor(12.6315)\n",
      "tensor(12.5310)\n",
      "tensor(0.4027)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 209.763443\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "tensor(22.1134)\n",
      "tensor(12.8448)\n",
      "tensor(12.7265)\n",
      "tensor(0.1528)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 209.661682\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "tensor(21.6326)\n",
      "tensor(14.6673)\n",
      "tensor(12.2899)\n",
      "tensor(0.0155)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 209.548462\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "tensor(26.1620)\n",
      "tensor(13.9395)\n",
      "tensor(11.5836)\n",
      "tensor(0.1864)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 209.389191\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "tensor(26.3255)\n",
      "tensor(15.5939)\n",
      "tensor(11.4349)\n",
      "tensor(0.6343)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 209.180679\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "tensor(49.4342)\n",
      "tensor(15.9186)\n",
      "tensor(13.8203)\n",
      "tensor(1.0914)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 208.988159\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "tensor(28.2277)\n",
      "tensor(12.9660)\n",
      "tensor(12.2611)\n",
      "tensor(0.8086)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 208.760941\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "tensor(41.2246)\n",
      "tensor(18.3507)\n",
      "tensor(12.2297)\n",
      "tensor(0.2373)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 208.566711\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "tensor(19.4643)\n",
      "tensor(13.6435)\n",
      "tensor(12.7764)\n",
      "tensor(0.0182)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 208.300995\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "tensor(21.4213)\n",
      "tensor(13.9433)\n",
      "tensor(11.5848)\n",
      "tensor(0.1886)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 208.076248\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "tensor(20.6967)\n",
      "tensor(12.1952)\n",
      "tensor(11.6893)\n",
      "tensor(0.6266)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 207.890778\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "tensor(55.5590)\n",
      "tensor(18.9254)\n",
      "tensor(12.8940)\n",
      "tensor(0.8776)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 207.755646\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "tensor(52.1653)\n",
      "tensor(20.3385)\n",
      "tensor(11.6100)\n",
      "tensor(0.7445)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 207.655029\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "tensor(64.7351)\n",
      "tensor(23.3688)\n",
      "tensor(11.1121)\n",
      "tensor(0.5411)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 207.569778\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "tensor(26.5668)\n",
      "tensor(14.1626)\n",
      "tensor(11.5236)\n",
      "tensor(0.3803)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 207.470169\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "tensor(22.5578)\n",
      "tensor(14.3767)\n",
      "tensor(11.5770)\n",
      "tensor(0.3180)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 207.421524\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "tensor(19.2917)\n",
      "tensor(12.2704)\n",
      "tensor(11.3955)\n",
      "tensor(0.3272)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 207.386887\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "tensor(19.0388)\n",
      "tensor(11.9249)\n",
      "tensor(11.2461)\n",
      "tensor(0.3476)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 207.369110\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "tensor(19.0851)\n",
      "tensor(11.4891)\n",
      "tensor(11.1249)\n",
      "tensor(0.3904)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 207.342926\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "tensor(19.1922)\n",
      "tensor(11.8306)\n",
      "tensor(11.0854)\n",
      "tensor(0.4683)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 207.299759\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "tensor(18.0770)\n",
      "tensor(14.6715)\n",
      "tensor(11.2265)\n",
      "tensor(0.5885)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 207.230545\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "tensor(16.7651)\n",
      "tensor(15.1511)\n",
      "tensor(11.9235)\n",
      "tensor(0.7761)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 207.133163\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "tensor(26.1367)\n",
      "tensor(10.9740)\n",
      "tensor(13.1687)\n",
      "tensor(0.9495)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 207.013306\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "tensor(45.3023)\n",
      "tensor(15.1622)\n",
      "tensor(12.4814)\n",
      "tensor(0.7820)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 206.857178\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "tensor(27.2356)\n",
      "tensor(15.3788)\n",
      "tensor(11.3269)\n",
      "tensor(0.2385)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 206.681992\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "tensor(46.3590)\n",
      "tensor(19.7627)\n",
      "tensor(12.5192)\n",
      "tensor(0.0516)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 206.526215\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "tensor(23.3653)\n",
      "tensor(13.6997)\n",
      "tensor(12.8141)\n",
      "tensor(0.3715)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 206.241669\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "tensor(20.1130)\n",
      "tensor(11.6045)\n",
      "tensor(12.4501)\n",
      "tensor(0.9272)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 206.046417\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "tensor(26.8434)\n",
      "tensor(11.4842)\n",
      "tensor(13.8556)\n",
      "tensor(1.0650)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 205.868546\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "tensor(25.2911)\n",
      "tensor(10.8761)\n",
      "tensor(12.4665)\n",
      "tensor(0.6221)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 205.633759\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "tensor(19.7574)\n",
      "tensor(12.8164)\n",
      "tensor(12.3093)\n",
      "tensor(0.0546)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 205.408386\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "tensor(24.0508)\n",
      "tensor(14.3346)\n",
      "tensor(13.0014)\n",
      "tensor(0.1155)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 205.243546\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "tensor(23.0783)\n",
      "tensor(14.1082)\n",
      "tensor(11.8610)\n",
      "tensor(0.1645)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 205.118958\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "tensor(67.2015)\n",
      "tensor(24.8785)\n",
      "tensor(11.0634)\n",
      "tensor(0.5826)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 205.021942\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "tensor(58.2809)\n",
      "tensor(23.1806)\n",
      "tensor(11.6495)\n",
      "tensor(0.8527)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 204.934402\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "tensor(36.8161)\n",
      "tensor(14.8243)\n",
      "tensor(12.3269)\n",
      "tensor(0.9772)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 204.882584\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "tensor(24.3273)\n",
      "tensor(10.5356)\n",
      "tensor(12.6048)\n",
      "tensor(1.0024)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 204.857651\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "tensor(24.1939)\n",
      "tensor(10.9686)\n",
      "tensor(12.5281)\n",
      "tensor(0.9759)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 204.835419\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "tensor(23.8516)\n",
      "tensor(11.2118)\n",
      "tensor(12.1341)\n",
      "tensor(0.8947)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 204.802307\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "tensor(18.4261)\n",
      "tensor(14.7258)\n",
      "tensor(11.3867)\n",
      "tensor(0.7118)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 204.754654\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "tensor(18.2890)\n",
      "tensor(14.5107)\n",
      "tensor(11.1042)\n",
      "tensor(0.4673)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 204.691055\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "tensor(20.5428)\n",
      "tensor(12.0919)\n",
      "tensor(12.3340)\n",
      "tensor(0.2431)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 204.605392\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "tensor(32.7141)\n",
      "tensor(14.9847)\n",
      "tensor(13.3632)\n",
      "tensor(0.0933)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 204.483582\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "tensor(56.4963)\n",
      "tensor(22.0092)\n",
      "tensor(11.7529)\n",
      "tensor(0.1432)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 204.399338\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "tensor(29.1421)\n",
      "tensor(13.7043)\n",
      "tensor(13.3965)\n",
      "tensor(0.4575)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 204.165558\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "tensor(26.8163)\n",
      "tensor(11.4261)\n",
      "tensor(14.6300)\n",
      "tensor(0.9200)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 203.946426\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "tensor(19.4427)\n",
      "tensor(10.3872)\n",
      "tensor(12.2603)\n",
      "tensor(0.9443)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 203.707794\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "tensor(17.3374)\n",
      "tensor(15.0103)\n",
      "tensor(12.4620)\n",
      "tensor(0.5966)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 203.499710\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "tensor(18.0120)\n",
      "tensor(15.3148)\n",
      "tensor(13.4809)\n",
      "tensor(0.3926)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 203.276108\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "tensor(19.6196)\n",
      "tensor(12.2672)\n",
      "tensor(12.2757)\n",
      "tensor(0.4710)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 203.057480\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "tensor(17.9362)\n",
      "tensor(11.6840)\n",
      "tensor(11.2044)\n",
      "tensor(0.5898)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 202.852051\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "tensor(25.2148)\n",
      "tensor(13.4311)\n",
      "tensor(12.3580)\n",
      "tensor(0.6482)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 202.701279\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "tensor(23.5900)\n",
      "tensor(11.0654)\n",
      "tensor(11.8533)\n",
      "tensor(0.6397)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 202.567871\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "tensor(22.0065)\n",
      "tensor(13.4025)\n",
      "tensor(11.1686)\n",
      "tensor(0.5395)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 202.458206\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "tensor(20.7293)\n",
      "tensor(13.2614)\n",
      "tensor(11.3070)\n",
      "tensor(0.4734)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 202.373154\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "tensor(19.9691)\n",
      "tensor(11.1738)\n",
      "tensor(11.8957)\n",
      "tensor(0.4274)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 202.330124\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "tensor(19.9867)\n",
      "tensor(11.5860)\n",
      "tensor(12.1395)\n",
      "tensor(0.4015)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 202.302948\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "tensor(33.3418)\n",
      "tensor(14.4603)\n",
      "tensor(12.0982)\n",
      "tensor(0.3894)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 202.277954\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "tensor(49.6060)\n",
      "tensor(20.2838)\n",
      "tensor(11.7800)\n",
      "tensor(0.3808)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 202.249847\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "tensor(59.6048)\n",
      "tensor(22.7148)\n",
      "tensor(11.1881)\n",
      "tensor(0.4007)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 202.216217\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "tensor(42.3563)\n",
      "tensor(18.1915)\n",
      "tensor(11.4675)\n",
      "tensor(0.4727)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 202.152924\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "tensor(40.6988)\n",
      "tensor(16.4953)\n",
      "tensor(14.3872)\n",
      "tensor(0.6353)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 202.079575\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "tensor(48.6876)\n",
      "tensor(18.4735)\n",
      "tensor(17.5537)\n",
      "tensor(0.7700)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 202.015671\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "tensor(41.1430)\n",
      "tensor(15.8431)\n",
      "tensor(16.1297)\n",
      "tensor(0.6882)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 201.856735\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "tensor(16.8116)\n",
      "tensor(11.4810)\n",
      "tensor(11.2583)\n",
      "tensor(0.4141)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 201.627960\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "tensor(22.5811)\n",
      "tensor(15.0436)\n",
      "tensor(14.4805)\n",
      "tensor(0.2643)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 201.466919\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "tensor(31.9371)\n",
      "tensor(17.3725)\n",
      "tensor(17.0233)\n",
      "tensor(0.5762)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 201.277924\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "tensor(39.8808)\n",
      "tensor(18.5105)\n",
      "tensor(13.5619)\n",
      "tensor(1.0858)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 201.030365\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "tensor(44.6627)\n",
      "tensor(15.1705)\n",
      "tensor(15.2314)\n",
      "tensor(1.3123)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 200.810806\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "tensor(24.9165)\n",
      "tensor(11.3323)\n",
      "tensor(18.4563)\n",
      "tensor(0.9031)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 200.649551\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "tensor(22.2659)\n",
      "tensor(11.7745)\n",
      "tensor(15.0787)\n",
      "tensor(0.4617)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 200.463058\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "tensor(20.7702)\n",
      "tensor(10.9950)\n",
      "tensor(11.4229)\n",
      "tensor(0.2635)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 200.296326\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "tensor(20.4253)\n",
      "tensor(11.6555)\n",
      "tensor(13.6477)\n",
      "tensor(0.2242)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 200.167282\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "tensor(20.2747)\n",
      "tensor(14.6600)\n",
      "tensor(14.9099)\n",
      "tensor(0.3223)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 200.073563\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "tensor(20.2208)\n",
      "tensor(13.7904)\n",
      "tensor(14.4336)\n",
      "tensor(0.4782)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 200.008957\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "tensor(19.6775)\n",
      "tensor(13.5520)\n",
      "tensor(13.6970)\n",
      "tensor(0.5897)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 199.960312\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "tensor(17.6988)\n",
      "tensor(14.2536)\n",
      "tensor(13.0981)\n",
      "tensor(0.6578)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 199.924744\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "tensor(16.8364)\n",
      "tensor(14.3746)\n",
      "tensor(12.5286)\n",
      "tensor(0.7163)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 199.893875\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "tensor(18.6998)\n",
      "tensor(10.3121)\n",
      "tensor(11.9730)\n",
      "tensor(0.7814)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 199.861176\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "tensor(21.2581)\n",
      "tensor(10.2726)\n",
      "tensor(11.5228)\n",
      "tensor(0.8379)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 199.820145\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "tensor(22.3776)\n",
      "tensor(11.1247)\n",
      "tensor(11.9939)\n",
      "tensor(0.8314)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 199.757568\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "tensor(19.2783)\n",
      "tensor(11.3971)\n",
      "tensor(13.0418)\n",
      "tensor(0.7125)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 199.664169\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "tensor(23.6065)\n",
      "tensor(13.2925)\n",
      "tensor(13.1195)\n",
      "tensor(0.5444)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 199.529709\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "tensor(16.6019)\n",
      "tensor(11.3796)\n",
      "tensor(11.0804)\n",
      "tensor(0.4908)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 199.346695\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "tensor(61.1376)\n",
      "tensor(22.3928)\n",
      "tensor(12.2867)\n",
      "tensor(0.4796)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 199.218231\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "tensor(29.5413)\n",
      "tensor(16.0028)\n",
      "tensor(11.1347)\n",
      "tensor(0.4982)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 198.934006\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "tensor(22.4184)\n",
      "tensor(11.3680)\n",
      "tensor(11.6791)\n",
      "tensor(0.5719)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 198.750092\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "tensor(45.2672)\n",
      "tensor(16.9716)\n",
      "tensor(11.3981)\n",
      "tensor(0.4279)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 198.554108\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "tensor(16.4452)\n",
      "tensor(12.4703)\n",
      "tensor(11.4466)\n",
      "tensor(0.1575)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 198.296829\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "tensor(48.6118)\n",
      "tensor(19.1777)\n",
      "tensor(11.3719)\n",
      "tensor(0.1982)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 198.139435\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "tensor(17.1203)\n",
      "tensor(13.3740)\n",
      "tensor(10.9292)\n",
      "tensor(0.5314)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 197.897751\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "tensor(19.3290)\n",
      "tensor(10.1796)\n",
      "tensor(11.8221)\n",
      "tensor(0.9042)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 197.735519\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "tensor(46.7843)\n",
      "tensor(17.3357)\n",
      "tensor(12.1701)\n",
      "tensor(1.0247)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 197.637955\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "tensor(56.8241)\n",
      "tensor(20.0375)\n",
      "tensor(11.8866)\n",
      "tensor(0.9508)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 197.526520\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "tensor(43.4943)\n",
      "tensor(17.2436)\n",
      "tensor(12.3095)\n",
      "tensor(0.7893)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 197.470795\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "tensor(26.3021)\n",
      "tensor(13.0852)\n",
      "tensor(12.6157)\n",
      "tensor(0.6802)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 197.426941\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "tensor(41.5108)\n",
      "tensor(16.8266)\n",
      "tensor(12.3046)\n",
      "tensor(0.6244)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 197.395889\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "tensor(24.9861)\n",
      "tensor(13.2503)\n",
      "tensor(11.7955)\n",
      "tensor(0.5898)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 197.362549\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "tensor(16.3527)\n",
      "tensor(11.0962)\n",
      "tensor(11.0916)\n",
      "tensor(0.5562)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 197.335312\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "tensor(16.1365)\n",
      "tensor(10.6977)\n",
      "tensor(11.0640)\n",
      "tensor(0.5275)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 197.301437\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "tensor(18.8650)\n",
      "tensor(10.9224)\n",
      "tensor(12.7096)\n",
      "tensor(0.5129)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 197.253998\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "tensor(58.6726)\n",
      "tensor(18.4941)\n",
      "tensor(14.6041)\n",
      "tensor(0.5325)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 197.197098\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "tensor(27.6377)\n",
      "tensor(12.3487)\n",
      "tensor(11.6085)\n",
      "tensor(0.5499)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 197.055481\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "tensor(24.6091)\n",
      "tensor(13.5095)\n",
      "tensor(12.6242)\n",
      "tensor(0.6139)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 196.914658\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "tensor(30.8952)\n",
      "tensor(13.4009)\n",
      "tensor(15.4404)\n",
      "tensor(0.8624)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 196.765472\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "tensor(20.4095)\n",
      "tensor(9.7996)\n",
      "tensor(12.2433)\n",
      "tensor(1.0787)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 196.534653\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "tensor(22.4634)\n",
      "tensor(10.3880)\n",
      "tensor(15.1440)\n",
      "tensor(0.8537)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 196.330139\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "tensor(19.0304)\n",
      "tensor(15.0086)\n",
      "tensor(15.3474)\n",
      "tensor(0.4473)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 196.103546\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "tensor(16.5994)\n",
      "tensor(11.6822)\n",
      "tensor(11.7463)\n",
      "tensor(0.3612)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 195.871399\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "tensor(24.9636)\n",
      "tensor(12.9703)\n",
      "tensor(11.8181)\n",
      "tensor(0.5234)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 195.664902\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "tensor(21.2391)\n",
      "tensor(10.7963)\n",
      "tensor(12.0103)\n",
      "tensor(0.7613)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 195.468475\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "tensor(41.0195)\n",
      "tensor(16.1675)\n",
      "tensor(11.4013)\n",
      "tensor(0.7775)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 195.302475\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "tensor(14.6723)\n",
      "tensor(13.8743)\n",
      "tensor(11.2519)\n",
      "tensor(0.6472)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 195.166061\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "tensor(75.8319)\n",
      "tensor(26.1591)\n",
      "tensor(11.3991)\n",
      "tensor(0.5657)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 195.104202\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "tensor(30.2048)\n",
      "tensor(13.6775)\n",
      "tensor(10.9270)\n",
      "tensor(0.4711)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 195.008713\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "tensor(21.3774)\n",
      "tensor(12.0508)\n",
      "tensor(11.0791)\n",
      "tensor(0.4201)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 194.968216\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "tensor(35.5775)\n",
      "tensor(15.1547)\n",
      "tensor(11.3397)\n",
      "tensor(0.4031)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 194.944733\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "tensor(30.2553)\n",
      "tensor(15.0552)\n",
      "tensor(11.6169)\n",
      "tensor(0.4005)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 194.920441\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "tensor(25.4817)\n",
      "tensor(13.7814)\n",
      "tensor(11.7796)\n",
      "tensor(0.4252)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 194.892502\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "tensor(20.0560)\n",
      "tensor(10.8300)\n",
      "tensor(11.6144)\n",
      "tensor(0.4928)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 194.858276\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "tensor(44.4599)\n",
      "tensor(16.5991)\n",
      "tensor(11.1030)\n",
      "tensor(0.5723)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 194.796951\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "tensor(45.8764)\n",
      "tensor(16.2948)\n",
      "tensor(10.9219)\n",
      "tensor(0.5983)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 194.724655\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "tensor(31.0515)\n",
      "tensor(15.7934)\n",
      "tensor(10.9917)\n",
      "tensor(0.5406)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 194.614182\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "tensor(38.6398)\n",
      "tensor(17.5522)\n",
      "tensor(11.2321)\n",
      "tensor(0.6437)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 194.480911\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "tensor(15.6481)\n",
      "tensor(10.2161)\n",
      "tensor(11.4318)\n",
      "tensor(0.8846)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 194.311157\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "tensor(15.4766)\n",
      "tensor(10.3731)\n",
      "tensor(11.6759)\n",
      "tensor(1.0336)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 194.118881\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "tensor(19.1381)\n",
      "tensor(9.3566)\n",
      "tensor(11.7054)\n",
      "tensor(0.9757)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 193.902817\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "tensor(32.4876)\n",
      "tensor(14.3858)\n",
      "tensor(10.9362)\n",
      "tensor(0.6457)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 193.704346\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "tensor(17.9247)\n",
      "tensor(13.0739)\n",
      "tensor(11.7860)\n",
      "tensor(0.4367)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 193.467285\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "tensor(17.3819)\n",
      "tensor(10.8869)\n",
      "tensor(12.5268)\n",
      "tensor(0.5848)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 193.278427\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "tensor(39.7027)\n",
      "tensor(14.2056)\n",
      "tensor(11.5666)\n",
      "tensor(0.7834)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 193.117432\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "tensor(15.8846)\n",
      "tensor(9.9828)\n",
      "tensor(12.0524)\n",
      "tensor(0.8283)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 192.934845\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "tensor(27.4853)\n",
      "tensor(11.5535)\n",
      "tensor(13.4227)\n",
      "tensor(0.7930)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 192.813049\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "tensor(48.4638)\n",
      "tensor(17.5797)\n",
      "tensor(12.5694)\n",
      "tensor(0.6922)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 192.710144\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "tensor(33.2973)\n",
      "tensor(15.3752)\n",
      "tensor(11.6605)\n",
      "tensor(0.5709)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 192.663498\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "tensor(32.8070)\n",
      "tensor(15.4795)\n",
      "tensor(11.0747)\n",
      "tensor(0.5348)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 192.618271\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "tensor(32.4194)\n",
      "tensor(15.5224)\n",
      "tensor(10.8456)\n",
      "tensor(0.5422)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 192.574738\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "tensor(41.8235)\n",
      "tensor(17.6221)\n",
      "tensor(10.8074)\n",
      "tensor(0.5746)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 192.552277\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "tensor(46.5963)\n",
      "tensor(17.2410)\n",
      "tensor(10.9250)\n",
      "tensor(0.6224)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 192.529312\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "tensor(15.0469)\n",
      "tensor(10.2580)\n",
      "tensor(11.1282)\n",
      "tensor(0.6742)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 192.485626\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "tensor(37.1318)\n",
      "tensor(13.3468)\n",
      "tensor(11.3560)\n",
      "tensor(0.7343)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 192.445526\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "tensor(41.7061)\n",
      "tensor(14.7753)\n",
      "tensor(11.1122)\n",
      "tensor(0.7191)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 192.361694\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "tensor(23.4463)\n",
      "tensor(10.9428)\n",
      "tensor(11.0781)\n",
      "tensor(0.5976)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 192.250732\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "tensor(25.8465)\n",
      "tensor(13.2375)\n",
      "tensor(11.7017)\n",
      "tensor(0.4203)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 192.122803\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "tensor(19.1403)\n",
      "tensor(12.7210)\n",
      "tensor(10.8625)\n",
      "tensor(0.5484)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 191.945572\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "tensor(16.8113)\n",
      "tensor(10.4969)\n",
      "tensor(12.6902)\n",
      "tensor(0.9361)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 191.756760\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "tensor(25.2228)\n",
      "tensor(11.8584)\n",
      "tensor(13.8518)\n",
      "tensor(1.1044)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 191.555832\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "tensor(37.6845)\n",
      "tensor(14.2446)\n",
      "tensor(11.3376)\n",
      "tensor(0.8972)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 191.347885\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "tensor(28.1443)\n",
      "tensor(12.1284)\n",
      "tensor(13.9182)\n",
      "tensor(0.4849)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 191.143417\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "tensor(24.1593)\n",
      "tensor(12.0654)\n",
      "tensor(11.8915)\n",
      "tensor(0.3545)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 190.934982\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "tensor(45.2769)\n",
      "tensor(16.5419)\n",
      "tensor(12.3158)\n",
      "tensor(0.4945)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 190.771362\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "tensor(18.1068)\n",
      "tensor(10.9047)\n",
      "tensor(13.4355)\n",
      "tensor(0.6152)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 190.593369\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "tensor(15.8215)\n",
      "tensor(10.7651)\n",
      "tensor(11.9995)\n",
      "tensor(0.6937)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 190.459305\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "tensor(17.0979)\n",
      "tensor(10.1645)\n",
      "tensor(10.9652)\n",
      "tensor(0.7204)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 190.359711\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "tensor(31.7775)\n",
      "tensor(15.1095)\n",
      "tensor(10.8483)\n",
      "tensor(0.6897)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 190.293060\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "tensor(19.8207)\n",
      "tensor(12.8261)\n",
      "tensor(10.8560)\n",
      "tensor(0.6631)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 190.247528\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "tensor(19.4518)\n",
      "tensor(12.8424)\n",
      "tensor(10.7994)\n",
      "tensor(0.6508)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 190.213837\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "tensor(31.3279)\n",
      "tensor(15.0561)\n",
      "tensor(10.7559)\n",
      "tensor(0.6418)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 190.188049\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "tensor(14.1891)\n",
      "tensor(10.4121)\n",
      "tensor(10.8162)\n",
      "tensor(0.6419)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 190.159927\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "tensor(14.3815)\n",
      "tensor(10.1599)\n",
      "tensor(11.0391)\n",
      "tensor(0.6394)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 190.119308\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "tensor(55.9782)\n",
      "tensor(19.3635)\n",
      "tensor(11.3663)\n",
      "tensor(0.6303)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 190.058243\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "tensor(13.9242)\n",
      "tensor(10.5741)\n",
      "tensor(10.7842)\n",
      "tensor(0.5652)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 189.959183\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "tensor(44.5055)\n",
      "tensor(16.8861)\n",
      "tensor(11.0841)\n",
      "tensor(0.5380)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 189.895508\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "tensor(56.9261)\n",
      "tensor(21.3773)\n",
      "tensor(13.8756)\n",
      "tensor(0.5034)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 189.771515\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "tensor(35.0173)\n",
      "tensor(13.7626)\n",
      "tensor(16.6616)\n",
      "tensor(0.6689)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 189.598312\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "tensor(22.5898)\n",
      "tensor(9.9838)\n",
      "tensor(11.4911)\n",
      "tensor(0.9548)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 189.396103\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "tensor(49.3147)\n",
      "tensor(16.5899)\n",
      "tensor(16.1644)\n",
      "tensor(0.9204)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 189.235962\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "tensor(18.3324)\n",
      "tensor(10.2769)\n",
      "tensor(13.7357)\n",
      "tensor(0.6388)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 189.009415\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "tensor(46.5750)\n",
      "tensor(17.0757)\n",
      "tensor(11.3939)\n",
      "tensor(0.4986)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 188.833054\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "tensor(40.3723)\n",
      "tensor(14.7577)\n",
      "tensor(16.7675)\n",
      "tensor(0.5898)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 188.635696\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "tensor(20.5794)\n",
      "tensor(10.0207)\n",
      "tensor(13.9362)\n",
      "tensor(0.8925)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 188.440369\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "tensor(14.4134)\n",
      "tensor(9.9330)\n",
      "tensor(11.8481)\n",
      "tensor(1.1290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 188.278656\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "tensor(46.8395)\n",
      "tensor(16.5507)\n",
      "tensor(14.1865)\n",
      "tensor(1.1408)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 188.184692\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "tensor(15.7290)\n",
      "tensor(14.1100)\n",
      "tensor(13.8950)\n",
      "tensor(0.9364)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 188.069275\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "tensor(31.0411)\n",
      "tensor(15.1087)\n",
      "tensor(12.8567)\n",
      "tensor(0.7768)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 188.002396\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "tensor(16.1500)\n",
      "tensor(12.4191)\n",
      "tensor(11.9503)\n",
      "tensor(0.6670)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 187.951675\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "tensor(14.6588)\n",
      "tensor(9.9267)\n",
      "tensor(11.4809)\n",
      "tensor(0.6018)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 187.921448\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "tensor(14.0487)\n",
      "tensor(9.9578)\n",
      "tensor(11.0952)\n",
      "tensor(0.5603)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 187.896698\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "tensor(13.9065)\n",
      "tensor(10.0297)\n",
      "tensor(10.7740)\n",
      "tensor(0.5285)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 187.865494\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "tensor(31.2579)\n",
      "tensor(13.0287)\n",
      "tensor(10.7739)\n",
      "tensor(0.5125)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 187.830032\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "tensor(27.9645)\n",
      "tensor(13.4693)\n",
      "tensor(11.5075)\n",
      "tensor(0.5023)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 187.760681\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "tensor(31.7643)\n",
      "tensor(12.9969)\n",
      "tensor(12.1594)\n",
      "tensor(0.6063)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 187.667999\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "tensor(40.9459)\n",
      "tensor(16.6306)\n",
      "tensor(11.9198)\n",
      "tensor(0.7357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 187.621567\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "tensor(45.6520)\n",
      "tensor(15.8614)\n",
      "tensor(11.4499)\n",
      "tensor(0.8537)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 187.492279\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "tensor(13.8979)\n",
      "tensor(10.0674)\n",
      "tensor(11.0059)\n",
      "tensor(0.7508)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 187.250702\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "tensor(13.5551)\n",
      "tensor(9.8940)\n",
      "tensor(10.6212)\n",
      "tensor(0.5846)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 187.071671\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "tensor(16.4452)\n",
      "tensor(12.2557)\n",
      "tensor(10.7552)\n",
      "tensor(0.4897)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 186.873825\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "tensor(21.3840)\n",
      "tensor(12.9844)\n",
      "tensor(11.4290)\n",
      "tensor(0.6836)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 186.661118\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "tensor(22.3823)\n",
      "tensor(10.1989)\n",
      "tensor(12.4978)\n",
      "tensor(0.8462)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 186.463196\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "tensor(15.4470)\n",
      "tensor(10.1707)\n",
      "tensor(11.0733)\n",
      "tensor(0.7486)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 186.262787\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "tensor(14.4787)\n",
      "tensor(10.8574)\n",
      "tensor(10.5480)\n",
      "tensor(0.5696)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 186.085602\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "tensor(15.2686)\n",
      "tensor(10.6722)\n",
      "tensor(10.6265)\n",
      "tensor(0.5611)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 185.926025\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "tensor(47.6262)\n",
      "tensor(17.8460)\n",
      "tensor(10.5485)\n",
      "tensor(0.6728)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 185.790436\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "tensor(17.4916)\n",
      "tensor(12.6847)\n",
      "tensor(10.6438)\n",
      "tensor(0.7366)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 185.695175\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "tensor(24.9024)\n",
      "tensor(13.7040)\n",
      "tensor(10.7460)\n",
      "tensor(0.8152)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 185.636627\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "tensor(24.5338)\n",
      "tensor(13.7876)\n",
      "tensor(10.9181)\n",
      "tensor(0.8618)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 185.587341\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "tensor(24.3477)\n",
      "tensor(13.8408)\n",
      "tensor(11.0640)\n",
      "tensor(0.8822)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 185.547714\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "tensor(29.7980)\n",
      "tensor(12.2031)\n",
      "tensor(11.1776)\n",
      "tensor(0.8885)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 185.522232\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "tensor(21.1193)\n",
      "tensor(10.7348)\n",
      "tensor(11.1524)\n",
      "tensor(0.8681)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 185.494904\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "tensor(48.1926)\n",
      "tensor(15.7720)\n",
      "tensor(10.9603)\n",
      "tensor(0.8078)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 185.465759\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "tensor(50.4109)\n",
      "tensor(17.5823)\n",
      "tensor(10.6154)\n",
      "tensor(0.6432)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 185.418579\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "tensor(44.9457)\n",
      "tensor(16.9380)\n",
      "tensor(12.0723)\n",
      "tensor(0.3740)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 185.345596\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "tensor(48.7084)\n",
      "tensor(18.0205)\n",
      "tensor(14.5307)\n",
      "tensor(0.1354)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 185.282593\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "tensor(47.7540)\n",
      "tensor(17.4216)\n",
      "tensor(13.6755)\n",
      "tensor(0.1530)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 185.144608\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "tensor(32.1519)\n",
      "tensor(13.2195)\n",
      "tensor(10.6183)\n",
      "tensor(0.6789)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 184.977249\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "tensor(16.4129)\n",
      "tensor(10.1537)\n",
      "tensor(14.0560)\n",
      "tensor(1.2927)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 184.793732\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "tensor(16.0714)\n",
      "tensor(10.3057)\n",
      "tensor(14.4377)\n",
      "tensor(1.4368)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 184.594101\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "tensor(30.1761)\n",
      "tensor(14.0934)\n",
      "tensor(11.2052)\n",
      "tensor(0.9879)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 184.376511\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "tensor(23.1090)\n",
      "tensor(10.9660)\n",
      "tensor(12.2054)\n",
      "tensor(0.4979)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 184.178864\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "tensor(39.6400)\n",
      "tensor(15.3860)\n",
      "tensor(11.7590)\n",
      "tensor(0.3394)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 183.991959\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "tensor(21.0598)\n",
      "tensor(11.3090)\n",
      "tensor(10.9192)\n",
      "tensor(0.4219)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 183.815430\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "tensor(48.8779)\n",
      "tensor(17.3601)\n",
      "tensor(11.3391)\n",
      "tensor(0.6824)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 183.678482\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "tensor(13.6375)\n",
      "tensor(9.5383)\n",
      "tensor(11.3627)\n",
      "tensor(0.8187)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 183.564484\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "tensor(15.0782)\n",
      "tensor(9.4102)\n",
      "tensor(11.0616)\n",
      "tensor(0.8661)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 183.475464\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "tensor(13.1247)\n",
      "tensor(9.8567)\n",
      "tensor(10.8335)\n",
      "tensor(0.8300)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 183.412460\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "tensor(21.9381)\n",
      "tensor(12.8832)\n",
      "tensor(10.7240)\n",
      "tensor(0.7921)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 183.374069\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "tensor(29.3658)\n",
      "tensor(13.9838)\n",
      "tensor(10.6871)\n",
      "tensor(0.7700)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 183.344696\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "tensor(17.0430)\n",
      "tensor(11.7533)\n",
      "tensor(10.6752)\n",
      "tensor(0.7577)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 183.320877\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "tensor(31.2008)\n",
      "tensor(12.6399)\n",
      "tensor(10.6593)\n",
      "tensor(0.7376)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 183.296768\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "tensor(34.8509)\n",
      "tensor(13.6956)\n",
      "tensor(10.6253)\n",
      "tensor(0.6901)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 183.270523\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "tensor(34.6543)\n",
      "tensor(13.7921)\n",
      "tensor(10.5455)\n",
      "tensor(0.6122)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 183.204437\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "tensor(31.5085)\n",
      "tensor(12.9632)\n",
      "tensor(10.6288)\n",
      "tensor(0.5403)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 183.131683\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "tensor(27.0040)\n",
      "tensor(11.9794)\n",
      "tensor(11.4252)\n",
      "tensor(0.5225)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 183.042252\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "tensor(23.3955)\n",
      "tensor(11.5652)\n",
      "tensor(11.7637)\n",
      "tensor(0.6017)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 182.913391\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "tensor(14.5906)\n",
      "tensor(12.1076)\n",
      "tensor(10.7097)\n",
      "tensor(0.8128)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 182.744385\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "tensor(17.4864)\n",
      "tensor(11.0705)\n",
      "tensor(14.5728)\n",
      "tensor(1.0213)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 182.581207\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "tensor(24.0780)\n",
      "tensor(11.2883)\n",
      "tensor(13.8145)\n",
      "tensor(0.9319)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 182.384857\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "tensor(14.6876)\n",
      "tensor(9.8430)\n",
      "tensor(10.4225)\n",
      "tensor(0.5394)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 182.164322\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "tensor(17.0857)\n",
      "tensor(10.6411)\n",
      "tensor(12.1408)\n",
      "tensor(0.2656)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 181.951141\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "tensor(26.4642)\n",
      "tensor(13.1376)\n",
      "tensor(11.0453)\n",
      "tensor(0.4339)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 181.737061\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "tensor(46.0507)\n",
      "tensor(16.7808)\n",
      "tensor(10.9483)\n",
      "tensor(0.8602)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 181.562943\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "tensor(12.3509)\n",
      "tensor(10.2346)\n",
      "tensor(11.6321)\n",
      "tensor(1.0527)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 181.404404\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "tensor(36.1306)\n",
      "tensor(13.3371)\n",
      "tensor(11.4191)\n",
      "tensor(1.0349)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 181.300766\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "tensor(35.9376)\n",
      "tensor(15.3473)\n",
      "tensor(10.6558)\n",
      "tensor(0.8630)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 181.208649\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "tensor(22.0925)\n",
      "tensor(12.5665)\n",
      "tensor(10.4195)\n",
      "tensor(0.7317)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 181.149963\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "tensor(16.0013)\n",
      "tensor(12.1254)\n",
      "tensor(10.3260)\n",
      "tensor(0.6621)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 181.107178\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "tensor(17.3695)\n",
      "tensor(10.5989)\n",
      "tensor(10.3094)\n",
      "tensor(0.6388)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 181.081940\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "tensor(11.9076)\n",
      "tensor(9.9806)\n",
      "tensor(10.3375)\n",
      "tensor(0.6287)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 181.061478\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "tensor(45.9069)\n",
      "tensor(16.9290)\n",
      "tensor(10.4089)\n",
      "tensor(0.6306)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 181.038544\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "tensor(33.1551)\n",
      "tensor(13.3758)\n",
      "tensor(10.3980)\n",
      "tensor(0.6341)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 181.005646\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "tensor(56.7727)\n",
      "tensor(19.3754)\n",
      "tensor(10.3184)\n",
      "tensor(0.6479)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 180.956085\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "tensor(16.7902)\n",
      "tensor(9.5971)\n",
      "tensor(11.0429)\n",
      "tensor(0.6252)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 180.869553\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "tensor(43.3012)\n",
      "tensor(16.3196)\n",
      "tensor(11.5967)\n",
      "tensor(0.6251)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 180.782272\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "tensor(65.3561)\n",
      "tensor(23.0254)\n",
      "tensor(11.3583)\n",
      "tensor(0.6928)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 180.700974\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "tensor(17.3096)\n",
      "tensor(9.7450)\n",
      "tensor(12.1958)\n",
      "tensor(0.7786)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 180.494873\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "tensor(12.5909)\n",
      "tensor(9.1717)\n",
      "tensor(10.7985)\n",
      "tensor(0.9256)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 180.325043\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "tensor(33.8518)\n",
      "tensor(11.8783)\n",
      "tensor(12.4236)\n",
      "tensor(0.9577)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 180.147308\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "tensor(21.3612)\n",
      "tensor(10.0713)\n",
      "tensor(11.3862)\n",
      "tensor(0.7319)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 179.967926\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "tensor(19.1239)\n",
      "tensor(10.0562)\n",
      "tensor(10.6808)\n",
      "tensor(0.5183)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 179.775299\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "tensor(23.4206)\n",
      "tensor(11.5183)\n",
      "tensor(11.1008)\n",
      "tensor(0.5966)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 179.610382\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "tensor(34.6033)\n",
      "tensor(14.4041)\n",
      "tensor(10.8738)\n",
      "tensor(0.9297)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 179.456787\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "tensor(30.4464)\n",
      "tensor(11.4990)\n",
      "tensor(11.7056)\n",
      "tensor(1.1200)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 179.308502\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "tensor(14.4334)\n",
      "tensor(9.4594)\n",
      "tensor(11.3236)\n",
      "tensor(1.0177)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 179.200302\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "tensor(38.3024)\n",
      "tensor(14.7873)\n",
      "tensor(10.7264)\n",
      "tensor(0.8094)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 179.119583\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "tensor(19.8940)\n",
      "tensor(10.3482)\n",
      "tensor(10.4084)\n",
      "tensor(0.6356)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 179.065186\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "tensor(20.1370)\n",
      "tensor(10.7143)\n",
      "tensor(10.3773)\n",
      "tensor(0.5447)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 179.024551\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "tensor(18.0205)\n",
      "tensor(10.3961)\n",
      "tensor(10.3939)\n",
      "tensor(0.5115)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 178.999374\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "tensor(17.9592)\n",
      "tensor(10.6513)\n",
      "tensor(10.3973)\n",
      "tensor(0.5066)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 178.976639\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "tensor(12.6451)\n",
      "tensor(9.7120)\n",
      "tensor(10.3777)\n",
      "tensor(0.5261)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 178.947632\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "tensor(15.8398)\n",
      "tensor(9.6401)\n",
      "tensor(10.3389)\n",
      "tensor(0.5844)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 178.904922\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "tensor(28.9728)\n",
      "tensor(13.2337)\n",
      "tensor(10.3547)\n",
      "tensor(0.6775)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 178.853760\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "tensor(27.6127)\n",
      "tensor(13.4525)\n",
      "tensor(10.6852)\n",
      "tensor(0.7986)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 178.761108\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "tensor(32.1432)\n",
      "tensor(14.6206)\n",
      "tensor(11.4389)\n",
      "tensor(0.9159)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 178.699844\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "tensor(41.3791)\n",
      "tensor(14.4292)\n",
      "tensor(11.3380)\n",
      "tensor(0.9204)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 178.584366\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "tensor(20.9276)\n",
      "tensor(10.6743)\n",
      "tensor(10.9669)\n",
      "tensor(0.6932)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 178.353088\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "tensor(17.3189)\n",
      "tensor(10.1540)\n",
      "tensor(12.1223)\n",
      "tensor(0.5029)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 178.191605\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "tensor(27.3927)\n",
      "tensor(12.3873)\n",
      "tensor(10.3174)\n",
      "tensor(0.5191)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 177.996552\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "tensor(45.8832)\n",
      "tensor(16.4601)\n",
      "tensor(10.9146)\n",
      "tensor(0.6963)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 177.812469\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "tensor(12.8471)\n",
      "tensor(9.1199)\n",
      "tensor(10.4125)\n",
      "tensor(0.7096)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 177.610291\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "tensor(15.7938)\n",
      "tensor(11.5531)\n",
      "tensor(10.2434)\n",
      "tensor(0.7118)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 177.424347\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "tensor(11.8805)\n",
      "tensor(9.4865)\n",
      "tensor(10.4876)\n",
      "tensor(0.8474)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 177.250519\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "tensor(59.5424)\n",
      "tensor(19.9324)\n",
      "tensor(10.7654)\n",
      "tensor(0.9370)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 177.147400\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "tensor(31.1400)\n",
      "tensor(13.3945)\n",
      "tensor(10.9154)\n",
      "tensor(0.8456)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 177.016968\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "tensor(24.4992)\n",
      "tensor(11.0669)\n",
      "tensor(11.2747)\n",
      "tensor(0.7634)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 176.931763\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "tensor(15.8012)\n",
      "tensor(9.4451)\n",
      "tensor(10.8357)\n",
      "tensor(0.7050)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 176.866333\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "tensor(14.8375)\n",
      "tensor(9.7809)\n",
      "tensor(10.3452)\n",
      "tensor(0.6752)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 176.827698\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "tensor(12.1977)\n",
      "tensor(9.5781)\n",
      "tensor(10.1722)\n",
      "tensor(0.6698)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 176.801376\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "tensor(30.2660)\n",
      "tensor(12.4906)\n",
      "tensor(10.1898)\n",
      "tensor(0.6787)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 176.785645\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "tensor(34.1633)\n",
      "tensor(13.2016)\n",
      "tensor(10.3258)\n",
      "tensor(0.6957)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 176.757004\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "tensor(19.5440)\n",
      "tensor(10.6689)\n",
      "tensor(10.4553)\n",
      "tensor(0.7308)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 176.718567\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "tensor(50.9827)\n",
      "tensor(17.4051)\n",
      "tensor(10.5529)\n",
      "tensor(0.7980)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 176.677780\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "tensor(31.5502)\n",
      "tensor(13.6288)\n",
      "tensor(10.2746)\n",
      "tensor(0.8059)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 176.600601\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "tensor(36.1568)\n",
      "tensor(14.0433)\n",
      "tensor(10.5965)\n",
      "tensor(0.8007)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 176.514267\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "tensor(58.9839)\n",
      "tensor(20.8589)\n",
      "tensor(11.2244)\n",
      "tensor(0.7660)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 176.430664\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "tensor(18.9407)\n",
      "tensor(9.7723)\n",
      "tensor(12.6056)\n",
      "tensor(0.6542)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 176.246613\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "tensor(26.0838)\n",
      "tensor(11.3972)\n",
      "tensor(10.2677)\n",
      "tensor(0.6820)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 176.070679\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "tensor(15.5868)\n",
      "tensor(10.1280)\n",
      "tensor(12.0208)\n",
      "tensor(0.7320)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 175.884186\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "tensor(32.9230)\n",
      "tensor(12.0857)\n",
      "tensor(12.6325)\n",
      "tensor(0.7902)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 175.698669\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "tensor(32.2096)\n",
      "tensor(13.6610)\n",
      "tensor(10.1797)\n",
      "tensor(0.6957)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 175.539978\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "tensor(23.0771)\n",
      "tensor(10.6150)\n",
      "tensor(12.3705)\n",
      "tensor(0.7192)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 175.360428\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "tensor(13.5301)\n",
      "tensor(9.4183)\n",
      "tensor(10.7012)\n",
      "tensor(0.7953)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 175.195282\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "tensor(12.0326)\n",
      "tensor(9.6739)\n",
      "tensor(11.0374)\n",
      "tensor(0.8059)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 175.060638\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "tensor(18.6673)\n",
      "tensor(10.6290)\n",
      "tensor(12.2043)\n",
      "tensor(0.7529)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 174.957062\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "tensor(58.5432)\n",
      "tensor(19.4547)\n",
      "tensor(11.6915)\n",
      "tensor(0.6810)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 174.872330\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "tensor(20.6416)\n",
      "tensor(11.0675)\n",
      "tensor(10.2352)\n",
      "tensor(0.5906)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 174.804474\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "tensor(27.2001)\n",
      "tensor(11.9232)\n",
      "tensor(10.2653)\n",
      "tensor(0.5544)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 174.775574\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "tensor(28.1790)\n",
      "tensor(12.1875)\n",
      "tensor(10.6087)\n",
      "tensor(0.5438)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 174.749329\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "tensor(16.5435)\n",
      "tensor(9.4209)\n",
      "tensor(10.8625)\n",
      "tensor(0.5430)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 174.728439\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "tensor(21.2830)\n",
      "tensor(10.1940)\n",
      "tensor(10.8756)\n",
      "tensor(0.5554)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 174.705795\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "tensor(18.1960)\n",
      "tensor(10.2186)\n",
      "tensor(10.5309)\n",
      "tensor(0.5847)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 174.666687\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "tensor(38.1460)\n",
      "tensor(14.1920)\n",
      "tensor(10.1457)\n",
      "tensor(0.6657)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 174.634964\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "tensor(36.1298)\n",
      "tensor(14.4671)\n",
      "tensor(10.2988)\n",
      "tensor(0.7662)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 174.548508\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "tensor(18.2014)\n",
      "tensor(12.1237)\n",
      "tensor(10.6073)\n",
      "tensor(0.9023)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 174.451019\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "tensor(45.3313)\n",
      "tensor(16.8295)\n",
      "tensor(11.1051)\n",
      "tensor(1.0282)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 174.332733\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "tensor(15.9707)\n",
      "tensor(9.5182)\n",
      "tensor(10.5551)\n",
      "tensor(0.9013)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 174.188461\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "tensor(31.1469)\n",
      "tensor(12.3597)\n",
      "tensor(10.3740)\n",
      "tensor(0.6544)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 174.067261\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "tensor(29.7082)\n",
      "tensor(12.3253)\n",
      "tensor(10.4083)\n",
      "tensor(0.4668)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 173.853333\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "tensor(13.3282)\n",
      "tensor(9.1853)\n",
      "tensor(10.2605)\n",
      "tensor(0.5563)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 173.678955\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "tensor(21.9659)\n",
      "tensor(10.5430)\n",
      "tensor(10.7105)\n",
      "tensor(0.9079)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 173.528397\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "tensor(21.7853)\n",
      "tensor(10.7143)\n",
      "tensor(11.2734)\n",
      "tensor(1.1221)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 173.340988\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "tensor(14.2208)\n",
      "tensor(9.6573)\n",
      "tensor(10.8971)\n",
      "tensor(1.0624)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 173.178787\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "tensor(12.7312)\n",
      "tensor(9.5303)\n",
      "tensor(10.2658)\n",
      "tensor(0.8569)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 173.037491\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "tensor(52.9779)\n",
      "tensor(18.4390)\n",
      "tensor(9.9526)\n",
      "tensor(0.6835)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 172.940186\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "tensor(22.7543)\n",
      "tensor(11.8121)\n",
      "tensor(10.1425)\n",
      "tensor(0.5445)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 172.856537\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "tensor(29.7032)\n",
      "tensor(12.5871)\n",
      "tensor(10.1023)\n",
      "tensor(0.5213)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 172.794922\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "tensor(13.8856)\n",
      "tensor(9.3381)\n",
      "tensor(10.0276)\n",
      "tensor(0.5367)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 172.759384\n",
      "Epoch 1001\n",
      "-------------------------------\n",
      "tensor(17.9341)\n",
      "tensor(9.9809)\n",
      "tensor(9.9543)\n",
      "tensor(0.5653)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 172.735229\n",
      "Epoch 1002\n",
      "-------------------------------\n",
      "tensor(14.8730)\n",
      "tensor(9.7438)\n",
      "tensor(9.9454)\n",
      "tensor(0.5987)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 172.714706\n",
      "Epoch 1003\n",
      "-------------------------------\n",
      "tensor(14.0338)\n",
      "tensor(9.5315)\n",
      "tensor(10.0618)\n",
      "tensor(0.6474)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 172.687759\n",
      "Epoch 1004\n",
      "-------------------------------\n",
      "tensor(11.4117)\n",
      "tensor(9.6666)\n",
      "tensor(10.5097)\n",
      "tensor(0.7198)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 172.646973\n",
      "Epoch 1005\n",
      "-------------------------------\n",
      "tensor(11.9346)\n",
      "tensor(10.0764)\n",
      "tensor(11.3540)\n",
      "tensor(0.8239)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 172.585663\n",
      "Epoch 1006\n",
      "-------------------------------\n",
      "tensor(33.7022)\n",
      "tensor(14.7643)\n",
      "tensor(11.8687)\n",
      "tensor(0.9297)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 172.519318\n",
      "Epoch 1007\n",
      "-------------------------------\n",
      "tensor(26.2697)\n",
      "tensor(13.3315)\n",
      "tensor(10.9928)\n",
      "tensor(0.9729)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 172.401321\n",
      "Epoch 1008\n",
      "-------------------------------\n",
      "tensor(42.5195)\n",
      "tensor(15.3393)\n",
      "tensor(10.7526)\n",
      "tensor(0.8831)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 172.281250\n",
      "Epoch 1009\n",
      "-------------------------------\n",
      "tensor(69.7893)\n",
      "tensor(24.3031)\n",
      "tensor(13.3491)\n",
      "tensor(0.6037)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 172.224228\n",
      "Epoch 1010\n",
      "-------------------------------\n",
      "tensor(46.3120)\n",
      "tensor(16.0198)\n",
      "tensor(19.6603)\n",
      "tensor(0.1982)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 172.025116\n",
      "Epoch 1011\n",
      "-------------------------------\n",
      "tensor(33.3860)\n",
      "tensor(14.0194)\n",
      "tensor(12.2752)\n",
      "tensor(0.2292)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 171.819870\n",
      "Epoch 1012\n",
      "-------------------------------\n",
      "tensor(21.5725)\n",
      "tensor(10.8344)\n",
      "tensor(13.0727)\n",
      "tensor(0.6925)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 171.632431\n",
      "Epoch 1013\n",
      "-------------------------------\n",
      "tensor(21.4166)\n",
      "tensor(11.2105)\n",
      "tensor(16.3124)\n",
      "tensor(1.1289)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 171.478760\n",
      "Epoch 1014\n",
      "-------------------------------\n",
      "tensor(28.1647)\n",
      "tensor(11.3754)\n",
      "tensor(11.7539)\n",
      "tensor(1.1475)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 171.308380\n",
      "Epoch 1015\n",
      "-------------------------------\n",
      "tensor(19.7020)\n",
      "tensor(9.0123)\n",
      "tensor(12.4897)\n",
      "tensor(0.8070)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 171.163315\n",
      "Epoch 1016\n",
      "-------------------------------\n",
      "tensor(20.7233)\n",
      "tensor(8.7593)\n",
      "tensor(12.7806)\n",
      "tensor(0.5280)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 171.035980\n",
      "Epoch 1017\n",
      "-------------------------------\n",
      "tensor(34.1348)\n",
      "tensor(12.8960)\n",
      "tensor(10.4964)\n",
      "tensor(0.4986)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 170.943237\n",
      "Epoch 1018\n",
      "-------------------------------\n",
      "tensor(34.8069)\n",
      "tensor(12.7910)\n",
      "tensor(10.1611)\n",
      "tensor(0.5847)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 170.847519\n",
      "Epoch 1019\n",
      "-------------------------------\n",
      "tensor(12.3292)\n",
      "tensor(8.7571)\n",
      "tensor(10.2379)\n",
      "tensor(0.6955)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 170.794586\n",
      "Epoch 1020\n",
      "-------------------------------\n",
      "tensor(17.3412)\n",
      "tensor(9.4380)\n",
      "tensor(10.3430)\n",
      "tensor(0.7805)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 170.761658\n",
      "Epoch 1021\n",
      "-------------------------------\n",
      "tensor(17.2593)\n",
      "tensor(9.4581)\n",
      "tensor(10.3938)\n",
      "tensor(0.8318)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 170.737991\n",
      "Epoch 1022\n",
      "-------------------------------\n",
      "tensor(19.2398)\n",
      "tensor(9.5151)\n",
      "tensor(10.4068)\n",
      "tensor(0.8648)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 170.719696\n",
      "Epoch 1023\n",
      "-------------------------------\n",
      "tensor(19.5247)\n",
      "tensor(9.5653)\n",
      "tensor(10.3847)\n",
      "tensor(0.8843)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 170.695740\n",
      "Epoch 1024\n",
      "-------------------------------\n",
      "tensor(14.8943)\n",
      "tensor(9.0536)\n",
      "tensor(10.3020)\n",
      "tensor(0.8821)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 170.656158\n",
      "Epoch 1025\n",
      "-------------------------------\n",
      "tensor(47.5475)\n",
      "tensor(16.7029)\n",
      "tensor(10.1685)\n",
      "tensor(0.8403)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 170.625534\n",
      "Epoch 1026\n",
      "-------------------------------\n",
      "tensor(29.0262)\n",
      "tensor(11.9446)\n",
      "tensor(10.2326)\n",
      "tensor(0.6894)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 170.542175\n",
      "Epoch 1027\n",
      "-------------------------------\n",
      "tensor(29.3871)\n",
      "tensor(12.2481)\n",
      "tensor(10.9594)\n",
      "tensor(0.5415)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 170.435059\n",
      "Epoch 1028\n",
      "-------------------------------\n",
      "tensor(30.5200)\n",
      "tensor(12.6472)\n",
      "tensor(10.9149)\n",
      "tensor(0.5275)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 170.331772\n",
      "Epoch 1029\n",
      "-------------------------------\n",
      "tensor(49.5602)\n",
      "tensor(17.6548)\n",
      "tensor(9.9397)\n",
      "tensor(0.6795)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 170.189133\n",
      "Epoch 1030\n",
      "-------------------------------\n",
      "tensor(24.4207)\n",
      "tensor(11.1300)\n",
      "tensor(10.0875)\n",
      "tensor(0.8136)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 170.009460\n",
      "Epoch 1031\n",
      "-------------------------------\n",
      "tensor(16.5597)\n",
      "tensor(9.8484)\n",
      "tensor(10.1860)\n",
      "tensor(0.9096)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 169.848206\n",
      "Epoch 1032\n",
      "-------------------------------\n",
      "tensor(11.2438)\n",
      "tensor(9.6214)\n",
      "tensor(10.3895)\n",
      "tensor(0.8834)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 169.678162\n",
      "Epoch 1033\n",
      "-------------------------------\n",
      "tensor(34.2255)\n",
      "tensor(13.9555)\n",
      "tensor(10.5402)\n",
      "tensor(0.7740)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 169.554398\n",
      "Epoch 1034\n",
      "-------------------------------\n",
      "tensor(29.6183)\n",
      "tensor(12.1116)\n",
      "tensor(9.7946)\n",
      "tensor(0.6283)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 169.355255\n",
      "Epoch 1035\n",
      "-------------------------------\n",
      "tensor(16.4043)\n",
      "tensor(9.5890)\n",
      "tensor(10.0046)\n",
      "tensor(0.5718)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 169.230896\n",
      "Epoch 1036\n",
      "-------------------------------\n",
      "tensor(13.9154)\n",
      "tensor(9.3800)\n",
      "tensor(9.7715)\n",
      "tensor(0.6236)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 169.109787\n",
      "Epoch 1037\n",
      "-------------------------------\n",
      "tensor(11.6307)\n",
      "tensor(9.5080)\n",
      "tensor(9.9486)\n",
      "tensor(0.7073)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 169.005402\n",
      "Epoch 1038\n",
      "-------------------------------\n",
      "tensor(13.4602)\n",
      "tensor(10.1397)\n",
      "tensor(10.8119)\n",
      "tensor(0.7567)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 168.925674\n",
      "Epoch 1039\n",
      "-------------------------------\n",
      "tensor(21.5786)\n",
      "tensor(11.0776)\n",
      "tensor(11.1299)\n",
      "tensor(0.7816)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 168.870422\n",
      "Epoch 1040\n",
      "-------------------------------\n",
      "tensor(21.4571)\n",
      "tensor(11.2767)\n",
      "tensor(10.8740)\n",
      "tensor(0.7860)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 168.831009\n",
      "Epoch 1041\n",
      "-------------------------------\n",
      "tensor(11.2302)\n",
      "tensor(10.1829)\n",
      "tensor(10.5168)\n",
      "tensor(0.7815)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 168.804382\n",
      "Epoch 1042\n",
      "-------------------------------\n",
      "tensor(11.1174)\n",
      "tensor(10.0542)\n",
      "tensor(10.2280)\n",
      "tensor(0.7737)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 168.782349\n",
      "Epoch 1043\n",
      "-------------------------------\n",
      "tensor(21.7728)\n",
      "tensor(11.1619)\n",
      "tensor(9.9618)\n",
      "tensor(0.7616)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 168.759750\n",
      "Epoch 1044\n",
      "-------------------------------\n",
      "tensor(22.1174)\n",
      "tensor(11.0156)\n",
      "tensor(9.7888)\n",
      "tensor(0.7356)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 168.718689\n",
      "Epoch 1045\n",
      "-------------------------------\n",
      "tensor(25.9138)\n",
      "tensor(11.3341)\n",
      "tensor(9.9657)\n",
      "tensor(0.6869)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 168.677032\n",
      "Epoch 1046\n",
      "-------------------------------\n",
      "tensor(27.1150)\n",
      "tensor(11.5741)\n",
      "tensor(10.2510)\n",
      "tensor(0.6189)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 168.604630\n",
      "Epoch 1047\n",
      "-------------------------------\n",
      "tensor(32.9139)\n",
      "tensor(13.0193)\n",
      "tensor(9.9158)\n",
      "tensor(0.5650)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 168.469391\n",
      "Epoch 1048\n",
      "-------------------------------\n",
      "tensor(40.6069)\n",
      "tensor(14.9689)\n",
      "tensor(9.6671)\n",
      "tensor(0.6054)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 168.365097\n",
      "Epoch 1049\n",
      "-------------------------------\n",
      "tensor(65.7744)\n",
      "tensor(22.1696)\n",
      "tensor(9.7023)\n",
      "tensor(0.7290)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 168.310410\n",
      "Epoch 1050\n",
      "-------------------------------\n",
      "tensor(49.4325)\n",
      "tensor(17.8927)\n",
      "tensor(13.1188)\n",
      "tensor(0.7444)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 168.123581\n",
      "Epoch 1051\n",
      "-------------------------------\n",
      "tensor(26.4000)\n",
      "tensor(10.3534)\n",
      "tensor(14.4904)\n",
      "tensor(0.7771)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 167.944183\n",
      "Epoch 1052\n",
      "-------------------------------\n",
      "tensor(28.2119)\n",
      "tensor(11.4142)\n",
      "tensor(10.0708)\n",
      "tensor(0.8949)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 167.772842\n",
      "Epoch 1053\n",
      "-------------------------------\n",
      "tensor(24.8234)\n",
      "tensor(11.1740)\n",
      "tensor(13.6292)\n",
      "tensor(0.9226)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 167.595123\n",
      "Epoch 1054\n",
      "-------------------------------\n",
      "tensor(33.6517)\n",
      "tensor(11.9616)\n",
      "tensor(11.6160)\n",
      "tensor(0.8414)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 167.450089\n",
      "Epoch 1055\n",
      "-------------------------------\n",
      "tensor(16.0877)\n",
      "tensor(8.3177)\n",
      "tensor(10.4062)\n",
      "tensor(0.6688)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 167.320694\n",
      "Epoch 1056\n",
      "-------------------------------\n",
      "tensor(20.1034)\n",
      "tensor(8.6713)\n",
      "tensor(11.6850)\n",
      "tensor(0.5854)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 167.214630\n",
      "Epoch 1057\n",
      "-------------------------------\n",
      "tensor(16.9114)\n",
      "tensor(8.6404)\n",
      "tensor(10.2443)\n",
      "tensor(0.5971)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 167.107513\n",
      "Epoch 1058\n",
      "-------------------------------\n",
      "tensor(12.0255)\n",
      "tensor(8.7791)\n",
      "tensor(9.8734)\n",
      "tensor(0.6437)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 167.021942\n",
      "Epoch 1059\n",
      "-------------------------------\n",
      "tensor(14.5652)\n",
      "tensor(9.5135)\n",
      "tensor(10.7085)\n",
      "tensor(0.6902)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 166.966034\n",
      "Epoch 1060\n",
      "-------------------------------\n",
      "tensor(29.4231)\n",
      "tensor(11.6327)\n",
      "tensor(11.1426)\n",
      "tensor(0.7220)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 166.937988\n",
      "Epoch 1061\n",
      "-------------------------------\n",
      "tensor(29.2659)\n",
      "tensor(11.5972)\n",
      "tensor(10.9944)\n",
      "tensor(0.7340)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 166.913773\n",
      "Epoch 1062\n",
      "-------------------------------\n",
      "tensor(26.1303)\n",
      "tensor(11.0458)\n",
      "tensor(10.6142)\n",
      "tensor(0.7346)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 166.885208\n",
      "Epoch 1063\n",
      "-------------------------------\n",
      "tensor(11.4311)\n",
      "tensor(8.9576)\n",
      "tensor(10.1027)\n",
      "tensor(0.7302)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 166.858002\n",
      "Epoch 1064\n",
      "-------------------------------\n",
      "tensor(29.4537)\n",
      "tensor(11.5299)\n",
      "tensor(9.8756)\n",
      "tensor(0.7216)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 166.834061\n",
      "Epoch 1065\n",
      "-------------------------------\n",
      "tensor(32.1515)\n",
      "tensor(12.3598)\n",
      "tensor(10.7554)\n",
      "tensor(0.6905)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 166.790390\n",
      "Epoch 1066\n",
      "-------------------------------\n",
      "tensor(22.6449)\n",
      "tensor(9.8592)\n",
      "tensor(12.4895)\n",
      "tensor(0.6249)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 166.717911\n",
      "Epoch 1067\n",
      "-------------------------------\n",
      "tensor(42.3011)\n",
      "tensor(15.5242)\n",
      "tensor(11.3776)\n",
      "tensor(0.5713)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 166.635696\n",
      "Epoch 1068\n",
      "-------------------------------\n",
      "tensor(28.6593)\n",
      "tensor(12.0323)\n",
      "tensor(9.6744)\n",
      "tensor(0.5486)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 166.529861\n",
      "Epoch 1069\n",
      "-------------------------------\n",
      "tensor(26.4596)\n",
      "tensor(11.5441)\n",
      "tensor(11.0020)\n",
      "tensor(0.6547)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 166.408356\n",
      "Epoch 1070\n",
      "-------------------------------\n",
      "tensor(49.7468)\n",
      "tensor(17.2400)\n",
      "tensor(10.9044)\n",
      "tensor(0.8278)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 166.234146\n",
      "Epoch 1071\n",
      "-------------------------------\n",
      "tensor(17.0068)\n",
      "tensor(8.8817)\n",
      "tensor(11.3541)\n",
      "tensor(0.8655)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 166.023483\n",
      "Epoch 1072\n",
      "-------------------------------\n",
      "tensor(38.0340)\n",
      "tensor(13.4042)\n",
      "tensor(12.2975)\n",
      "tensor(0.8903)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 165.944305\n",
      "Epoch 1073\n",
      "-------------------------------\n",
      "tensor(37.6002)\n",
      "tensor(13.4190)\n",
      "tensor(10.1767)\n",
      "tensor(0.7716)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 165.747772\n",
      "Epoch 1074\n",
      "-------------------------------\n",
      "tensor(13.3585)\n",
      "tensor(8.8986)\n",
      "tensor(9.5497)\n",
      "tensor(0.6834)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 165.581451\n",
      "Epoch 1075\n",
      "-------------------------------\n",
      "tensor(12.3094)\n",
      "tensor(9.6207)\n",
      "tensor(10.7834)\n",
      "tensor(0.7080)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 165.472122\n",
      "Epoch 1076\n",
      "-------------------------------\n",
      "tensor(12.7148)\n",
      "tensor(9.8879)\n",
      "tensor(11.6092)\n",
      "tensor(0.7739)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 165.367905\n",
      "Epoch 1077\n",
      "-------------------------------\n",
      "tensor(18.2144)\n",
      "tensor(10.3937)\n",
      "tensor(10.8229)\n",
      "tensor(0.8140)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 165.273895\n",
      "Epoch 1078\n",
      "-------------------------------\n",
      "tensor(17.7231)\n",
      "tensor(10.3624)\n",
      "tensor(9.9115)\n",
      "tensor(0.8100)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 165.202255\n",
      "Epoch 1079\n",
      "-------------------------------\n",
      "tensor(16.6592)\n",
      "tensor(10.1060)\n",
      "tensor(9.6436)\n",
      "tensor(0.7944)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 165.145874\n",
      "Epoch 1080\n",
      "-------------------------------\n",
      "tensor(12.9218)\n",
      "tensor(9.2507)\n",
      "tensor(9.6462)\n",
      "tensor(0.7794)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 165.108551\n",
      "Epoch 1081\n",
      "-------------------------------\n",
      "tensor(12.9747)\n",
      "tensor(9.2414)\n",
      "tensor(9.6460)\n",
      "tensor(0.7678)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 165.083771\n",
      "Epoch 1082\n",
      "-------------------------------\n",
      "tensor(13.1643)\n",
      "tensor(9.1910)\n",
      "tensor(9.6177)\n",
      "tensor(0.7567)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 165.062180\n",
      "Epoch 1083\n",
      "-------------------------------\n",
      "tensor(12.6094)\n",
      "tensor(9.2242)\n",
      "tensor(9.5750)\n",
      "tensor(0.7377)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 165.034485\n",
      "Epoch 1084\n",
      "-------------------------------\n",
      "tensor(11.6830)\n",
      "tensor(9.3205)\n",
      "tensor(9.6710)\n",
      "tensor(0.7036)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 164.990997\n",
      "Epoch 1085\n",
      "-------------------------------\n",
      "tensor(16.2513)\n",
      "tensor(10.2226)\n",
      "tensor(10.3115)\n",
      "tensor(0.6487)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 164.925293\n",
      "Epoch 1086\n",
      "-------------------------------\n",
      "tensor(33.0748)\n",
      "tensor(12.9426)\n",
      "tensor(11.1338)\n",
      "tensor(0.5989)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 164.835388\n",
      "Epoch 1087\n",
      "-------------------------------\n",
      "tensor(31.3690)\n",
      "tensor(12.8450)\n",
      "tensor(10.0002)\n",
      "tensor(0.5772)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 164.733704\n",
      "Epoch 1088\n",
      "-------------------------------\n",
      "tensor(36.3196)\n",
      "tensor(14.0456)\n",
      "tensor(10.1187)\n",
      "tensor(0.5960)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 164.704483\n",
      "Epoch 1089\n",
      "-------------------------------\n",
      "tensor(35.7739)\n",
      "tensor(13.6321)\n",
      "tensor(13.3422)\n",
      "tensor(0.6351)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 164.626724\n",
      "Epoch 1090\n",
      "-------------------------------\n",
      "tensor(49.8677)\n",
      "tensor(17.5558)\n",
      "tensor(11.0597)\n",
      "tensor(0.6468)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 164.414978\n",
      "Epoch 1091\n",
      "-------------------------------\n",
      "tensor(71.4301)\n",
      "tensor(23.9569)\n",
      "tensor(9.6406)\n",
      "tensor(0.6284)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 164.329834\n",
      "Epoch 1092\n",
      "-------------------------------\n",
      "tensor(19.8033)\n",
      "tensor(8.5927)\n",
      "tensor(11.4886)\n",
      "tensor(0.6173)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 164.061447\n",
      "Epoch 1093\n",
      "-------------------------------\n",
      "tensor(25.3586)\n",
      "tensor(10.3897)\n",
      "tensor(9.7712)\n",
      "tensor(0.8419)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 163.938385\n",
      "Epoch 1094\n",
      "-------------------------------\n",
      "tensor(29.4995)\n",
      "tensor(11.5471)\n",
      "tensor(11.0801)\n",
      "tensor(1.0286)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 163.792450\n",
      "Epoch 1095\n",
      "-------------------------------\n",
      "tensor(17.3017)\n",
      "tensor(9.9353)\n",
      "tensor(10.7548)\n",
      "tensor(1.0035)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 163.660400\n",
      "Epoch 1096\n",
      "-------------------------------\n",
      "tensor(17.3320)\n",
      "tensor(9.3550)\n",
      "tensor(9.7963)\n",
      "tensor(0.8438)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 163.551926\n",
      "Epoch 1097\n",
      "-------------------------------\n",
      "tensor(14.2020)\n",
      "tensor(8.2929)\n",
      "tensor(9.5852)\n",
      "tensor(0.6594)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 163.457733\n",
      "Epoch 1098\n",
      "-------------------------------\n",
      "tensor(13.9403)\n",
      "tensor(8.3940)\n",
      "tensor(9.5538)\n",
      "tensor(0.5537)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 163.383835\n",
      "Epoch 1099\n",
      "-------------------------------\n",
      "tensor(13.2077)\n",
      "tensor(8.5455)\n",
      "tensor(9.5773)\n",
      "tensor(0.5231)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 163.327850\n",
      "Epoch 1100\n",
      "-------------------------------\n",
      "tensor(12.3007)\n",
      "tensor(8.7499)\n",
      "tensor(9.6830)\n",
      "tensor(0.5289)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 163.289749\n",
      "Epoch 1101\n",
      "-------------------------------\n",
      "tensor(12.0589)\n",
      "tensor(8.8218)\n",
      "tensor(9.7747)\n",
      "tensor(0.5513)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 163.263702\n",
      "Epoch 1102\n",
      "-------------------------------\n",
      "tensor(16.4537)\n",
      "tensor(9.4149)\n",
      "tensor(9.8407)\n",
      "tensor(0.5808)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 163.242233\n",
      "Epoch 1103\n",
      "-------------------------------\n",
      "tensor(11.8875)\n",
      "tensor(9.2600)\n",
      "tensor(9.8853)\n",
      "tensor(0.6235)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 163.213882\n",
      "Epoch 1104\n",
      "-------------------------------\n",
      "tensor(19.8821)\n",
      "tensor(10.2177)\n",
      "tensor(9.9338)\n",
      "tensor(0.6927)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 163.179321\n",
      "Epoch 1105\n",
      "-------------------------------\n",
      "tensor(28.7727)\n",
      "tensor(11.5748)\n",
      "tensor(9.9900)\n",
      "tensor(0.7780)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 163.132904\n",
      "Epoch 1106\n",
      "-------------------------------\n",
      "tensor(34.4058)\n",
      "tensor(12.3085)\n",
      "tensor(10.1397)\n",
      "tensor(0.8170)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 163.045914\n",
      "Epoch 1107\n",
      "-------------------------------\n",
      "tensor(41.6489)\n",
      "tensor(14.2707)\n",
      "tensor(10.8981)\n",
      "tensor(0.7428)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 162.978241\n",
      "Epoch 1108\n",
      "-------------------------------\n",
      "tensor(32.1766)\n",
      "tensor(12.1198)\n",
      "tensor(12.3364)\n",
      "tensor(0.5346)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 162.880737\n",
      "Epoch 1109\n",
      "-------------------------------\n",
      "tensor(21.3513)\n",
      "tensor(9.6685)\n",
      "tensor(10.6527)\n",
      "tensor(0.4270)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 162.731354\n",
      "Epoch 1110\n",
      "-------------------------------\n",
      "tensor(42.3809)\n",
      "tensor(15.6106)\n",
      "tensor(10.6940)\n",
      "tensor(0.6233)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 162.607361\n",
      "Epoch 1111\n",
      "-------------------------------\n",
      "tensor(52.0874)\n",
      "tensor(17.7404)\n",
      "tensor(11.5787)\n",
      "tensor(0.9213)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 162.461411\n",
      "Epoch 1112\n",
      "-------------------------------\n",
      "tensor(17.2900)\n",
      "tensor(9.0726)\n",
      "tensor(11.4493)\n",
      "tensor(0.9917)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 162.280426\n",
      "Epoch 1113\n",
      "-------------------------------\n",
      "tensor(19.3144)\n",
      "tensor(8.9140)\n",
      "tensor(12.0912)\n",
      "tensor(0.8980)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 162.124069\n",
      "Epoch 1114\n",
      "-------------------------------\n",
      "tensor(27.3027)\n",
      "tensor(11.3984)\n",
      "tensor(9.4905)\n",
      "tensor(0.7502)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 161.974747\n",
      "Epoch 1115\n",
      "-------------------------------\n",
      "tensor(17.2753)\n",
      "tensor(10.0713)\n",
      "tensor(11.8456)\n",
      "tensor(0.5941)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 161.818390\n",
      "Epoch 1116\n",
      "-------------------------------\n",
      "tensor(23.4478)\n",
      "tensor(10.9770)\n",
      "tensor(11.5912)\n",
      "tensor(0.5570)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 161.708847\n",
      "Epoch 1117\n",
      "-------------------------------\n",
      "tensor(18.2167)\n",
      "tensor(10.3698)\n",
      "tensor(9.6367)\n",
      "tensor(0.5868)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 161.601212\n",
      "Epoch 1118\n",
      "-------------------------------\n",
      "tensor(33.1026)\n",
      "tensor(12.6294)\n",
      "tensor(9.3012)\n",
      "tensor(0.6380)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 161.528473\n",
      "Epoch 1119\n",
      "-------------------------------\n",
      "tensor(14.4734)\n",
      "tensor(8.9066)\n",
      "tensor(9.8586)\n",
      "tensor(0.6775)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 161.480865\n",
      "Epoch 1120\n",
      "-------------------------------\n",
      "tensor(21.7576)\n",
      "tensor(10.4900)\n",
      "tensor(10.0232)\n",
      "tensor(0.7062)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 161.457382\n",
      "Epoch 1121\n",
      "-------------------------------\n",
      "tensor(30.3875)\n",
      "tensor(12.2032)\n",
      "tensor(9.8889)\n",
      "tensor(0.7192)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 161.436722\n",
      "Epoch 1122\n",
      "-------------------------------\n",
      "tensor(24.5648)\n",
      "tensor(10.6917)\n",
      "tensor(9.7189)\n",
      "tensor(0.7235)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 161.418121\n",
      "Epoch 1123\n",
      "-------------------------------\n",
      "tensor(16.6645)\n",
      "tensor(9.4783)\n",
      "tensor(9.4852)\n",
      "tensor(0.7216)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 161.390854\n",
      "Epoch 1124\n",
      "-------------------------------\n",
      "tensor(17.9032)\n",
      "tensor(9.7741)\n",
      "tensor(9.3963)\n",
      "tensor(0.7158)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 161.359482\n",
      "Epoch 1125\n",
      "-------------------------------\n",
      "tensor(29.5944)\n",
      "tensor(12.1344)\n",
      "tensor(10.1080)\n",
      "tensor(0.6940)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 161.325531\n",
      "Epoch 1126\n",
      "-------------------------------\n",
      "tensor(37.6990)\n",
      "tensor(13.2663)\n",
      "tensor(10.8836)\n",
      "tensor(0.6548)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 161.256912\n",
      "Epoch 1127\n",
      "-------------------------------\n",
      "tensor(18.2930)\n",
      "tensor(10.1894)\n",
      "tensor(9.5837)\n",
      "tensor(0.6055)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 161.171539\n",
      "Epoch 1128\n",
      "-------------------------------\n",
      "tensor(17.8269)\n",
      "tensor(9.3159)\n",
      "tensor(9.3759)\n",
      "tensor(0.6250)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 161.083954\n",
      "Epoch 1129\n",
      "-------------------------------\n",
      "tensor(18.8184)\n",
      "tensor(9.2175)\n",
      "tensor(9.8455)\n",
      "tensor(0.7657)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 160.963287\n",
      "Epoch 1130\n",
      "-------------------------------\n",
      "tensor(68.3601)\n",
      "tensor(23.1639)\n",
      "tensor(9.6462)\n",
      "tensor(0.8912)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 160.828552\n",
      "Epoch 1131\n",
      "-------------------------------\n",
      "tensor(35.8386)\n",
      "tensor(13.4968)\n",
      "tensor(10.9900)\n",
      "tensor(0.6668)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 160.685303\n",
      "Epoch 1132\n",
      "-------------------------------\n",
      "tensor(48.7207)\n",
      "tensor(17.3207)\n",
      "tensor(11.5745)\n",
      "tensor(0.5307)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 160.577850\n",
      "Epoch 1133\n",
      "-------------------------------\n",
      "tensor(26.5003)\n",
      "tensor(10.9587)\n",
      "tensor(10.3873)\n",
      "tensor(0.5387)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 160.373123\n",
      "Epoch 1134\n",
      "-------------------------------\n",
      "tensor(28.4977)\n",
      "tensor(11.0517)\n",
      "tensor(9.5689)\n",
      "tensor(0.7241)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 160.229691\n",
      "Epoch 1135\n",
      "-------------------------------\n",
      "tensor(20.3191)\n",
      "tensor(9.9006)\n",
      "tensor(10.6139)\n",
      "tensor(0.8685)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 160.130188\n",
      "Epoch 1136\n",
      "-------------------------------\n",
      "tensor(19.0633)\n",
      "tensor(9.4703)\n",
      "tensor(10.1108)\n",
      "tensor(0.8556)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 160.030640\n",
      "Epoch 1137\n",
      "-------------------------------\n",
      "tensor(25.2937)\n",
      "tensor(10.2493)\n",
      "tensor(9.4546)\n",
      "tensor(0.7320)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 159.945297\n",
      "Epoch 1138\n",
      "-------------------------------\n",
      "tensor(14.1781)\n",
      "tensor(8.1965)\n",
      "tensor(9.3711)\n",
      "tensor(0.6097)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 159.880981\n",
      "Epoch 1139\n",
      "-------------------------------\n",
      "tensor(14.2381)\n",
      "tensor(8.2421)\n",
      "tensor(9.3524)\n",
      "tensor(0.5448)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 159.833893\n",
      "Epoch 1140\n",
      "-------------------------------\n",
      "tensor(13.1946)\n",
      "tensor(8.3432)\n",
      "tensor(9.2891)\n",
      "tensor(0.5240)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 159.800568\n",
      "Epoch 1141\n",
      "-------------------------------\n",
      "tensor(12.7597)\n",
      "tensor(8.4223)\n",
      "tensor(9.2775)\n",
      "tensor(0.5301)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 159.777069\n",
      "Epoch 1142\n",
      "-------------------------------\n",
      "tensor(18.2182)\n",
      "tensor(9.3202)\n",
      "tensor(9.3149)\n",
      "tensor(0.5496)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 159.758209\n",
      "Epoch 1143\n",
      "-------------------------------\n",
      "tensor(19.2460)\n",
      "tensor(9.5541)\n",
      "tensor(9.4145)\n",
      "tensor(0.5854)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 159.733459\n",
      "Epoch 1144\n",
      "-------------------------------\n",
      "tensor(13.5110)\n",
      "tensor(8.8645)\n",
      "tensor(9.6128)\n",
      "tensor(0.6485)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 159.693588\n",
      "Epoch 1145\n",
      "-------------------------------\n",
      "tensor(25.3943)\n",
      "tensor(10.9141)\n",
      "tensor(9.9310)\n",
      "tensor(0.7435)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 159.646515\n",
      "Epoch 1146\n",
      "-------------------------------\n",
      "tensor(29.0284)\n",
      "tensor(11.4725)\n",
      "tensor(9.9406)\n",
      "tensor(0.8182)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 159.562820\n",
      "Epoch 1147\n",
      "-------------------------------\n",
      "tensor(35.5147)\n",
      "tensor(12.7074)\n",
      "tensor(9.9909)\n",
      "tensor(0.8205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 159.525925\n",
      "Epoch 1148\n",
      "-------------------------------\n",
      "tensor(34.2382)\n",
      "tensor(12.8116)\n",
      "tensor(11.1156)\n",
      "tensor(0.6487)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 159.472855\n",
      "Epoch 1149\n",
      "-------------------------------\n",
      "tensor(34.0906)\n",
      "tensor(13.2980)\n",
      "tensor(11.0936)\n",
      "tensor(0.4295)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 159.307724\n",
      "Epoch 1150\n",
      "-------------------------------\n",
      "tensor(22.1924)\n",
      "tensor(9.8602)\n",
      "tensor(9.4330)\n",
      "tensor(0.4689)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 159.153229\n",
      "Epoch 1151\n",
      "-------------------------------\n",
      "tensor(62.5517)\n",
      "tensor(20.7901)\n",
      "tensor(10.7470)\n",
      "tensor(0.8125)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 159.061600\n",
      "Epoch 1152\n",
      "-------------------------------\n",
      "tensor(16.4764)\n",
      "tensor(9.0115)\n",
      "tensor(10.0261)\n",
      "tensor(0.9651)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 158.879044\n",
      "Epoch 1153\n",
      "-------------------------------\n",
      "tensor(16.7500)\n",
      "tensor(8.7743)\n",
      "tensor(10.3704)\n",
      "tensor(0.9257)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 158.743500\n",
      "Epoch 1154\n",
      "-------------------------------\n",
      "tensor(28.2458)\n",
      "tensor(11.0399)\n",
      "tensor(9.3451)\n",
      "tensor(0.7791)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 158.647720\n",
      "Epoch 1155\n",
      "-------------------------------\n",
      "tensor(25.1253)\n",
      "tensor(10.7781)\n",
      "tensor(10.2789)\n",
      "tensor(0.6095)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 158.515045\n",
      "Epoch 1156\n",
      "-------------------------------\n",
      "tensor(12.4400)\n",
      "tensor(9.2229)\n",
      "tensor(9.7802)\n",
      "tensor(0.5385)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 158.370224\n",
      "Epoch 1157\n",
      "-------------------------------\n",
      "tensor(18.4397)\n",
      "tensor(10.1839)\n",
      "tensor(9.0879)\n",
      "tensor(0.6082)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 158.283844\n",
      "Epoch 1158\n",
      "-------------------------------\n",
      "tensor(12.8109)\n",
      "tensor(9.3463)\n",
      "tensor(9.0873)\n",
      "tensor(0.6932)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 158.213989\n",
      "Epoch 1159\n",
      "-------------------------------\n",
      "tensor(12.7705)\n",
      "tensor(9.3483)\n",
      "tensor(9.2538)\n",
      "tensor(0.7429)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 158.162369\n",
      "Epoch 1160\n",
      "-------------------------------\n",
      "tensor(12.4323)\n",
      "tensor(9.3823)\n",
      "tensor(9.3126)\n",
      "tensor(0.7570)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 158.126205\n",
      "Epoch 1161\n",
      "-------------------------------\n",
      "tensor(12.0993)\n",
      "tensor(9.4204)\n",
      "tensor(9.3281)\n",
      "tensor(0.7526)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 158.100845\n",
      "Epoch 1162\n",
      "-------------------------------\n",
      "tensor(11.8140)\n",
      "tensor(9.4619)\n",
      "tensor(9.3473)\n",
      "tensor(0.7389)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 158.078979\n",
      "Epoch 1163\n",
      "-------------------------------\n",
      "tensor(11.5885)\n",
      "tensor(9.0646)\n",
      "tensor(9.4057)\n",
      "tensor(0.7114)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 158.052185\n",
      "Epoch 1164\n",
      "-------------------------------\n",
      "tensor(23.1416)\n",
      "tensor(10.6443)\n",
      "tensor(9.6083)\n",
      "tensor(0.6549)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 158.011368\n",
      "Epoch 1165\n",
      "-------------------------------\n",
      "tensor(27.2148)\n",
      "tensor(11.4677)\n",
      "tensor(9.8136)\n",
      "tensor(0.5647)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 157.963165\n",
      "Epoch 1166\n",
      "-------------------------------\n",
      "tensor(38.5370)\n",
      "tensor(13.9436)\n",
      "tensor(9.4508)\n",
      "tensor(0.4737)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 157.935822\n",
      "Epoch 1167\n",
      "-------------------------------\n",
      "tensor(35.5547)\n",
      "tensor(13.5304)\n",
      "tensor(9.2517)\n",
      "tensor(0.4177)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 157.880249\n",
      "Epoch 1168\n",
      "-------------------------------\n",
      "tensor(28.5053)\n",
      "tensor(10.9921)\n",
      "tensor(11.4385)\n",
      "tensor(0.4926)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 157.734558\n",
      "Epoch 1169\n",
      "-------------------------------\n",
      "tensor(25.6882)\n",
      "tensor(10.1424)\n",
      "tensor(10.5214)\n",
      "tensor(0.7699)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 157.588455\n",
      "Epoch 1170\n",
      "-------------------------------\n",
      "tensor(41.5654)\n",
      "tensor(15.7933)\n",
      "tensor(10.3929)\n",
      "tensor(1.0630)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 157.486847\n",
      "Epoch 1171\n",
      "-------------------------------\n",
      "tensor(33.9410)\n",
      "tensor(13.4136)\n",
      "tensor(10.9083)\n",
      "tensor(1.0076)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 157.368347\n",
      "Epoch 1172\n",
      "-------------------------------\n",
      "tensor(66.1896)\n",
      "tensor(22.3688)\n",
      "tensor(9.0028)\n",
      "tensor(0.7112)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 157.217072\n",
      "Epoch 1173\n",
      "-------------------------------\n",
      "tensor(35.3284)\n",
      "tensor(12.5333)\n",
      "tensor(16.8587)\n",
      "tensor(0.2649)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 157.104477\n",
      "Epoch 1174\n",
      "-------------------------------\n",
      "tensor(31.4473)\n",
      "tensor(11.4244)\n",
      "tensor(12.9654)\n",
      "tensor(0.3314)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 156.945648\n",
      "Epoch 1175\n",
      "-------------------------------\n",
      "tensor(27.5365)\n",
      "tensor(10.6653)\n",
      "tensor(9.7033)\n",
      "tensor(0.7271)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 156.840515\n",
      "Epoch 1176\n",
      "-------------------------------\n",
      "tensor(34.4981)\n",
      "tensor(12.0636)\n",
      "tensor(13.6618)\n",
      "tensor(0.9978)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 156.740601\n",
      "Epoch 1177\n",
      "-------------------------------\n",
      "tensor(27.1054)\n",
      "tensor(10.8547)\n",
      "tensor(11.7564)\n",
      "tensor(0.9993)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 156.633957\n",
      "Epoch 1178\n",
      "-------------------------------\n",
      "tensor(12.6296)\n",
      "tensor(8.6560)\n",
      "tensor(9.6528)\n",
      "tensor(0.8625)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 156.579483\n",
      "Epoch 1179\n",
      "-------------------------------\n",
      "tensor(25.9983)\n",
      "tensor(10.3789)\n",
      "tensor(9.7070)\n",
      "tensor(0.7146)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 156.543762\n",
      "Epoch 1180\n",
      "-------------------------------\n",
      "tensor(16.6385)\n",
      "tensor(8.4294)\n",
      "tensor(9.9432)\n",
      "tensor(0.6080)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 156.519669\n",
      "Epoch 1181\n",
      "-------------------------------\n",
      "tensor(20.5807)\n",
      "tensor(8.7612)\n",
      "tensor(9.7875)\n",
      "tensor(0.5470)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 156.502029\n",
      "Epoch 1182\n",
      "-------------------------------\n",
      "tensor(15.0856)\n",
      "tensor(7.8935)\n",
      "tensor(9.4712)\n",
      "tensor(0.5089)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 156.484039\n",
      "Epoch 1183\n",
      "-------------------------------\n",
      "tensor(13.8150)\n",
      "tensor(8.0472)\n",
      "tensor(9.0933)\n",
      "tensor(0.4842)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 156.461761\n",
      "Epoch 1184\n",
      "-------------------------------\n",
      "tensor(19.1287)\n",
      "tensor(9.3725)\n",
      "tensor(9.1530)\n",
      "tensor(0.4837)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 156.429047\n",
      "Epoch 1185\n",
      "-------------------------------\n",
      "tensor(26.9515)\n",
      "tensor(10.9753)\n",
      "tensor(10.4583)\n",
      "tensor(0.5325)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 156.382462\n",
      "Epoch 1186\n",
      "-------------------------------\n",
      "tensor(26.0864)\n",
      "tensor(11.0908)\n",
      "tensor(11.4354)\n",
      "tensor(0.6428)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 156.320496\n",
      "Epoch 1187\n",
      "-------------------------------\n",
      "tensor(32.6835)\n",
      "tensor(11.9878)\n",
      "tensor(10.0622)\n",
      "tensor(0.7776)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 156.249924\n",
      "Epoch 1188\n",
      "-------------------------------\n",
      "tensor(26.1389)\n",
      "tensor(10.0564)\n",
      "tensor(10.5274)\n",
      "tensor(0.8069)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 156.147720\n",
      "Epoch 1189\n",
      "-------------------------------\n",
      "tensor(37.7102)\n",
      "tensor(13.0426)\n",
      "tensor(12.2154)\n",
      "tensor(0.6681)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 156.034119\n",
      "Epoch 1190\n",
      "-------------------------------\n",
      "tensor(26.6494)\n",
      "tensor(10.4353)\n",
      "tensor(10.2014)\n",
      "tensor(0.4831)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 155.930603\n",
      "Epoch 1191\n",
      "-------------------------------\n",
      "tensor(21.9839)\n",
      "tensor(10.1559)\n",
      "tensor(10.1873)\n",
      "tensor(0.5375)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 155.781357\n",
      "Epoch 1192\n",
      "-------------------------------\n",
      "tensor(60.5112)\n",
      "tensor(20.4615)\n",
      "tensor(12.3265)\n",
      "tensor(0.8334)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 155.704193\n",
      "Epoch 1193\n",
      "-------------------------------\n",
      "tensor(17.4646)\n",
      "tensor(9.1722)\n",
      "tensor(10.6685)\n",
      "tensor(0.9064)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 155.511902\n",
      "Epoch 1194\n",
      "-------------------------------\n",
      "tensor(29.3373)\n",
      "tensor(11.1040)\n",
      "tensor(12.4562)\n",
      "tensor(0.8384)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 155.401169\n",
      "Epoch 1195\n",
      "-------------------------------\n",
      "tensor(31.8435)\n",
      "tensor(11.7145)\n",
      "tensor(9.0116)\n",
      "tensor(0.6906)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 155.304672\n",
      "Epoch 1196\n",
      "-------------------------------\n",
      "tensor(25.7232)\n",
      "tensor(10.8460)\n",
      "tensor(9.6058)\n",
      "tensor(0.5485)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 155.194046\n",
      "Epoch 1197\n",
      "-------------------------------\n",
      "tensor(12.4252)\n",
      "tensor(9.1957)\n",
      "tensor(10.6667)\n",
      "tensor(0.5099)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 155.093277\n",
      "Epoch 1198\n",
      "-------------------------------\n",
      "tensor(12.0729)\n",
      "tensor(9.1111)\n",
      "tensor(10.2118)\n",
      "tensor(0.5607)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 155.034439\n",
      "Epoch 1199\n",
      "-------------------------------\n",
      "tensor(11.8865)\n",
      "tensor(9.0742)\n",
      "tensor(9.4894)\n",
      "tensor(0.6225)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 154.988724\n",
      "Epoch 1200\n",
      "-------------------------------\n",
      "tensor(11.7690)\n",
      "tensor(8.9233)\n",
      "tensor(9.0911)\n",
      "tensor(0.6702)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 154.956711\n",
      "Epoch 1201\n",
      "-------------------------------\n",
      "tensor(17.6978)\n",
      "tensor(9.4364)\n",
      "tensor(8.9600)\n",
      "tensor(0.6995)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 154.936081\n",
      "Epoch 1202\n",
      "-------------------------------\n",
      "tensor(17.6398)\n",
      "tensor(9.4496)\n",
      "tensor(8.9160)\n",
      "tensor(0.7134)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 154.917007\n",
      "Epoch 1203\n",
      "-------------------------------\n",
      "tensor(12.5465)\n",
      "tensor(8.6957)\n",
      "tensor(8.9062)\n",
      "tensor(0.7216)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 154.892899\n",
      "Epoch 1204\n",
      "-------------------------------\n",
      "tensor(12.3272)\n",
      "tensor(8.6985)\n",
      "tensor(8.8979)\n",
      "tensor(0.7147)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 154.855911\n",
      "Epoch 1205\n",
      "-------------------------------\n",
      "tensor(12.2090)\n",
      "tensor(9.1725)\n",
      "tensor(8.8547)\n",
      "tensor(0.6864)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 154.797150\n",
      "Epoch 1206\n",
      "-------------------------------\n",
      "tensor(35.0287)\n",
      "tensor(12.9099)\n",
      "tensor(8.8912)\n",
      "tensor(0.6421)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 154.748856\n",
      "Epoch 1207\n",
      "-------------------------------\n",
      "tensor(39.7217)\n",
      "tensor(14.2329)\n",
      "tensor(8.6865)\n",
      "tensor(0.5584)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 154.720901\n",
      "Epoch 1208\n",
      "-------------------------------\n",
      "tensor(35.1609)\n",
      "tensor(13.4410)\n",
      "tensor(9.4395)\n",
      "tensor(0.4701)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 154.621414\n",
      "Epoch 1209\n",
      "-------------------------------\n",
      "tensor(30.8288)\n",
      "tensor(11.4341)\n",
      "tensor(10.7322)\n",
      "tensor(0.5137)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 154.504761\n",
      "Epoch 1210\n",
      "-------------------------------\n",
      "tensor(23.2403)\n",
      "tensor(9.5991)\n",
      "tensor(9.0075)\n",
      "tensor(0.7339)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 154.373520\n",
      "Epoch 1211\n",
      "-------------------------------\n",
      "tensor(19.5659)\n",
      "tensor(10.0581)\n",
      "tensor(10.9688)\n",
      "tensor(0.9451)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 154.238312\n",
      "Epoch 1212\n",
      "-------------------------------\n",
      "tensor(67.7492)\n",
      "tensor(22.8410)\n",
      "tensor(11.5054)\n",
      "tensor(0.9997)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 154.178955\n",
      "Epoch 1213\n",
      "-------------------------------\n",
      "tensor(34.7741)\n",
      "tensor(11.7535)\n",
      "tensor(13.0000)\n",
      "tensor(0.5960)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 154.022293\n",
      "Epoch 1214\n",
      "-------------------------------\n",
      "tensor(45.9265)\n",
      "tensor(15.9822)\n",
      "tensor(14.5842)\n",
      "tensor(0.3656)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 153.967407\n",
      "Epoch 1215\n",
      "-------------------------------\n",
      "tensor(26.3443)\n",
      "tensor(10.8127)\n",
      "tensor(9.4486)\n",
      "tensor(0.4085)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 153.797150\n",
      "Epoch 1216\n",
      "-------------------------------\n",
      "tensor(13.6214)\n",
      "tensor(9.1944)\n",
      "tensor(10.4268)\n",
      "tensor(0.6182)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 153.657043\n",
      "Epoch 1217\n",
      "-------------------------------\n",
      "tensor(28.2968)\n",
      "tensor(11.2553)\n",
      "tensor(12.7475)\n",
      "tensor(0.8097)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 153.608536\n",
      "Epoch 1218\n",
      "-------------------------------\n",
      "tensor(28.7017)\n",
      "tensor(10.9485)\n",
      "tensor(11.2161)\n",
      "tensor(0.8484)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 153.544815\n",
      "Epoch 1219\n",
      "-------------------------------\n",
      "tensor(27.0224)\n",
      "tensor(10.4401)\n",
      "tensor(9.4944)\n",
      "tensor(0.7890)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 153.482620\n",
      "Epoch 1220\n",
      "-------------------------------\n",
      "tensor(13.2738)\n",
      "tensor(7.9194)\n",
      "tensor(9.1792)\n",
      "tensor(0.7131)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 153.448532\n",
      "Epoch 1221\n",
      "-------------------------------\n",
      "tensor(14.3587)\n",
      "tensor(7.7839)\n",
      "tensor(9.3231)\n",
      "tensor(0.6532)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 153.431396\n",
      "Epoch 1222\n",
      "-------------------------------\n",
      "tensor(14.8367)\n",
      "tensor(7.7614)\n",
      "tensor(9.3651)\n",
      "tensor(0.6017)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 153.415710\n",
      "Epoch 1223\n",
      "-------------------------------\n",
      "tensor(14.7153)\n",
      "tensor(7.8128)\n",
      "tensor(9.2045)\n",
      "tensor(0.5426)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 153.394028\n",
      "Epoch 1224\n",
      "-------------------------------\n",
      "tensor(13.6741)\n",
      "tensor(7.9946)\n",
      "tensor(8.8263)\n",
      "tensor(0.4696)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 153.358002\n",
      "Epoch 1225\n",
      "-------------------------------\n",
      "tensor(35.6488)\n",
      "tensor(12.8856)\n",
      "tensor(8.8866)\n",
      "tensor(0.4098)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 153.325745\n",
      "Epoch 1226\n",
      "-------------------------------\n",
      "tensor(35.0866)\n",
      "tensor(12.7563)\n",
      "tensor(9.4394)\n",
      "tensor(0.3881)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 153.286652\n",
      "Epoch 1227\n",
      "-------------------------------\n",
      "tensor(27.1336)\n",
      "tensor(11.1952)\n",
      "tensor(8.9534)\n",
      "tensor(0.4427)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 153.184509\n",
      "Epoch 1228\n",
      "-------------------------------\n",
      "tensor(25.2737)\n",
      "tensor(10.3878)\n",
      "tensor(8.7014)\n",
      "tensor(0.6210)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 153.091080\n",
      "Epoch 1229\n",
      "-------------------------------\n",
      "tensor(22.4071)\n",
      "tensor(9.6249)\n",
      "tensor(9.9395)\n",
      "tensor(0.8573)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 152.989334\n",
      "Epoch 1230\n",
      "-------------------------------\n",
      "tensor(18.8266)\n",
      "tensor(9.5683)\n",
      "tensor(9.4787)\n",
      "tensor(0.9701)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 152.885956\n",
      "Epoch 1231\n",
      "-------------------------------\n",
      "tensor(28.5001)\n",
      "tensor(10.8890)\n",
      "tensor(9.8818)\n",
      "tensor(0.8647)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 152.764847\n",
      "Epoch 1232\n",
      "-------------------------------\n",
      "tensor(61.1711)\n",
      "tensor(20.1773)\n",
      "tensor(9.0385)\n",
      "tensor(0.6642)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 152.636719\n",
      "Epoch 1233\n",
      "-------------------------------\n",
      "tensor(26.1951)\n",
      "tensor(8.5834)\n",
      "tensor(13.6632)\n",
      "tensor(0.4300)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 152.496384\n",
      "Epoch 1234\n",
      "-------------------------------\n",
      "tensor(21.8335)\n",
      "tensor(8.7178)\n",
      "tensor(10.4443)\n",
      "tensor(0.5133)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 152.357178\n",
      "Epoch 1235\n",
      "-------------------------------\n",
      "tensor(25.7015)\n",
      "tensor(10.7504)\n",
      "tensor(10.6412)\n",
      "tensor(0.7234)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 152.227951\n",
      "Epoch 1236\n",
      "-------------------------------\n",
      "tensor(29.9779)\n",
      "tensor(11.6314)\n",
      "tensor(13.5292)\n",
      "tensor(0.8539)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 152.173676\n",
      "Epoch 1237\n",
      "-------------------------------\n",
      "tensor(35.0951)\n",
      "tensor(12.1464)\n",
      "tensor(10.5135)\n",
      "tensor(0.8103)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 152.105087\n",
      "Epoch 1238\n",
      "-------------------------------\n",
      "tensor(29.3229)\n",
      "tensor(11.1427)\n",
      "tensor(8.6892)\n",
      "tensor(0.6562)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 152.012421\n",
      "Epoch 1239\n",
      "-------------------------------\n",
      "tensor(17.6772)\n",
      "tensor(8.3167)\n",
      "tensor(9.7554)\n",
      "tensor(0.5363)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 151.974396\n",
      "Epoch 1240\n",
      "-------------------------------\n",
      "tensor(18.3198)\n",
      "tensor(9.0479)\n",
      "tensor(9.9426)\n",
      "tensor(0.4775)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 151.953506\n",
      "Epoch 1241\n",
      "-------------------------------\n",
      "tensor(17.2082)\n",
      "tensor(9.0392)\n",
      "tensor(9.4287)\n",
      "tensor(0.4567)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 151.935059\n",
      "Epoch 1242\n",
      "-------------------------------\n",
      "tensor(15.0828)\n",
      "tensor(9.0090)\n",
      "tensor(8.7871)\n",
      "tensor(0.4545)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 151.917450\n",
      "Epoch 1243\n",
      "-------------------------------\n",
      "tensor(12.9617)\n",
      "tensor(9.0616)\n",
      "tensor(8.3706)\n",
      "tensor(0.4731)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 151.895218\n",
      "Epoch 1244\n",
      "-------------------------------\n",
      "tensor(20.9757)\n",
      "tensor(10.4397)\n",
      "tensor(9.2651)\n",
      "tensor(0.5319)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 151.870026\n",
      "Epoch 1245\n",
      "-------------------------------\n",
      "tensor(37.2804)\n",
      "tensor(12.7626)\n",
      "tensor(11.7670)\n",
      "tensor(0.6529)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 151.865295\n",
      "Epoch 1246\n",
      "-------------------------------\n",
      "tensor(43.9112)\n",
      "tensor(13.6785)\n",
      "tensor(12.0339)\n",
      "tensor(0.7847)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 151.811707\n",
      "Epoch 1247\n",
      "-------------------------------\n",
      "tensor(17.0428)\n",
      "tensor(8.9809)\n",
      "tensor(8.9399)\n",
      "tensor(0.8389)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 151.703644\n",
      "Epoch 1248\n",
      "-------------------------------\n",
      "tensor(24.3602)\n",
      "tensor(8.9580)\n",
      "tensor(12.2774)\n",
      "tensor(0.7957)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 151.645706\n",
      "Epoch 1249\n",
      "-------------------------------\n",
      "tensor(29.1520)\n",
      "tensor(10.4686)\n",
      "tensor(10.4170)\n",
      "tensor(0.6373)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 151.547256\n",
      "Epoch 1250\n",
      "-------------------------------\n",
      "tensor(39.5296)\n",
      "tensor(14.1170)\n",
      "tensor(9.4148)\n",
      "tensor(0.5175)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 151.431046\n",
      "Epoch 1251\n",
      "-------------------------------\n",
      "tensor(36.5783)\n",
      "tensor(13.0068)\n",
      "tensor(10.8583)\n",
      "tensor(0.5926)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 151.312393\n",
      "Epoch 1252\n",
      "-------------------------------\n",
      "tensor(47.1073)\n",
      "tensor(16.8993)\n",
      "tensor(8.7004)\n",
      "tensor(0.7726)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 151.160339\n",
      "Epoch 1253\n",
      "-------------------------------\n",
      "tensor(50.2148)\n",
      "tensor(17.5684)\n",
      "tensor(13.5929)\n",
      "tensor(0.6906)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 151.097778\n",
      "Epoch 1254\n",
      "-------------------------------\n",
      "tensor(33.2555)\n",
      "tensor(12.1266)\n",
      "tensor(12.9218)\n",
      "tensor(0.4334)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 150.924561\n",
      "Epoch 1255\n",
      "-------------------------------\n",
      "tensor(30.1209)\n",
      "tensor(11.4115)\n",
      "tensor(8.5374)\n",
      "tensor(0.3743)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 150.831711\n",
      "Epoch 1256\n",
      "-------------------------------\n",
      "tensor(20.5964)\n",
      "tensor(9.5606)\n",
      "tensor(10.4909)\n",
      "tensor(0.4979)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 150.752350\n",
      "Epoch 1257\n",
      "-------------------------------\n",
      "tensor(20.4027)\n",
      "tensor(9.4941)\n",
      "tensor(10.7327)\n",
      "tensor(0.6639)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 150.684402\n",
      "Epoch 1258\n",
      "-------------------------------\n",
      "tensor(18.0298)\n",
      "tensor(8.9137)\n",
      "tensor(9.4232)\n",
      "tensor(0.7574)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 150.631088\n",
      "Epoch 1259\n",
      "-------------------------------\n",
      "tensor(24.3823)\n",
      "tensor(9.6240)\n",
      "tensor(8.9231)\n",
      "tensor(0.7713)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 150.598587\n",
      "Epoch 1260\n",
      "-------------------------------\n",
      "tensor(24.4610)\n",
      "tensor(9.4841)\n",
      "tensor(8.9702)\n",
      "tensor(0.7407)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 150.569824\n",
      "Epoch 1261\n",
      "-------------------------------\n",
      "tensor(14.1064)\n",
      "tensor(7.4865)\n",
      "tensor(9.0484)\n",
      "tensor(0.7021)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 150.551483\n",
      "Epoch 1262\n",
      "-------------------------------\n",
      "tensor(24.8482)\n",
      "tensor(9.4927)\n",
      "tensor(8.9893)\n",
      "tensor(0.6640)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 150.541763\n",
      "Epoch 1263\n",
      "-------------------------------\n",
      "tensor(24.7862)\n",
      "tensor(9.5424)\n",
      "tensor(8.7939)\n",
      "tensor(0.6099)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 150.523560\n",
      "Epoch 1264\n",
      "-------------------------------\n",
      "tensor(13.7410)\n",
      "tensor(7.5397)\n",
      "tensor(8.4870)\n",
      "tensor(0.5303)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 150.493896\n",
      "Epoch 1265\n",
      "-------------------------------\n",
      "tensor(21.0882)\n",
      "tensor(9.7819)\n",
      "tensor(8.4151)\n",
      "tensor(0.4446)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 150.456009\n",
      "Epoch 1266\n",
      "-------------------------------\n",
      "tensor(27.0025)\n",
      "tensor(11.0657)\n",
      "tensor(9.1561)\n",
      "tensor(0.4023)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 150.410873\n",
      "Epoch 1267\n",
      "-------------------------------\n",
      "tensor(34.0062)\n",
      "tensor(12.3911)\n",
      "tensor(9.2123)\n",
      "tensor(0.4563)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 150.369186\n",
      "Epoch 1268\n",
      "-------------------------------\n",
      "tensor(21.4048)\n",
      "tensor(9.5398)\n",
      "tensor(8.2931)\n",
      "tensor(0.5874)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 150.275665\n",
      "Epoch 1269\n",
      "-------------------------------\n",
      "tensor(29.9383)\n",
      "tensor(10.6399)\n",
      "tensor(9.3184)\n",
      "tensor(0.7404)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 150.163162\n",
      "Epoch 1270\n",
      "-------------------------------\n",
      "tensor(15.1013)\n",
      "tensor(7.9317)\n",
      "tensor(8.9641)\n",
      "tensor(0.7704)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 150.039978\n",
      "Epoch 1271\n",
      "-------------------------------\n",
      "tensor(26.5126)\n",
      "tensor(10.4457)\n",
      "tensor(9.1933)\n",
      "tensor(0.7441)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 149.937851\n",
      "Epoch 1272\n",
      "-------------------------------\n",
      "tensor(21.3096)\n",
      "tensor(9.7862)\n",
      "tensor(9.4763)\n",
      "tensor(0.6949)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 149.812302\n",
      "Epoch 1273\n",
      "-------------------------------\n",
      "tensor(56.3243)\n",
      "tensor(19.2859)\n",
      "tensor(8.1384)\n",
      "tensor(0.6797)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 149.740891\n",
      "Epoch 1274\n",
      "-------------------------------\n",
      "tensor(29.8422)\n",
      "tensor(10.3824)\n",
      "tensor(12.8417)\n",
      "tensor(0.5429)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 149.600586\n",
      "Epoch 1275\n",
      "-------------------------------\n",
      "tensor(29.0365)\n",
      "tensor(11.3892)\n",
      "tensor(10.2068)\n",
      "tensor(0.5024)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 149.488007\n",
      "Epoch 1276\n",
      "-------------------------------\n",
      "tensor(30.7753)\n",
      "tensor(11.9613)\n",
      "tensor(8.3752)\n",
      "tensor(0.5589)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 149.417755\n",
      "Epoch 1277\n",
      "-------------------------------\n",
      "tensor(30.8605)\n",
      "tensor(11.7763)\n",
      "tensor(10.2424)\n",
      "tensor(0.6132)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 149.337036\n",
      "Epoch 1278\n",
      "-------------------------------\n",
      "tensor(16.0848)\n",
      "tensor(9.5815)\n",
      "tensor(9.6652)\n",
      "tensor(0.6345)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 149.279358\n",
      "Epoch 1279\n",
      "-------------------------------\n",
      "tensor(11.8377)\n",
      "tensor(8.5130)\n",
      "tensor(8.7584)\n",
      "tensor(0.6396)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 149.247070\n",
      "Epoch 1280\n",
      "-------------------------------\n",
      "tensor(12.1644)\n",
      "tensor(8.3522)\n",
      "tensor(8.3406)\n",
      "tensor(0.6350)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 149.224487\n",
      "Epoch 1281\n",
      "-------------------------------\n",
      "tensor(12.4748)\n",
      "tensor(8.2754)\n",
      "tensor(8.1982)\n",
      "tensor(0.6275)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 149.207901\n",
      "Epoch 1282\n",
      "-------------------------------\n",
      "tensor(18.1717)\n",
      "tensor(8.9894)\n",
      "tensor(8.1425)\n",
      "tensor(0.6193)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 149.193893\n",
      "Epoch 1283\n",
      "-------------------------------\n",
      "tensor(23.8671)\n",
      "tensor(9.9409)\n",
      "tensor(8.1049)\n",
      "tensor(0.6084)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 149.181015\n",
      "Epoch 1284\n",
      "-------------------------------\n",
      "tensor(19.9719)\n",
      "tensor(9.2845)\n",
      "tensor(8.0616)\n",
      "tensor(0.5885)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 149.156357\n",
      "Epoch 1285\n",
      "-------------------------------\n",
      "tensor(19.9522)\n",
      "tensor(9.2895)\n",
      "tensor(8.0330)\n",
      "tensor(0.5651)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 149.107529\n",
      "Epoch 1286\n",
      "-------------------------------\n",
      "tensor(28.3502)\n",
      "tensor(11.3270)\n",
      "tensor(8.0594)\n",
      "tensor(0.5631)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 149.043320\n",
      "Epoch 1287\n",
      "-------------------------------\n",
      "tensor(26.7740)\n",
      "tensor(10.9038)\n",
      "tensor(8.0671)\n",
      "tensor(0.6062)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 149.006821\n",
      "Epoch 1288\n",
      "-------------------------------\n",
      "tensor(36.4471)\n",
      "tensor(12.4379)\n",
      "tensor(8.2170)\n",
      "tensor(0.6615)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 148.950134\n",
      "Epoch 1289\n",
      "-------------------------------\n",
      "tensor(27.5799)\n",
      "tensor(10.2500)\n",
      "tensor(8.8553)\n",
      "tensor(0.6560)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 148.831543\n",
      "Epoch 1290\n",
      "-------------------------------\n",
      "tensor(19.4929)\n",
      "tensor(8.5650)\n",
      "tensor(8.3660)\n",
      "tensor(0.6523)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 148.724960\n",
      "Epoch 1291\n",
      "-------------------------------\n",
      "tensor(18.5506)\n",
      "tensor(9.3073)\n",
      "tensor(8.8651)\n",
      "tensor(0.6909)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 148.619629\n",
      "Epoch 1292\n",
      "-------------------------------\n",
      "tensor(46.6256)\n",
      "tensor(16.3041)\n",
      "tensor(9.9731)\n",
      "tensor(0.7070)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 148.538986\n",
      "Epoch 1293\n",
      "-------------------------------\n",
      "tensor(22.8789)\n",
      "tensor(9.0669)\n",
      "tensor(8.5400)\n",
      "tensor(0.5866)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 148.395737\n",
      "Epoch 1294\n",
      "-------------------------------\n",
      "tensor(52.5951)\n",
      "tensor(17.7779)\n",
      "tensor(9.7228)\n",
      "tensor(0.5175)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 148.339035\n",
      "Epoch 1295\n",
      "-------------------------------\n",
      "tensor(37.2561)\n",
      "tensor(12.9764)\n",
      "tensor(10.6560)\n",
      "tensor(0.4012)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 148.217392\n",
      "Epoch 1296\n",
      "-------------------------------\n",
      "tensor(39.9398)\n",
      "tensor(13.8904)\n",
      "tensor(8.6518)\n",
      "tensor(0.4116)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 148.132172\n",
      "Epoch 1297\n",
      "-------------------------------\n",
      "tensor(16.2848)\n",
      "tensor(8.1978)\n",
      "tensor(8.0128)\n",
      "tensor(0.5032)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 148.039032\n",
      "Epoch 1298\n",
      "-------------------------------\n",
      "tensor(16.5122)\n",
      "tensor(9.0111)\n",
      "tensor(8.7589)\n",
      "tensor(0.6042)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 147.989487\n",
      "Epoch 1299\n",
      "-------------------------------\n",
      "tensor(16.7565)\n",
      "tensor(9.1189)\n",
      "tensor(9.2270)\n",
      "tensor(0.6646)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 147.952972\n",
      "Epoch 1300\n",
      "-------------------------------\n",
      "tensor(12.9149)\n",
      "tensor(8.6749)\n",
      "tensor(9.1432)\n",
      "tensor(0.6833)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 147.923569\n",
      "Epoch 1301\n",
      "-------------------------------\n",
      "tensor(12.7130)\n",
      "tensor(8.5887)\n",
      "tensor(8.9087)\n",
      "tensor(0.6811)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 147.903488\n",
      "Epoch 1302\n",
      "-------------------------------\n",
      "tensor(12.2556)\n",
      "tensor(7.9869)\n",
      "tensor(8.6538)\n",
      "tensor(0.6681)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 147.885757\n",
      "Epoch 1303\n",
      "-------------------------------\n",
      "tensor(12.2585)\n",
      "tensor(7.8287)\n",
      "tensor(8.3718)\n",
      "tensor(0.6412)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 147.863800\n",
      "Epoch 1304\n",
      "-------------------------------\n",
      "tensor(26.0617)\n",
      "tensor(9.9759)\n",
      "tensor(8.1478)\n",
      "tensor(0.5923)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 147.839264\n",
      "Epoch 1305\n",
      "-------------------------------\n",
      "tensor(31.2698)\n",
      "tensor(11.2132)\n",
      "tensor(8.3225)\n",
      "tensor(0.5136)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 147.813080\n",
      "Epoch 1306\n",
      "-------------------------------\n",
      "tensor(39.2401)\n",
      "tensor(13.6525)\n",
      "tensor(9.3230)\n",
      "tensor(0.4242)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 147.795074\n",
      "Epoch 1307\n",
      "-------------------------------\n",
      "tensor(34.5899)\n",
      "tensor(12.2877)\n",
      "tensor(10.6718)\n",
      "tensor(0.3589)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 147.746429\n",
      "Epoch 1308\n",
      "-------------------------------\n",
      "tensor(30.6754)\n",
      "tensor(11.0607)\n",
      "tensor(9.0472)\n",
      "tensor(0.3900)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 147.637329\n",
      "Epoch 1309\n",
      "-------------------------------\n",
      "tensor(13.5007)\n",
      "tensor(8.3626)\n",
      "tensor(8.3188)\n",
      "tensor(0.6017)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 147.554947\n",
      "Epoch 1310\n",
      "-------------------------------\n",
      "tensor(15.8818)\n",
      "tensor(9.7319)\n",
      "tensor(11.5567)\n",
      "tensor(0.9145)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 147.483475\n",
      "Epoch 1311\n",
      "-------------------------------\n",
      "tensor(14.6758)\n",
      "tensor(9.6218)\n",
      "tensor(10.1347)\n",
      "tensor(1.0212)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 147.377350\n",
      "Epoch 1312\n",
      "-------------------------------\n",
      "tensor(41.4049)\n",
      "tensor(14.8216)\n",
      "tensor(8.5359)\n",
      "tensor(0.7643)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 147.267044\n",
      "Epoch 1313\n",
      "-------------------------------\n",
      "tensor(44.0532)\n",
      "tensor(15.0385)\n",
      "tensor(10.8041)\n",
      "tensor(0.3191)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 147.179535\n",
      "Epoch 1314\n",
      "-------------------------------\n",
      "tensor(28.4548)\n",
      "tensor(11.2285)\n",
      "tensor(9.8312)\n",
      "tensor(0.1745)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 147.056778\n",
      "Epoch 1315\n",
      "-------------------------------\n",
      "tensor(27.8806)\n",
      "tensor(10.9667)\n",
      "tensor(8.2040)\n",
      "tensor(0.4278)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 146.952744\n",
      "Epoch 1316\n",
      "-------------------------------\n",
      "tensor(18.6410)\n",
      "tensor(9.4744)\n",
      "tensor(9.8805)\n",
      "tensor(0.7273)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 146.836029\n",
      "Epoch 1317\n",
      "-------------------------------\n",
      "tensor(16.1538)\n",
      "tensor(9.2494)\n",
      "tensor(9.9263)\n",
      "tensor(0.8598)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 146.765167\n",
      "Epoch 1318\n",
      "-------------------------------\n",
      "tensor(20.0128)\n",
      "tensor(9.3329)\n",
      "tensor(9.0386)\n",
      "tensor(0.8156)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 146.717636\n",
      "Epoch 1319\n",
      "-------------------------------\n",
      "tensor(20.0731)\n",
      "tensor(9.1024)\n",
      "tensor(8.2994)\n",
      "tensor(0.6917)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 146.680038\n",
      "Epoch 1320\n",
      "-------------------------------\n",
      "tensor(19.9449)\n",
      "tensor(9.0698)\n",
      "tensor(7.9421)\n",
      "tensor(0.5808)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 146.648392\n",
      "Epoch 1321\n",
      "-------------------------------\n",
      "tensor(24.5131)\n",
      "tensor(10.0046)\n",
      "tensor(7.8202)\n",
      "tensor(0.5111)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 146.632111\n",
      "Epoch 1322\n",
      "-------------------------------\n",
      "tensor(24.5075)\n",
      "tensor(10.0721)\n",
      "tensor(7.7818)\n",
      "tensor(0.4639)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 146.621582\n",
      "Epoch 1323\n",
      "-------------------------------\n",
      "tensor(24.0344)\n",
      "tensor(10.5248)\n",
      "tensor(7.7761)\n",
      "tensor(0.4210)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 146.602036\n",
      "Epoch 1324\n",
      "-------------------------------\n",
      "tensor(16.4172)\n",
      "tensor(9.2614)\n",
      "tensor(7.8155)\n",
      "tensor(0.3850)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 146.578079\n",
      "Epoch 1325\n",
      "-------------------------------\n",
      "tensor(27.5878)\n",
      "tensor(11.3294)\n",
      "tensor(7.9152)\n",
      "tensor(0.4006)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 146.562851\n",
      "Epoch 1326\n",
      "-------------------------------\n",
      "tensor(31.3959)\n",
      "tensor(11.7813)\n",
      "tensor(7.9219)\n",
      "tensor(0.4960)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 146.534134\n",
      "Epoch 1327\n",
      "-------------------------------\n",
      "tensor(29.6613)\n",
      "tensor(10.6962)\n",
      "tensor(7.8297)\n",
      "tensor(0.6371)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 146.456726\n",
      "Epoch 1328\n",
      "-------------------------------\n",
      "tensor(24.6170)\n",
      "tensor(9.5371)\n",
      "tensor(8.4143)\n",
      "tensor(0.7677)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 146.384521\n",
      "Epoch 1329\n",
      "-------------------------------\n",
      "tensor(37.8295)\n",
      "tensor(13.9742)\n",
      "tensor(8.5540)\n",
      "tensor(0.8130)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 146.307281\n",
      "Epoch 1330\n",
      "-------------------------------\n",
      "tensor(29.1910)\n",
      "tensor(11.0531)\n",
      "tensor(7.9342)\n",
      "tensor(0.6725)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 146.214508\n",
      "Epoch 1331\n",
      "-------------------------------\n",
      "tensor(54.3496)\n",
      "tensor(17.6343)\n",
      "tensor(7.8511)\n",
      "tensor(0.5612)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 146.180191\n",
      "Epoch 1332\n",
      "-------------------------------\n",
      "tensor(27.2469)\n",
      "tensor(9.5770)\n",
      "tensor(9.9262)\n",
      "tensor(0.3997)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 146.049942\n",
      "Epoch 1333\n",
      "-------------------------------\n",
      "tensor(44.6595)\n",
      "tensor(15.5862)\n",
      "tensor(8.5019)\n",
      "tensor(0.4894)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 145.924194\n",
      "Epoch 1334\n",
      "-------------------------------\n",
      "tensor(19.2726)\n",
      "tensor(8.8568)\n",
      "tensor(7.8394)\n",
      "tensor(0.5938)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 145.818420\n",
      "Epoch 1335\n",
      "-------------------------------\n",
      "tensor(18.1189)\n",
      "tensor(9.0479)\n",
      "tensor(8.6787)\n",
      "tensor(0.6602)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 145.721802\n",
      "Epoch 1336\n",
      "-------------------------------\n",
      "tensor(21.4992)\n",
      "tensor(9.6039)\n",
      "tensor(8.8916)\n",
      "tensor(0.6269)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 145.637222\n",
      "Epoch 1337\n",
      "-------------------------------\n",
      "tensor(28.8929)\n",
      "tensor(10.5012)\n",
      "tensor(8.0480)\n",
      "tensor(0.5344)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 145.558624\n",
      "Epoch 1338\n",
      "-------------------------------\n",
      "tensor(24.1541)\n",
      "tensor(9.4845)\n",
      "tensor(7.9319)\n",
      "tensor(0.4402)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 145.514816\n",
      "Epoch 1339\n",
      "-------------------------------\n",
      "tensor(17.3504)\n",
      "tensor(7.8428)\n",
      "tensor(8.2197)\n",
      "tensor(0.3951)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 145.481277\n",
      "Epoch 1340\n",
      "-------------------------------\n",
      "tensor(20.7966)\n",
      "tensor(8.6556)\n",
      "tensor(8.1419)\n",
      "tensor(0.3898)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 145.457932\n",
      "Epoch 1341\n",
      "-------------------------------\n",
      "tensor(21.1637)\n",
      "tensor(8.7980)\n",
      "tensor(7.9733)\n",
      "tensor(0.4021)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 145.444916\n",
      "Epoch 1342\n",
      "-------------------------------\n",
      "tensor(28.8358)\n",
      "tensor(10.6353)\n",
      "tensor(7.8328)\n",
      "tensor(0.4228)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 145.435776\n",
      "Epoch 1343\n",
      "-------------------------------\n",
      "tensor(30.2138)\n",
      "tensor(10.9153)\n",
      "tensor(7.7614)\n",
      "tensor(0.4542)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 145.421097\n",
      "Epoch 1344\n",
      "-------------------------------\n",
      "tensor(14.2071)\n",
      "tensor(7.6138)\n",
      "tensor(7.7895)\n",
      "tensor(0.5051)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 145.392731\n",
      "Epoch 1345\n",
      "-------------------------------\n",
      "tensor(24.1285)\n",
      "tensor(9.4801)\n",
      "tensor(8.0976)\n",
      "tensor(0.5813)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 145.371399\n",
      "Epoch 1346\n",
      "-------------------------------\n",
      "tensor(24.2590)\n",
      "tensor(9.4989)\n",
      "tensor(8.3721)\n",
      "tensor(0.6568)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 145.320770\n",
      "Epoch 1347\n",
      "-------------------------------\n",
      "tensor(22.2685)\n",
      "tensor(9.4324)\n",
      "tensor(8.0488)\n",
      "tensor(0.6996)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 145.278381\n",
      "Epoch 1348\n",
      "-------------------------------\n",
      "tensor(32.0864)\n",
      "tensor(11.3408)\n",
      "tensor(7.8822)\n",
      "tensor(0.6470)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 145.229385\n",
      "Epoch 1349\n",
      "-------------------------------\n",
      "tensor(21.9177)\n",
      "tensor(9.0190)\n",
      "tensor(8.4752)\n",
      "tensor(0.5089)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 145.112320\n",
      "Epoch 1350\n",
      "-------------------------------\n",
      "tensor(21.9055)\n",
      "tensor(8.7835)\n",
      "tensor(7.6809)\n",
      "tensor(0.5469)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 145.026855\n",
      "Epoch 1351\n",
      "-------------------------------\n",
      "tensor(16.5823)\n",
      "tensor(8.7839)\n",
      "tensor(8.7730)\n",
      "tensor(0.7514)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 144.923920\n",
      "Epoch 1352\n",
      "-------------------------------\n",
      "tensor(46.3901)\n",
      "tensor(15.8823)\n",
      "tensor(9.4439)\n",
      "tensor(0.8992)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 144.858368\n",
      "Epoch 1353\n",
      "-------------------------------\n",
      "tensor(26.6699)\n",
      "tensor(9.7214)\n",
      "tensor(9.7469)\n",
      "tensor(0.6732)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 144.734894\n",
      "Epoch 1354\n",
      "-------------------------------\n",
      "tensor(45.5238)\n",
      "tensor(16.1223)\n",
      "tensor(9.4727)\n",
      "tensor(0.3700)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 144.662003\n",
      "Epoch 1355\n",
      "-------------------------------\n",
      "tensor(20.2485)\n",
      "tensor(9.9360)\n",
      "tensor(8.3539)\n",
      "tensor(0.2042)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 144.542725\n",
      "Epoch 1356\n",
      "-------------------------------\n",
      "tensor(25.5940)\n",
      "tensor(10.5829)\n",
      "tensor(8.6657)\n",
      "tensor(0.3458)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 144.484222\n",
      "Epoch 1357\n",
      "-------------------------------\n",
      "tensor(22.8229)\n",
      "tensor(9.8787)\n",
      "tensor(9.6196)\n",
      "tensor(0.5637)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 144.427063\n",
      "Epoch 1358\n",
      "-------------------------------\n",
      "tensor(17.4665)\n",
      "tensor(9.0613)\n",
      "tensor(9.0342)\n",
      "tensor(0.6987)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 144.375671\n",
      "Epoch 1359\n",
      "-------------------------------\n",
      "tensor(12.1598)\n",
      "tensor(8.3671)\n",
      "tensor(8.3873)\n",
      "tensor(0.7308)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 144.339996\n",
      "Epoch 1360\n",
      "-------------------------------\n",
      "tensor(16.8097)\n",
      "tensor(8.6948)\n",
      "tensor(8.0725)\n",
      "tensor(0.7152)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 144.316879\n",
      "Epoch 1361\n",
      "-------------------------------\n",
      "tensor(12.7641)\n",
      "tensor(8.0559)\n",
      "tensor(7.8922)\n",
      "tensor(0.6838)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 144.300003\n",
      "Epoch 1362\n",
      "-------------------------------\n",
      "tensor(24.2515)\n",
      "tensor(9.7453)\n",
      "tensor(7.7531)\n",
      "tensor(0.6474)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 144.287857\n",
      "Epoch 1363\n",
      "-------------------------------\n",
      "tensor(23.6270)\n",
      "tensor(9.6254)\n",
      "tensor(7.5864)\n",
      "tensor(0.5923)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 144.268799\n",
      "Epoch 1364\n",
      "-------------------------------\n",
      "tensor(13.5500)\n",
      "tensor(8.5273)\n",
      "tensor(7.4370)\n",
      "tensor(0.5082)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 144.242752\n",
      "Epoch 1365\n",
      "-------------------------------\n",
      "tensor(18.7692)\n",
      "tensor(9.4706)\n",
      "tensor(7.4408)\n",
      "tensor(0.4093)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 144.203613\n",
      "Epoch 1366\n",
      "-------------------------------\n",
      "tensor(31.8371)\n",
      "tensor(12.1755)\n",
      "tensor(7.6756)\n",
      "tensor(0.3607)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 144.188538\n",
      "Epoch 1367\n",
      "-------------------------------\n",
      "tensor(33.3865)\n",
      "tensor(11.9227)\n",
      "tensor(7.5173)\n",
      "tensor(0.4000)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 144.169617\n",
      "Epoch 1368\n",
      "-------------------------------\n",
      "tensor(32.0752)\n",
      "tensor(11.2362)\n",
      "tensor(7.5932)\n",
      "tensor(0.5219)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 144.084198\n",
      "Epoch 1369\n",
      "-------------------------------\n",
      "tensor(27.4713)\n",
      "tensor(9.8230)\n",
      "tensor(8.4833)\n",
      "tensor(0.6665)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 143.961395\n",
      "Epoch 1370\n",
      "-------------------------------\n",
      "tensor(14.4504)\n",
      "tensor(7.7467)\n",
      "tensor(7.9152)\n",
      "tensor(0.7551)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 143.884445\n",
      "Epoch 1371\n",
      "-------------------------------\n",
      "tensor(15.5219)\n",
      "tensor(8.9687)\n",
      "tensor(9.2956)\n",
      "tensor(0.7587)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 143.802200\n",
      "Epoch 1372\n",
      "-------------------------------\n",
      "tensor(14.4109)\n",
      "tensor(8.7889)\n",
      "tensor(9.3497)\n",
      "tensor(0.7034)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 143.693314\n",
      "Epoch 1373\n",
      "-------------------------------\n",
      "tensor(26.4381)\n",
      "tensor(9.8170)\n",
      "tensor(7.5266)\n",
      "tensor(0.6374)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 143.594788\n",
      "Epoch 1374\n",
      "-------------------------------\n",
      "tensor(48.1833)\n",
      "tensor(16.2943)\n",
      "tensor(8.6186)\n",
      "tensor(0.5040)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 143.499268\n",
      "Epoch 1375\n",
      "-------------------------------\n",
      "tensor(25.6440)\n",
      "tensor(9.2194)\n",
      "tensor(10.5122)\n",
      "tensor(0.2957)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 143.403412\n",
      "Epoch 1376\n",
      "-------------------------------\n",
      "tensor(14.3186)\n",
      "tensor(7.7405)\n",
      "tensor(7.6421)\n",
      "tensor(0.2830)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 143.308792\n",
      "Epoch 1377\n",
      "-------------------------------\n",
      "tensor(54.6368)\n",
      "tensor(17.4518)\n",
      "tensor(9.2674)\n",
      "tensor(0.4197)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 143.267807\n",
      "Epoch 1378\n",
      "-------------------------------\n",
      "tensor(28.5287)\n",
      "tensor(10.5781)\n",
      "tensor(8.1766)\n",
      "tensor(0.5025)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 143.214050\n",
      "Epoch 1379\n",
      "-------------------------------\n",
      "tensor(12.9989)\n",
      "tensor(8.1863)\n",
      "tensor(7.5415)\n",
      "tensor(0.5458)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 143.172913\n",
      "Epoch 1380\n",
      "-------------------------------\n",
      "tensor(27.0677)\n",
      "tensor(10.9442)\n",
      "tensor(7.6755)\n",
      "tensor(0.5647)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 143.155701\n",
      "Epoch 1381\n",
      "-------------------------------\n",
      "tensor(18.8338)\n",
      "tensor(8.7773)\n",
      "tensor(7.8010)\n",
      "tensor(0.5633)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 143.142975\n",
      "Epoch 1382\n",
      "-------------------------------\n",
      "tensor(23.7246)\n",
      "tensor(9.1454)\n",
      "tensor(7.7935)\n",
      "tensor(0.5546)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 143.132751\n",
      "Epoch 1383\n",
      "-------------------------------\n",
      "tensor(43.6193)\n",
      "tensor(14.7578)\n",
      "tensor(7.6652)\n",
      "tensor(0.5346)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 143.117950\n",
      "Epoch 1384\n",
      "-------------------------------\n",
      "tensor(14.0873)\n",
      "tensor(7.2145)\n",
      "tensor(7.5854)\n",
      "tensor(0.4828)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 143.091217\n",
      "Epoch 1385\n",
      "-------------------------------\n",
      "tensor(31.9724)\n",
      "tensor(11.2820)\n",
      "tensor(7.3860)\n",
      "tensor(0.4291)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 143.078979\n",
      "Epoch 1386\n",
      "-------------------------------\n",
      "tensor(33.0174)\n",
      "tensor(11.5184)\n",
      "tensor(7.3833)\n",
      "tensor(0.3963)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 143.062103\n",
      "Epoch 1387\n",
      "-------------------------------\n",
      "tensor(22.4075)\n",
      "tensor(9.3856)\n",
      "tensor(7.3959)\n",
      "tensor(0.4109)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 143.008987\n",
      "Epoch 1388\n",
      "-------------------------------\n",
      "tensor(21.1378)\n",
      "tensor(9.0017)\n",
      "tensor(7.3539)\n",
      "tensor(0.5371)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 142.939758\n",
      "Epoch 1389\n",
      "-------------------------------\n",
      "tensor(14.5260)\n",
      "tensor(7.6560)\n",
      "tensor(7.7768)\n",
      "tensor(0.7410)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 142.853775\n",
      "Epoch 1390\n",
      "-------------------------------\n",
      "tensor(31.0797)\n",
      "tensor(10.6087)\n",
      "tensor(8.3800)\n",
      "tensor(0.8313)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 142.781662\n",
      "Epoch 1391\n",
      "-------------------------------\n",
      "tensor(14.8474)\n",
      "tensor(7.3007)\n",
      "tensor(7.7513)\n",
      "tensor(0.6906)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 142.680420\n",
      "Epoch 1392\n",
      "-------------------------------\n",
      "tensor(23.7857)\n",
      "tensor(9.1054)\n",
      "tensor(7.2771)\n",
      "tensor(0.4749)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 142.576874\n",
      "Epoch 1393\n",
      "-------------------------------\n",
      "tensor(18.2272)\n",
      "tensor(8.3345)\n",
      "tensor(7.3694)\n",
      "tensor(0.4352)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 142.481293\n",
      "Epoch 1394\n",
      "-------------------------------\n",
      "tensor(43.8394)\n",
      "tensor(15.1443)\n",
      "tensor(7.6130)\n",
      "tensor(0.5417)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 142.412308\n",
      "Epoch 1395\n",
      "-------------------------------\n",
      "tensor(21.1142)\n",
      "tensor(9.2489)\n",
      "tensor(7.9027)\n",
      "tensor(0.5192)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 142.307053\n",
      "Epoch 1396\n",
      "-------------------------------\n",
      "tensor(45.3994)\n",
      "tensor(15.7989)\n",
      "tensor(7.7207)\n",
      "tensor(0.4505)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 142.240875\n",
      "Epoch 1397\n",
      "-------------------------------\n",
      "tensor(18.1126)\n",
      "tensor(8.0297)\n",
      "tensor(7.9613)\n",
      "tensor(0.3487)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 142.170593\n",
      "Epoch 1398\n",
      "-------------------------------\n",
      "tensor(22.1038)\n",
      "tensor(9.2541)\n",
      "tensor(7.3127)\n",
      "tensor(0.3478)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 142.134018\n",
      "Epoch 1399\n",
      "-------------------------------\n",
      "tensor(30.6253)\n",
      "tensor(11.1847)\n",
      "tensor(7.2922)\n",
      "tensor(0.3963)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 142.114822\n",
      "Epoch 1400\n",
      "-------------------------------\n",
      "tensor(36.6252)\n",
      "tensor(12.4401)\n",
      "tensor(7.4045)\n",
      "tensor(0.4423)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 142.094330\n",
      "Epoch 1401\n",
      "-------------------------------\n",
      "tensor(28.4692)\n",
      "tensor(10.4945)\n",
      "tensor(7.3330)\n",
      "tensor(0.4729)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 142.071854\n",
      "Epoch 1402\n",
      "-------------------------------\n",
      "tensor(13.8939)\n",
      "tensor(7.6907)\n",
      "tensor(7.2593)\n",
      "tensor(0.5020)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 142.062256\n",
      "Epoch 1403\n",
      "-------------------------------\n",
      "tensor(14.0925)\n",
      "tensor(7.6244)\n",
      "tensor(7.2685)\n",
      "tensor(0.5389)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 142.054214\n",
      "Epoch 1404\n",
      "-------------------------------\n",
      "tensor(13.3751)\n",
      "tensor(7.4261)\n",
      "tensor(7.3634)\n",
      "tensor(0.5864)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 142.038116\n",
      "Epoch 1405\n",
      "-------------------------------\n",
      "tensor(13.1425)\n",
      "tensor(7.4777)\n",
      "tensor(7.4580)\n",
      "tensor(0.6273)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 142.009445\n",
      "Epoch 1406\n",
      "-------------------------------\n",
      "tensor(30.8113)\n",
      "tensor(11.0485)\n",
      "tensor(7.6256)\n",
      "tensor(0.6349)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 141.974152\n",
      "Epoch 1407\n",
      "-------------------------------\n",
      "tensor(21.7151)\n",
      "tensor(9.3442)\n",
      "tensor(7.3472)\n",
      "tensor(0.5698)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 141.923462\n",
      "Epoch 1408\n",
      "-------------------------------\n",
      "tensor(25.2408)\n",
      "tensor(10.0395)\n",
      "tensor(7.1566)\n",
      "tensor(0.4805)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 141.859558\n",
      "Epoch 1409\n",
      "-------------------------------\n",
      "tensor(14.9070)\n",
      "tensor(7.1582)\n",
      "tensor(7.2818)\n",
      "tensor(0.4821)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 141.768784\n",
      "Epoch 1410\n",
      "-------------------------------\n",
      "tensor(27.1015)\n",
      "tensor(9.8554)\n",
      "tensor(7.3174)\n",
      "tensor(0.6310)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 141.689743\n",
      "Epoch 1411\n",
      "-------------------------------\n",
      "tensor(50.7943)\n",
      "tensor(16.8530)\n",
      "tensor(7.6869)\n",
      "tensor(0.7267)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 141.628098\n",
      "Epoch 1412\n",
      "-------------------------------\n",
      "tensor(27.7164)\n",
      "tensor(9.4478)\n",
      "tensor(10.0752)\n",
      "tensor(0.4803)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 141.530502\n",
      "Epoch 1413\n",
      "-------------------------------\n",
      "tensor(44.5519)\n",
      "tensor(14.9747)\n",
      "tensor(8.0355)\n",
      "tensor(0.2968)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 141.414459\n",
      "Epoch 1414\n",
      "-------------------------------\n",
      "tensor(34.8517)\n",
      "tensor(12.6985)\n",
      "tensor(7.3338)\n",
      "tensor(0.3042)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 141.354874\n",
      "Epoch 1415\n",
      "-------------------------------\n",
      "tensor(21.6090)\n",
      "tensor(9.3168)\n",
      "tensor(7.1331)\n",
      "tensor(0.4910)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 141.268738\n",
      "Epoch 1416\n",
      "-------------------------------\n",
      "tensor(25.2346)\n",
      "tensor(9.3942)\n",
      "tensor(7.6678)\n",
      "tensor(0.6648)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 141.214462\n",
      "Epoch 1417\n",
      "-------------------------------\n",
      "tensor(13.4167)\n",
      "tensor(7.3882)\n",
      "tensor(7.7612)\n",
      "tensor(0.6828)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 141.155746\n",
      "Epoch 1418\n",
      "-------------------------------\n",
      "tensor(13.2227)\n",
      "tensor(7.2762)\n",
      "tensor(7.4696)\n",
      "tensor(0.6028)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 141.113861\n",
      "Epoch 1419\n",
      "-------------------------------\n",
      "tensor(30.8611)\n",
      "tensor(10.6244)\n",
      "tensor(7.3451)\n",
      "tensor(0.5173)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 141.095398\n",
      "Epoch 1420\n",
      "-------------------------------\n",
      "tensor(23.7099)\n",
      "tensor(9.7448)\n",
      "tensor(7.2254)\n",
      "tensor(0.4500)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 141.070251\n",
      "Epoch 1421\n",
      "-------------------------------\n",
      "tensor(19.9257)\n",
      "tensor(8.4183)\n",
      "tensor(7.1659)\n",
      "tensor(0.4168)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 141.053467\n",
      "Epoch 1422\n",
      "-------------------------------\n",
      "tensor(19.9625)\n",
      "tensor(8.4041)\n",
      "tensor(7.1349)\n",
      "tensor(0.4036)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 141.044250\n",
      "Epoch 1423\n",
      "-------------------------------\n",
      "tensor(17.8284)\n",
      "tensor(8.6186)\n",
      "tensor(7.1145)\n",
      "tensor(0.4019)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 141.028351\n",
      "Epoch 1424\n",
      "-------------------------------\n",
      "tensor(24.0825)\n",
      "tensor(9.2211)\n",
      "tensor(7.1029)\n",
      "tensor(0.4228)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 141.007767\n",
      "Epoch 1425\n",
      "-------------------------------\n",
      "tensor(17.0128)\n",
      "tensor(7.6716)\n",
      "tensor(7.1452)\n",
      "tensor(0.4760)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 140.976212\n",
      "Epoch 1426\n",
      "-------------------------------\n",
      "tensor(29.7845)\n",
      "tensor(10.8224)\n",
      "tensor(7.2780)\n",
      "tensor(0.5592)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 140.938171\n",
      "Epoch 1427\n",
      "-------------------------------\n",
      "tensor(25.2548)\n",
      "tensor(9.3196)\n",
      "tensor(7.3944)\n",
      "tensor(0.6054)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 140.910706\n",
      "Epoch 1428\n",
      "-------------------------------\n",
      "tensor(45.2606)\n",
      "tensor(14.9658)\n",
      "tensor(7.1538)\n",
      "tensor(0.5532)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 140.862579\n",
      "Epoch 1429\n",
      "-------------------------------\n",
      "tensor(20.9109)\n",
      "tensor(7.8796)\n",
      "tensor(8.3253)\n",
      "tensor(0.3709)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 140.764877\n",
      "Epoch 1430\n",
      "-------------------------------\n",
      "tensor(25.6216)\n",
      "tensor(10.0691)\n",
      "tensor(7.2876)\n",
      "tensor(0.3862)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 140.686859\n",
      "Epoch 1431\n",
      "-------------------------------\n",
      "tensor(45.5212)\n",
      "tensor(15.2680)\n",
      "tensor(7.6770)\n",
      "tensor(0.5995)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 140.609589\n",
      "Epoch 1432\n",
      "-------------------------------\n",
      "tensor(39.7139)\n",
      "tensor(13.9606)\n",
      "tensor(7.9766)\n",
      "tensor(0.6769)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 140.530884\n",
      "Epoch 1433\n",
      "-------------------------------\n",
      "tensor(20.3417)\n",
      "tensor(7.2039)\n",
      "tensor(9.5275)\n",
      "tensor(0.5011)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 140.438354\n",
      "Epoch 1434\n",
      "-------------------------------\n",
      "tensor(25.1292)\n",
      "tensor(9.4748)\n",
      "tensor(7.2237)\n",
      "tensor(0.3865)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 140.368225\n",
      "Epoch 1435\n",
      "-------------------------------\n",
      "tensor(25.7336)\n",
      "tensor(9.8512)\n",
      "tensor(9.5611)\n",
      "tensor(0.4070)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 140.269775\n",
      "Epoch 1436\n",
      "-------------------------------\n",
      "tensor(21.9514)\n",
      "tensor(8.8807)\n",
      "tensor(7.7680)\n",
      "tensor(0.5418)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 140.217163\n",
      "Epoch 1437\n",
      "-------------------------------\n",
      "tensor(29.0937)\n",
      "tensor(10.0612)\n",
      "tensor(7.6340)\n",
      "tensor(0.6185)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 140.178650\n",
      "Epoch 1438\n",
      "-------------------------------\n",
      "tensor(27.5004)\n",
      "tensor(10.1325)\n",
      "tensor(8.5267)\n",
      "tensor(0.5888)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 140.132248\n",
      "Epoch 1439\n",
      "-------------------------------\n",
      "tensor(20.5511)\n",
      "tensor(8.5499)\n",
      "tensor(8.2727)\n",
      "tensor(0.5212)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 140.106323\n",
      "Epoch 1440\n",
      "-------------------------------\n",
      "tensor(19.2330)\n",
      "tensor(7.7759)\n",
      "tensor(7.4609)\n",
      "tensor(0.4651)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 140.085602\n",
      "Epoch 1441\n",
      "-------------------------------\n",
      "tensor(14.0391)\n",
      "tensor(7.0098)\n",
      "tensor(7.0208)\n",
      "tensor(0.4333)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 140.069702\n",
      "Epoch 1442\n",
      "-------------------------------\n",
      "tensor(15.8724)\n",
      "tensor(7.5904)\n",
      "tensor(6.9672)\n",
      "tensor(0.4177)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 140.058899\n",
      "Epoch 1443\n",
      "-------------------------------\n",
      "tensor(15.8508)\n",
      "tensor(8.4736)\n",
      "tensor(7.3516)\n",
      "tensor(0.4119)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 140.044968\n",
      "Epoch 1444\n",
      "-------------------------------\n",
      "tensor(25.8768)\n",
      "tensor(9.7833)\n",
      "tensor(8.2932)\n",
      "tensor(0.4274)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 140.031509\n",
      "Epoch 1445\n",
      "-------------------------------\n",
      "tensor(33.2246)\n",
      "tensor(11.1278)\n",
      "tensor(8.8390)\n",
      "tensor(0.4731)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 140.004166\n",
      "Epoch 1446\n",
      "-------------------------------\n",
      "tensor(17.4662)\n",
      "tensor(8.6048)\n",
      "tensor(7.4846)\n",
      "tensor(0.5277)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 139.952545\n",
      "Epoch 1447\n",
      "-------------------------------\n",
      "tensor(30.3876)\n",
      "tensor(10.3502)\n",
      "tensor(7.5421)\n",
      "tensor(0.5855)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 139.940735\n",
      "Epoch 1448\n",
      "-------------------------------\n",
      "tensor(29.4445)\n",
      "tensor(9.9009)\n",
      "tensor(9.4651)\n",
      "tensor(0.5530)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 139.927734\n",
      "Epoch 1449\n",
      "-------------------------------\n",
      "tensor(26.8497)\n",
      "tensor(9.5543)\n",
      "tensor(7.4460)\n",
      "tensor(0.4204)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 139.836899\n",
      "Epoch 1450\n",
      "-------------------------------\n",
      "tensor(15.2838)\n",
      "tensor(8.2761)\n",
      "tensor(8.1465)\n",
      "tensor(0.4202)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 139.726913\n",
      "Epoch 1451\n",
      "-------------------------------\n",
      "tensor(25.0213)\n",
      "tensor(10.0571)\n",
      "tensor(9.5340)\n",
      "tensor(0.6683)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 139.655762\n",
      "Epoch 1452\n",
      "-------------------------------\n",
      "tensor(16.4996)\n",
      "tensor(8.2738)\n",
      "tensor(8.1985)\n",
      "tensor(0.8382)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 139.570068\n",
      "Epoch 1453\n",
      "-------------------------------\n",
      "tensor(35.3023)\n",
      "tensor(12.1138)\n",
      "tensor(8.5441)\n",
      "tensor(0.6875)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 139.511551\n",
      "Epoch 1454\n",
      "-------------------------------\n",
      "tensor(47.1202)\n",
      "tensor(15.7659)\n",
      "tensor(7.3767)\n",
      "tensor(0.3009)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 139.413788\n",
      "Epoch 1455\n",
      "-------------------------------\n",
      "tensor(17.7166)\n",
      "tensor(7.9035)\n",
      "tensor(8.5968)\n",
      "tensor(0.0510)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 139.331238\n",
      "Epoch 1456\n",
      "-------------------------------\n",
      "tensor(17.6275)\n",
      "tensor(8.5571)\n",
      "tensor(7.4686)\n",
      "tensor(0.2156)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 139.268982\n",
      "Epoch 1457\n",
      "-------------------------------\n",
      "tensor(46.9395)\n",
      "tensor(15.5928)\n",
      "tensor(8.4199)\n",
      "tensor(0.5158)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 139.227570\n",
      "Epoch 1458\n",
      "-------------------------------\n",
      "tensor(28.1552)\n",
      "tensor(10.0956)\n",
      "tensor(7.7161)\n",
      "tensor(0.6622)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 139.187271\n",
      "Epoch 1459\n",
      "-------------------------------\n",
      "tensor(23.9598)\n",
      "tensor(9.6482)\n",
      "tensor(7.8162)\n",
      "tensor(0.6788)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 139.156952\n",
      "Epoch 1460\n",
      "-------------------------------\n",
      "tensor(30.1201)\n",
      "tensor(10.8587)\n",
      "tensor(7.8730)\n",
      "tensor(0.6338)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 139.135437\n",
      "Epoch 1461\n",
      "-------------------------------\n",
      "tensor(20.8668)\n",
      "tensor(8.9075)\n",
      "tensor(7.7194)\n",
      "tensor(0.5752)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 139.116333\n",
      "Epoch 1462\n",
      "-------------------------------\n",
      "tensor(14.8812)\n",
      "tensor(6.8726)\n",
      "tensor(7.4006)\n",
      "tensor(0.5173)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 139.105362\n",
      "Epoch 1463\n",
      "-------------------------------\n",
      "tensor(14.2068)\n",
      "tensor(6.9035)\n",
      "tensor(6.9414)\n",
      "tensor(0.4467)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 139.093170\n",
      "Epoch 1464\n",
      "-------------------------------\n",
      "tensor(23.7829)\n",
      "tensor(9.1979)\n",
      "tensor(6.8594)\n",
      "tensor(0.3586)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 139.078613\n",
      "Epoch 1465\n",
      "-------------------------------\n",
      "tensor(30.2553)\n",
      "tensor(10.8787)\n",
      "tensor(7.9574)\n",
      "tensor(0.2809)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 139.060181\n",
      "Epoch 1466\n",
      "-------------------------------\n",
      "tensor(31.4551)\n",
      "tensor(11.0745)\n",
      "tensor(8.2649)\n",
      "tensor(0.2864)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 139.035080\n",
      "Epoch 1467\n",
      "-------------------------------\n",
      "tensor(30.1368)\n",
      "tensor(10.8670)\n",
      "tensor(6.7565)\n",
      "tensor(0.4317)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 138.980789\n",
      "Epoch 1468\n",
      "-------------------------------\n",
      "tensor(21.1243)\n",
      "tensor(8.3324)\n",
      "tensor(9.2254)\n",
      "tensor(0.6485)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 138.947205\n",
      "Epoch 1469\n",
      "-------------------------------\n",
      "tensor(16.1091)\n",
      "tensor(7.2217)\n",
      "tensor(8.3840)\n",
      "tensor(0.7449)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 138.890900\n",
      "Epoch 1470\n",
      "-------------------------------\n",
      "tensor(21.1762)\n",
      "tensor(8.6935)\n",
      "tensor(9.1511)\n",
      "tensor(0.6200)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 138.815720\n",
      "Epoch 1471\n",
      "-------------------------------\n",
      "tensor(25.4720)\n",
      "tensor(9.5211)\n",
      "tensor(9.6307)\n",
      "tensor(0.4925)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 138.758408\n",
      "Epoch 1472\n",
      "-------------------------------\n",
      "tensor(28.7922)\n",
      "tensor(10.0425)\n",
      "tensor(7.2258)\n",
      "tensor(0.4742)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 138.653214\n",
      "Epoch 1473\n",
      "-------------------------------\n",
      "tensor(25.7690)\n",
      "tensor(8.3932)\n",
      "tensor(10.5358)\n",
      "tensor(0.4696)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 138.592773\n",
      "Epoch 1474\n",
      "-------------------------------\n",
      "tensor(19.2334)\n",
      "tensor(8.8040)\n",
      "tensor(6.6503)\n",
      "tensor(0.4337)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 138.511047\n",
      "Epoch 1475\n",
      "-------------------------------\n",
      "tensor(31.5784)\n",
      "tensor(11.2790)\n",
      "tensor(10.2257)\n",
      "tensor(0.4501)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 138.454956\n",
      "Epoch 1476\n",
      "-------------------------------\n",
      "tensor(31.9073)\n",
      "tensor(10.8838)\n",
      "tensor(8.0561)\n",
      "tensor(0.4895)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 138.384521\n",
      "Epoch 1477\n",
      "-------------------------------\n",
      "tensor(15.7389)\n",
      "tensor(7.0061)\n",
      "tensor(7.2972)\n",
      "tensor(0.5057)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 138.345413\n",
      "Epoch 1478\n",
      "-------------------------------\n",
      "tensor(17.6526)\n",
      "tensor(6.9781)\n",
      "tensor(8.1166)\n",
      "tensor(0.4885)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 138.318863\n",
      "Epoch 1479\n",
      "-------------------------------\n",
      "tensor(15.9226)\n",
      "tensor(6.9701)\n",
      "tensor(7.1882)\n",
      "tensor(0.4621)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 138.290558\n",
      "Epoch 1480\n",
      "-------------------------------\n",
      "tensor(14.0175)\n",
      "tensor(7.0621)\n",
      "tensor(6.6003)\n",
      "tensor(0.4446)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 138.268402\n",
      "Epoch 1481\n",
      "-------------------------------\n",
      "tensor(13.0913)\n",
      "tensor(7.2071)\n",
      "tensor(6.7632)\n",
      "tensor(0.4372)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 138.252594\n",
      "Epoch 1482\n",
      "-------------------------------\n",
      "tensor(13.0119)\n",
      "tensor(7.6574)\n",
      "tensor(7.2494)\n",
      "tensor(0.4351)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 138.239090\n",
      "Epoch 1483\n",
      "-------------------------------\n",
      "tensor(17.6161)\n",
      "tensor(8.4520)\n",
      "tensor(7.9213)\n",
      "tensor(0.4433)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 138.224533\n",
      "Epoch 1484\n",
      "-------------------------------\n",
      "tensor(41.1304)\n",
      "tensor(13.9433)\n",
      "tensor(8.4939)\n",
      "tensor(0.4663)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 138.202454\n",
      "Epoch 1485\n",
      "-------------------------------\n",
      "tensor(28.6073)\n",
      "tensor(10.6583)\n",
      "tensor(7.4276)\n",
      "tensor(0.4868)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 138.170410\n",
      "Epoch 1486\n",
      "-------------------------------\n",
      "tensor(36.6247)\n",
      "tensor(12.4670)\n",
      "tensor(7.0713)\n",
      "tensor(0.5132)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 138.161743\n",
      "Epoch 1487\n",
      "-------------------------------\n",
      "tensor(32.6705)\n",
      "tensor(11.3518)\n",
      "tensor(10.9218)\n",
      "tensor(0.4715)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 138.139603\n",
      "Epoch 1488\n",
      "-------------------------------\n",
      "tensor(44.0894)\n",
      "tensor(14.7416)\n",
      "tensor(10.1682)\n",
      "tensor(0.3671)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 138.087326\n",
      "Epoch 1489\n",
      "-------------------------------\n",
      "tensor(38.5497)\n",
      "tensor(12.8027)\n",
      "tensor(7.2912)\n",
      "tensor(0.2708)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 138.029190\n",
      "Epoch 1490\n",
      "-------------------------------\n",
      "tensor(28.1992)\n",
      "tensor(9.7798)\n",
      "tensor(7.6752)\n",
      "tensor(0.4241)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 137.949310\n",
      "Epoch 1491\n",
      "-------------------------------\n",
      "tensor(13.8036)\n",
      "tensor(7.5869)\n",
      "tensor(7.6070)\n",
      "tensor(0.7565)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 137.874329\n",
      "Epoch 1492\n",
      "-------------------------------\n",
      "tensor(14.4976)\n",
      "tensor(8.3079)\n",
      "tensor(8.3657)\n",
      "tensor(0.8705)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 137.805954\n",
      "Epoch 1493\n",
      "-------------------------------\n",
      "tensor(13.9301)\n",
      "tensor(6.6275)\n",
      "tensor(6.7736)\n",
      "tensor(0.5628)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 137.716476\n",
      "Epoch 1494\n",
      "-------------------------------\n",
      "tensor(53.0793)\n",
      "tensor(17.2524)\n",
      "tensor(7.4372)\n",
      "tensor(0.2149)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 137.682571\n",
      "Epoch 1495\n",
      "-------------------------------\n",
      "tensor(24.6058)\n",
      "tensor(9.1655)\n",
      "tensor(9.7538)\n",
      "tensor(0.0294)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 137.588104\n",
      "Epoch 1496\n",
      "-------------------------------\n",
      "tensor(40.9433)\n",
      "tensor(13.4918)\n",
      "tensor(8.8765)\n",
      "tensor(0.1653)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 137.521927\n",
      "Epoch 1497\n",
      "-------------------------------\n",
      "tensor(27.9575)\n",
      "tensor(9.7329)\n",
      "tensor(7.1596)\n",
      "tensor(0.3467)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 137.479034\n",
      "Epoch 1498\n",
      "-------------------------------\n",
      "tensor(29.0294)\n",
      "tensor(10.2758)\n",
      "tensor(6.7911)\n",
      "tensor(0.4797)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 137.440475\n",
      "Epoch 1499\n",
      "-------------------------------\n",
      "tensor(25.3070)\n",
      "tensor(9.0254)\n",
      "tensor(7.1566)\n",
      "tensor(0.5364)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 137.423660\n",
      "Epoch 1500\n",
      "-------------------------------\n",
      "tensor(25.3404)\n",
      "tensor(9.0299)\n",
      "tensor(7.1887)\n",
      "tensor(0.5385)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 137.406982\n",
      "Epoch 1501\n",
      "-------------------------------\n",
      "tensor(34.0109)\n",
      "tensor(10.9342)\n",
      "tensor(7.0293)\n",
      "tensor(0.5204)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 137.389938\n",
      "Epoch 1502\n",
      "-------------------------------\n",
      "tensor(19.7132)\n",
      "tensor(7.7946)\n",
      "tensor(6.7863)\n",
      "tensor(0.4946)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 137.380157\n",
      "Epoch 1503\n",
      "-------------------------------\n",
      "tensor(19.6762)\n",
      "tensor(8.3724)\n",
      "tensor(6.6434)\n",
      "tensor(0.4575)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 137.370972\n",
      "Epoch 1504\n",
      "-------------------------------\n",
      "tensor(21.7196)\n",
      "tensor(8.7206)\n",
      "tensor(6.7254)\n",
      "tensor(0.4025)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 137.362656\n",
      "Epoch 1505\n",
      "-------------------------------\n",
      "tensor(29.8271)\n",
      "tensor(10.7876)\n",
      "tensor(7.0406)\n",
      "tensor(0.3381)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 137.352493\n",
      "Epoch 1506\n",
      "-------------------------------\n",
      "tensor(19.9803)\n",
      "tensor(7.4835)\n",
      "tensor(7.3787)\n",
      "tensor(0.2984)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 137.332077\n",
      "Epoch 1507\n",
      "-------------------------------\n",
      "tensor(20.6256)\n",
      "tensor(7.9566)\n",
      "tensor(6.5734)\n",
      "tensor(0.3588)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 137.290512\n",
      "Epoch 1508\n",
      "-------------------------------\n",
      "tensor(14.7544)\n",
      "tensor(7.4420)\n",
      "tensor(7.3519)\n",
      "tensor(0.5413)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 137.234039\n",
      "Epoch 1509\n",
      "-------------------------------\n",
      "tensor(32.7438)\n",
      "tensor(10.4893)\n",
      "tensor(8.6493)\n",
      "tensor(0.7346)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 137.190323\n",
      "Epoch 1510\n",
      "-------------------------------\n",
      "tensor(18.4762)\n",
      "tensor(7.4835)\n",
      "tensor(7.3832)\n",
      "tensor(0.6497)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 137.109482\n",
      "Epoch 1511\n",
      "-------------------------------\n",
      "tensor(19.9144)\n",
      "tensor(8.2206)\n",
      "tensor(7.3351)\n",
      "tensor(0.3743)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 137.029556\n",
      "Epoch 1512\n",
      "-------------------------------\n",
      "tensor(29.7399)\n",
      "tensor(10.6191)\n",
      "tensor(7.1400)\n",
      "tensor(0.2542)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 136.991058\n",
      "Epoch 1513\n",
      "-------------------------------\n",
      "tensor(29.3904)\n",
      "tensor(10.8302)\n",
      "tensor(7.1021)\n",
      "tensor(0.3919)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 136.922714\n",
      "Epoch 1514\n",
      "-------------------------------\n",
      "tensor(15.2316)\n",
      "tensor(7.2795)\n",
      "tensor(6.7583)\n",
      "tensor(0.5801)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 136.818878\n",
      "Epoch 1515\n",
      "-------------------------------\n",
      "tensor(14.0436)\n",
      "tensor(7.3674)\n",
      "tensor(6.9562)\n",
      "tensor(0.6410)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 136.761093\n",
      "Epoch 1516\n",
      "-------------------------------\n",
      "tensor(17.6850)\n",
      "tensor(8.2482)\n",
      "tensor(7.0235)\n",
      "tensor(0.5521)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 136.714142\n",
      "Epoch 1517\n",
      "-------------------------------\n",
      "tensor(32.8525)\n",
      "tensor(11.3144)\n",
      "tensor(7.5008)\n",
      "tensor(0.4614)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 136.691635\n",
      "Epoch 1518\n",
      "-------------------------------\n",
      "tensor(27.1709)\n",
      "tensor(10.2221)\n",
      "tensor(6.4745)\n",
      "tensor(0.4009)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 136.634949\n",
      "Epoch 1519\n",
      "-------------------------------\n",
      "tensor(14.5269)\n",
      "tensor(8.3386)\n",
      "tensor(6.2305)\n",
      "tensor(0.3921)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 136.601654\n",
      "Epoch 1520\n",
      "-------------------------------\n",
      "tensor(18.9828)\n",
      "tensor(8.7506)\n",
      "tensor(6.3385)\n",
      "tensor(0.4110)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 136.588089\n",
      "Epoch 1521\n",
      "-------------------------------\n",
      "tensor(24.9857)\n",
      "tensor(9.9650)\n",
      "tensor(6.3415)\n",
      "tensor(0.4317)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 136.578598\n",
      "Epoch 1522\n",
      "-------------------------------\n",
      "tensor(14.2683)\n",
      "tensor(7.1040)\n",
      "tensor(6.3121)\n",
      "tensor(0.4500)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 136.568039\n",
      "Epoch 1523\n",
      "-------------------------------\n",
      "tensor(22.2361)\n",
      "tensor(8.7199)\n",
      "tensor(6.2901)\n",
      "tensor(0.4741)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 136.562775\n",
      "Epoch 1524\n",
      "-------------------------------\n",
      "tensor(29.1391)\n",
      "tensor(10.1920)\n",
      "tensor(6.5320)\n",
      "tensor(0.5024)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 136.551987\n",
      "Epoch 1525\n",
      "-------------------------------\n",
      "tensor(23.7579)\n",
      "tensor(9.1131)\n",
      "tensor(7.0025)\n",
      "tensor(0.5207)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 136.523880\n",
      "Epoch 1526\n",
      "-------------------------------\n",
      "tensor(28.1264)\n",
      "tensor(9.8977)\n",
      "tensor(7.0669)\n",
      "tensor(0.5276)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 136.516083\n",
      "Epoch 1527\n",
      "-------------------------------\n",
      "tensor(26.2931)\n",
      "tensor(9.6563)\n",
      "tensor(6.3092)\n",
      "tensor(0.4893)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 136.482422\n",
      "Epoch 1528\n",
      "-------------------------------\n",
      "tensor(18.6040)\n",
      "tensor(8.3974)\n",
      "tensor(6.7531)\n",
      "tensor(0.4242)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 136.424118\n",
      "Epoch 1529\n",
      "-------------------------------\n",
      "tensor(23.4409)\n",
      "tensor(9.2637)\n",
      "tensor(6.2958)\n",
      "tensor(0.4226)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 136.389618\n",
      "Epoch 1530\n",
      "-------------------------------\n",
      "tensor(25.0456)\n",
      "tensor(9.0353)\n",
      "tensor(6.7986)\n",
      "tensor(0.5039)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 136.311783\n",
      "Epoch 1531\n",
      "-------------------------------\n",
      "tensor(13.3180)\n",
      "tensor(7.1034)\n",
      "tensor(6.8058)\n",
      "tensor(0.6121)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 136.246887\n",
      "Epoch 1532\n",
      "-------------------------------\n",
      "tensor(25.5337)\n",
      "tensor(8.9839)\n",
      "tensor(6.6800)\n",
      "tensor(0.5978)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 136.184723\n",
      "Epoch 1533\n",
      "-------------------------------\n",
      "tensor(24.1161)\n",
      "tensor(8.5830)\n",
      "tensor(6.3954)\n",
      "tensor(0.4096)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 136.140060\n",
      "Epoch 1534\n",
      "-------------------------------\n",
      "tensor(25.9161)\n",
      "tensor(9.3853)\n",
      "tensor(6.3643)\n",
      "tensor(0.2528)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 136.077240\n",
      "Epoch 1535\n",
      "-------------------------------\n",
      "tensor(22.9690)\n",
      "tensor(9.4253)\n",
      "tensor(6.3951)\n",
      "tensor(0.2688)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 135.992798\n",
      "Epoch 1536\n",
      "-------------------------------\n",
      "tensor(41.0445)\n",
      "tensor(13.9415)\n",
      "tensor(6.2759)\n",
      "tensor(0.4215)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 135.950775\n",
      "Epoch 1537\n",
      "-------------------------------\n",
      "tensor(44.6940)\n",
      "tensor(15.1547)\n",
      "tensor(6.8587)\n",
      "tensor(0.4845)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 135.918396\n",
      "Epoch 1538\n",
      "-------------------------------\n",
      "tensor(20.4879)\n",
      "tensor(7.2847)\n",
      "tensor(8.3586)\n",
      "tensor(0.4343)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 135.879623\n",
      "Epoch 1539\n",
      "-------------------------------\n",
      "tensor(19.0055)\n",
      "tensor(6.6999)\n",
      "tensor(7.6221)\n",
      "tensor(0.3903)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 135.850449\n",
      "Epoch 1540\n",
      "-------------------------------\n",
      "tensor(44.6465)\n",
      "tensor(14.6446)\n",
      "tensor(6.5611)\n",
      "tensor(0.3668)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 135.836380\n",
      "Epoch 1541\n",
      "-------------------------------\n",
      "tensor(14.4718)\n",
      "tensor(6.2833)\n",
      "tensor(6.2950)\n",
      "tensor(0.3471)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 135.815201\n",
      "Epoch 1542\n",
      "-------------------------------\n",
      "tensor(23.1755)\n",
      "tensor(8.4405)\n",
      "tensor(6.1882)\n",
      "tensor(0.3435)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 135.812363\n",
      "Epoch 1543\n",
      "-------------------------------\n",
      "tensor(29.7426)\n",
      "tensor(10.0986)\n",
      "tensor(6.2792)\n",
      "tensor(0.3509)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 135.806000\n",
      "Epoch 1544\n",
      "-------------------------------\n",
      "tensor(28.6820)\n",
      "tensor(9.9986)\n",
      "tensor(6.4968)\n",
      "tensor(0.3764)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 135.797943\n",
      "Epoch 1545\n",
      "-------------------------------\n",
      "tensor(21.4861)\n",
      "tensor(8.7371)\n",
      "tensor(6.5383)\n",
      "tensor(0.4177)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 135.773132\n",
      "Epoch 1546\n",
      "-------------------------------\n",
      "tensor(24.7198)\n",
      "tensor(9.2930)\n",
      "tensor(6.3740)\n",
      "tensor(0.4739)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 135.744217\n",
      "Epoch 1547\n",
      "-------------------------------\n",
      "tensor(26.6372)\n",
      "tensor(9.9204)\n",
      "tensor(6.4417)\n",
      "tensor(0.5021)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 135.705688\n",
      "Epoch 1548\n",
      "-------------------------------\n",
      "tensor(22.6441)\n",
      "tensor(7.7995)\n",
      "tensor(7.2100)\n",
      "tensor(0.4897)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 135.701645\n",
      "Epoch 1549\n",
      "-------------------------------\n",
      "tensor(37.1400)\n",
      "tensor(12.3686)\n",
      "tensor(6.2414)\n",
      "tensor(0.4239)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 135.663940\n",
      "Epoch 1550\n",
      "-------------------------------\n",
      "tensor(42.3209)\n",
      "tensor(13.7200)\n",
      "tensor(6.2282)\n",
      "tensor(0.3218)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 135.627609\n",
      "Epoch 1551\n",
      "-------------------------------\n",
      "tensor(27.5635)\n",
      "tensor(9.2012)\n",
      "tensor(7.5981)\n",
      "tensor(0.3051)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 135.503174\n",
      "Epoch 1552\n",
      "-------------------------------\n",
      "tensor(23.7283)\n",
      "tensor(8.1328)\n",
      "tensor(6.6889)\n",
      "tensor(0.4630)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 135.431259\n",
      "Epoch 1553\n",
      "-------------------------------\n",
      "tensor(18.5439)\n",
      "tensor(8.0265)\n",
      "tensor(7.1473)\n",
      "tensor(0.5775)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 135.394547\n",
      "Epoch 1554\n",
      "-------------------------------\n",
      "tensor(24.3314)\n",
      "tensor(9.5713)\n",
      "tensor(7.8717)\n",
      "tensor(0.5383)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 135.355713\n",
      "Epoch 1555\n",
      "-------------------------------\n",
      "tensor(28.5573)\n",
      "tensor(10.2999)\n",
      "tensor(6.0602)\n",
      "tensor(0.4046)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 135.310349\n",
      "Epoch 1556\n",
      "-------------------------------\n",
      "tensor(21.3172)\n",
      "tensor(7.4050)\n",
      "tensor(7.5184)\n",
      "tensor(0.3053)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 135.247681\n",
      "Epoch 1557\n",
      "-------------------------------\n",
      "tensor(18.1462)\n",
      "tensor(6.7854)\n",
      "tensor(6.8503)\n",
      "tensor(0.3076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 135.206558\n",
      "Epoch 1558\n",
      "-------------------------------\n",
      "tensor(22.8536)\n",
      "tensor(8.4644)\n",
      "tensor(6.0197)\n",
      "tensor(0.3637)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 135.173248\n",
      "Epoch 1559\n",
      "-------------------------------\n",
      "tensor(13.2693)\n",
      "tensor(6.8812)\n",
      "tensor(6.6082)\n",
      "tensor(0.4194)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 135.151276\n",
      "Epoch 1560\n",
      "-------------------------------\n",
      "tensor(23.5209)\n",
      "tensor(8.7557)\n",
      "tensor(7.0820)\n",
      "tensor(0.4575)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 135.138245\n",
      "Epoch 1561\n",
      "-------------------------------\n",
      "tensor(23.4901)\n",
      "tensor(8.7445)\n",
      "tensor(7.0567)\n",
      "tensor(0.4762)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 135.128845\n",
      "Epoch 1562\n",
      "-------------------------------\n",
      "tensor(23.3195)\n",
      "tensor(8.6820)\n",
      "tensor(6.8065)\n",
      "tensor(0.4826)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 135.115158\n",
      "Epoch 1563\n",
      "-------------------------------\n",
      "tensor(17.2313)\n",
      "tensor(7.4221)\n",
      "tensor(6.4082)\n",
      "tensor(0.4816)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 135.104446\n",
      "Epoch 1564\n",
      "-------------------------------\n",
      "tensor(28.5654)\n",
      "tensor(10.2815)\n",
      "tensor(6.1685)\n",
      "tensor(0.4646)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 135.096573\n",
      "Epoch 1565\n",
      "-------------------------------\n",
      "tensor(20.1295)\n",
      "tensor(8.2969)\n",
      "tensor(6.6406)\n",
      "tensor(0.4092)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 135.075317\n",
      "Epoch 1566\n",
      "-------------------------------\n",
      "tensor(26.9602)\n",
      "tensor(10.0265)\n",
      "tensor(6.9544)\n",
      "tensor(0.3254)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 135.038635\n",
      "Epoch 1567\n",
      "-------------------------------\n",
      "tensor(19.2695)\n",
      "tensor(7.8143)\n",
      "tensor(6.2742)\n",
      "tensor(0.2667)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 134.997360\n",
      "Epoch 1568\n",
      "-------------------------------\n",
      "tensor(25.7868)\n",
      "tensor(9.5541)\n",
      "tensor(6.9019)\n",
      "tensor(0.3477)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 134.997406\n",
      "Epoch 1569\n",
      "-------------------------------\n",
      "tensor(27.8394)\n",
      "tensor(9.8706)\n",
      "tensor(7.4122)\n",
      "tensor(0.5519)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 134.965988\n",
      "Epoch 1570\n",
      "-------------------------------\n",
      "tensor(20.8398)\n",
      "tensor(8.1840)\n",
      "tensor(7.1648)\n",
      "tensor(0.6775)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 134.902710\n",
      "Epoch 1571\n",
      "-------------------------------\n",
      "tensor(27.1240)\n",
      "tensor(9.1602)\n",
      "tensor(7.1580)\n",
      "tensor(0.5427)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 134.831070\n",
      "Epoch 1572\n",
      "-------------------------------\n",
      "tensor(38.1901)\n",
      "tensor(12.5623)\n",
      "tensor(6.1258)\n",
      "tensor(0.2771)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 134.724426\n",
      "Epoch 1573\n",
      "-------------------------------\n",
      "tensor(51.3186)\n",
      "tensor(16.8327)\n",
      "tensor(6.5027)\n",
      "tensor(0.2097)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 134.714050\n",
      "Epoch 1574\n",
      "-------------------------------\n",
      "tensor(25.7608)\n",
      "tensor(8.8846)\n",
      "tensor(10.1566)\n",
      "tensor(0.2726)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 134.640305\n",
      "Epoch 1575\n",
      "-------------------------------\n",
      "tensor(21.2711)\n",
      "tensor(7.7109)\n",
      "tensor(6.2404)\n",
      "tensor(0.4587)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 134.589386\n",
      "Epoch 1576\n",
      "-------------------------------\n",
      "tensor(48.5106)\n",
      "tensor(15.3526)\n",
      "tensor(8.2913)\n",
      "tensor(0.5890)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 134.587021\n",
      "Epoch 1577\n",
      "-------------------------------\n",
      "tensor(18.1333)\n",
      "tensor(7.5384)\n",
      "tensor(6.5913)\n",
      "tensor(0.5375)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 134.512726\n",
      "Epoch 1578\n",
      "-------------------------------\n",
      "tensor(15.0536)\n",
      "tensor(6.4195)\n",
      "tensor(6.0593)\n",
      "tensor(0.4507)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 134.478271\n",
      "Epoch 1579\n",
      "-------------------------------\n",
      "tensor(14.9565)\n",
      "tensor(6.1211)\n",
      "tensor(6.2138)\n",
      "tensor(0.3854)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 134.454590\n",
      "Epoch 1580\n",
      "-------------------------------\n",
      "tensor(23.2756)\n",
      "tensor(8.2295)\n",
      "tensor(6.0947)\n",
      "tensor(0.3516)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 134.439957\n",
      "Epoch 1581\n",
      "-------------------------------\n",
      "tensor(23.1626)\n",
      "tensor(8.2783)\n",
      "tensor(5.9743)\n",
      "tensor(0.3355)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 134.424698\n",
      "Epoch 1582\n",
      "-------------------------------\n",
      "tensor(16.7221)\n",
      "tensor(7.7072)\n",
      "tensor(5.9065)\n",
      "tensor(0.3306)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 134.415421\n",
      "Epoch 1583\n",
      "-------------------------------\n",
      "tensor(16.4474)\n",
      "tensor(7.7559)\n",
      "tensor(5.9067)\n",
      "tensor(0.3343)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 134.402832\n",
      "Epoch 1584\n",
      "-------------------------------\n",
      "tensor(17.9242)\n",
      "tensor(7.3211)\n",
      "tensor(6.1737)\n",
      "tensor(0.3524)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 134.383453\n",
      "Epoch 1585\n",
      "-------------------------------\n",
      "tensor(31.2634)\n",
      "tensor(10.5781)\n",
      "tensor(6.7329)\n",
      "tensor(0.3975)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 134.397308\n",
      "Epoch 1586\n",
      "-------------------------------\n",
      "tensor(32.9949)\n",
      "tensor(10.9338)\n",
      "tensor(6.3737)\n",
      "tensor(0.4325)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 134.401123\n",
      "Epoch 1587\n",
      "-------------------------------\n",
      "tensor(35.2022)\n",
      "tensor(12.2141)\n",
      "tensor(6.3866)\n",
      "tensor(0.4070)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 134.364151\n",
      "Epoch 1588\n",
      "-------------------------------\n",
      "tensor(27.0597)\n",
      "tensor(8.6210)\n",
      "tensor(9.1806)\n",
      "tensor(0.3074)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 134.281357\n",
      "Epoch 1589\n",
      "-------------------------------\n",
      "tensor(19.5640)\n",
      "tensor(6.6600)\n",
      "tensor(6.9189)\n",
      "tensor(0.3139)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 134.255798\n",
      "Epoch 1590\n",
      "-------------------------------\n",
      "tensor(25.2204)\n",
      "tensor(8.8330)\n",
      "tensor(8.7911)\n",
      "tensor(0.4988)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 134.223343\n",
      "Epoch 1591\n",
      "-------------------------------\n",
      "tensor(15.6539)\n",
      "tensor(8.6075)\n",
      "tensor(8.7956)\n",
      "tensor(0.7318)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 134.183945\n",
      "Epoch 1592\n",
      "-------------------------------\n",
      "tensor(15.5236)\n",
      "tensor(7.8868)\n",
      "tensor(7.3929)\n",
      "tensor(0.7071)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 134.119659\n",
      "Epoch 1593\n",
      "-------------------------------\n",
      "tensor(23.7511)\n",
      "tensor(8.2536)\n",
      "tensor(6.5238)\n",
      "tensor(0.3796)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 134.081223\n",
      "Epoch 1594\n",
      "-------------------------------\n",
      "tensor(25.5181)\n",
      "tensor(9.7302)\n",
      "tensor(6.6849)\n",
      "tensor(0.1131)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 134.006561\n",
      "Epoch 1595\n",
      "-------------------------------\n",
      "tensor(27.6563)\n",
      "tensor(10.1795)\n",
      "tensor(7.0478)\n",
      "tensor(0.1780)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 133.949249\n",
      "Epoch 1596\n",
      "-------------------------------\n",
      "tensor(24.7197)\n",
      "tensor(8.9960)\n",
      "tensor(5.8272)\n",
      "tensor(0.3919)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 133.887344\n",
      "Epoch 1597\n",
      "-------------------------------\n",
      "tensor(16.7656)\n",
      "tensor(7.1171)\n",
      "tensor(6.4987)\n",
      "tensor(0.5284)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 133.853088\n",
      "Epoch 1598\n",
      "-------------------------------\n",
      "tensor(19.7128)\n",
      "tensor(7.8034)\n",
      "tensor(6.3353)\n",
      "tensor(0.5366)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 133.825760\n",
      "Epoch 1599\n",
      "-------------------------------\n",
      "tensor(19.5369)\n",
      "tensor(8.3698)\n",
      "tensor(5.9984)\n",
      "tensor(0.4890)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 133.807709\n",
      "Epoch 1600\n",
      "-------------------------------\n",
      "tensor(19.4424)\n",
      "tensor(8.4092)\n",
      "tensor(5.9683)\n",
      "tensor(0.4394)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 133.788345\n",
      "Epoch 1601\n",
      "-------------------------------\n",
      "tensor(13.4390)\n",
      "tensor(7.4803)\n",
      "tensor(6.0383)\n",
      "tensor(0.4055)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 133.772781\n",
      "Epoch 1602\n",
      "-------------------------------\n",
      "tensor(13.1359)\n",
      "tensor(6.9101)\n",
      "tensor(6.1219)\n",
      "tensor(0.3833)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 133.762085\n",
      "Epoch 1603\n",
      "-------------------------------\n",
      "tensor(17.7452)\n",
      "tensor(7.6888)\n",
      "tensor(6.1993)\n",
      "tensor(0.3653)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 133.749069\n",
      "Epoch 1604\n",
      "-------------------------------\n",
      "tensor(22.5059)\n",
      "tensor(8.6041)\n",
      "tensor(6.1455)\n",
      "tensor(0.3548)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 133.732590\n",
      "Epoch 1605\n",
      "-------------------------------\n",
      "tensor(26.9969)\n",
      "tensor(10.1159)\n",
      "tensor(5.8002)\n",
      "tensor(0.3634)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 133.725266\n",
      "Epoch 1606\n",
      "-------------------------------\n",
      "tensor(33.4812)\n",
      "tensor(11.6078)\n",
      "tensor(5.8087)\n",
      "tensor(0.3870)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 133.726227\n",
      "Epoch 1607\n",
      "-------------------------------\n",
      "tensor(33.6255)\n",
      "tensor(11.6749)\n",
      "tensor(7.1716)\n",
      "tensor(0.3849)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 133.691833\n",
      "Epoch 1608\n",
      "-------------------------------\n",
      "tensor(27.8478)\n",
      "tensor(9.1637)\n",
      "tensor(7.5127)\n",
      "tensor(0.3578)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 133.629593\n",
      "Epoch 1609\n",
      "-------------------------------\n",
      "tensor(18.5645)\n",
      "tensor(7.0170)\n",
      "tensor(5.7780)\n",
      "tensor(0.4056)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 133.607117\n",
      "Epoch 1610\n",
      "-------------------------------\n",
      "tensor(39.2930)\n",
      "tensor(13.1602)\n",
      "tensor(8.8893)\n",
      "tensor(0.5537)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 133.595581\n",
      "Epoch 1611\n",
      "-------------------------------\n",
      "tensor(36.3125)\n",
      "tensor(12.4753)\n",
      "tensor(6.2591)\n",
      "tensor(0.6157)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 133.547501\n",
      "Epoch 1612\n",
      "-------------------------------\n",
      "tensor(26.7787)\n",
      "tensor(8.7965)\n",
      "tensor(10.9186)\n",
      "tensor(0.4279)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 133.479004\n",
      "Epoch 1613\n",
      "-------------------------------\n",
      "tensor(15.5408)\n",
      "tensor(7.1306)\n",
      "tensor(5.9358)\n",
      "tensor(0.2347)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 133.403915\n",
      "Epoch 1614\n",
      "-------------------------------\n",
      "tensor(26.4313)\n",
      "tensor(9.2902)\n",
      "tensor(10.7002)\n",
      "tensor(0.3054)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 133.364746\n",
      "Epoch 1615\n",
      "-------------------------------\n",
      "tensor(30.1886)\n",
      "tensor(10.2925)\n",
      "tensor(7.8923)\n",
      "tensor(0.4834)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 133.307343\n",
      "Epoch 1616\n",
      "-------------------------------\n",
      "tensor(28.2797)\n",
      "tensor(9.1135)\n",
      "tensor(8.1784)\n",
      "tensor(0.4991)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 133.251328\n",
      "Epoch 1617\n",
      "-------------------------------\n",
      "tensor(26.8517)\n",
      "tensor(8.2667)\n",
      "tensor(10.1198)\n",
      "tensor(0.3578)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 133.223160\n",
      "Epoch 1618\n",
      "-------------------------------\n",
      "tensor(40.7252)\n",
      "tensor(13.3130)\n",
      "tensor(7.4369)\n",
      "tensor(0.2118)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 133.187454\n",
      "Epoch 1619\n",
      "-------------------------------\n",
      "tensor(18.9863)\n",
      "tensor(7.4209)\n",
      "tensor(6.3037)\n",
      "tensor(0.1131)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 133.169327\n",
      "Epoch 1620\n",
      "-------------------------------\n",
      "tensor(13.9308)\n",
      "tensor(6.6167)\n",
      "tensor(6.1899)\n",
      "tensor(0.1031)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 133.151718\n",
      "Epoch 1621\n",
      "-------------------------------\n",
      "tensor(15.7302)\n",
      "tensor(7.1229)\n",
      "tensor(6.4692)\n",
      "tensor(0.1351)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 133.141159\n",
      "Epoch 1622\n",
      "-------------------------------\n",
      "tensor(15.7716)\n",
      "tensor(7.1197)\n",
      "tensor(6.7141)\n",
      "tensor(0.1865)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 133.130951\n",
      "Epoch 1623\n",
      "-------------------------------\n",
      "tensor(13.5739)\n",
      "tensor(6.6807)\n",
      "tensor(6.8671)\n",
      "tensor(0.2704)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 133.116913\n",
      "Epoch 1624\n",
      "-------------------------------\n",
      "tensor(34.8686)\n",
      "tensor(11.3151)\n",
      "tensor(6.8815)\n",
      "tensor(0.4156)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 133.114822\n",
      "Epoch 1625\n",
      "-------------------------------\n",
      "tensor(36.3848)\n",
      "tensor(11.4858)\n",
      "tensor(6.6768)\n",
      "tensor(0.5906)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 133.105972\n",
      "Epoch 1626\n",
      "-------------------------------\n",
      "tensor(23.7030)\n",
      "tensor(8.7612)\n",
      "tensor(8.8821)\n",
      "tensor(0.6712)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 133.084030\n",
      "Epoch 1627\n",
      "-------------------------------\n",
      "tensor(28.1682)\n",
      "tensor(9.5887)\n",
      "tensor(9.6321)\n",
      "tensor(0.5407)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 133.059845\n",
      "Epoch 1628\n",
      "-------------------------------\n",
      "tensor(49.1729)\n",
      "tensor(16.0407)\n",
      "tensor(6.3186)\n",
      "tensor(0.2335)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 133.015320\n",
      "Epoch 1629\n",
      "-------------------------------\n",
      "tensor(26.2462)\n",
      "tensor(9.1953)\n",
      "tensor(8.3189)\n",
      "tensor(0.0463)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 132.992981\n",
      "Epoch 1630\n",
      "-------------------------------\n",
      "tensor(36.2691)\n",
      "tensor(11.4944)\n",
      "tensor(6.0896)\n",
      "tensor(0.2886)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 132.939301\n",
      "Epoch 1631\n",
      "-------------------------------\n",
      "tensor(20.3415)\n",
      "tensor(8.5087)\n",
      "tensor(8.8080)\n",
      "tensor(0.8422)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 132.898880\n",
      "Epoch 1632\n",
      "-------------------------------\n",
      "tensor(14.1034)\n",
      "tensor(8.0382)\n",
      "tensor(8.4669)\n",
      "tensor(0.8875)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 132.847794\n",
      "Epoch 1633\n",
      "-------------------------------\n",
      "tensor(30.3861)\n",
      "tensor(10.7308)\n",
      "tensor(7.2784)\n",
      "tensor(0.4039)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 132.832504\n",
      "Epoch 1634\n",
      "-------------------------------\n",
      "tensor(19.8412)\n",
      "tensor(8.8239)\n",
      "tensor(7.1163)\n",
      "tensor(0.0326)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 132.758499\n",
      "Epoch 1635\n",
      "-------------------------------\n",
      "tensor(22.9799)\n",
      "tensor(8.8317)\n",
      "tensor(6.7636)\n",
      "tensor(0.0152)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 132.719482\n",
      "Epoch 1636\n",
      "-------------------------------\n",
      "tensor(22.7211)\n",
      "tensor(9.0118)\n",
      "tensor(5.8344)\n",
      "tensor(0.2895)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 132.670685\n",
      "Epoch 1637\n",
      "-------------------------------\n",
      "tensor(25.9897)\n",
      "tensor(8.9499)\n",
      "tensor(6.2225)\n",
      "tensor(0.4809)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 132.634583\n",
      "Epoch 1638\n",
      "-------------------------------\n",
      "tensor(17.2685)\n",
      "tensor(7.3510)\n",
      "tensor(6.5330)\n",
      "tensor(0.5195)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 132.609589\n",
      "Epoch 1639\n",
      "-------------------------------\n",
      "tensor(13.2056)\n",
      "tensor(6.6878)\n",
      "tensor(6.3265)\n",
      "tensor(0.4773)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 132.589081\n",
      "Epoch 1640\n",
      "-------------------------------\n",
      "tensor(13.1625)\n",
      "tensor(6.5955)\n",
      "tensor(6.0680)\n",
      "tensor(0.4226)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 132.573792\n",
      "Epoch 1641\n",
      "-------------------------------\n",
      "tensor(22.6522)\n",
      "tensor(8.3187)\n",
      "tensor(5.8859)\n",
      "tensor(0.3816)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 132.568314\n",
      "Epoch 1642\n",
      "-------------------------------\n",
      "tensor(22.5762)\n",
      "tensor(8.3002)\n",
      "tensor(5.6970)\n",
      "tensor(0.3471)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 132.562347\n",
      "Epoch 1643\n",
      "-------------------------------\n",
      "tensor(29.0874)\n",
      "tensor(9.8748)\n",
      "tensor(5.5078)\n",
      "tensor(0.3092)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 132.552917\n",
      "Epoch 1644\n",
      "-------------------------------\n",
      "tensor(24.5702)\n",
      "tensor(9.4978)\n",
      "tensor(5.4679)\n",
      "tensor(0.2643)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 132.536713\n",
      "Epoch 1645\n",
      "-------------------------------\n",
      "tensor(23.3296)\n",
      "tensor(9.0537)\n",
      "tensor(6.0106)\n",
      "tensor(0.2374)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 132.533905\n",
      "Epoch 1646\n",
      "-------------------------------\n",
      "tensor(26.5088)\n",
      "tensor(8.9748)\n",
      "tensor(6.2062)\n",
      "tensor(0.2566)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 132.527786\n",
      "Epoch 1647\n",
      "-------------------------------\n",
      "tensor(25.3243)\n",
      "tensor(8.5691)\n",
      "tensor(5.4725)\n",
      "tensor(0.3459)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 132.500534\n",
      "Epoch 1648\n",
      "-------------------------------\n",
      "tensor(19.0092)\n",
      "tensor(8.0600)\n",
      "tensor(6.0674)\n",
      "tensor(0.4878)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 132.438263\n",
      "Epoch 1649\n",
      "-------------------------------\n",
      "tensor(31.9248)\n",
      "tensor(11.0309)\n",
      "tensor(7.0279)\n",
      "tensor(0.6335)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 132.416519\n",
      "Epoch 1650\n",
      "-------------------------------\n",
      "tensor(17.5412)\n",
      "tensor(6.9346)\n",
      "tensor(6.6035)\n",
      "tensor(0.5908)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 132.367722\n",
      "Epoch 1651\n",
      "-------------------------------\n",
      "tensor(16.2207)\n",
      "tensor(6.0755)\n",
      "tensor(5.9472)\n",
      "tensor(0.4081)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 132.319199\n",
      "Epoch 1652\n",
      "-------------------------------\n",
      "tensor(31.8667)\n",
      "tensor(10.3266)\n",
      "tensor(7.0946)\n",
      "tensor(0.3098)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 132.277084\n",
      "Epoch 1653\n",
      "-------------------------------\n",
      "tensor(16.3833)\n",
      "tensor(7.7878)\n",
      "tensor(5.4170)\n",
      "tensor(0.3947)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 132.211563\n",
      "Epoch 1654\n",
      "-------------------------------\n",
      "tensor(21.4936)\n",
      "tensor(8.6051)\n",
      "tensor(6.1930)\n",
      "tensor(0.4946)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 132.183121\n",
      "Epoch 1655\n",
      "-------------------------------\n",
      "tensor(42.0552)\n",
      "tensor(13.8209)\n",
      "tensor(5.5310)\n",
      "tensor(0.3945)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 132.138626\n",
      "Epoch 1656\n",
      "-------------------------------\n",
      "tensor(23.0157)\n",
      "tensor(8.6284)\n",
      "tensor(5.7681)\n",
      "tensor(0.1516)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 132.067154\n",
      "Epoch 1657\n",
      "-------------------------------\n",
      "tensor(27.9959)\n",
      "tensor(10.2156)\n",
      "tensor(5.8121)\n",
      "tensor(0.1012)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 132.055511\n",
      "Epoch 1658\n",
      "-------------------------------\n",
      "tensor(26.8779)\n",
      "tensor(9.6354)\n",
      "tensor(5.4678)\n",
      "tensor(0.1738)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 132.031219\n",
      "Epoch 1659\n",
      "-------------------------------\n",
      "tensor(17.9889)\n",
      "tensor(7.1389)\n",
      "tensor(5.2858)\n",
      "tensor(0.2746)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 132.015152\n",
      "Epoch 1660\n",
      "-------------------------------\n",
      "tensor(21.2761)\n",
      "tensor(7.8513)\n",
      "tensor(5.3620)\n",
      "tensor(0.3469)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 131.999969\n",
      "Epoch 1661\n",
      "-------------------------------\n",
      "tensor(22.9634)\n",
      "tensor(8.2102)\n",
      "tensor(5.4857)\n",
      "tensor(0.3886)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 131.989395\n",
      "Epoch 1662\n",
      "-------------------------------\n",
      "tensor(18.0525)\n",
      "tensor(7.9501)\n",
      "tensor(5.5646)\n",
      "tensor(0.4148)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 131.984283\n",
      "Epoch 1663\n",
      "-------------------------------\n",
      "tensor(26.5044)\n",
      "tensor(9.5960)\n",
      "tensor(5.6327)\n",
      "tensor(0.4359)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 131.980927\n",
      "Epoch 1664\n",
      "-------------------------------\n",
      "tensor(20.5911)\n",
      "tensor(7.6769)\n",
      "tensor(5.5593)\n",
      "tensor(0.4440)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 131.974075\n",
      "Epoch 1665\n",
      "-------------------------------\n",
      "tensor(15.6917)\n",
      "tensor(6.5842)\n",
      "tensor(5.4045)\n",
      "tensor(0.4302)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 131.954056\n",
      "Epoch 1666\n",
      "-------------------------------\n",
      "tensor(24.4735)\n",
      "tensor(9.1336)\n",
      "tensor(5.3160)\n",
      "tensor(0.4032)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 131.956055\n",
      "Epoch 1667\n",
      "-------------------------------\n",
      "tensor(26.5893)\n",
      "tensor(9.6233)\n",
      "tensor(5.2901)\n",
      "tensor(0.3548)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 131.933182\n",
      "Epoch 1668\n",
      "-------------------------------\n",
      "tensor(26.6779)\n",
      "tensor(9.6020)\n",
      "tensor(5.3846)\n",
      "tensor(0.3268)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 131.882019\n",
      "Epoch 1669\n",
      "-------------------------------\n",
      "tensor(25.3233)\n",
      "tensor(8.4228)\n",
      "tensor(5.3467)\n",
      "tensor(0.3683)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 131.848434\n",
      "Epoch 1670\n",
      "-------------------------------\n",
      "tensor(22.2638)\n",
      "tensor(7.7649)\n",
      "tensor(5.3858)\n",
      "tensor(0.4609)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 131.823120\n",
      "Epoch 1671\n",
      "-------------------------------\n",
      "tensor(38.9320)\n",
      "tensor(13.1203)\n",
      "tensor(5.6713)\n",
      "tensor(0.4995)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 131.771469\n",
      "Epoch 1672\n",
      "-------------------------------\n",
      "tensor(36.9184)\n",
      "tensor(12.0678)\n",
      "tensor(7.1876)\n",
      "tensor(0.3813)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 131.745102\n",
      "Epoch 1673\n",
      "-------------------------------\n",
      "tensor(21.1897)\n",
      "tensor(6.4398)\n",
      "tensor(8.0873)\n",
      "tensor(0.1660)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 131.699081\n",
      "Epoch 1674\n",
      "-------------------------------\n",
      "tensor(17.7646)\n",
      "tensor(7.1704)\n",
      "tensor(6.6007)\n",
      "tensor(0.2075)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 131.645401\n",
      "Epoch 1675\n",
      "-------------------------------\n",
      "tensor(17.4673)\n",
      "tensor(8.1645)\n",
      "tensor(9.2679)\n",
      "tensor(0.4183)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 131.591919\n",
      "Epoch 1676\n",
      "-------------------------------\n",
      "tensor(30.4342)\n",
      "tensor(10.2273)\n",
      "tensor(6.9599)\n",
      "tensor(0.5447)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 131.578140\n",
      "Epoch 1677\n",
      "-------------------------------\n",
      "tensor(34.2834)\n",
      "tensor(10.8154)\n",
      "tensor(7.1589)\n",
      "tensor(0.4372)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 131.543365\n",
      "Epoch 1678\n",
      "-------------------------------\n",
      "tensor(27.7391)\n",
      "tensor(9.5333)\n",
      "tensor(8.7804)\n",
      "tensor(0.2526)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 131.518677\n",
      "Epoch 1679\n",
      "-------------------------------\n",
      "tensor(27.2851)\n",
      "tensor(9.7867)\n",
      "tensor(7.7951)\n",
      "tensor(0.1336)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 131.496506\n",
      "Epoch 1680\n",
      "-------------------------------\n",
      "tensor(16.6292)\n",
      "tensor(6.0015)\n",
      "tensor(6.4418)\n",
      "tensor(0.0952)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 131.481369\n",
      "Epoch 1681\n",
      "-------------------------------\n",
      "tensor(17.2585)\n",
      "tensor(6.6679)\n",
      "tensor(5.7074)\n",
      "tensor(0.1081)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 131.475113\n",
      "Epoch 1682\n",
      "-------------------------------\n",
      "tensor(15.9570)\n",
      "tensor(6.5686)\n",
      "tensor(5.6774)\n",
      "tensor(0.1474)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 131.469803\n",
      "Epoch 1683\n",
      "-------------------------------\n",
      "tensor(15.9539)\n",
      "tensor(6.7228)\n",
      "tensor(6.3193)\n",
      "tensor(0.2228)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 131.460968\n",
      "Epoch 1684\n",
      "-------------------------------\n",
      "tensor(15.1723)\n",
      "tensor(7.5843)\n",
      "tensor(7.6416)\n",
      "tensor(0.3621)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 131.447281\n",
      "Epoch 1685\n",
      "-------------------------------\n",
      "tensor(29.8890)\n",
      "tensor(10.0669)\n",
      "tensor(8.6964)\n",
      "tensor(0.5720)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 131.445404\n",
      "Epoch 1686\n",
      "-------------------------------\n",
      "tensor(38.5449)\n",
      "tensor(11.3202)\n",
      "tensor(7.8137)\n",
      "tensor(0.7440)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 131.442032\n",
      "Epoch 1687\n",
      "-------------------------------\n",
      "tensor(26.1141)\n",
      "tensor(8.4155)\n",
      "tensor(9.8676)\n",
      "tensor(0.6426)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 131.402496\n",
      "Epoch 1688\n",
      "-------------------------------\n",
      "tensor(26.2068)\n",
      "tensor(7.3506)\n",
      "tensor(10.6664)\n",
      "tensor(0.2492)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 131.369675\n",
      "Epoch 1689\n",
      "-------------------------------\n",
      "tensor(24.2295)\n",
      "tensor(9.5736)\n",
      "tensor(7.9295)\n",
      "tensor(0.0619)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 131.340958\n",
      "Epoch 1690\n",
      "-------------------------------\n",
      "tensor(28.9087)\n",
      "tensor(9.9443)\n",
      "tensor(11.6923)\n",
      "tensor(0.2182)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 131.312531\n",
      "Epoch 1691\n",
      "-------------------------------\n",
      "tensor(19.6142)\n",
      "tensor(8.6466)\n",
      "tensor(7.9651)\n",
      "tensor(0.8355)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 131.276978\n",
      "Epoch 1692\n",
      "-------------------------------\n",
      "tensor(22.6099)\n",
      "tensor(8.4555)\n",
      "tensor(11.4977)\n",
      "tensor(0.8460)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 131.223740\n",
      "Epoch 1693\n",
      "-------------------------------\n",
      "tensor(26.1486)\n",
      "tensor(9.8822)\n",
      "tensor(5.1139)\n",
      "tensor(0.2209)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 131.177811\n",
      "Epoch 1694\n",
      "-------------------------------\n",
      "tensor(27.2475)\n",
      "tensor(10.7837)\n",
      "tensor(10.2580)\n",
      "tensor(0.1729)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 131.121460\n",
      "Epoch 1695\n",
      "-------------------------------\n",
      "tensor(47.9075)\n",
      "tensor(14.9036)\n",
      "tensor(6.3462)\n",
      "tensor(0.0795)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 131.084869\n",
      "Epoch 1696\n",
      "-------------------------------\n",
      "tensor(32.8136)\n",
      "tensor(11.1993)\n",
      "tensor(9.6057)\n",
      "tensor(0.3657)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 131.064133\n",
      "Epoch 1697\n",
      "-------------------------------\n",
      "tensor(22.3707)\n",
      "tensor(6.4813)\n",
      "tensor(10.5460)\n",
      "tensor(0.4818)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 131.021545\n",
      "Epoch 1698\n",
      "-------------------------------\n",
      "tensor(27.6867)\n",
      "tensor(9.8116)\n",
      "tensor(5.8966)\n",
      "tensor(0.4623)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 131.014160\n",
      "Epoch 1699\n",
      "-------------------------------\n",
      "tensor(29.2563)\n",
      "tensor(9.5791)\n",
      "tensor(5.9558)\n",
      "tensor(0.3863)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.998978\n",
      "Epoch 1700\n",
      "-------------------------------\n",
      "tensor(21.3394)\n",
      "tensor(7.9470)\n",
      "tensor(6.9761)\n",
      "tensor(0.3326)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 130.986420\n",
      "Epoch 1701\n",
      "-------------------------------\n",
      "tensor(21.6917)\n",
      "tensor(8.7248)\n",
      "tensor(7.0973)\n",
      "tensor(0.3082)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.973969\n",
      "Epoch 1702\n",
      "-------------------------------\n",
      "tensor(26.8922)\n",
      "tensor(9.0936)\n",
      "tensor(6.6417)\n",
      "tensor(0.3011)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 130.964432\n",
      "Epoch 1703\n",
      "-------------------------------\n",
      "tensor(22.9154)\n",
      "tensor(8.9803)\n",
      "tensor(5.6571)\n",
      "tensor(0.3045)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 130.947891\n",
      "Epoch 1704\n",
      "-------------------------------\n",
      "tensor(20.0205)\n",
      "tensor(7.2112)\n",
      "tensor(5.0262)\n",
      "tensor(0.3319)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 130.942459\n",
      "Epoch 1705\n",
      "-------------------------------\n",
      "tensor(23.1289)\n",
      "tensor(8.4779)\n",
      "tensor(6.8307)\n",
      "tensor(0.3807)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 130.942749\n",
      "Epoch 1706\n",
      "-------------------------------\n",
      "tensor(29.1328)\n",
      "tensor(9.2117)\n",
      "tensor(8.0701)\n",
      "tensor(0.4140)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 130.946609\n",
      "Epoch 1707\n",
      "-------------------------------\n",
      "tensor(26.9892)\n",
      "tensor(8.7877)\n",
      "tensor(5.7457)\n",
      "tensor(0.3671)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 130.914200\n",
      "Epoch 1708\n",
      "-------------------------------\n",
      "tensor(22.8953)\n",
      "tensor(7.9257)\n",
      "tensor(6.2328)\n",
      "tensor(0.2869)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 130.886307\n",
      "Epoch 1709\n",
      "-------------------------------\n",
      "tensor(40.3044)\n",
      "tensor(13.5966)\n",
      "tensor(7.6176)\n",
      "tensor(0.3074)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 130.860611\n",
      "Epoch 1710\n",
      "-------------------------------\n",
      "tensor(19.2262)\n",
      "tensor(6.0475)\n",
      "tensor(6.9645)\n",
      "tensor(0.3505)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 130.815765\n",
      "Epoch 1711\n",
      "-------------------------------\n",
      "tensor(26.2002)\n",
      "tensor(8.3009)\n",
      "tensor(7.1792)\n",
      "tensor(0.3789)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 130.777786\n",
      "Epoch 1712\n",
      "-------------------------------\n",
      "tensor(24.2540)\n",
      "tensor(8.2599)\n",
      "tensor(5.6208)\n",
      "tensor(0.3236)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 130.718872\n",
      "Epoch 1713\n",
      "-------------------------------\n",
      "tensor(32.1331)\n",
      "tensor(10.7235)\n",
      "tensor(6.5236)\n",
      "tensor(0.3205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 130.706863\n",
      "Epoch 1714\n",
      "-------------------------------\n",
      "tensor(24.0816)\n",
      "tensor(8.7619)\n",
      "tensor(6.0990)\n",
      "tensor(0.3162)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 130.651093\n",
      "Epoch 1715\n",
      "-------------------------------\n",
      "tensor(24.5090)\n",
      "tensor(7.7545)\n",
      "tensor(7.1193)\n",
      "tensor(0.2917)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 130.609695\n",
      "Epoch 1716\n",
      "-------------------------------\n",
      "tensor(17.7266)\n",
      "tensor(6.4155)\n",
      "tensor(4.9614)\n",
      "tensor(0.2801)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 130.587982\n",
      "Epoch 1717\n",
      "-------------------------------\n",
      "tensor(17.8819)\n",
      "tensor(6.9209)\n",
      "tensor(6.3075)\n",
      "tensor(0.3096)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 130.560379\n",
      "Epoch 1718\n",
      "-------------------------------\n",
      "tensor(16.3625)\n",
      "tensor(6.6929)\n",
      "tensor(6.8052)\n",
      "tensor(0.3606)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 130.539825\n",
      "Epoch 1719\n",
      "-------------------------------\n",
      "tensor(24.2565)\n",
      "tensor(8.2178)\n",
      "tensor(6.0942)\n",
      "tensor(0.3869)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.519531\n",
      "Epoch 1720\n",
      "-------------------------------\n",
      "tensor(16.1672)\n",
      "tensor(7.6037)\n",
      "tensor(5.3240)\n",
      "tensor(0.3887)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 130.504929\n",
      "Epoch 1721\n",
      "-------------------------------\n",
      "tensor(19.5803)\n",
      "tensor(8.1313)\n",
      "tensor(5.0837)\n",
      "tensor(0.3805)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.496216\n",
      "Epoch 1722\n",
      "-------------------------------\n",
      "tensor(19.2786)\n",
      "tensor(6.9065)\n",
      "tensor(5.0480)\n",
      "tensor(0.3662)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 130.488403\n",
      "Epoch 1723\n",
      "-------------------------------\n",
      "tensor(22.5572)\n",
      "tensor(7.6659)\n",
      "tensor(5.0801)\n",
      "tensor(0.3406)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 130.475952\n",
      "Epoch 1724\n",
      "-------------------------------\n",
      "tensor(25.9396)\n",
      "tensor(8.5853)\n",
      "tensor(5.1686)\n",
      "tensor(0.3011)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 130.475815\n",
      "Epoch 1725\n",
      "-------------------------------\n",
      "tensor(27.1637)\n",
      "tensor(9.0480)\n",
      "tensor(5.2045)\n",
      "tensor(0.2416)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 130.473312\n",
      "Epoch 1726\n",
      "-------------------------------\n",
      "tensor(35.6399)\n",
      "tensor(11.7101)\n",
      "tensor(5.1274)\n",
      "tensor(0.1924)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 130.455139\n",
      "Epoch 1727\n",
      "-------------------------------\n",
      "tensor(21.5513)\n",
      "tensor(8.3553)\n",
      "tensor(5.3952)\n",
      "tensor(0.2082)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 130.432297\n",
      "Epoch 1728\n",
      "-------------------------------\n",
      "tensor(15.4256)\n",
      "tensor(7.0846)\n",
      "tensor(4.8796)\n",
      "tensor(0.3479)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 130.407715\n",
      "Epoch 1729\n",
      "-------------------------------\n",
      "tensor(14.4466)\n",
      "tensor(7.6327)\n",
      "tensor(6.4605)\n",
      "tensor(0.5345)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 130.378830\n",
      "Epoch 1730\n",
      "-------------------------------\n",
      "tensor(28.2216)\n",
      "tensor(9.8420)\n",
      "tensor(6.8712)\n",
      "tensor(0.5927)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 130.358597\n",
      "Epoch 1731\n",
      "-------------------------------\n",
      "tensor(23.0577)\n",
      "tensor(8.7824)\n",
      "tensor(5.8910)\n",
      "tensor(0.3825)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 130.293854\n",
      "Epoch 1732\n",
      "-------------------------------\n",
      "tensor(21.6473)\n",
      "tensor(7.3876)\n",
      "tensor(6.0727)\n",
      "tensor(0.1412)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 130.253906\n",
      "Epoch 1733\n",
      "-------------------------------\n",
      "tensor(31.6023)\n",
      "tensor(10.4600)\n",
      "tensor(6.2545)\n",
      "tensor(0.1742)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 130.228714\n",
      "Epoch 1734\n",
      "-------------------------------\n",
      "tensor(38.1664)\n",
      "tensor(12.3048)\n",
      "tensor(5.2794)\n",
      "tensor(0.3958)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 130.207001\n",
      "Epoch 1735\n",
      "-------------------------------\n",
      "tensor(21.7039)\n",
      "tensor(7.1103)\n",
      "tensor(7.3354)\n",
      "tensor(0.4319)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 130.169052\n",
      "Epoch 1736\n",
      "-------------------------------\n",
      "tensor(20.6850)\n",
      "tensor(7.0378)\n",
      "tensor(5.7635)\n",
      "tensor(0.3512)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 130.144104\n",
      "Epoch 1737\n",
      "-------------------------------\n",
      "tensor(32.8888)\n",
      "tensor(10.7788)\n",
      "tensor(5.3012)\n",
      "tensor(0.2658)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 130.122208\n",
      "Epoch 1738\n",
      "-------------------------------\n",
      "tensor(13.8293)\n",
      "tensor(6.2270)\n",
      "tensor(6.1416)\n",
      "tensor(0.2259)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 130.086212\n",
      "Epoch 1739\n",
      "-------------------------------\n",
      "tensor(22.5412)\n",
      "tensor(8.0033)\n",
      "tensor(5.9331)\n",
      "tensor(0.2535)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.077438\n",
      "Epoch 1740\n",
      "-------------------------------\n",
      "tensor(22.3322)\n",
      "tensor(7.8943)\n",
      "tensor(5.3352)\n",
      "tensor(0.2902)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 130.063187\n",
      "Epoch 1741\n",
      "-------------------------------\n",
      "tensor(22.2337)\n",
      "tensor(7.8158)\n",
      "tensor(4.9672)\n",
      "tensor(0.3173)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 130.047623\n",
      "Epoch 1742\n",
      "-------------------------------\n",
      "tensor(13.8044)\n",
      "tensor(5.9174)\n",
      "tensor(4.8182)\n",
      "tensor(0.3381)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 130.033142\n",
      "Epoch 1743\n",
      "-------------------------------\n",
      "tensor(22.6294)\n",
      "tensor(7.8075)\n",
      "tensor(4.8739)\n",
      "tensor(0.3651)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 130.033524\n",
      "Epoch 1744\n",
      "-------------------------------\n",
      "tensor(26.1855)\n",
      "tensor(8.6473)\n",
      "tensor(5.1225)\n",
      "tensor(0.3895)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 130.032593\n",
      "Epoch 1745\n",
      "-------------------------------\n",
      "tensor(29.9671)\n",
      "tensor(9.6431)\n",
      "tensor(5.3591)\n",
      "tensor(0.3879)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 130.026169\n",
      "Epoch 1746\n",
      "-------------------------------\n",
      "tensor(27.7238)\n",
      "tensor(10.0286)\n",
      "tensor(5.4058)\n",
      "tensor(0.3398)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 130.028412\n",
      "Epoch 1747\n",
      "-------------------------------\n",
      "tensor(26.3579)\n",
      "tensor(9.7697)\n",
      "tensor(4.9135)\n",
      "tensor(0.2521)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 130.001999\n",
      "Epoch 1748\n",
      "-------------------------------\n",
      "tensor(33.5706)\n",
      "tensor(11.4736)\n",
      "tensor(4.8974)\n",
      "tensor(0.2258)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.984375\n",
      "Epoch 1749\n",
      "-------------------------------\n",
      "tensor(22.7206)\n",
      "tensor(8.5144)\n",
      "tensor(4.8474)\n",
      "tensor(0.2961)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.956970\n",
      "Epoch 1750\n",
      "-------------------------------\n",
      "tensor(45.6155)\n",
      "tensor(15.4783)\n",
      "tensor(5.2566)\n",
      "tensor(0.4476)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 129.958450\n",
      "Epoch 1751\n",
      "-------------------------------\n",
      "tensor(22.6929)\n",
      "tensor(7.5850)\n",
      "tensor(8.7558)\n",
      "tensor(0.3019)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.929626\n",
      "Epoch 1752\n",
      "-------------------------------\n",
      "tensor(24.3666)\n",
      "tensor(8.0716)\n",
      "tensor(5.0262)\n",
      "tensor(0.1877)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.911392\n",
      "Epoch 1753\n",
      "-------------------------------\n",
      "tensor(25.3090)\n",
      "tensor(8.3140)\n",
      "tensor(7.9111)\n",
      "tensor(0.2463)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 129.867371\n",
      "Epoch 1754\n",
      "-------------------------------\n",
      "tensor(16.7315)\n",
      "tensor(6.2627)\n",
      "tensor(5.1025)\n",
      "tensor(0.3864)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 129.782944\n",
      "Epoch 1755\n",
      "-------------------------------\n",
      "tensor(20.4646)\n",
      "tensor(6.6225)\n",
      "tensor(6.4859)\n",
      "tensor(0.4004)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 129.742325\n",
      "Epoch 1756\n",
      "-------------------------------\n",
      "tensor(24.8594)\n",
      "tensor(8.1356)\n",
      "tensor(5.2382)\n",
      "tensor(0.2608)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 129.708878\n",
      "Epoch 1757\n",
      "-------------------------------\n",
      "tensor(25.9908)\n",
      "tensor(8.8160)\n",
      "tensor(4.9454)\n",
      "tensor(0.1512)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 129.692429\n",
      "Epoch 1758\n",
      "-------------------------------\n",
      "tensor(19.1922)\n",
      "tensor(7.0696)\n",
      "tensor(5.1789)\n",
      "tensor(0.1308)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 129.662384\n",
      "Epoch 1759\n",
      "-------------------------------\n",
      "tensor(26.3590)\n",
      "tensor(8.8651)\n",
      "tensor(5.0407)\n",
      "tensor(0.1746)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 129.642654\n",
      "Epoch 1760\n",
      "-------------------------------\n",
      "tensor(22.2925)\n",
      "tensor(7.6549)\n",
      "tensor(4.7991)\n",
      "tensor(0.2246)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 129.628647\n",
      "Epoch 1761\n",
      "-------------------------------\n",
      "tensor(28.5750)\n",
      "tensor(9.2666)\n",
      "tensor(4.7400)\n",
      "tensor(0.2626)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 129.617645\n",
      "Epoch 1762\n",
      "-------------------------------\n",
      "tensor(19.9511)\n",
      "tensor(6.8799)\n",
      "tensor(4.7976)\n",
      "tensor(0.2920)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 129.609863\n",
      "Epoch 1763\n",
      "-------------------------------\n",
      "tensor(20.0533)\n",
      "tensor(6.8433)\n",
      "tensor(4.9547)\n",
      "tensor(0.3236)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 129.602997\n",
      "Epoch 1764\n",
      "-------------------------------\n",
      "tensor(26.0383)\n",
      "tensor(8.4111)\n",
      "tensor(5.1586)\n",
      "tensor(0.3610)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 129.605255\n",
      "Epoch 1765\n",
      "-------------------------------\n",
      "tensor(31.4632)\n",
      "tensor(9.8858)\n",
      "tensor(5.2232)\n",
      "tensor(0.3805)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 129.601028\n",
      "Epoch 1766\n",
      "-------------------------------\n",
      "tensor(17.5225)\n",
      "tensor(7.1624)\n",
      "tensor(5.1588)\n",
      "tensor(0.3483)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 129.586731\n",
      "Epoch 1767\n",
      "-------------------------------\n",
      "tensor(17.6455)\n",
      "tensor(8.5773)\n",
      "tensor(4.6650)\n",
      "tensor(0.2888)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 129.573502\n",
      "Epoch 1768\n",
      "-------------------------------\n",
      "tensor(14.6472)\n",
      "tensor(6.9330)\n",
      "tensor(5.8679)\n",
      "tensor(0.2619)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.543365\n",
      "Epoch 1769\n",
      "-------------------------------\n",
      "tensor(25.9269)\n",
      "tensor(9.5980)\n",
      "tensor(5.9249)\n",
      "tensor(0.3588)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.519348\n",
      "Epoch 1770\n",
      "-------------------------------\n",
      "tensor(24.0759)\n",
      "tensor(8.0272)\n",
      "tensor(5.6268)\n",
      "tensor(0.4600)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 129.481934\n",
      "Epoch 1771\n",
      "-------------------------------\n",
      "tensor(28.4860)\n",
      "tensor(9.1705)\n",
      "tensor(6.1661)\n",
      "tensor(0.3772)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.441666\n",
      "Epoch 1772\n",
      "-------------------------------\n",
      "tensor(29.8648)\n",
      "tensor(9.9226)\n",
      "tensor(4.7198)\n",
      "tensor(0.1983)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.413040\n",
      "Epoch 1773\n",
      "-------------------------------\n",
      "tensor(16.9584)\n",
      "tensor(6.6684)\n",
      "tensor(5.1311)\n",
      "tensor(0.1974)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 129.385300\n",
      "Epoch 1774\n",
      "-------------------------------\n",
      "tensor(16.2221)\n",
      "tensor(6.7972)\n",
      "tensor(5.2647)\n",
      "tensor(0.3964)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 129.363220\n",
      "Epoch 1775\n",
      "-------------------------------\n",
      "tensor(15.8722)\n",
      "tensor(7.8498)\n",
      "tensor(5.6341)\n",
      "tensor(0.5181)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 129.332413\n",
      "Epoch 1776\n",
      "-------------------------------\n",
      "tensor(14.0526)\n",
      "tensor(7.4374)\n",
      "tensor(5.1782)\n",
      "tensor(0.4526)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 129.291428\n",
      "Epoch 1777\n",
      "-------------------------------\n",
      "tensor(23.9063)\n",
      "tensor(8.3530)\n",
      "tensor(4.7814)\n",
      "tensor(0.3226)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 129.272186\n",
      "Epoch 1778\n",
      "-------------------------------\n",
      "tensor(28.9662)\n",
      "tensor(9.7507)\n",
      "tensor(4.6427)\n",
      "tensor(0.2249)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 129.247757\n",
      "Epoch 1779\n",
      "-------------------------------\n",
      "tensor(21.6340)\n",
      "tensor(7.8595)\n",
      "tensor(4.5970)\n",
      "tensor(0.1861)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 129.219635\n",
      "Epoch 1780\n",
      "-------------------------------\n",
      "tensor(17.6387)\n",
      "tensor(6.7875)\n",
      "tensor(4.5821)\n",
      "tensor(0.1931)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 129.206955\n",
      "Epoch 1781\n",
      "-------------------------------\n",
      "tensor(17.5741)\n",
      "tensor(6.8055)\n",
      "tensor(4.5369)\n",
      "tensor(0.2142)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 129.195770\n",
      "Epoch 1782\n",
      "-------------------------------\n",
      "tensor(24.8113)\n",
      "tensor(8.5982)\n",
      "tensor(4.5126)\n",
      "tensor(0.2474)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 129.198166\n",
      "Epoch 1783\n",
      "-------------------------------\n",
      "tensor(24.7580)\n",
      "tensor(8.4995)\n",
      "tensor(4.5581)\n",
      "tensor(0.2937)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 129.197525\n",
      "Epoch 1784\n",
      "-------------------------------\n",
      "tensor(31.4134)\n",
      "tensor(10.0564)\n",
      "tensor(4.7483)\n",
      "tensor(0.3558)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 129.184952\n",
      "Epoch 1785\n",
      "-------------------------------\n",
      "tensor(21.0259)\n",
      "tensor(8.4114)\n",
      "tensor(4.8990)\n",
      "tensor(0.4214)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 129.174179\n",
      "Epoch 1786\n",
      "-------------------------------\n",
      "tensor(22.6694)\n",
      "tensor(8.7171)\n",
      "tensor(4.9997)\n",
      "tensor(0.4533)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 129.166779\n",
      "Epoch 1787\n",
      "-------------------------------\n",
      "tensor(24.3740)\n",
      "tensor(8.1733)\n",
      "tensor(4.7694)\n",
      "tensor(0.4102)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 129.166031\n",
      "Epoch 1788\n",
      "-------------------------------\n",
      "tensor(14.6416)\n",
      "tensor(5.6261)\n",
      "tensor(4.5556)\n",
      "tensor(0.2806)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.126678\n",
      "Epoch 1789\n",
      "-------------------------------\n",
      "tensor(14.2224)\n",
      "tensor(5.6986)\n",
      "tensor(5.2860)\n",
      "tensor(0.2328)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.090759\n",
      "Epoch 1790\n",
      "-------------------------------\n",
      "tensor(55.4726)\n",
      "tensor(17.4951)\n",
      "tensor(5.3732)\n",
      "tensor(0.3614)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 129.106049\n",
      "Epoch 1791\n",
      "-------------------------------\n",
      "tensor(35.8498)\n",
      "tensor(12.3668)\n",
      "tensor(15.3952)\n",
      "tensor(0.2417)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 129.066788\n",
      "Epoch 1792\n",
      "-------------------------------\n",
      "tensor(23.9421)\n",
      "tensor(8.0790)\n",
      "tensor(8.8833)\n",
      "tensor(0.0815)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 129.019608\n",
      "Epoch 1793\n",
      "-------------------------------\n",
      "tensor(42.2541)\n",
      "tensor(12.1934)\n",
      "tensor(12.9074)\n",
      "tensor(0.1833)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 129.014725\n",
      "Epoch 1794\n",
      "-------------------------------\n",
      "tensor(26.2134)\n",
      "tensor(8.4260)\n",
      "tensor(6.8596)\n",
      "tensor(0.4368)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.967621\n",
      "Epoch 1795\n",
      "-------------------------------\n",
      "tensor(22.8181)\n",
      "tensor(6.4006)\n",
      "tensor(10.8924)\n",
      "tensor(0.5042)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.947937\n",
      "Epoch 1796\n",
      "-------------------------------\n",
      "tensor(20.5562)\n",
      "tensor(5.2126)\n",
      "tensor(9.0341)\n",
      "tensor(0.2726)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.913391\n",
      "Epoch 1797\n",
      "-------------------------------\n",
      "tensor(17.6811)\n",
      "tensor(6.5519)\n",
      "tensor(5.4703)\n",
      "tensor(0.0442)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.873062\n",
      "Epoch 1798\n",
      "-------------------------------\n",
      "tensor(34.4258)\n",
      "tensor(11.1254)\n",
      "tensor(8.8733)\n",
      "tensor(0.0087)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.856186\n",
      "Epoch 1799\n",
      "-------------------------------\n",
      "tensor(30.6247)\n",
      "tensor(9.9557)\n",
      "tensor(7.5337)\n",
      "tensor(0.0831)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.834885\n",
      "Epoch 1800\n",
      "-------------------------------\n",
      "tensor(18.2806)\n",
      "tensor(6.6049)\n",
      "tensor(5.2404)\n",
      "tensor(0.1743)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 128.829239\n",
      "Epoch 1801\n",
      "-------------------------------\n",
      "tensor(18.1317)\n",
      "tensor(6.2957)\n",
      "tensor(4.6155)\n",
      "tensor(0.2420)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.825470\n",
      "Epoch 1802\n",
      "-------------------------------\n",
      "tensor(19.8278)\n",
      "tensor(6.5511)\n",
      "tensor(4.9907)\n",
      "tensor(0.2917)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.820129\n",
      "Epoch 1803\n",
      "-------------------------------\n",
      "tensor(25.6228)\n",
      "tensor(8.0827)\n",
      "tensor(5.7963)\n",
      "tensor(0.3371)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.813583\n",
      "Epoch 1804\n",
      "-------------------------------\n",
      "tensor(22.5672)\n",
      "tensor(7.0308)\n",
      "tensor(6.7084)\n",
      "tensor(0.3715)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.808228\n",
      "Epoch 1805\n",
      "-------------------------------\n",
      "tensor(28.0995)\n",
      "tensor(8.7726)\n",
      "tensor(6.4100)\n",
      "tensor(0.3631)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.801956\n",
      "Epoch 1806\n",
      "-------------------------------\n",
      "tensor(30.3237)\n",
      "tensor(10.3485)\n",
      "tensor(4.8407)\n",
      "tensor(0.2789)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.803375\n",
      "Epoch 1807\n",
      "-------------------------------\n",
      "tensor(24.5553)\n",
      "tensor(9.0219)\n",
      "tensor(4.9727)\n",
      "tensor(0.1417)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 128.791809\n",
      "Epoch 1808\n",
      "-------------------------------\n",
      "tensor(15.0241)\n",
      "tensor(6.7376)\n",
      "tensor(5.5187)\n",
      "tensor(0.1418)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 128.766403\n",
      "Epoch 1809\n",
      "-------------------------------\n",
      "tensor(14.2949)\n",
      "tensor(5.8225)\n",
      "tensor(4.7865)\n",
      "tensor(0.3751)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 128.742783\n",
      "Epoch 1810\n",
      "-------------------------------\n",
      "tensor(17.7886)\n",
      "tensor(7.3783)\n",
      "tensor(5.8508)\n",
      "tensor(0.6053)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 128.707153\n",
      "Epoch 1811\n",
      "-------------------------------\n",
      "tensor(23.5781)\n",
      "tensor(9.6591)\n",
      "tensor(5.1765)\n",
      "tensor(0.5019)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 128.654205\n",
      "Epoch 1812\n",
      "-------------------------------\n",
      "tensor(30.4758)\n",
      "tensor(10.1060)\n",
      "tensor(4.4175)\n",
      "tensor(0.1981)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 128.646637\n",
      "Epoch 1813\n",
      "-------------------------------\n",
      "tensor(27.3488)\n",
      "tensor(9.6838)\n",
      "tensor(5.3676)\n",
      "tensor(0.0394)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 128.613647\n",
      "Epoch 1814\n",
      "-------------------------------\n",
      "tensor(19.5466)\n",
      "tensor(7.2866)\n",
      "tensor(4.4277)\n",
      "tensor(0.1879)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.562286\n",
      "Epoch 1815\n",
      "-------------------------------\n",
      "tensor(23.7539)\n",
      "tensor(8.3987)\n",
      "tensor(5.7339)\n",
      "tensor(0.4493)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.540176\n",
      "Epoch 1816\n",
      "-------------------------------\n",
      "tensor(15.8260)\n",
      "tensor(8.0178)\n",
      "tensor(6.1470)\n",
      "tensor(0.5280)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.522049\n",
      "Epoch 1817\n",
      "-------------------------------\n",
      "tensor(19.0956)\n",
      "tensor(8.2635)\n",
      "tensor(5.1717)\n",
      "tensor(0.4362)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.495865\n",
      "Epoch 1818\n",
      "-------------------------------\n",
      "tensor(14.3044)\n",
      "tensor(7.2771)\n",
      "tensor(4.4254)\n",
      "tensor(0.3094)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.473999\n",
      "Epoch 1819\n",
      "-------------------------------\n",
      "tensor(20.3970)\n",
      "tensor(9.0616)\n",
      "tensor(4.3767)\n",
      "tensor(0.2337)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.462036\n",
      "Epoch 1820\n",
      "-------------------------------\n",
      "tensor(19.6401)\n",
      "tensor(7.3021)\n",
      "tensor(4.4358)\n",
      "tensor(0.2068)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 128.448334\n",
      "Epoch 1821\n",
      "-------------------------------\n",
      "tensor(26.2153)\n",
      "tensor(8.9583)\n",
      "tensor(4.4682)\n",
      "tensor(0.2067)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.438522\n",
      "Epoch 1822\n",
      "-------------------------------\n",
      "tensor(29.7913)\n",
      "tensor(9.8654)\n",
      "tensor(4.4205)\n",
      "tensor(0.2211)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.438538\n",
      "Epoch 1823\n",
      "-------------------------------\n",
      "tensor(24.5838)\n",
      "tensor(8.4371)\n",
      "tensor(4.3246)\n",
      "tensor(0.2472)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.435150\n",
      "Epoch 1824\n",
      "-------------------------------\n",
      "tensor(27.6770)\n",
      "tensor(9.1443)\n",
      "tensor(4.3254)\n",
      "tensor(0.2953)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.428680\n",
      "Epoch 1825\n",
      "-------------------------------\n",
      "tensor(21.0844)\n",
      "tensor(7.3716)\n",
      "tensor(4.6427)\n",
      "tensor(0.3622)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.424545\n",
      "Epoch 1826\n",
      "-------------------------------\n",
      "tensor(23.6226)\n",
      "tensor(8.0075)\n",
      "tensor(4.7846)\n",
      "tensor(0.4151)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.413406\n",
      "Epoch 1827\n",
      "-------------------------------\n",
      "tensor(17.9928)\n",
      "tensor(7.7080)\n",
      "tensor(4.6057)\n",
      "tensor(0.4017)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 128.400589\n",
      "Epoch 1828\n",
      "-------------------------------\n",
      "tensor(18.4882)\n",
      "tensor(7.7565)\n",
      "tensor(5.2310)\n",
      "tensor(0.3237)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 128.381744\n",
      "Epoch 1829\n",
      "-------------------------------\n",
      "tensor(19.9365)\n",
      "tensor(8.6863)\n",
      "tensor(4.8702)\n",
      "tensor(0.2827)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 128.350113\n",
      "Epoch 1830\n",
      "-------------------------------\n",
      "tensor(28.7113)\n",
      "tensor(9.1512)\n",
      "tensor(4.4713)\n",
      "tensor(0.3054)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 128.311096\n",
      "Epoch 1831\n",
      "-------------------------------\n",
      "tensor(23.9761)\n",
      "tensor(7.4943)\n",
      "tensor(6.2921)\n",
      "tensor(0.2946)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 128.318787\n",
      "Epoch 1832\n",
      "-------------------------------\n",
      "tensor(46.9083)\n",
      "tensor(14.4868)\n",
      "tensor(4.4140)\n",
      "tensor(0.1965)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 128.285049\n",
      "Epoch 1833\n",
      "-------------------------------\n",
      "tensor(28.7818)\n",
      "tensor(9.5196)\n",
      "tensor(7.4462)\n",
      "tensor(0.0082)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 128.236633\n",
      "Epoch 1834\n",
      "-------------------------------\n",
      "tensor(29.2417)\n",
      "tensor(9.2451)\n",
      "tensor(6.1538)\n",
      "tensor(0.1015)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.208160\n",
      "Epoch 1835\n",
      "-------------------------------\n",
      "tensor(18.9520)\n",
      "tensor(7.4475)\n",
      "tensor(4.4762)\n",
      "tensor(0.2942)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.190643\n",
      "Epoch 1836\n",
      "-------------------------------\n",
      "tensor(21.3415)\n",
      "tensor(8.1054)\n",
      "tensor(5.9710)\n",
      "tensor(0.4197)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.185226\n",
      "Epoch 1837\n",
      "-------------------------------\n",
      "tensor(20.3974)\n",
      "tensor(7.0099)\n",
      "tensor(5.2271)\n",
      "tensor(0.4047)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.155426\n",
      "Epoch 1838\n",
      "-------------------------------\n",
      "tensor(30.6648)\n",
      "tensor(9.5034)\n",
      "tensor(4.6422)\n",
      "tensor(0.3274)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.153229\n",
      "Epoch 1839\n",
      "-------------------------------\n",
      "tensor(28.4113)\n",
      "tensor(8.9482)\n",
      "tensor(5.3344)\n",
      "tensor(0.2376)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.138062\n",
      "Epoch 1840\n",
      "-------------------------------\n",
      "tensor(23.2180)\n",
      "tensor(7.1976)\n",
      "tensor(5.8705)\n",
      "tensor(0.1792)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 128.124649\n",
      "Epoch 1841\n",
      "-------------------------------\n",
      "tensor(16.6625)\n",
      "tensor(4.8578)\n",
      "tensor(5.7599)\n",
      "tensor(0.1529)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 128.116821\n",
      "Epoch 1842\n",
      "-------------------------------\n",
      "tensor(16.0104)\n",
      "tensor(4.8979)\n",
      "tensor(5.2463)\n",
      "tensor(0.1457)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 128.112961\n",
      "Epoch 1843\n",
      "-------------------------------\n",
      "tensor(14.9720)\n",
      "tensor(5.0187)\n",
      "tensor(4.5633)\n",
      "tensor(0.1534)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 128.105347\n",
      "Epoch 1844\n",
      "-------------------------------\n",
      "tensor(16.0893)\n",
      "tensor(5.8205)\n",
      "tensor(4.7949)\n",
      "tensor(0.1886)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 128.091583\n",
      "Epoch 1845\n",
      "-------------------------------\n",
      "tensor(28.9891)\n",
      "tensor(8.9910)\n",
      "tensor(6.7867)\n",
      "tensor(0.2726)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 128.085144\n",
      "Epoch 1846\n",
      "-------------------------------\n",
      "tensor(33.8506)\n",
      "tensor(10.6595)\n",
      "tensor(6.7868)\n",
      "tensor(0.3710)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 128.075897\n",
      "Epoch 1847\n",
      "-------------------------------\n",
      "tensor(31.5017)\n",
      "tensor(10.4891)\n",
      "tensor(5.4233)\n",
      "tensor(0.3965)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 128.070526\n",
      "Epoch 1848\n",
      "-------------------------------\n",
      "tensor(26.1615)\n",
      "tensor(7.0961)\n",
      "tensor(10.4700)\n",
      "tensor(0.2397)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 128.058029\n",
      "Epoch 1849\n",
      "-------------------------------\n",
      "tensor(19.5446)\n",
      "tensor(8.3335)\n",
      "tensor(5.7851)\n",
      "tensor(0.0231)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 128.019409\n",
      "Epoch 1850\n",
      "-------------------------------\n",
      "tensor(26.3802)\n",
      "tensor(8.6291)\n",
      "tensor(10.8118)\n",
      "tensor(0.1428)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 128.016373\n",
      "Epoch 1851\n",
      "-------------------------------\n",
      "tensor(14.8434)\n",
      "tensor(7.2679)\n",
      "tensor(7.4684)\n",
      "tensor(0.5654)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.956451\n",
      "Epoch 1852\n",
      "-------------------------------\n",
      "tensor(24.8806)\n",
      "tensor(9.3940)\n",
      "tensor(9.3956)\n",
      "tensor(0.6864)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.932243\n",
      "Epoch 1853\n",
      "-------------------------------\n",
      "tensor(34.6927)\n",
      "tensor(11.3789)\n",
      "tensor(5.8357)\n",
      "tensor(0.2135)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.904488\n",
      "Epoch 1854\n",
      "-------------------------------\n",
      "tensor(23.9398)\n",
      "tensor(9.3111)\n",
      "tensor(6.9552)\n",
      "tensor(0.1744)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.884987\n",
      "Epoch 1855\n",
      "-------------------------------\n",
      "tensor(14.5710)\n",
      "tensor(6.4392)\n",
      "tensor(6.9322)\n",
      "tensor(0.0230)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.855682\n",
      "Epoch 1856\n",
      "-------------------------------\n",
      "tensor(15.0807)\n",
      "tensor(7.7555)\n",
      "tensor(6.2325)\n",
      "tensor(0.4742)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.839012\n",
      "Epoch 1857\n",
      "-------------------------------\n",
      "tensor(17.2260)\n",
      "tensor(7.9883)\n",
      "tensor(7.1549)\n",
      "tensor(0.6990)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.818863\n",
      "Epoch 1858\n",
      "-------------------------------\n",
      "tensor(19.1807)\n",
      "tensor(7.8356)\n",
      "tensor(6.5013)\n",
      "tensor(0.6367)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.792358\n",
      "Epoch 1859\n",
      "-------------------------------\n",
      "tensor(23.5460)\n",
      "tensor(8.1242)\n",
      "tensor(5.0915)\n",
      "tensor(0.4703)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.783577\n",
      "Epoch 1860\n",
      "-------------------------------\n",
      "tensor(23.6768)\n",
      "tensor(8.9203)\n",
      "tensor(4.2679)\n",
      "tensor(0.3237)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 127.770828\n",
      "Epoch 1861\n",
      "-------------------------------\n",
      "tensor(24.5869)\n",
      "tensor(8.3287)\n",
      "tensor(4.1267)\n",
      "tensor(0.2286)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.759682\n",
      "Epoch 1862\n",
      "-------------------------------\n",
      "tensor(19.4117)\n",
      "tensor(7.0305)\n",
      "tensor(4.2564)\n",
      "tensor(0.1648)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.749657\n",
      "Epoch 1863\n",
      "-------------------------------\n",
      "tensor(16.0391)\n",
      "tensor(6.2542)\n",
      "tensor(4.5112)\n",
      "tensor(0.1158)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.741432\n",
      "Epoch 1864\n",
      "-------------------------------\n",
      "tensor(23.7713)\n",
      "tensor(10.7794)\n",
      "tensor(4.8061)\n",
      "tensor(0.0971)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.744949\n",
      "Epoch 1865\n",
      "-------------------------------\n",
      "tensor(38.9307)\n",
      "tensor(12.2576)\n",
      "tensor(4.7422)\n",
      "tensor(0.1478)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.757500\n",
      "Epoch 1866\n",
      "-------------------------------\n",
      "tensor(31.7533)\n",
      "tensor(10.2122)\n",
      "tensor(4.1955)\n",
      "tensor(0.2403)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.748672\n",
      "Epoch 1867\n",
      "-------------------------------\n",
      "tensor(21.2241)\n",
      "tensor(6.7844)\n",
      "tensor(6.3998)\n",
      "tensor(0.3513)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.714378\n",
      "Epoch 1868\n",
      "-------------------------------\n",
      "tensor(25.7925)\n",
      "tensor(9.1765)\n",
      "tensor(5.1024)\n",
      "tensor(0.3988)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.700989\n",
      "Epoch 1869\n",
      "-------------------------------\n",
      "tensor(24.0132)\n",
      "tensor(7.8339)\n",
      "tensor(5.1636)\n",
      "tensor(0.3109)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.685371\n",
      "Epoch 1870\n",
      "-------------------------------\n",
      "tensor(18.5278)\n",
      "tensor(6.4117)\n",
      "tensor(5.3250)\n",
      "tensor(0.2379)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 127.669022\n",
      "Epoch 1871\n",
      "-------------------------------\n",
      "tensor(27.7853)\n",
      "tensor(8.7533)\n",
      "tensor(4.3632)\n",
      "tensor(0.2967)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.653671\n",
      "Epoch 1872\n",
      "-------------------------------\n",
      "tensor(19.0457)\n",
      "tensor(5.6606)\n",
      "tensor(6.6494)\n",
      "tensor(0.3163)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.635017\n",
      "Epoch 1873\n",
      "-------------------------------\n",
      "tensor(20.2417)\n",
      "tensor(7.8416)\n",
      "tensor(4.4787)\n",
      "tensor(0.2552)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.609055\n",
      "Epoch 1874\n",
      "-------------------------------\n",
      "tensor(26.1029)\n",
      "tensor(9.1887)\n",
      "tensor(6.3581)\n",
      "tensor(0.2244)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.575851\n",
      "Epoch 1875\n",
      "-------------------------------\n",
      "tensor(34.3294)\n",
      "tensor(10.9143)\n",
      "tensor(4.3120)\n",
      "tensor(0.2404)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.541176\n",
      "Epoch 1876\n",
      "-------------------------------\n",
      "tensor(34.4472)\n",
      "tensor(10.7990)\n",
      "tensor(8.2033)\n",
      "tensor(0.1509)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.548035\n",
      "Epoch 1877\n",
      "-------------------------------\n",
      "tensor(25.7821)\n",
      "tensor(9.8793)\n",
      "tensor(8.0871)\n",
      "tensor(0.0311)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.522041\n",
      "Epoch 1878\n",
      "-------------------------------\n",
      "tensor(22.5096)\n",
      "tensor(7.7158)\n",
      "tensor(5.0022)\n",
      "tensor(0.0081)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.476036\n",
      "Epoch 1879\n",
      "-------------------------------\n",
      "tensor(22.8011)\n",
      "tensor(7.8712)\n",
      "tensor(5.0988)\n",
      "tensor(0.0617)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.449219\n",
      "Epoch 1880\n",
      "-------------------------------\n",
      "tensor(29.7879)\n",
      "tensor(9.5398)\n",
      "tensor(5.6468)\n",
      "tensor(0.1351)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 127.442299\n",
      "Epoch 1881\n",
      "-------------------------------\n",
      "tensor(23.8892)\n",
      "tensor(7.8987)\n",
      "tensor(5.3630)\n",
      "tensor(0.1927)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.435852\n",
      "Epoch 1882\n",
      "-------------------------------\n",
      "tensor(22.9829)\n",
      "tensor(7.5979)\n",
      "tensor(4.8551)\n",
      "tensor(0.2407)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.430534\n",
      "Epoch 1883\n",
      "-------------------------------\n",
      "tensor(22.2316)\n",
      "tensor(7.3296)\n",
      "tensor(4.4087)\n",
      "tensor(0.2923)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.424599\n",
      "Epoch 1884\n",
      "-------------------------------\n",
      "tensor(18.4136)\n",
      "tensor(6.1752)\n",
      "tensor(4.9299)\n",
      "tensor(0.3469)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.418045\n",
      "Epoch 1885\n",
      "-------------------------------\n",
      "tensor(26.3490)\n",
      "tensor(8.2123)\n",
      "tensor(6.3690)\n",
      "tensor(0.3724)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.413513\n",
      "Epoch 1886\n",
      "-------------------------------\n",
      "tensor(25.1412)\n",
      "tensor(7.7527)\n",
      "tensor(6.5201)\n",
      "tensor(0.3081)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.420456\n",
      "Epoch 1887\n",
      "-------------------------------\n",
      "tensor(23.8900)\n",
      "tensor(8.6841)\n",
      "tensor(4.3935)\n",
      "tensor(0.1500)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.413315\n",
      "Epoch 1888\n",
      "-------------------------------\n",
      "tensor(26.9472)\n",
      "tensor(11.0570)\n",
      "tensor(6.4888)\n",
      "tensor(0.0617)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.390068\n",
      "Epoch 1889\n",
      "-------------------------------\n",
      "tensor(19.7567)\n",
      "tensor(7.6525)\n",
      "tensor(4.8416)\n",
      "tensor(0.2456)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.362801\n",
      "Epoch 1890\n",
      "-------------------------------\n",
      "tensor(21.5107)\n",
      "tensor(7.6310)\n",
      "tensor(6.0514)\n",
      "tensor(0.5392)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 127.341209\n",
      "Epoch 1891\n",
      "-------------------------------\n",
      "tensor(22.3076)\n",
      "tensor(7.7346)\n",
      "tensor(5.6628)\n",
      "tensor(0.5033)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.319244\n",
      "Epoch 1892\n",
      "-------------------------------\n",
      "tensor(31.9139)\n",
      "tensor(10.1210)\n",
      "tensor(4.7616)\n",
      "tensor(0.1985)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.307159\n",
      "Epoch 1893\n",
      "-------------------------------\n",
      "tensor(22.8087)\n",
      "tensor(7.9454)\n",
      "tensor(4.6924)\n",
      "tensor(0.0462)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.259796\n",
      "Epoch 1894\n",
      "-------------------------------\n",
      "tensor(15.8180)\n",
      "tensor(5.6681)\n",
      "tensor(4.3616)\n",
      "tensor(0.2101)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.251259\n",
      "Epoch 1895\n",
      "-------------------------------\n",
      "tensor(22.5605)\n",
      "tensor(7.8498)\n",
      "tensor(5.1685)\n",
      "tensor(0.3856)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.236481\n",
      "Epoch 1896\n",
      "-------------------------------\n",
      "tensor(15.7272)\n",
      "tensor(6.6690)\n",
      "tensor(5.8307)\n",
      "tensor(0.3915)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.211365\n",
      "Epoch 1897\n",
      "-------------------------------\n",
      "tensor(16.1517)\n",
      "tensor(6.5027)\n",
      "tensor(4.9111)\n",
      "tensor(0.2960)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.183502\n",
      "Epoch 1898\n",
      "-------------------------------\n",
      "tensor(21.8783)\n",
      "tensor(7.6749)\n",
      "tensor(4.1964)\n",
      "tensor(0.2034)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.161491\n",
      "Epoch 1899\n",
      "-------------------------------\n",
      "tensor(24.0671)\n",
      "tensor(8.2530)\n",
      "tensor(4.0419)\n",
      "tensor(0.1552)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.149620\n",
      "Epoch 1900\n",
      "-------------------------------\n",
      "tensor(27.2826)\n",
      "tensor(10.0349)\n",
      "tensor(4.0764)\n",
      "tensor(0.1412)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 127.138519\n",
      "Epoch 1901\n",
      "-------------------------------\n",
      "tensor(24.7112)\n",
      "tensor(9.3350)\n",
      "tensor(4.1031)\n",
      "tensor(0.1443)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 127.129936\n",
      "Epoch 1902\n",
      "-------------------------------\n",
      "tensor(24.1789)\n",
      "tensor(9.0764)\n",
      "tensor(4.0891)\n",
      "tensor(0.1567)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 127.124275\n",
      "Epoch 1903\n",
      "-------------------------------\n",
      "tensor(16.7314)\n",
      "tensor(6.1074)\n",
      "tensor(4.0294)\n",
      "tensor(0.1789)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 127.117172\n",
      "Epoch 1904\n",
      "-------------------------------\n",
      "tensor(16.8555)\n",
      "tensor(6.2324)\n",
      "tensor(3.9954)\n",
      "tensor(0.2254)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 127.109016\n",
      "Epoch 1905\n",
      "-------------------------------\n",
      "tensor(27.7946)\n",
      "tensor(8.9097)\n",
      "tensor(4.5703)\n",
      "tensor(0.3069)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 127.117531\n",
      "Epoch 1906\n",
      "-------------------------------\n",
      "tensor(33.2313)\n",
      "tensor(10.1148)\n",
      "tensor(4.9395)\n",
      "tensor(0.3836)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 127.119675\n",
      "Epoch 1907\n",
      "-------------------------------\n",
      "tensor(24.9978)\n",
      "tensor(8.1517)\n",
      "tensor(4.5479)\n",
      "tensor(0.3858)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.109947\n",
      "Epoch 1908\n",
      "-------------------------------\n",
      "tensor(25.3665)\n",
      "tensor(8.0008)\n",
      "tensor(5.6519)\n",
      "tensor(0.2844)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.070824\n",
      "Epoch 1909\n",
      "-------------------------------\n",
      "tensor(23.0439)\n",
      "tensor(7.2793)\n",
      "tensor(4.6974)\n",
      "tensor(0.2014)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.068039\n",
      "Epoch 1910\n",
      "-------------------------------\n",
      "tensor(20.0275)\n",
      "tensor(7.7277)\n",
      "tensor(5.2809)\n",
      "tensor(0.2789)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 127.070351\n",
      "Epoch 1911\n",
      "-------------------------------\n",
      "tensor(18.8342)\n",
      "tensor(7.9783)\n",
      "tensor(5.4307)\n",
      "tensor(0.4788)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 127.047096\n",
      "Epoch 1912\n",
      "-------------------------------\n",
      "tensor(19.6527)\n",
      "tensor(6.9348)\n",
      "tensor(5.6202)\n",
      "tensor(0.4790)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 127.032524\n",
      "Epoch 1913\n",
      "-------------------------------\n",
      "tensor(26.7706)\n",
      "tensor(8.7122)\n",
      "tensor(4.2469)\n",
      "tensor(0.1553)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 127.005447\n",
      "Epoch 1914\n",
      "-------------------------------\n",
      "tensor(21.4661)\n",
      "tensor(10.0291)\n",
      "tensor(5.3447)\n",
      "tensor(0.1001)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.951820\n",
      "Epoch 1915\n",
      "-------------------------------\n",
      "tensor(19.7380)\n",
      "tensor(7.4004)\n",
      "tensor(4.9410)\n",
      "tensor(0.0034)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.918839\n",
      "Epoch 1916\n",
      "-------------------------------\n",
      "tensor(45.1507)\n",
      "tensor(13.9867)\n",
      "tensor(4.4583)\n",
      "tensor(0.2440)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.902069\n",
      "Epoch 1917\n",
      "-------------------------------\n",
      "tensor(18.1745)\n",
      "tensor(5.1988)\n",
      "tensor(7.0384)\n",
      "tensor(0.2568)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.865067\n",
      "Epoch 1918\n",
      "-------------------------------\n",
      "tensor(21.3966)\n",
      "tensor(6.4996)\n",
      "tensor(6.3961)\n",
      "tensor(0.1973)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.846214\n",
      "Epoch 1919\n",
      "-------------------------------\n",
      "tensor(25.3743)\n",
      "tensor(8.2744)\n",
      "tensor(4.4483)\n",
      "tensor(0.1448)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.849213\n",
      "Epoch 1920\n",
      "-------------------------------\n",
      "tensor(28.2931)\n",
      "tensor(9.2461)\n",
      "tensor(4.1115)\n",
      "tensor(0.1180)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 126.846344\n",
      "Epoch 1921\n",
      "-------------------------------\n",
      "tensor(31.1144)\n",
      "tensor(10.0358)\n",
      "tensor(4.3376)\n",
      "tensor(0.1123)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.840500\n",
      "Epoch 1922\n",
      "-------------------------------\n",
      "tensor(31.4533)\n",
      "tensor(10.0901)\n",
      "tensor(4.3794)\n",
      "tensor(0.1180)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.830566\n",
      "Epoch 1923\n",
      "-------------------------------\n",
      "tensor(23.5670)\n",
      "tensor(7.7801)\n",
      "tensor(4.1603)\n",
      "tensor(0.1372)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.815414\n",
      "Epoch 1924\n",
      "-------------------------------\n",
      "tensor(17.1551)\n",
      "tensor(5.8053)\n",
      "tensor(3.9767)\n",
      "tensor(0.1901)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.810295\n",
      "Epoch 1925\n",
      "-------------------------------\n",
      "tensor(20.4255)\n",
      "tensor(7.7102)\n",
      "tensor(4.4720)\n",
      "tensor(0.2859)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.821022\n",
      "Epoch 1926\n",
      "-------------------------------\n",
      "tensor(18.9174)\n",
      "tensor(7.4668)\n",
      "tensor(4.8713)\n",
      "tensor(0.3748)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.824974\n",
      "Epoch 1927\n",
      "-------------------------------\n",
      "tensor(28.0529)\n",
      "tensor(9.5691)\n",
      "tensor(4.3386)\n",
      "tensor(0.3897)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.826790\n",
      "Epoch 1928\n",
      "-------------------------------\n",
      "tensor(17.7697)\n",
      "tensor(6.1009)\n",
      "tensor(4.2129)\n",
      "tensor(0.2892)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.795517\n",
      "Epoch 1929\n",
      "-------------------------------\n",
      "tensor(21.7696)\n",
      "tensor(7.0368)\n",
      "tensor(4.2633)\n",
      "tensor(0.2268)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.792358\n",
      "Epoch 1930\n",
      "-------------------------------\n",
      "tensor(26.9203)\n",
      "tensor(10.7005)\n",
      "tensor(4.0377)\n",
      "tensor(0.2718)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 126.774216\n",
      "Epoch 1931\n",
      "-------------------------------\n",
      "tensor(15.8780)\n",
      "tensor(5.4873)\n",
      "tensor(4.7794)\n",
      "tensor(0.3204)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.725311\n",
      "Epoch 1932\n",
      "-------------------------------\n",
      "tensor(16.1520)\n",
      "tensor(6.2386)\n",
      "tensor(4.8322)\n",
      "tensor(0.2838)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.691818\n",
      "Epoch 1933\n",
      "-------------------------------\n",
      "tensor(27.0075)\n",
      "tensor(8.8790)\n",
      "tensor(6.1146)\n",
      "tensor(0.2102)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.678383\n",
      "Epoch 1934\n",
      "-------------------------------\n",
      "tensor(32.4819)\n",
      "tensor(10.6255)\n",
      "tensor(4.4572)\n",
      "tensor(0.1583)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.660843\n",
      "Epoch 1935\n",
      "-------------------------------\n",
      "tensor(22.0131)\n",
      "tensor(6.6818)\n",
      "tensor(7.2306)\n",
      "tensor(0.0688)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.599007\n",
      "Epoch 1936\n",
      "-------------------------------\n",
      "tensor(19.4353)\n",
      "tensor(8.1414)\n",
      "tensor(4.0879)\n",
      "tensor(0.0802)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.577980\n",
      "Epoch 1937\n",
      "-------------------------------\n",
      "tensor(24.7887)\n",
      "tensor(9.1207)\n",
      "tensor(6.4315)\n",
      "tensor(0.1936)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.573936\n",
      "Epoch 1938\n",
      "-------------------------------\n",
      "tensor(21.9480)\n",
      "tensor(8.4479)\n",
      "tensor(6.2856)\n",
      "tensor(0.3074)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.552460\n",
      "Epoch 1939\n",
      "-------------------------------\n",
      "tensor(24.0054)\n",
      "tensor(7.9992)\n",
      "tensor(4.8946)\n",
      "tensor(0.3731)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.549126\n",
      "Epoch 1940\n",
      "-------------------------------\n",
      "tensor(23.7490)\n",
      "tensor(7.9051)\n",
      "tensor(4.3803)\n",
      "tensor(0.3797)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 126.540253\n",
      "Epoch 1941\n",
      "-------------------------------\n",
      "tensor(15.3096)\n",
      "tensor(5.8338)\n",
      "tensor(4.4095)\n",
      "tensor(0.3600)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.529549\n",
      "Epoch 1942\n",
      "-------------------------------\n",
      "tensor(22.9431)\n",
      "tensor(7.5737)\n",
      "tensor(4.4330)\n",
      "tensor(0.3330)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.528458\n",
      "Epoch 1943\n",
      "-------------------------------\n",
      "tensor(22.5960)\n",
      "tensor(7.4265)\n",
      "tensor(4.3023)\n",
      "tensor(0.2870)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.522247\n",
      "Epoch 1944\n",
      "-------------------------------\n",
      "tensor(24.0807)\n",
      "tensor(7.8565)\n",
      "tensor(4.0276)\n",
      "tensor(0.2151)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.520981\n",
      "Epoch 1945\n",
      "-------------------------------\n",
      "tensor(30.0897)\n",
      "tensor(9.7424)\n",
      "tensor(4.0538)\n",
      "tensor(0.1280)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.522888\n",
      "Epoch 1946\n",
      "-------------------------------\n",
      "tensor(21.7234)\n",
      "tensor(7.3336)\n",
      "tensor(4.4629)\n",
      "tensor(0.0744)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.508347\n",
      "Epoch 1947\n",
      "-------------------------------\n",
      "tensor(16.2828)\n",
      "tensor(7.0448)\n",
      "tensor(4.1688)\n",
      "tensor(0.1564)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.497185\n",
      "Epoch 1948\n",
      "-------------------------------\n",
      "tensor(39.2732)\n",
      "tensor(12.1267)\n",
      "tensor(4.4216)\n",
      "tensor(0.3640)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.488449\n",
      "Epoch 1949\n",
      "-------------------------------\n",
      "tensor(18.3382)\n",
      "tensor(7.1073)\n",
      "tensor(7.1169)\n",
      "tensor(0.3918)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.471626\n",
      "Epoch 1950\n",
      "-------------------------------\n",
      "tensor(19.4715)\n",
      "tensor(6.3896)\n",
      "tensor(4.0542)\n",
      "tensor(0.2141)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 126.461037\n",
      "Epoch 1951\n",
      "-------------------------------\n",
      "tensor(42.1512)\n",
      "tensor(12.4479)\n",
      "tensor(7.2920)\n",
      "tensor(0.0760)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.440727\n",
      "Epoch 1952\n",
      "-------------------------------\n",
      "tensor(27.4175)\n",
      "tensor(8.6593)\n",
      "tensor(6.6834)\n",
      "tensor(0.1011)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.411819\n",
      "Epoch 1953\n",
      "-------------------------------\n",
      "tensor(25.3728)\n",
      "tensor(7.4706)\n",
      "tensor(8.0048)\n",
      "tensor(0.1894)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.396042\n",
      "Epoch 1954\n",
      "-------------------------------\n",
      "tensor(17.9181)\n",
      "tensor(6.2844)\n",
      "tensor(5.1042)\n",
      "tensor(0.1984)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.365166\n",
      "Epoch 1955\n",
      "-------------------------------\n",
      "tensor(33.3463)\n",
      "tensor(9.8525)\n",
      "tensor(8.0564)\n",
      "tensor(0.2547)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.361084\n",
      "Epoch 1956\n",
      "-------------------------------\n",
      "tensor(22.8524)\n",
      "tensor(8.3268)\n",
      "tensor(4.2694)\n",
      "tensor(0.2812)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.325912\n",
      "Epoch 1957\n",
      "-------------------------------\n",
      "tensor(20.3020)\n",
      "tensor(7.0053)\n",
      "tensor(7.6537)\n",
      "tensor(0.2421)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.322464\n",
      "Epoch 1958\n",
      "-------------------------------\n",
      "tensor(18.8606)\n",
      "tensor(5.3986)\n",
      "tensor(6.0911)\n",
      "tensor(0.1740)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.309402\n",
      "Epoch 1959\n",
      "-------------------------------\n",
      "tensor(20.9788)\n",
      "tensor(6.8052)\n",
      "tensor(3.9851)\n",
      "tensor(0.1311)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.297661\n",
      "Epoch 1960\n",
      "-------------------------------\n",
      "tensor(28.1162)\n",
      "tensor(11.1674)\n",
      "tensor(4.3645)\n",
      "tensor(0.1262)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 126.292397\n",
      "Epoch 1961\n",
      "-------------------------------\n",
      "tensor(25.9139)\n",
      "tensor(8.2891)\n",
      "tensor(4.7463)\n",
      "tensor(0.1376)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.283279\n",
      "Epoch 1962\n",
      "-------------------------------\n",
      "tensor(19.3518)\n",
      "tensor(6.5452)\n",
      "tensor(4.7183)\n",
      "tensor(0.1549)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.272057\n",
      "Epoch 1963\n",
      "-------------------------------\n",
      "tensor(18.1555)\n",
      "tensor(6.2303)\n",
      "tensor(4.3746)\n",
      "tensor(0.1887)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.270386\n",
      "Epoch 1964\n",
      "-------------------------------\n",
      "tensor(18.0405)\n",
      "tensor(6.1708)\n",
      "tensor(3.9715)\n",
      "tensor(0.2422)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.264145\n",
      "Epoch 1965\n",
      "-------------------------------\n",
      "tensor(20.3491)\n",
      "tensor(6.7152)\n",
      "tensor(4.2979)\n",
      "tensor(0.3016)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.250862\n",
      "Epoch 1966\n",
      "-------------------------------\n",
      "tensor(29.7207)\n",
      "tensor(10.1040)\n",
      "tensor(5.3029)\n",
      "tensor(0.3345)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.275917\n",
      "Epoch 1967\n",
      "-------------------------------\n",
      "tensor(26.3788)\n",
      "tensor(9.2862)\n",
      "tensor(5.3409)\n",
      "tensor(0.2414)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.282768\n",
      "Epoch 1968\n",
      "-------------------------------\n",
      "tensor(26.6142)\n",
      "tensor(8.7847)\n",
      "tensor(4.1742)\n",
      "tensor(0.0632)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.259613\n",
      "Epoch 1969\n",
      "-------------------------------\n",
      "tensor(25.3488)\n",
      "tensor(8.4558)\n",
      "tensor(5.0653)\n",
      "tensor(0.0316)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.214531\n",
      "Epoch 1970\n",
      "-------------------------------\n",
      "tensor(32.9838)\n",
      "tensor(10.4975)\n",
      "tensor(3.8676)\n",
      "tensor(0.2804)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 126.184929\n",
      "Epoch 1971\n",
      "-------------------------------\n",
      "tensor(20.9539)\n",
      "tensor(6.8101)\n",
      "tensor(7.3573)\n",
      "tensor(0.4251)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 126.183235\n",
      "Epoch 1972\n",
      "-------------------------------\n",
      "tensor(21.8943)\n",
      "tensor(7.3192)\n",
      "tensor(3.9627)\n",
      "tensor(0.2796)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 126.155876\n",
      "Epoch 1973\n",
      "-------------------------------\n",
      "tensor(23.5736)\n",
      "tensor(7.7900)\n",
      "tensor(6.8261)\n",
      "tensor(0.1362)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.146126\n",
      "Epoch 1974\n",
      "-------------------------------\n",
      "tensor(28.2903)\n",
      "tensor(9.9366)\n",
      "tensor(3.7939)\n",
      "tensor(0.1962)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.119797\n",
      "Epoch 1975\n",
      "-------------------------------\n",
      "tensor(25.7073)\n",
      "tensor(9.0616)\n",
      "tensor(6.7450)\n",
      "tensor(0.2702)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.098373\n",
      "Epoch 1976\n",
      "-------------------------------\n",
      "tensor(17.8566)\n",
      "tensor(5.8244)\n",
      "tensor(4.8787)\n",
      "tensor(0.2450)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 126.081474\n",
      "Epoch 1977\n",
      "-------------------------------\n",
      "tensor(19.1389)\n",
      "tensor(6.7660)\n",
      "tensor(4.8698)\n",
      "tensor(0.2033)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.071030\n",
      "Epoch 1978\n",
      "-------------------------------\n",
      "tensor(44.4294)\n",
      "tensor(13.1891)\n",
      "tensor(6.7427)\n",
      "tensor(0.1982)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.058281\n",
      "Epoch 1979\n",
      "-------------------------------\n",
      "tensor(19.6637)\n",
      "tensor(6.8343)\n",
      "tensor(4.3272)\n",
      "tensor(0.1699)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.042130\n",
      "Epoch 1980\n",
      "-------------------------------\n",
      "tensor(19.8589)\n",
      "tensor(6.7085)\n",
      "tensor(3.7658)\n",
      "tensor(0.1691)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 126.034447\n",
      "Epoch 1981\n",
      "-------------------------------\n",
      "tensor(17.2891)\n",
      "tensor(5.8174)\n",
      "tensor(4.0592)\n",
      "tensor(0.1751)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 126.026031\n",
      "Epoch 1982\n",
      "-------------------------------\n",
      "tensor(17.4522)\n",
      "tensor(5.8041)\n",
      "tensor(4.2138)\n",
      "tensor(0.1834)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 126.017303\n",
      "Epoch 1983\n",
      "-------------------------------\n",
      "tensor(17.1951)\n",
      "tensor(5.8002)\n",
      "tensor(4.0780)\n",
      "tensor(0.1946)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 126.003380\n",
      "Epoch 1984\n",
      "-------------------------------\n",
      "tensor(18.0387)\n",
      "tensor(6.2445)\n",
      "tensor(3.7684)\n",
      "tensor(0.2114)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.981560\n",
      "Epoch 1985\n",
      "-------------------------------\n",
      "tensor(31.9507)\n",
      "tensor(9.8967)\n",
      "tensor(4.3721)\n",
      "tensor(0.2474)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 126.002045\n",
      "Epoch 1986\n",
      "-------------------------------\n",
      "tensor(34.2318)\n",
      "tensor(11.1268)\n",
      "tensor(4.5062)\n",
      "tensor(0.2659)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 126.036453\n",
      "Epoch 1987\n",
      "-------------------------------\n",
      "tensor(30.7180)\n",
      "tensor(10.5702)\n",
      "tensor(4.0721)\n",
      "tensor(0.2114)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 126.016716\n",
      "Epoch 1988\n",
      "-------------------------------\n",
      "tensor(24.7500)\n",
      "tensor(10.3715)\n",
      "tensor(6.9173)\n",
      "tensor(0.1153)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.978477\n",
      "Epoch 1989\n",
      "-------------------------------\n",
      "tensor(21.2854)\n",
      "tensor(6.7273)\n",
      "tensor(4.4349)\n",
      "tensor(0.1302)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.960922\n",
      "Epoch 1990\n",
      "-------------------------------\n",
      "tensor(28.6161)\n",
      "tensor(8.9998)\n",
      "tensor(6.5213)\n",
      "tensor(0.3120)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 125.938049\n",
      "Epoch 1991\n",
      "-------------------------------\n",
      "tensor(14.4709)\n",
      "tensor(6.3524)\n",
      "tensor(4.9560)\n",
      "tensor(0.4767)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.914146\n",
      "Epoch 1992\n",
      "-------------------------------\n",
      "tensor(22.8461)\n",
      "tensor(8.3685)\n",
      "tensor(5.4658)\n",
      "tensor(0.3514)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.883789\n",
      "Epoch 1993\n",
      "-------------------------------\n",
      "tensor(24.8881)\n",
      "tensor(8.2200)\n",
      "tensor(4.2613)\n",
      "tensor(0.0398)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.894150\n",
      "Epoch 1994\n",
      "-------------------------------\n",
      "tensor(27.4094)\n",
      "tensor(9.5042)\n",
      "tensor(5.3096)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.879845\n",
      "Epoch 1995\n",
      "-------------------------------\n",
      "tensor(27.5002)\n",
      "tensor(9.0412)\n",
      "tensor(3.9970)\n",
      "tensor(0.0683)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.823021\n",
      "Epoch 1996\n",
      "-------------------------------\n",
      "tensor(16.6814)\n",
      "tensor(6.8479)\n",
      "tensor(4.8525)\n",
      "tensor(0.2655)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.811523\n",
      "Epoch 1997\n",
      "-------------------------------\n",
      "tensor(18.4488)\n",
      "tensor(6.2919)\n",
      "tensor(4.5214)\n",
      "tensor(0.3256)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.815094\n",
      "Epoch 1998\n",
      "-------------------------------\n",
      "tensor(18.0675)\n",
      "tensor(6.3212)\n",
      "tensor(4.1342)\n",
      "tensor(0.2745)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.803650\n",
      "Epoch 1999\n",
      "-------------------------------\n",
      "tensor(18.2541)\n",
      "tensor(6.3895)\n",
      "tensor(4.5319)\n",
      "tensor(0.2088)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.786263\n",
      "Epoch 2000\n",
      "-------------------------------\n",
      "tensor(25.0286)\n",
      "tensor(8.0790)\n",
      "tensor(4.5760)\n",
      "tensor(0.1730)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 125.773483\n",
      "Epoch 2001\n",
      "-------------------------------\n",
      "tensor(24.8212)\n",
      "tensor(8.0426)\n",
      "tensor(4.2446)\n",
      "tensor(0.1580)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.759979\n",
      "Epoch 2002\n",
      "-------------------------------\n",
      "tensor(22.4496)\n",
      "tensor(7.3884)\n",
      "tensor(3.8796)\n",
      "tensor(0.1551)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.750916\n",
      "Epoch 2003\n",
      "-------------------------------\n",
      "tensor(23.8174)\n",
      "tensor(7.7085)\n",
      "tensor(3.7299)\n",
      "tensor(0.1642)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.749878\n",
      "Epoch 2004\n",
      "-------------------------------\n",
      "tensor(17.7749)\n",
      "tensor(7.0953)\n",
      "tensor(4.2805)\n",
      "tensor(0.1804)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.743164\n",
      "Epoch 2005\n",
      "-------------------------------\n",
      "tensor(21.3899)\n",
      "tensor(7.9599)\n",
      "tensor(4.8232)\n",
      "tensor(0.2050)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.731438\n",
      "Epoch 2006\n",
      "-------------------------------\n",
      "tensor(26.5118)\n",
      "tensor(8.4326)\n",
      "tensor(4.1459)\n",
      "tensor(0.2263)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.729881\n",
      "Epoch 2007\n",
      "-------------------------------\n",
      "tensor(29.0191)\n",
      "tensor(9.1470)\n",
      "tensor(3.8285)\n",
      "tensor(0.2139)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.718552\n",
      "Epoch 2008\n",
      "-------------------------------\n",
      "tensor(30.7001)\n",
      "tensor(9.6188)\n",
      "tensor(3.8496)\n",
      "tensor(0.2021)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.704338\n",
      "Epoch 2009\n",
      "-------------------------------\n",
      "tensor(19.0634)\n",
      "tensor(6.0089)\n",
      "tensor(4.9437)\n",
      "tensor(0.2270)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.715141\n",
      "Epoch 2010\n",
      "-------------------------------\n",
      "tensor(17.8330)\n",
      "tensor(6.0667)\n",
      "tensor(3.7886)\n",
      "tensor(0.2459)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 125.700333\n",
      "Epoch 2011\n",
      "-------------------------------\n",
      "tensor(19.1889)\n",
      "tensor(7.7223)\n",
      "tensor(5.8890)\n",
      "tensor(0.2745)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.683144\n",
      "Epoch 2012\n",
      "-------------------------------\n",
      "tensor(19.8375)\n",
      "tensor(6.9361)\n",
      "tensor(4.5024)\n",
      "tensor(0.3082)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.659760\n",
      "Epoch 2013\n",
      "-------------------------------\n",
      "tensor(28.6737)\n",
      "tensor(9.1469)\n",
      "tensor(4.5037)\n",
      "tensor(0.2312)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.605927\n",
      "Epoch 2014\n",
      "-------------------------------\n",
      "tensor(20.0453)\n",
      "tensor(6.3386)\n",
      "tensor(5.2784)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.574638\n",
      "Epoch 2015\n",
      "-------------------------------\n",
      "tensor(26.9326)\n",
      "tensor(8.9694)\n",
      "tensor(4.6775)\n",
      "tensor(0.0586)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.557892\n",
      "Epoch 2016\n",
      "-------------------------------\n",
      "tensor(28.6992)\n",
      "tensor(9.9552)\n",
      "tensor(5.0367)\n",
      "tensor(0.1677)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.553497\n",
      "Epoch 2017\n",
      "-------------------------------\n",
      "tensor(14.8477)\n",
      "tensor(6.8921)\n",
      "tensor(3.8307)\n",
      "tensor(0.2666)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.527382\n",
      "Epoch 2018\n",
      "-------------------------------\n",
      "tensor(15.0348)\n",
      "tensor(5.5567)\n",
      "tensor(4.1945)\n",
      "tensor(0.3039)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.517555\n",
      "Epoch 2019\n",
      "-------------------------------\n",
      "tensor(24.0107)\n",
      "tensor(7.8359)\n",
      "tensor(3.9626)\n",
      "tensor(0.2908)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.512962\n",
      "Epoch 2020\n",
      "-------------------------------\n",
      "tensor(17.4140)\n",
      "tensor(6.1574)\n",
      "tensor(3.7648)\n",
      "tensor(0.2593)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 125.505486\n",
      "Epoch 2021\n",
      "-------------------------------\n",
      "tensor(21.3668)\n",
      "tensor(7.1763)\n",
      "tensor(3.7371)\n",
      "tensor(0.2331)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.498482\n",
      "Epoch 2022\n",
      "-------------------------------\n",
      "tensor(21.3712)\n",
      "tensor(7.1865)\n",
      "tensor(3.7967)\n",
      "tensor(0.2124)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.490593\n",
      "Epoch 2023\n",
      "-------------------------------\n",
      "tensor(14.9223)\n",
      "tensor(5.6148)\n",
      "tensor(3.8828)\n",
      "tensor(0.1922)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.478630\n",
      "Epoch 2024\n",
      "-------------------------------\n",
      "tensor(29.3658)\n",
      "tensor(9.3051)\n",
      "tensor(3.9531)\n",
      "tensor(0.1842)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.488472\n",
      "Epoch 2025\n",
      "-------------------------------\n",
      "tensor(29.1900)\n",
      "tensor(9.3054)\n",
      "tensor(3.6854)\n",
      "tensor(0.1757)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.482658\n",
      "Epoch 2026\n",
      "-------------------------------\n",
      "tensor(26.9136)\n",
      "tensor(9.6144)\n",
      "tensor(4.0953)\n",
      "tensor(0.1702)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.456970\n",
      "Epoch 2027\n",
      "-------------------------------\n",
      "tensor(20.9652)\n",
      "tensor(6.5190)\n",
      "tensor(5.3258)\n",
      "tensor(0.1903)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.481194\n",
      "Epoch 2028\n",
      "-------------------------------\n",
      "tensor(19.6848)\n",
      "tensor(7.7163)\n",
      "tensor(3.6608)\n",
      "tensor(0.2066)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.495308\n",
      "Epoch 2029\n",
      "-------------------------------\n",
      "tensor(20.3577)\n",
      "tensor(6.9032)\n",
      "tensor(6.1743)\n",
      "tensor(0.2260)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.479584\n",
      "Epoch 2030\n",
      "-------------------------------\n",
      "tensor(30.7788)\n",
      "tensor(9.4188)\n",
      "tensor(4.6864)\n",
      "tensor(0.2817)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 125.444664\n",
      "Epoch 2031\n",
      "-------------------------------\n",
      "tensor(22.1907)\n",
      "tensor(6.3360)\n",
      "tensor(7.8710)\n",
      "tensor(0.2407)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.391922\n",
      "Epoch 2032\n",
      "-------------------------------\n",
      "tensor(20.1391)\n",
      "tensor(6.4880)\n",
      "tensor(4.1313)\n",
      "tensor(0.1080)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.355759\n",
      "Epoch 2033\n",
      "-------------------------------\n",
      "tensor(31.4023)\n",
      "tensor(9.5928)\n",
      "tensor(7.2806)\n",
      "tensor(0.1385)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.377357\n",
      "Epoch 2034\n",
      "-------------------------------\n",
      "tensor(24.0591)\n",
      "tensor(8.7369)\n",
      "tensor(3.9310)\n",
      "tensor(0.2813)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.362053\n",
      "Epoch 2035\n",
      "-------------------------------\n",
      "tensor(19.4410)\n",
      "tensor(5.8490)\n",
      "tensor(6.4470)\n",
      "tensor(0.2680)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.315041\n",
      "Epoch 2036\n",
      "-------------------------------\n",
      "tensor(15.2434)\n",
      "tensor(5.1114)\n",
      "tensor(3.8461)\n",
      "tensor(0.1543)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.290871\n",
      "Epoch 2037\n",
      "-------------------------------\n",
      "tensor(22.7033)\n",
      "tensor(7.5815)\n",
      "tensor(5.5508)\n",
      "tensor(0.0918)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.267044\n",
      "Epoch 2038\n",
      "-------------------------------\n",
      "tensor(18.2659)\n",
      "tensor(7.5720)\n",
      "tensor(5.7884)\n",
      "tensor(0.1235)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.255569\n",
      "Epoch 2039\n",
      "-------------------------------\n",
      "tensor(24.7528)\n",
      "tensor(8.0122)\n",
      "tensor(4.5589)\n",
      "tensor(0.1779)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.245872\n",
      "Epoch 2040\n",
      "-------------------------------\n",
      "tensor(29.9248)\n",
      "tensor(9.3989)\n",
      "tensor(3.7718)\n",
      "tensor(0.2121)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 125.240128\n",
      "Epoch 2041\n",
      "-------------------------------\n",
      "tensor(33.9411)\n",
      "tensor(10.6045)\n",
      "tensor(3.8508)\n",
      "tensor(0.2221)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.231049\n",
      "Epoch 2042\n",
      "-------------------------------\n",
      "tensor(22.3872)\n",
      "tensor(7.1826)\n",
      "tensor(4.4057)\n",
      "tensor(0.2173)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.220459\n",
      "Epoch 2043\n",
      "-------------------------------\n",
      "tensor(16.7983)\n",
      "tensor(6.7459)\n",
      "tensor(5.0455)\n",
      "tensor(0.2064)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.222992\n",
      "Epoch 2044\n",
      "-------------------------------\n",
      "tensor(17.7236)\n",
      "tensor(5.5163)\n",
      "tensor(4.9072)\n",
      "tensor(0.1793)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.222214\n",
      "Epoch 2045\n",
      "-------------------------------\n",
      "tensor(16.4517)\n",
      "tensor(5.5333)\n",
      "tensor(3.7363)\n",
      "tensor(0.1407)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.216339\n",
      "Epoch 2046\n",
      "-------------------------------\n",
      "tensor(24.0371)\n",
      "tensor(7.8378)\n",
      "tensor(5.1557)\n",
      "tensor(0.1294)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.213760\n",
      "Epoch 2047\n",
      "-------------------------------\n",
      "tensor(36.2276)\n",
      "tensor(11.4457)\n",
      "tensor(6.1202)\n",
      "tensor(0.1953)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.203712\n",
      "Epoch 2048\n",
      "-------------------------------\n",
      "tensor(18.4174)\n",
      "tensor(6.0423)\n",
      "tensor(5.1862)\n",
      "tensor(0.3024)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.199760\n",
      "Epoch 2049\n",
      "-------------------------------\n",
      "tensor(24.2956)\n",
      "tensor(7.5126)\n",
      "tensor(6.9765)\n",
      "tensor(0.2956)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.196922\n",
      "Epoch 2050\n",
      "-------------------------------\n",
      "tensor(28.2870)\n",
      "tensor(8.9499)\n",
      "tensor(4.1663)\n",
      "tensor(0.1364)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 125.175377\n",
      "Epoch 2051\n",
      "-------------------------------\n",
      "tensor(14.9843)\n",
      "tensor(5.4852)\n",
      "tensor(5.4521)\n",
      "tensor(0.1064)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 125.158531\n",
      "Epoch 2052\n",
      "-------------------------------\n",
      "tensor(14.8467)\n",
      "tensor(6.9341)\n",
      "tensor(3.9063)\n",
      "tensor(0.2970)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 125.137405\n",
      "Epoch 2053\n",
      "-------------------------------\n",
      "tensor(14.7752)\n",
      "tensor(5.7147)\n",
      "tensor(4.5463)\n",
      "tensor(0.3551)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 125.095497\n",
      "Epoch 2054\n",
      "-------------------------------\n",
      "tensor(32.7786)\n",
      "tensor(10.2219)\n",
      "tensor(3.9733)\n",
      "tensor(0.2004)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 125.115059\n",
      "Epoch 2055\n",
      "-------------------------------\n",
      "tensor(25.9422)\n",
      "tensor(8.7906)\n",
      "tensor(4.1987)\n",
      "tensor(0.0051)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 125.086983\n",
      "Epoch 2056\n",
      "-------------------------------\n",
      "tensor(24.1210)\n",
      "tensor(8.1698)\n",
      "tensor(4.6385)\n",
      "tensor(0.0322)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 125.049866\n",
      "Epoch 2057\n",
      "-------------------------------\n",
      "tensor(27.2100)\n",
      "tensor(9.8535)\n",
      "tensor(3.7295)\n",
      "tensor(0.0884)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.033066\n",
      "Epoch 2058\n",
      "-------------------------------\n",
      "tensor(21.5509)\n",
      "tensor(7.1503)\n",
      "tensor(3.6219)\n",
      "tensor(0.2168)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.029282\n",
      "Epoch 2059\n",
      "-------------------------------\n",
      "tensor(18.6265)\n",
      "tensor(6.5476)\n",
      "tensor(3.8897)\n",
      "tensor(0.2878)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.020485\n",
      "Epoch 2060\n",
      "-------------------------------\n",
      "tensor(15.8398)\n",
      "tensor(6.0258)\n",
      "tensor(4.0498)\n",
      "tensor(0.3104)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 125.013420\n",
      "Epoch 2061\n",
      "-------------------------------\n",
      "tensor(14.2287)\n",
      "tensor(5.7133)\n",
      "tensor(4.0930)\n",
      "tensor(0.3091)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 125.008507\n",
      "Epoch 2062\n",
      "-------------------------------\n",
      "tensor(14.6626)\n",
      "tensor(6.9732)\n",
      "tensor(4.0851)\n",
      "tensor(0.2964)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 125.003128\n",
      "Epoch 2063\n",
      "-------------------------------\n",
      "tensor(21.9511)\n",
      "tensor(7.3282)\n",
      "tensor(4.0284)\n",
      "tensor(0.2724)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 125.000908\n",
      "Epoch 2064\n",
      "-------------------------------\n",
      "tensor(22.3835)\n",
      "tensor(7.4114)\n",
      "tensor(3.8129)\n",
      "tensor(0.2228)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.989967\n",
      "Epoch 2065\n",
      "-------------------------------\n",
      "tensor(20.9780)\n",
      "tensor(6.9780)\n",
      "tensor(3.5898)\n",
      "tensor(0.1518)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.970490\n",
      "Epoch 2066\n",
      "-------------------------------\n",
      "tensor(30.6164)\n",
      "tensor(10.8143)\n",
      "tensor(3.8513)\n",
      "tensor(0.0966)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.986908\n",
      "Epoch 2067\n",
      "-------------------------------\n",
      "tensor(30.3430)\n",
      "tensor(9.9088)\n",
      "tensor(4.9401)\n",
      "tensor(0.0753)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.986778\n",
      "Epoch 2068\n",
      "-------------------------------\n",
      "tensor(30.5839)\n",
      "tensor(9.9208)\n",
      "tensor(4.7147)\n",
      "tensor(0.0985)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.953751\n",
      "Epoch 2069\n",
      "-------------------------------\n",
      "tensor(17.1797)\n",
      "tensor(5.8126)\n",
      "tensor(3.6293)\n",
      "tensor(0.1834)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.942528\n",
      "Epoch 2070\n",
      "-------------------------------\n",
      "tensor(16.8200)\n",
      "tensor(6.4248)\n",
      "tensor(5.5461)\n",
      "tensor(0.3189)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 124.947212\n",
      "Epoch 2071\n",
      "-------------------------------\n",
      "tensor(22.7724)\n",
      "tensor(8.5515)\n",
      "tensor(4.9498)\n",
      "tensor(0.3707)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.933182\n",
      "Epoch 2072\n",
      "-------------------------------\n",
      "tensor(17.1241)\n",
      "tensor(6.8206)\n",
      "tensor(5.0580)\n",
      "tensor(0.2093)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.911789\n",
      "Epoch 2073\n",
      "-------------------------------\n",
      "tensor(21.9435)\n",
      "tensor(7.3342)\n",
      "tensor(4.0210)\n",
      "tensor(0.0129)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.891472\n",
      "Epoch 2074\n",
      "-------------------------------\n",
      "tensor(22.1262)\n",
      "tensor(7.5327)\n",
      "tensor(5.1906)\n",
      "tensor(0.0282)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.847786\n",
      "Epoch 2075\n",
      "-------------------------------\n",
      "tensor(22.4578)\n",
      "tensor(7.4204)\n",
      "tensor(4.0155)\n",
      "tensor(0.2074)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.816460\n",
      "Epoch 2076\n",
      "-------------------------------\n",
      "tensor(29.0802)\n",
      "tensor(9.1322)\n",
      "tensor(4.6521)\n",
      "tensor(0.2966)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.814842\n",
      "Epoch 2077\n",
      "-------------------------------\n",
      "tensor(26.9516)\n",
      "tensor(8.5341)\n",
      "tensor(5.0466)\n",
      "tensor(0.2177)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.801208\n",
      "Epoch 2078\n",
      "-------------------------------\n",
      "tensor(20.0326)\n",
      "tensor(6.4447)\n",
      "tensor(4.2270)\n",
      "tensor(0.1060)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.781158\n",
      "Epoch 2079\n",
      "-------------------------------\n",
      "tensor(19.4112)\n",
      "tensor(6.4929)\n",
      "tensor(3.8178)\n",
      "tensor(0.0561)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.770363\n",
      "Epoch 2080\n",
      "-------------------------------\n",
      "tensor(19.2849)\n",
      "tensor(6.5455)\n",
      "tensor(4.1451)\n",
      "tensor(0.0571)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 124.756454\n",
      "Epoch 2081\n",
      "-------------------------------\n",
      "tensor(20.9839)\n",
      "tensor(7.0157)\n",
      "tensor(4.3587)\n",
      "tensor(0.0810)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.746101\n",
      "Epoch 2082\n",
      "-------------------------------\n",
      "tensor(26.1736)\n",
      "tensor(8.3981)\n",
      "tensor(4.3268)\n",
      "tensor(0.1156)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.738678\n",
      "Epoch 2083\n",
      "-------------------------------\n",
      "tensor(24.1508)\n",
      "tensor(7.8139)\n",
      "tensor(3.9727)\n",
      "tensor(0.1687)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.733849\n",
      "Epoch 2084\n",
      "-------------------------------\n",
      "tensor(25.1846)\n",
      "tensor(8.1023)\n",
      "tensor(3.7092)\n",
      "tensor(0.2525)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.740005\n",
      "Epoch 2085\n",
      "-------------------------------\n",
      "tensor(25.2923)\n",
      "tensor(8.1640)\n",
      "tensor(4.8702)\n",
      "tensor(0.3443)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.745674\n",
      "Epoch 2086\n",
      "-------------------------------\n",
      "tensor(22.3938)\n",
      "tensor(7.2908)\n",
      "tensor(6.1020)\n",
      "tensor(0.3615)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.732948\n",
      "Epoch 2087\n",
      "-------------------------------\n",
      "tensor(15.4625)\n",
      "tensor(5.2917)\n",
      "tensor(4.1568)\n",
      "tensor(0.2360)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.715057\n",
      "Epoch 2088\n",
      "-------------------------------\n",
      "tensor(38.8087)\n",
      "tensor(12.2490)\n",
      "tensor(5.7473)\n",
      "tensor(0.0627)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.721642\n",
      "Epoch 2089\n",
      "-------------------------------\n",
      "tensor(29.2404)\n",
      "tensor(10.7777)\n",
      "tensor(4.5356)\n",
      "tensor(0.0545)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.692032\n",
      "Epoch 2090\n",
      "-------------------------------\n",
      "tensor(39.5154)\n",
      "tensor(13.3317)\n",
      "tensor(7.8676)\n",
      "tensor(0.0994)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 124.691429\n",
      "Epoch 2091\n",
      "-------------------------------\n",
      "tensor(24.9833)\n",
      "tensor(6.9903)\n",
      "tensor(8.9705)\n",
      "tensor(0.1115)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.671547\n",
      "Epoch 2092\n",
      "-------------------------------\n",
      "tensor(27.9807)\n",
      "tensor(8.5634)\n",
      "tensor(7.4816)\n",
      "tensor(0.1421)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.666016\n",
      "Epoch 2093\n",
      "-------------------------------\n",
      "tensor(34.1456)\n",
      "tensor(10.4300)\n",
      "tensor(6.7524)\n",
      "tensor(0.3011)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.658607\n",
      "Epoch 2094\n",
      "-------------------------------\n",
      "tensor(22.1257)\n",
      "tensor(6.0101)\n",
      "tensor(9.5260)\n",
      "tensor(0.2813)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.609154\n",
      "Epoch 2095\n",
      "-------------------------------\n",
      "tensor(20.1100)\n",
      "tensor(5.5266)\n",
      "tensor(6.9449)\n",
      "tensor(0.0631)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.579727\n",
      "Epoch 2096\n",
      "-------------------------------\n",
      "tensor(32.2901)\n",
      "tensor(10.4088)\n",
      "tensor(6.0623)\n",
      "tensor(0.0472)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.582916\n",
      "Epoch 2097\n",
      "-------------------------------\n",
      "tensor(25.9996)\n",
      "tensor(8.4215)\n",
      "tensor(6.2685)\n",
      "tensor(0.0210)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.566078\n",
      "Epoch 2098\n",
      "-------------------------------\n",
      "tensor(26.1190)\n",
      "tensor(8.3312)\n",
      "tensor(3.7586)\n",
      "tensor(0.1475)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.550743\n",
      "Epoch 2099\n",
      "-------------------------------\n",
      "tensor(23.7552)\n",
      "tensor(7.5407)\n",
      "tensor(4.5878)\n",
      "tensor(0.2115)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.530930\n",
      "Epoch 2100\n",
      "-------------------------------\n",
      "tensor(27.8205)\n",
      "tensor(8.7512)\n",
      "tensor(5.4630)\n",
      "tensor(0.2237)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 124.529999\n",
      "Epoch 2101\n",
      "-------------------------------\n",
      "tensor(27.8009)\n",
      "tensor(8.7539)\n",
      "tensor(5.4796)\n",
      "tensor(0.2058)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.521339\n",
      "Epoch 2102\n",
      "-------------------------------\n",
      "tensor(16.3837)\n",
      "tensor(4.9877)\n",
      "tensor(5.0927)\n",
      "tensor(0.1768)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.514626\n",
      "Epoch 2103\n",
      "-------------------------------\n",
      "tensor(16.3847)\n",
      "tensor(6.7365)\n",
      "tensor(4.1656)\n",
      "tensor(0.1375)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.514046\n",
      "Epoch 2104\n",
      "-------------------------------\n",
      "tensor(15.5884)\n",
      "tensor(6.8037)\n",
      "tensor(3.7898)\n",
      "tensor(0.0799)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.508675\n",
      "Epoch 2105\n",
      "-------------------------------\n",
      "tensor(26.4262)\n",
      "tensor(9.3501)\n",
      "tensor(6.1851)\n",
      "tensor(0.0386)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.507278\n",
      "Epoch 2106\n",
      "-------------------------------\n",
      "tensor(33.3903)\n",
      "tensor(10.1835)\n",
      "tensor(6.5228)\n",
      "tensor(0.0790)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.506813\n",
      "Epoch 2107\n",
      "-------------------------------\n",
      "tensor(19.5339)\n",
      "tensor(6.3855)\n",
      "tensor(4.0670)\n",
      "tensor(0.2200)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.497269\n",
      "Epoch 2108\n",
      "-------------------------------\n",
      "tensor(24.3238)\n",
      "tensor(7.4611)\n",
      "tensor(8.5026)\n",
      "tensor(0.3395)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.493973\n",
      "Epoch 2109\n",
      "-------------------------------\n",
      "tensor(35.4211)\n",
      "tensor(11.2245)\n",
      "tensor(4.1105)\n",
      "tensor(0.2395)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.477386\n",
      "Epoch 2110\n",
      "-------------------------------\n",
      "tensor(18.8555)\n",
      "tensor(6.3902)\n",
      "tensor(4.6747)\n",
      "tensor(0.0099)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 124.459023\n",
      "Epoch 2111\n",
      "-------------------------------\n",
      "tensor(15.3078)\n",
      "tensor(5.5930)\n",
      "tensor(4.9966)\n",
      "tensor(0.0581)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.437126\n",
      "Epoch 2112\n",
      "-------------------------------\n",
      "tensor(23.4903)\n",
      "tensor(7.8614)\n",
      "tensor(4.5749)\n",
      "tensor(0.3655)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.458496\n",
      "Epoch 2113\n",
      "-------------------------------\n",
      "tensor(28.8303)\n",
      "tensor(9.1356)\n",
      "tensor(5.2061)\n",
      "tensor(0.3531)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.439026\n",
      "Epoch 2114\n",
      "-------------------------------\n",
      "tensor(23.9423)\n",
      "tensor(7.9897)\n",
      "tensor(4.1735)\n",
      "tensor(0.0257)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.361931\n",
      "Epoch 2115\n",
      "-------------------------------\n",
      "tensor(25.3407)\n",
      "tensor(9.8411)\n",
      "tensor(4.9097)\n",
      "tensor(0.0912)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.359543\n",
      "Epoch 2116\n",
      "-------------------------------\n",
      "tensor(22.9852)\n",
      "tensor(8.7498)\n",
      "tensor(3.9642)\n",
      "tensor(0.0747)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.374100\n",
      "Epoch 2117\n",
      "-------------------------------\n",
      "tensor(21.8718)\n",
      "tensor(7.3608)\n",
      "tensor(3.8323)\n",
      "tensor(0.2786)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.380005\n",
      "Epoch 2118\n",
      "-------------------------------\n",
      "tensor(22.3252)\n",
      "tensor(7.6202)\n",
      "tensor(4.3509)\n",
      "tensor(0.3612)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.370209\n",
      "Epoch 2119\n",
      "-------------------------------\n",
      "tensor(23.3049)\n",
      "tensor(7.8073)\n",
      "tensor(4.1785)\n",
      "tensor(0.3409)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.348923\n",
      "Epoch 2120\n",
      "-------------------------------\n",
      "tensor(19.8348)\n",
      "tensor(6.8716)\n",
      "tensor(3.8566)\n",
      "tensor(0.2908)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 124.335609\n",
      "Epoch 2121\n",
      "-------------------------------\n",
      "tensor(14.7488)\n",
      "tensor(5.5687)\n",
      "tensor(3.6653)\n",
      "tensor(0.2467)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.330338\n",
      "Epoch 2122\n",
      "-------------------------------\n",
      "tensor(14.6806)\n",
      "tensor(5.4751)\n",
      "tensor(3.6039)\n",
      "tensor(0.2087)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.325745\n",
      "Epoch 2123\n",
      "-------------------------------\n",
      "tensor(14.5673)\n",
      "tensor(5.4143)\n",
      "tensor(3.7220)\n",
      "tensor(0.1661)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.317726\n",
      "Epoch 2124\n",
      "-------------------------------\n",
      "tensor(24.0641)\n",
      "tensor(7.8763)\n",
      "tensor(4.1033)\n",
      "tensor(0.1227)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.319519\n",
      "Epoch 2125\n",
      "-------------------------------\n",
      "tensor(25.2844)\n",
      "tensor(9.2575)\n",
      "tensor(4.2027)\n",
      "tensor(0.0888)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.313576\n",
      "Epoch 2126\n",
      "-------------------------------\n",
      "tensor(26.6208)\n",
      "tensor(9.6750)\n",
      "tensor(3.6385)\n",
      "tensor(0.0892)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.279160\n",
      "Epoch 2127\n",
      "-------------------------------\n",
      "tensor(24.1716)\n",
      "tensor(7.7860)\n",
      "tensor(4.5279)\n",
      "tensor(0.1479)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.275024\n",
      "Epoch 2128\n",
      "-------------------------------\n",
      "tensor(29.0087)\n",
      "tensor(9.2395)\n",
      "tensor(4.8682)\n",
      "tensor(0.2002)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.315506\n",
      "Epoch 2129\n",
      "-------------------------------\n",
      "tensor(28.0277)\n",
      "tensor(9.0051)\n",
      "tensor(3.5381)\n",
      "tensor(0.1282)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.306335\n",
      "Epoch 2130\n",
      "-------------------------------\n",
      "tensor(25.6583)\n",
      "tensor(8.3149)\n",
      "tensor(4.2792)\n",
      "tensor(0.0550)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 124.242317\n",
      "Epoch 2131\n",
      "-------------------------------\n",
      "tensor(15.5690)\n",
      "tensor(5.2387)\n",
      "tensor(3.8002)\n",
      "tensor(0.1793)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.222069\n",
      "Epoch 2132\n",
      "-------------------------------\n",
      "tensor(23.0610)\n",
      "tensor(7.7241)\n",
      "tensor(4.0976)\n",
      "tensor(0.3402)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.241608\n",
      "Epoch 2133\n",
      "-------------------------------\n",
      "tensor(25.7553)\n",
      "tensor(9.1968)\n",
      "tensor(3.8764)\n",
      "tensor(0.2837)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.240120\n",
      "Epoch 2134\n",
      "-------------------------------\n",
      "tensor(18.9396)\n",
      "tensor(7.5044)\n",
      "tensor(3.7474)\n",
      "tensor(0.1261)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.211273\n",
      "Epoch 2135\n",
      "-------------------------------\n",
      "tensor(22.1580)\n",
      "tensor(7.2495)\n",
      "tensor(3.6984)\n",
      "tensor(0.0696)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.184990\n",
      "Epoch 2136\n",
      "-------------------------------\n",
      "tensor(22.0986)\n",
      "tensor(7.1939)\n",
      "tensor(3.5698)\n",
      "tensor(0.1224)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.164856\n",
      "Epoch 2137\n",
      "-------------------------------\n",
      "tensor(19.8072)\n",
      "tensor(6.6823)\n",
      "tensor(3.6775)\n",
      "tensor(0.1707)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.139778\n",
      "Epoch 2138\n",
      "-------------------------------\n",
      "tensor(14.5086)\n",
      "tensor(5.3846)\n",
      "tensor(3.7560)\n",
      "tensor(0.1791)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.114792\n",
      "Epoch 2139\n",
      "-------------------------------\n",
      "tensor(14.3845)\n",
      "tensor(5.3837)\n",
      "tensor(3.8148)\n",
      "tensor(0.1631)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.094925\n",
      "Epoch 2140\n",
      "-------------------------------\n",
      "tensor(14.2868)\n",
      "tensor(5.3816)\n",
      "tensor(3.8734)\n",
      "tensor(0.1426)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 124.078751\n",
      "Epoch 2141\n",
      "-------------------------------\n",
      "tensor(22.0668)\n",
      "tensor(7.3605)\n",
      "tensor(3.9096)\n",
      "tensor(0.1281)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 124.071228\n",
      "Epoch 2142\n",
      "-------------------------------\n",
      "tensor(24.6279)\n",
      "tensor(8.0682)\n",
      "tensor(3.8550)\n",
      "tensor(0.1152)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 124.065132\n",
      "Epoch 2143\n",
      "-------------------------------\n",
      "tensor(25.4964)\n",
      "tensor(8.3367)\n",
      "tensor(3.7169)\n",
      "tensor(0.0982)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 124.051414\n",
      "Epoch 2144\n",
      "-------------------------------\n",
      "tensor(26.4440)\n",
      "tensor(8.6309)\n",
      "tensor(3.6383)\n",
      "tensor(0.0852)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 124.061684\n",
      "Epoch 2145\n",
      "-------------------------------\n",
      "tensor(26.8900)\n",
      "tensor(8.7461)\n",
      "tensor(4.2702)\n",
      "tensor(0.0740)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 124.079117\n",
      "Epoch 2146\n",
      "-------------------------------\n",
      "tensor(26.2290)\n",
      "tensor(8.4722)\n",
      "tensor(4.9069)\n",
      "tensor(0.0629)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 124.072174\n",
      "Epoch 2147\n",
      "-------------------------------\n",
      "tensor(28.6720)\n",
      "tensor(10.2180)\n",
      "tensor(4.0581)\n",
      "tensor(0.0909)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 124.054909\n",
      "Epoch 2148\n",
      "-------------------------------\n",
      "tensor(22.7540)\n",
      "tensor(8.4791)\n",
      "tensor(3.6266)\n",
      "tensor(0.1978)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 124.052078\n",
      "Epoch 2149\n",
      "-------------------------------\n",
      "tensor(25.5954)\n",
      "tensor(8.4174)\n",
      "tensor(4.2453)\n",
      "tensor(0.3517)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.053230\n",
      "Epoch 2150\n",
      "-------------------------------\n",
      "tensor(18.0903)\n",
      "tensor(6.9144)\n",
      "tensor(4.5387)\n",
      "tensor(0.3891)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 124.019974\n",
      "Epoch 2151\n",
      "-------------------------------\n",
      "tensor(25.3018)\n",
      "tensor(8.1985)\n",
      "tensor(3.8486)\n",
      "tensor(0.2625)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 124.016502\n",
      "Epoch 2152\n",
      "-------------------------------\n",
      "tensor(16.7358)\n",
      "tensor(5.5933)\n",
      "tensor(3.7533)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.992844\n",
      "Epoch 2153\n",
      "-------------------------------\n",
      "tensor(14.9634)\n",
      "tensor(6.9757)\n",
      "tensor(4.3746)\n",
      "tensor(0.0839)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.959305\n",
      "Epoch 2154\n",
      "-------------------------------\n",
      "tensor(18.1508)\n",
      "tensor(6.5207)\n",
      "tensor(4.5759)\n",
      "tensor(0.1890)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.919205\n",
      "Epoch 2155\n",
      "-------------------------------\n",
      "tensor(27.2276)\n",
      "tensor(8.7539)\n",
      "tensor(4.2824)\n",
      "tensor(0.1967)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.916893\n",
      "Epoch 2156\n",
      "-------------------------------\n",
      "tensor(30.6590)\n",
      "tensor(10.1176)\n",
      "tensor(4.4231)\n",
      "tensor(0.0574)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.902214\n",
      "Epoch 2157\n",
      "-------------------------------\n",
      "tensor(26.9856)\n",
      "tensor(9.3185)\n",
      "tensor(4.8033)\n",
      "tensor(0.0764)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.856995\n",
      "Epoch 2158\n",
      "-------------------------------\n",
      "tensor(23.1982)\n",
      "tensor(8.0688)\n",
      "tensor(4.4636)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.840630\n",
      "Epoch 2159\n",
      "-------------------------------\n",
      "tensor(20.1067)\n",
      "tensor(8.2018)\n",
      "tensor(3.9713)\n",
      "tensor(0.0116)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.842804\n",
      "Epoch 2160\n",
      "-------------------------------\n",
      "tensor(20.0380)\n",
      "tensor(8.0324)\n",
      "tensor(3.8515)\n",
      "tensor(0.0692)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 123.839043\n",
      "Epoch 2161\n",
      "-------------------------------\n",
      "tensor(14.9694)\n",
      "tensor(5.5286)\n",
      "tensor(3.8917)\n",
      "tensor(0.1313)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.833572\n",
      "Epoch 2162\n",
      "-------------------------------\n",
      "tensor(14.9348)\n",
      "tensor(5.6184)\n",
      "tensor(3.9898)\n",
      "tensor(0.1833)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.829300\n",
      "Epoch 2163\n",
      "-------------------------------\n",
      "tensor(16.9342)\n",
      "tensor(6.1916)\n",
      "tensor(4.1428)\n",
      "tensor(0.2394)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.821571\n",
      "Epoch 2164\n",
      "-------------------------------\n",
      "tensor(25.4877)\n",
      "tensor(8.2528)\n",
      "tensor(4.3042)\n",
      "tensor(0.3035)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.816544\n",
      "Epoch 2165\n",
      "-------------------------------\n",
      "tensor(26.3301)\n",
      "tensor(8.5296)\n",
      "tensor(4.2798)\n",
      "tensor(0.3402)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.808723\n",
      "Epoch 2166\n",
      "-------------------------------\n",
      "tensor(27.2548)\n",
      "tensor(8.7574)\n",
      "tensor(4.7562)\n",
      "tensor(0.2905)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.812294\n",
      "Epoch 2167\n",
      "-------------------------------\n",
      "tensor(28.2572)\n",
      "tensor(9.0783)\n",
      "tensor(5.3255)\n",
      "tensor(0.1211)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.813698\n",
      "Epoch 2168\n",
      "-------------------------------\n",
      "tensor(28.4101)\n",
      "tensor(10.4341)\n",
      "tensor(4.9698)\n",
      "tensor(0.0517)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.783951\n",
      "Epoch 2169\n",
      "-------------------------------\n",
      "tensor(17.4989)\n",
      "tensor(7.1728)\n",
      "tensor(4.3622)\n",
      "tensor(0.0429)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.782990\n",
      "Epoch 2170\n",
      "-------------------------------\n",
      "tensor(23.9852)\n",
      "tensor(8.1521)\n",
      "tensor(5.1706)\n",
      "tensor(0.3977)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 123.811157\n",
      "Epoch 2171\n",
      "-------------------------------\n",
      "tensor(24.5544)\n",
      "tensor(8.6145)\n",
      "tensor(5.7175)\n",
      "tensor(0.5000)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.792358\n",
      "Epoch 2172\n",
      "-------------------------------\n",
      "tensor(16.9418)\n",
      "tensor(5.3150)\n",
      "tensor(4.7506)\n",
      "tensor(0.1416)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.766716\n",
      "Epoch 2173\n",
      "-------------------------------\n",
      "tensor(15.7910)\n",
      "tensor(6.2116)\n",
      "tensor(6.1020)\n",
      "tensor(0.1642)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.744118\n",
      "Epoch 2174\n",
      "-------------------------------\n",
      "tensor(17.2363)\n",
      "tensor(6.5230)\n",
      "tensor(6.1488)\n",
      "tensor(0.0052)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.705658\n",
      "Epoch 2175\n",
      "-------------------------------\n",
      "tensor(19.5986)\n",
      "tensor(6.8135)\n",
      "tensor(4.9284)\n",
      "tensor(0.2947)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.656731\n",
      "Epoch 2176\n",
      "-------------------------------\n",
      "tensor(30.1983)\n",
      "tensor(9.4169)\n",
      "tensor(5.9287)\n",
      "tensor(0.3070)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.674278\n",
      "Epoch 2177\n",
      "-------------------------------\n",
      "tensor(29.1524)\n",
      "tensor(9.5081)\n",
      "tensor(4.7155)\n",
      "tensor(0.0819)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.660149\n",
      "Epoch 2178\n",
      "-------------------------------\n",
      "tensor(32.7880)\n",
      "tensor(11.3316)\n",
      "tensor(4.5708)\n",
      "tensor(0.1079)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.622101\n",
      "Epoch 2179\n",
      "-------------------------------\n",
      "tensor(28.5865)\n",
      "tensor(10.0486)\n",
      "tensor(5.0615)\n",
      "tensor(0.1608)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.593704\n",
      "Epoch 2180\n",
      "-------------------------------\n",
      "tensor(15.6651)\n",
      "tensor(5.4780)\n",
      "tensor(4.8023)\n",
      "tensor(0.1181)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 123.587776\n",
      "Epoch 2181\n",
      "-------------------------------\n",
      "tensor(17.9650)\n",
      "tensor(6.1142)\n",
      "tensor(4.2484)\n",
      "tensor(0.0452)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.589340\n",
      "Epoch 2182\n",
      "-------------------------------\n",
      "tensor(14.9723)\n",
      "tensor(5.2009)\n",
      "tensor(3.7766)\n",
      "tensor(0.0368)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.588593\n",
      "Epoch 2183\n",
      "-------------------------------\n",
      "tensor(17.7780)\n",
      "tensor(7.4168)\n",
      "tensor(3.6025)\n",
      "tensor(0.1493)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.585281\n",
      "Epoch 2184\n",
      "-------------------------------\n",
      "tensor(23.5494)\n",
      "tensor(8.7607)\n",
      "tensor(4.3676)\n",
      "tensor(0.3097)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.588234\n",
      "Epoch 2185\n",
      "-------------------------------\n",
      "tensor(27.1987)\n",
      "tensor(8.9584)\n",
      "tensor(5.6926)\n",
      "tensor(0.4722)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.578461\n",
      "Epoch 2186\n",
      "-------------------------------\n",
      "tensor(22.3128)\n",
      "tensor(8.2210)\n",
      "tensor(5.8178)\n",
      "tensor(0.5118)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.567604\n",
      "Epoch 2187\n",
      "-------------------------------\n",
      "tensor(22.5859)\n",
      "tensor(7.5285)\n",
      "tensor(4.6621)\n",
      "tensor(0.3026)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.564796\n",
      "Epoch 2188\n",
      "-------------------------------\n",
      "tensor(25.1256)\n",
      "tensor(8.2720)\n",
      "tensor(5.0610)\n",
      "tensor(0.0612)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.590630\n",
      "Epoch 2189\n",
      "-------------------------------\n",
      "tensor(25.1517)\n",
      "tensor(9.8849)\n",
      "tensor(6.1356)\n",
      "tensor(0.2101)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.571007\n",
      "Epoch 2190\n",
      "-------------------------------\n",
      "tensor(24.4028)\n",
      "tensor(7.8879)\n",
      "tensor(3.8484)\n",
      "tensor(0.1281)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 123.509911\n",
      "Epoch 2191\n",
      "-------------------------------\n",
      "tensor(25.5235)\n",
      "tensor(8.9231)\n",
      "tensor(6.7884)\n",
      "tensor(0.5490)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.536705\n",
      "Epoch 2192\n",
      "-------------------------------\n",
      "tensor(24.4884)\n",
      "tensor(8.1020)\n",
      "tensor(5.0345)\n",
      "tensor(0.3488)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.532806\n",
      "Epoch 2193\n",
      "-------------------------------\n",
      "tensor(16.6945)\n",
      "tensor(6.0607)\n",
      "tensor(5.3075)\n",
      "tensor(0.1399)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.497696\n",
      "Epoch 2194\n",
      "-------------------------------\n",
      "tensor(22.4456)\n",
      "tensor(9.1023)\n",
      "tensor(6.1720)\n",
      "tensor(0.1502)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.494133\n",
      "Epoch 2195\n",
      "-------------------------------\n",
      "tensor(17.6310)\n",
      "tensor(7.4815)\n",
      "tensor(3.6941)\n",
      "tensor(0.1898)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.464050\n",
      "Epoch 2196\n",
      "-------------------------------\n",
      "tensor(15.6995)\n",
      "tensor(6.2540)\n",
      "tensor(5.3501)\n",
      "tensor(0.3880)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.433731\n",
      "Epoch 2197\n",
      "-------------------------------\n",
      "tensor(21.5481)\n",
      "tensor(7.3933)\n",
      "tensor(4.6176)\n",
      "tensor(0.3236)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.401878\n",
      "Epoch 2198\n",
      "-------------------------------\n",
      "tensor(24.5731)\n",
      "tensor(8.0311)\n",
      "tensor(3.9227)\n",
      "tensor(0.1683)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.389778\n",
      "Epoch 2199\n",
      "-------------------------------\n",
      "tensor(25.6600)\n",
      "tensor(8.4815)\n",
      "tensor(3.8408)\n",
      "tensor(0.0499)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.378761\n",
      "Epoch 2200\n",
      "-------------------------------\n",
      "tensor(19.6901)\n",
      "tensor(6.8192)\n",
      "tensor(3.9047)\n",
      "tensor(0.0043)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 123.364372\n",
      "Epoch 2201\n",
      "-------------------------------\n",
      "tensor(18.1755)\n",
      "tensor(6.3471)\n",
      "tensor(3.9254)\n",
      "tensor(0.0145)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.354538\n",
      "Epoch 2202\n",
      "-------------------------------\n",
      "tensor(20.5142)\n",
      "tensor(7.0126)\n",
      "tensor(3.8630)\n",
      "tensor(0.0031)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.344566\n",
      "Epoch 2203\n",
      "-------------------------------\n",
      "tensor(22.3772)\n",
      "tensor(7.5074)\n",
      "tensor(3.7038)\n",
      "tensor(0.0330)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.339348\n",
      "Epoch 2204\n",
      "-------------------------------\n",
      "tensor(26.3028)\n",
      "tensor(8.5530)\n",
      "tensor(3.5519)\n",
      "tensor(0.1116)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.350800\n",
      "Epoch 2205\n",
      "-------------------------------\n",
      "tensor(28.5259)\n",
      "tensor(9.0864)\n",
      "tensor(3.8677)\n",
      "tensor(0.2238)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.370552\n",
      "Epoch 2206\n",
      "-------------------------------\n",
      "tensor(29.0711)\n",
      "tensor(9.2618)\n",
      "tensor(4.4245)\n",
      "tensor(0.2986)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.370110\n",
      "Epoch 2207\n",
      "-------------------------------\n",
      "tensor(24.1263)\n",
      "tensor(7.9033)\n",
      "tensor(4.3435)\n",
      "tensor(0.2526)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.338333\n",
      "Epoch 2208\n",
      "-------------------------------\n",
      "tensor(22.2813)\n",
      "tensor(8.4280)\n",
      "tensor(3.7850)\n",
      "tensor(0.1236)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.329445\n",
      "Epoch 2209\n",
      "-------------------------------\n",
      "tensor(18.3707)\n",
      "tensor(7.4022)\n",
      "tensor(4.1877)\n",
      "tensor(0.0768)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.323883\n",
      "Epoch 2210\n",
      "-------------------------------\n",
      "tensor(16.8663)\n",
      "tensor(7.3182)\n",
      "tensor(4.1025)\n",
      "tensor(0.2203)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 123.307251\n",
      "Epoch 2211\n",
      "-------------------------------\n",
      "tensor(22.0428)\n",
      "tensor(7.6461)\n",
      "tensor(4.4887)\n",
      "tensor(0.3610)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.291252\n",
      "Epoch 2212\n",
      "-------------------------------\n",
      "tensor(27.3865)\n",
      "tensor(8.7753)\n",
      "tensor(4.2700)\n",
      "tensor(0.2141)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.282280\n",
      "Epoch 2213\n",
      "-------------------------------\n",
      "tensor(24.4531)\n",
      "tensor(8.3198)\n",
      "tensor(4.5250)\n",
      "tensor(0.0756)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.267380\n",
      "Epoch 2214\n",
      "-------------------------------\n",
      "tensor(19.7753)\n",
      "tensor(7.1434)\n",
      "tensor(4.7654)\n",
      "tensor(0.1209)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.224693\n",
      "Epoch 2215\n",
      "-------------------------------\n",
      "tensor(23.0015)\n",
      "tensor(7.6325)\n",
      "tensor(4.2139)\n",
      "tensor(0.1180)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.230171\n",
      "Epoch 2216\n",
      "-------------------------------\n",
      "tensor(24.9949)\n",
      "tensor(8.1351)\n",
      "tensor(4.4409)\n",
      "tensor(0.2885)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.227303\n",
      "Epoch 2217\n",
      "-------------------------------\n",
      "tensor(25.7626)\n",
      "tensor(8.3220)\n",
      "tensor(4.4118)\n",
      "tensor(0.2526)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.196609\n",
      "Epoch 2218\n",
      "-------------------------------\n",
      "tensor(19.2942)\n",
      "tensor(6.3845)\n",
      "tensor(3.9578)\n",
      "tensor(0.1302)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.156876\n",
      "Epoch 2219\n",
      "-------------------------------\n",
      "tensor(15.2236)\n",
      "tensor(5.2219)\n",
      "tensor(3.7127)\n",
      "tensor(0.0535)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.148659\n",
      "Epoch 2220\n",
      "-------------------------------\n",
      "tensor(14.8152)\n",
      "tensor(5.2615)\n",
      "tensor(3.8264)\n",
      "tensor(0.0298)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 123.140182\n",
      "Epoch 2221\n",
      "-------------------------------\n",
      "tensor(16.5873)\n",
      "tensor(5.8357)\n",
      "tensor(4.0452)\n",
      "tensor(0.0364)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 123.133835\n",
      "Epoch 2222\n",
      "-------------------------------\n",
      "tensor(16.5402)\n",
      "tensor(5.8679)\n",
      "tensor(4.2017)\n",
      "tensor(0.0571)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 123.126106\n",
      "Epoch 2223\n",
      "-------------------------------\n",
      "tensor(23.5110)\n",
      "tensor(7.7161)\n",
      "tensor(4.2426)\n",
      "tensor(0.0991)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 123.122726\n",
      "Epoch 2224\n",
      "-------------------------------\n",
      "tensor(25.3911)\n",
      "tensor(8.1967)\n",
      "tensor(3.9969)\n",
      "tensor(0.1719)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 123.122932\n",
      "Epoch 2225\n",
      "-------------------------------\n",
      "tensor(31.2066)\n",
      "tensor(9.7775)\n",
      "tensor(4.0111)\n",
      "tensor(0.2568)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 123.113197\n",
      "Epoch 2226\n",
      "-------------------------------\n",
      "tensor(30.4115)\n",
      "tensor(9.6917)\n",
      "tensor(5.7494)\n",
      "tensor(0.2819)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.103218\n",
      "Epoch 2227\n",
      "-------------------------------\n",
      "tensor(23.3679)\n",
      "tensor(7.1971)\n",
      "tensor(6.9111)\n",
      "tensor(0.1621)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.099236\n",
      "Epoch 2228\n",
      "-------------------------------\n",
      "tensor(19.3269)\n",
      "tensor(7.8706)\n",
      "tensor(4.3038)\n",
      "tensor(0.0363)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.088104\n",
      "Epoch 2229\n",
      "-------------------------------\n",
      "tensor(20.9896)\n",
      "tensor(8.0623)\n",
      "tensor(8.1737)\n",
      "tensor(0.0110)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.057854\n",
      "Epoch 2230\n",
      "-------------------------------\n",
      "tensor(29.7453)\n",
      "tensor(10.1477)\n",
      "tensor(4.8016)\n",
      "tensor(0.3517)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 123.051620\n",
      "Epoch 2231\n",
      "-------------------------------\n",
      "tensor(29.8451)\n",
      "tensor(9.5677)\n",
      "tensor(10.8866)\n",
      "tensor(0.4427)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 123.046867\n",
      "Epoch 2232\n",
      "-------------------------------\n",
      "tensor(23.3647)\n",
      "tensor(7.5426)\n",
      "tensor(5.1078)\n",
      "tensor(0.0056)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 123.031143\n",
      "Epoch 2233\n",
      "-------------------------------\n",
      "tensor(20.6214)\n",
      "tensor(7.2133)\n",
      "tensor(10.1869)\n",
      "tensor(0.2154)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 123.024689\n",
      "Epoch 2234\n",
      "-------------------------------\n",
      "tensor(23.3477)\n",
      "tensor(7.6309)\n",
      "tensor(5.5360)\n",
      "tensor(0.1816)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 123.023018\n",
      "Epoch 2235\n",
      "-------------------------------\n",
      "tensor(23.9709)\n",
      "tensor(8.2152)\n",
      "tensor(8.2049)\n",
      "tensor(0.4748)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.995560\n",
      "Epoch 2236\n",
      "-------------------------------\n",
      "tensor(19.9581)\n",
      "tensor(6.4717)\n",
      "tensor(7.0480)\n",
      "tensor(0.3056)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.967094\n",
      "Epoch 2237\n",
      "-------------------------------\n",
      "tensor(15.2490)\n",
      "tensor(5.5935)\n",
      "tensor(4.0709)\n",
      "tensor(0.0021)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.941994\n",
      "Epoch 2238\n",
      "-------------------------------\n",
      "tensor(16.1512)\n",
      "tensor(6.3177)\n",
      "tensor(7.5074)\n",
      "tensor(0.1345)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.918594\n",
      "Epoch 2239\n",
      "-------------------------------\n",
      "tensor(18.1503)\n",
      "tensor(6.7580)\n",
      "tensor(7.4869)\n",
      "tensor(0.1054)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.895844\n",
      "Epoch 2240\n",
      "-------------------------------\n",
      "tensor(23.6246)\n",
      "tensor(7.9894)\n",
      "tensor(5.7103)\n",
      "tensor(0.0181)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 122.892555\n",
      "Epoch 2241\n",
      "-------------------------------\n",
      "tensor(27.4523)\n",
      "tensor(8.9375)\n",
      "tensor(4.2311)\n",
      "tensor(0.0572)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.888298\n",
      "Epoch 2242\n",
      "-------------------------------\n",
      "tensor(27.1784)\n",
      "tensor(8.8101)\n",
      "tensor(3.6637)\n",
      "tensor(0.1176)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.881981\n",
      "Epoch 2243\n",
      "-------------------------------\n",
      "tensor(30.0256)\n",
      "tensor(9.6114)\n",
      "tensor(4.4676)\n",
      "tensor(0.1764)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.870010\n",
      "Epoch 2244\n",
      "-------------------------------\n",
      "tensor(21.4113)\n",
      "tensor(6.7265)\n",
      "tensor(6.6961)\n",
      "tensor(0.2305)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.865692\n",
      "Epoch 2245\n",
      "-------------------------------\n",
      "tensor(27.0297)\n",
      "tensor(8.5257)\n",
      "tensor(7.5241)\n",
      "tensor(0.2444)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.878830\n",
      "Epoch 2246\n",
      "-------------------------------\n",
      "tensor(24.2452)\n",
      "tensor(8.9992)\n",
      "tensor(4.6373)\n",
      "tensor(0.1550)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.886444\n",
      "Epoch 2247\n",
      "-------------------------------\n",
      "tensor(23.6300)\n",
      "tensor(8.7569)\n",
      "tensor(5.8233)\n",
      "tensor(0.0075)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.874344\n",
      "Epoch 2248\n",
      "-------------------------------\n",
      "tensor(22.4554)\n",
      "tensor(8.2485)\n",
      "tensor(7.3841)\n",
      "tensor(0.0374)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.852432\n",
      "Epoch 2249\n",
      "-------------------------------\n",
      "tensor(23.4001)\n",
      "tensor(8.0056)\n",
      "tensor(4.5077)\n",
      "tensor(0.3284)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.829002\n",
      "Epoch 2250\n",
      "-------------------------------\n",
      "tensor(34.5748)\n",
      "tensor(15.1765)\n",
      "tensor(8.9711)\n",
      "tensor(0.4354)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 122.823616\n",
      "Epoch 2251\n",
      "-------------------------------\n",
      "tensor(27.5392)\n",
      "tensor(8.6853)\n",
      "tensor(4.7176)\n",
      "tensor(0.1195)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.800995\n",
      "Epoch 2252\n",
      "-------------------------------\n",
      "tensor(23.0024)\n",
      "tensor(7.6174)\n",
      "tensor(6.6103)\n",
      "tensor(0.0658)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.785759\n",
      "Epoch 2253\n",
      "-------------------------------\n",
      "tensor(24.0986)\n",
      "tensor(7.8292)\n",
      "tensor(4.6470)\n",
      "tensor(0.1462)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.786446\n",
      "Epoch 2254\n",
      "-------------------------------\n",
      "tensor(23.4916)\n",
      "tensor(7.6300)\n",
      "tensor(6.0714)\n",
      "tensor(0.2674)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.766266\n",
      "Epoch 2255\n",
      "-------------------------------\n",
      "tensor(14.3477)\n",
      "tensor(5.5883)\n",
      "tensor(4.0851)\n",
      "tensor(0.1339)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.708511\n",
      "Epoch 2256\n",
      "-------------------------------\n",
      "tensor(24.3288)\n",
      "tensor(8.0914)\n",
      "tensor(6.6276)\n",
      "tensor(0.0364)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.703049\n",
      "Epoch 2257\n",
      "-------------------------------\n",
      "tensor(21.7055)\n",
      "tensor(7.4760)\n",
      "tensor(4.3891)\n",
      "tensor(0.0378)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.677780\n",
      "Epoch 2258\n",
      "-------------------------------\n",
      "tensor(25.4528)\n",
      "tensor(8.4062)\n",
      "tensor(3.8911)\n",
      "tensor(0.0821)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.669281\n",
      "Epoch 2259\n",
      "-------------------------------\n",
      "tensor(22.7542)\n",
      "tensor(7.4215)\n",
      "tensor(4.7795)\n",
      "tensor(0.1033)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.654945\n",
      "Epoch 2260\n",
      "-------------------------------\n",
      "tensor(26.5930)\n",
      "tensor(8.6767)\n",
      "tensor(4.6078)\n",
      "tensor(0.1058)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 122.641678\n",
      "Epoch 2261\n",
      "-------------------------------\n",
      "tensor(25.6234)\n",
      "tensor(8.3969)\n",
      "tensor(4.2066)\n",
      "tensor(0.1023)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.636742\n",
      "Epoch 2262\n",
      "-------------------------------\n",
      "tensor(25.4772)\n",
      "tensor(8.3765)\n",
      "tensor(3.8314)\n",
      "tensor(0.0979)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.635445\n",
      "Epoch 2263\n",
      "-------------------------------\n",
      "tensor(24.2196)\n",
      "tensor(9.1213)\n",
      "tensor(3.5834)\n",
      "tensor(0.0908)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.628532\n",
      "Epoch 2264\n",
      "-------------------------------\n",
      "tensor(15.4863)\n",
      "tensor(7.1127)\n",
      "tensor(3.8210)\n",
      "tensor(0.0902)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.627708\n",
      "Epoch 2265\n",
      "-------------------------------\n",
      "tensor(18.2354)\n",
      "tensor(6.4399)\n",
      "tensor(4.8155)\n",
      "tensor(0.1125)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.626289\n",
      "Epoch 2266\n",
      "-------------------------------\n",
      "tensor(23.6978)\n",
      "tensor(7.7755)\n",
      "tensor(5.0790)\n",
      "tensor(0.1791)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.626472\n",
      "Epoch 2267\n",
      "-------------------------------\n",
      "tensor(27.1306)\n",
      "tensor(8.8099)\n",
      "tensor(3.8870)\n",
      "tensor(0.2621)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.627113\n",
      "Epoch 2268\n",
      "-------------------------------\n",
      "tensor(26.8349)\n",
      "tensor(8.7057)\n",
      "tensor(6.7273)\n",
      "tensor(0.2563)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.613777\n",
      "Epoch 2269\n",
      "-------------------------------\n",
      "tensor(21.4348)\n",
      "tensor(8.1596)\n",
      "tensor(5.4740)\n",
      "tensor(0.1022)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.589455\n",
      "Epoch 2270\n",
      "-------------------------------\n",
      "tensor(24.7527)\n",
      "tensor(7.9651)\n",
      "tensor(5.9965)\n",
      "tensor(0.0303)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 122.603889\n",
      "Epoch 2271\n",
      "-------------------------------\n",
      "tensor(25.6345)\n",
      "tensor(8.2453)\n",
      "tensor(5.0291)\n",
      "tensor(0.1909)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.587570\n",
      "Epoch 2272\n",
      "-------------------------------\n",
      "tensor(17.7047)\n",
      "tensor(6.0040)\n",
      "tensor(6.7263)\n",
      "tensor(0.3001)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.539040\n",
      "Epoch 2273\n",
      "-------------------------------\n",
      "tensor(17.4873)\n",
      "tensor(6.0327)\n",
      "tensor(3.7752)\n",
      "tensor(0.1110)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.509079\n",
      "Epoch 2274\n",
      "-------------------------------\n",
      "tensor(28.4653)\n",
      "tensor(10.3422)\n",
      "tensor(7.0053)\n",
      "tensor(0.0700)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.500343\n",
      "Epoch 2275\n",
      "-------------------------------\n",
      "tensor(27.0143)\n",
      "tensor(9.0844)\n",
      "tensor(3.9844)\n",
      "tensor(0.0160)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.470772\n",
      "Epoch 2276\n",
      "-------------------------------\n",
      "tensor(21.1837)\n",
      "tensor(6.6502)\n",
      "tensor(5.9333)\n",
      "tensor(0.0975)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.426727\n",
      "Epoch 2277\n",
      "-------------------------------\n",
      "tensor(25.1140)\n",
      "tensor(8.1429)\n",
      "tensor(5.0277)\n",
      "tensor(0.1530)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.440445\n",
      "Epoch 2278\n",
      "-------------------------------\n",
      "tensor(24.5994)\n",
      "tensor(8.0895)\n",
      "tensor(3.6214)\n",
      "tensor(0.1311)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.430061\n",
      "Epoch 2279\n",
      "-------------------------------\n",
      "tensor(25.9334)\n",
      "tensor(8.4246)\n",
      "tensor(4.1315)\n",
      "tensor(0.1018)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.408829\n",
      "Epoch 2280\n",
      "-------------------------------\n",
      "tensor(19.8077)\n",
      "tensor(8.0044)\n",
      "tensor(4.2417)\n",
      "tensor(0.0914)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 122.394402\n",
      "Epoch 2281\n",
      "-------------------------------\n",
      "tensor(14.9982)\n",
      "tensor(7.0082)\n",
      "tensor(4.0508)\n",
      "tensor(0.0966)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.390549\n",
      "Epoch 2282\n",
      "-------------------------------\n",
      "tensor(21.9886)\n",
      "tensor(7.3534)\n",
      "tensor(3.8421)\n",
      "tensor(0.1094)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.389816\n",
      "Epoch 2283\n",
      "-------------------------------\n",
      "tensor(23.9311)\n",
      "tensor(7.8855)\n",
      "tensor(3.6225)\n",
      "tensor(0.1285)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.386391\n",
      "Epoch 2284\n",
      "-------------------------------\n",
      "tensor(17.7241)\n",
      "tensor(6.1544)\n",
      "tensor(3.7320)\n",
      "tensor(0.1548)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.377388\n",
      "Epoch 2285\n",
      "-------------------------------\n",
      "tensor(23.7640)\n",
      "tensor(7.8271)\n",
      "tensor(4.2945)\n",
      "tensor(0.1859)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.379097\n",
      "Epoch 2286\n",
      "-------------------------------\n",
      "tensor(26.9317)\n",
      "tensor(8.7580)\n",
      "tensor(4.3191)\n",
      "tensor(0.1807)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.376129\n",
      "Epoch 2287\n",
      "-------------------------------\n",
      "tensor(31.7804)\n",
      "tensor(10.2360)\n",
      "tensor(3.7153)\n",
      "tensor(0.1158)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.362747\n",
      "Epoch 2288\n",
      "-------------------------------\n",
      "tensor(20.2171)\n",
      "tensor(6.7368)\n",
      "tensor(3.9491)\n",
      "tensor(0.0379)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.349098\n",
      "Epoch 2289\n",
      "-------------------------------\n",
      "tensor(16.6661)\n",
      "tensor(7.2896)\n",
      "tensor(3.7365)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.341614\n",
      "Epoch 2290\n",
      "-------------------------------\n",
      "tensor(22.7473)\n",
      "tensor(8.6419)\n",
      "tensor(4.5984)\n",
      "tensor(0.2067)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 122.328560\n",
      "Epoch 2291\n",
      "-------------------------------\n",
      "tensor(22.0235)\n",
      "tensor(7.4919)\n",
      "tensor(4.1568)\n",
      "tensor(0.2536)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.309921\n",
      "Epoch 2292\n",
      "-------------------------------\n",
      "tensor(19.8369)\n",
      "tensor(6.5845)\n",
      "tensor(4.2229)\n",
      "tensor(0.0851)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.269264\n",
      "Epoch 2293\n",
      "-------------------------------\n",
      "tensor(25.4952)\n",
      "tensor(8.6208)\n",
      "tensor(4.2939)\n",
      "tensor(0.0445)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.269714\n",
      "Epoch 2294\n",
      "-------------------------------\n",
      "tensor(31.1477)\n",
      "tensor(10.1267)\n",
      "tensor(3.8882)\n",
      "tensor(0.0281)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.255478\n",
      "Epoch 2295\n",
      "-------------------------------\n",
      "tensor(24.8594)\n",
      "tensor(8.0970)\n",
      "tensor(4.9659)\n",
      "tensor(0.1427)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.208801\n",
      "Epoch 2296\n",
      "-------------------------------\n",
      "tensor(16.3196)\n",
      "tensor(5.5947)\n",
      "tensor(4.7555)\n",
      "tensor(0.1816)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.212677\n",
      "Epoch 2297\n",
      "-------------------------------\n",
      "tensor(31.0343)\n",
      "tensor(15.6242)\n",
      "tensor(3.9925)\n",
      "tensor(0.1528)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.205849\n",
      "Epoch 2298\n",
      "-------------------------------\n",
      "tensor(16.2807)\n",
      "tensor(6.0191)\n",
      "tensor(6.0135)\n",
      "tensor(0.1279)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.188469\n",
      "Epoch 2299\n",
      "-------------------------------\n",
      "tensor(19.6123)\n",
      "tensor(6.7737)\n",
      "tensor(5.8196)\n",
      "tensor(0.1365)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.176773\n",
      "Epoch 2300\n",
      "-------------------------------\n",
      "tensor(21.7417)\n",
      "tensor(7.3180)\n",
      "tensor(4.6753)\n",
      "tensor(0.1517)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 122.163689\n",
      "Epoch 2301\n",
      "-------------------------------\n",
      "tensor(23.7446)\n",
      "tensor(7.8567)\n",
      "tensor(3.8815)\n",
      "tensor(0.1616)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 122.154243\n",
      "Epoch 2302\n",
      "-------------------------------\n",
      "tensor(23.6022)\n",
      "tensor(7.8354)\n",
      "tensor(3.6795)\n",
      "tensor(0.1661)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 122.148628\n",
      "Epoch 2303\n",
      "-------------------------------\n",
      "tensor(15.4905)\n",
      "tensor(5.5054)\n",
      "tensor(4.1313)\n",
      "tensor(0.1626)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 122.138062\n",
      "Epoch 2304\n",
      "-------------------------------\n",
      "tensor(23.0346)\n",
      "tensor(7.5529)\n",
      "tensor(4.8660)\n",
      "tensor(0.1495)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.136208\n",
      "Epoch 2305\n",
      "-------------------------------\n",
      "tensor(24.7299)\n",
      "tensor(8.0839)\n",
      "tensor(4.8559)\n",
      "tensor(0.1070)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.130539\n",
      "Epoch 2306\n",
      "-------------------------------\n",
      "tensor(31.5640)\n",
      "tensor(10.3605)\n",
      "tensor(3.8651)\n",
      "tensor(0.0446)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.156487\n",
      "Epoch 2307\n",
      "-------------------------------\n",
      "tensor(25.1448)\n",
      "tensor(9.5785)\n",
      "tensor(4.0532)\n",
      "tensor(0.0235)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.146004\n",
      "Epoch 2308\n",
      "-------------------------------\n",
      "tensor(26.3034)\n",
      "tensor(9.7404)\n",
      "tensor(3.9770)\n",
      "tensor(0.0241)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.144333\n",
      "Epoch 2309\n",
      "-------------------------------\n",
      "tensor(28.7893)\n",
      "tensor(10.2906)\n",
      "tensor(3.8931)\n",
      "tensor(0.1643)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.104301\n",
      "Epoch 2310\n",
      "-------------------------------\n",
      "tensor(17.0786)\n",
      "tensor(5.9226)\n",
      "tensor(5.6045)\n",
      "tensor(0.2346)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 122.083321\n",
      "Epoch 2311\n",
      "-------------------------------\n",
      "tensor(20.8280)\n",
      "tensor(7.1703)\n",
      "tensor(4.4151)\n",
      "tensor(0.1805)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 122.086494\n",
      "Epoch 2312\n",
      "-------------------------------\n",
      "tensor(20.7283)\n",
      "tensor(7.1048)\n",
      "tensor(5.1367)\n",
      "tensor(0.1527)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 122.076874\n",
      "Epoch 2313\n",
      "-------------------------------\n",
      "tensor(24.8650)\n",
      "tensor(8.1977)\n",
      "tensor(4.2898)\n",
      "tensor(0.1803)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 122.062531\n",
      "Epoch 2314\n",
      "-------------------------------\n",
      "tensor(26.0242)\n",
      "tensor(8.5029)\n",
      "tensor(5.2618)\n",
      "tensor(0.0862)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 122.027740\n",
      "Epoch 2315\n",
      "-------------------------------\n",
      "tensor(15.7107)\n",
      "tensor(5.6711)\n",
      "tensor(3.9245)\n",
      "tensor(0.0208)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 122.002960\n",
      "Epoch 2316\n",
      "-------------------------------\n",
      "tensor(29.4027)\n",
      "tensor(9.1128)\n",
      "tensor(5.6587)\n",
      "tensor(0.0135)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 122.002922\n",
      "Epoch 2317\n",
      "-------------------------------\n",
      "tensor(22.9656)\n",
      "tensor(7.7496)\n",
      "tensor(4.0204)\n",
      "tensor(0.0737)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.980919\n",
      "Epoch 2318\n",
      "-------------------------------\n",
      "tensor(20.0464)\n",
      "tensor(6.8002)\n",
      "tensor(4.0330)\n",
      "tensor(0.1134)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.954544\n",
      "Epoch 2319\n",
      "-------------------------------\n",
      "tensor(18.3055)\n",
      "tensor(6.2104)\n",
      "tensor(4.3240)\n",
      "tensor(0.1136)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.926842\n",
      "Epoch 2320\n",
      "-------------------------------\n",
      "tensor(17.9950)\n",
      "tensor(6.1894)\n",
      "tensor(4.0012)\n",
      "tensor(0.0986)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 121.906387\n",
      "Epoch 2321\n",
      "-------------------------------\n",
      "tensor(22.0501)\n",
      "tensor(7.4229)\n",
      "tensor(3.7590)\n",
      "tensor(0.0882)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.906792\n",
      "Epoch 2322\n",
      "-------------------------------\n",
      "tensor(23.0233)\n",
      "tensor(7.7274)\n",
      "tensor(3.6978)\n",
      "tensor(0.0777)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.905640\n",
      "Epoch 2323\n",
      "-------------------------------\n",
      "tensor(26.5333)\n",
      "tensor(8.7294)\n",
      "tensor(3.8132)\n",
      "tensor(0.0647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.898766\n",
      "Epoch 2324\n",
      "-------------------------------\n",
      "tensor(28.4028)\n",
      "tensor(9.2562)\n",
      "tensor(3.9890)\n",
      "tensor(0.0500)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.884613\n",
      "Epoch 2325\n",
      "-------------------------------\n",
      "tensor(26.3080)\n",
      "tensor(8.6667)\n",
      "tensor(3.7441)\n",
      "tensor(0.0531)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.888702\n",
      "Epoch 2326\n",
      "-------------------------------\n",
      "tensor(26.1408)\n",
      "tensor(9.6819)\n",
      "tensor(4.0834)\n",
      "tensor(0.0871)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.905884\n",
      "Epoch 2327\n",
      "-------------------------------\n",
      "tensor(23.2423)\n",
      "tensor(8.8421)\n",
      "tensor(5.0405)\n",
      "tensor(0.1274)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.893471\n",
      "Epoch 2328\n",
      "-------------------------------\n",
      "tensor(19.7889)\n",
      "tensor(8.0489)\n",
      "tensor(3.8671)\n",
      "tensor(0.1587)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.856323\n",
      "Epoch 2329\n",
      "-------------------------------\n",
      "tensor(33.6365)\n",
      "tensor(16.3913)\n",
      "tensor(5.0173)\n",
      "tensor(0.2122)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.843826\n",
      "Epoch 2330\n",
      "-------------------------------\n",
      "tensor(18.4724)\n",
      "tensor(6.9937)\n",
      "tensor(5.1104)\n",
      "tensor(0.2748)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 121.812447\n",
      "Epoch 2331\n",
      "-------------------------------\n",
      "tensor(23.4361)\n",
      "tensor(8.0431)\n",
      "tensor(4.9187)\n",
      "tensor(0.2799)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.778824\n",
      "Epoch 2332\n",
      "-------------------------------\n",
      "tensor(30.6393)\n",
      "tensor(10.0810)\n",
      "tensor(5.3454)\n",
      "tensor(0.1002)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.819557\n",
      "Epoch 2333\n",
      "-------------------------------\n",
      "tensor(27.5315)\n",
      "tensor(9.5422)\n",
      "tensor(4.6142)\n",
      "tensor(0.1158)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.792320\n",
      "Epoch 2334\n",
      "-------------------------------\n",
      "tensor(23.3591)\n",
      "tensor(8.0511)\n",
      "tensor(4.3942)\n",
      "tensor(0.0347)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.755081\n",
      "Epoch 2335\n",
      "-------------------------------\n",
      "tensor(21.8096)\n",
      "tensor(7.5232)\n",
      "tensor(3.9635)\n",
      "tensor(0.1996)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.728867\n",
      "Epoch 2336\n",
      "-------------------------------\n",
      "tensor(18.3421)\n",
      "tensor(6.8471)\n",
      "tensor(4.6388)\n",
      "tensor(0.2897)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.735474\n",
      "Epoch 2337\n",
      "-------------------------------\n",
      "tensor(18.1631)\n",
      "tensor(6.6265)\n",
      "tensor(3.9336)\n",
      "tensor(0.1979)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.724792\n",
      "Epoch 2338\n",
      "-------------------------------\n",
      "tensor(14.6610)\n",
      "tensor(5.6942)\n",
      "tensor(3.9229)\n",
      "tensor(0.0771)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.705017\n",
      "Epoch 2339\n",
      "-------------------------------\n",
      "tensor(19.1881)\n",
      "tensor(6.8561)\n",
      "tensor(4.3432)\n",
      "tensor(0.0186)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.694511\n",
      "Epoch 2340\n",
      "-------------------------------\n",
      "tensor(19.1604)\n",
      "tensor(6.8584)\n",
      "tensor(4.3391)\n",
      "tensor(0.0085)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 121.681023\n",
      "Epoch 2341\n",
      "-------------------------------\n",
      "tensor(16.1113)\n",
      "tensor(6.0513)\n",
      "tensor(4.1470)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.668617\n",
      "Epoch 2342\n",
      "-------------------------------\n",
      "tensor(19.3519)\n",
      "tensor(6.8672)\n",
      "tensor(3.9380)\n",
      "tensor(0.0422)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.662445\n",
      "Epoch 2343\n",
      "-------------------------------\n",
      "tensor(18.4764)\n",
      "tensor(7.9191)\n",
      "tensor(3.7471)\n",
      "tensor(0.0770)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.652687\n",
      "Epoch 2344\n",
      "-------------------------------\n",
      "tensor(26.8389)\n",
      "tensor(9.8569)\n",
      "tensor(3.7621)\n",
      "tensor(0.1330)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.646790\n",
      "Epoch 2345\n",
      "-------------------------------\n",
      "tensor(27.6265)\n",
      "tensor(9.0505)\n",
      "tensor(4.3176)\n",
      "tensor(0.1891)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.641510\n",
      "Epoch 2346\n",
      "-------------------------------\n",
      "tensor(29.8693)\n",
      "tensor(9.7283)\n",
      "tensor(4.9467)\n",
      "tensor(0.1906)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.638283\n",
      "Epoch 2347\n",
      "-------------------------------\n",
      "tensor(24.0954)\n",
      "tensor(7.9945)\n",
      "tensor(4.6618)\n",
      "tensor(0.0903)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.636971\n",
      "Epoch 2348\n",
      "-------------------------------\n",
      "tensor(26.4378)\n",
      "tensor(8.8215)\n",
      "tensor(4.3500)\n",
      "tensor(0.0231)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.627960\n",
      "Epoch 2349\n",
      "-------------------------------\n",
      "tensor(28.8174)\n",
      "tensor(10.2602)\n",
      "tensor(4.5105)\n",
      "tensor(0.0485)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.576416\n",
      "Epoch 2350\n",
      "-------------------------------\n",
      "tensor(18.8753)\n",
      "tensor(7.0627)\n",
      "tensor(5.6850)\n",
      "tensor(0.3107)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 121.569450\n",
      "Epoch 2351\n",
      "-------------------------------\n",
      "tensor(22.4447)\n",
      "tensor(8.1546)\n",
      "tensor(4.8722)\n",
      "tensor(0.3548)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.586098\n",
      "Epoch 2352\n",
      "-------------------------------\n",
      "tensor(28.3205)\n",
      "tensor(9.0533)\n",
      "tensor(4.7120)\n",
      "tensor(0.1048)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.585793\n",
      "Epoch 2353\n",
      "-------------------------------\n",
      "tensor(20.8794)\n",
      "tensor(7.0884)\n",
      "tensor(4.3820)\n",
      "tensor(0.0547)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.564110\n",
      "Epoch 2354\n",
      "-------------------------------\n",
      "tensor(16.7114)\n",
      "tensor(7.2809)\n",
      "tensor(4.3805)\n",
      "tensor(0.0144)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.543518\n",
      "Epoch 2355\n",
      "-------------------------------\n",
      "tensor(20.2106)\n",
      "tensor(7.1073)\n",
      "tensor(4.4273)\n",
      "tensor(0.1320)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.521652\n",
      "Epoch 2356\n",
      "-------------------------------\n",
      "tensor(15.6768)\n",
      "tensor(6.1979)\n",
      "tensor(5.2043)\n",
      "tensor(0.1591)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.492241\n",
      "Epoch 2357\n",
      "-------------------------------\n",
      "tensor(15.2937)\n",
      "tensor(5.9683)\n",
      "tensor(4.3984)\n",
      "tensor(0.1070)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.455711\n",
      "Epoch 2358\n",
      "-------------------------------\n",
      "tensor(18.1387)\n",
      "tensor(6.5501)\n",
      "tensor(3.9410)\n",
      "tensor(0.0381)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.420616\n",
      "Epoch 2359\n",
      "-------------------------------\n",
      "tensor(17.7111)\n",
      "tensor(6.4076)\n",
      "tensor(3.9512)\n",
      "tensor(0.0060)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.395706\n",
      "Epoch 2360\n",
      "-------------------------------\n",
      "tensor(17.6542)\n",
      "tensor(6.4105)\n",
      "tensor(3.9779)\n",
      "tensor(0.0227)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 121.375000\n",
      "Epoch 2361\n",
      "-------------------------------\n",
      "tensor(24.4660)\n",
      "tensor(8.3994)\n",
      "tensor(3.9633)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.369781\n",
      "Epoch 2362\n",
      "-------------------------------\n",
      "tensor(30.7548)\n",
      "tensor(10.2493)\n",
      "tensor(3.9259)\n",
      "tensor(0.0071)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.374496\n",
      "Epoch 2363\n",
      "-------------------------------\n",
      "tensor(30.7000)\n",
      "tensor(10.1880)\n",
      "tensor(3.8728)\n",
      "tensor(0.0102)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.371849\n",
      "Epoch 2364\n",
      "-------------------------------\n",
      "tensor(31.8406)\n",
      "tensor(10.4966)\n",
      "tensor(3.9440)\n",
      "tensor(0.0397)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.366692\n",
      "Epoch 2365\n",
      "-------------------------------\n",
      "tensor(22.6027)\n",
      "tensor(8.7826)\n",
      "tensor(4.7199)\n",
      "tensor(0.0829)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.370514\n",
      "Epoch 2366\n",
      "-------------------------------\n",
      "tensor(23.1871)\n",
      "tensor(8.9411)\n",
      "tensor(4.7571)\n",
      "tensor(0.1321)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.374115\n",
      "Epoch 2367\n",
      "-------------------------------\n",
      "tensor(20.0397)\n",
      "tensor(7.0385)\n",
      "tensor(3.8707)\n",
      "tensor(0.1729)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.358421\n",
      "Epoch 2368\n",
      "-------------------------------\n",
      "tensor(16.7346)\n",
      "tensor(6.6365)\n",
      "tensor(5.0071)\n",
      "tensor(0.2436)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.355141\n",
      "Epoch 2369\n",
      "-------------------------------\n",
      "tensor(37.7360)\n",
      "tensor(17.4902)\n",
      "tensor(4.7754)\n",
      "tensor(0.3329)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.346313\n",
      "Epoch 2370\n",
      "-------------------------------\n",
      "tensor(19.0441)\n",
      "tensor(7.0596)\n",
      "tensor(4.7808)\n",
      "tensor(0.2625)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 121.320114\n",
      "Epoch 2371\n",
      "-------------------------------\n",
      "tensor(18.8605)\n",
      "tensor(6.4475)\n",
      "tensor(4.1395)\n",
      "tensor(0.0720)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.278519\n",
      "Epoch 2372\n",
      "-------------------------------\n",
      "tensor(31.2700)\n",
      "tensor(11.0197)\n",
      "tensor(4.6409)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.309547\n",
      "Epoch 2373\n",
      "-------------------------------\n",
      "tensor(24.5848)\n",
      "tensor(8.2843)\n",
      "tensor(4.2102)\n",
      "tensor(0.0624)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.280182\n",
      "Epoch 2374\n",
      "-------------------------------\n",
      "tensor(23.8548)\n",
      "tensor(8.0517)\n",
      "tensor(4.5462)\n",
      "tensor(0.0541)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.225334\n",
      "Epoch 2375\n",
      "-------------------------------\n",
      "tensor(24.1258)\n",
      "tensor(8.3108)\n",
      "tensor(3.9997)\n",
      "tensor(0.0105)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.183487\n",
      "Epoch 2376\n",
      "-------------------------------\n",
      "tensor(21.5126)\n",
      "tensor(7.5490)\n",
      "tensor(4.2536)\n",
      "tensor(0.0305)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.180511\n",
      "Epoch 2377\n",
      "-------------------------------\n",
      "tensor(24.3090)\n",
      "tensor(9.3133)\n",
      "tensor(3.8261)\n",
      "tensor(0.0877)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.185776\n",
      "Epoch 2378\n",
      "-------------------------------\n",
      "tensor(22.0152)\n",
      "tensor(7.5627)\n",
      "tensor(3.8423)\n",
      "tensor(0.1175)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.170235\n",
      "Epoch 2379\n",
      "-------------------------------\n",
      "tensor(18.4969)\n",
      "tensor(6.5756)\n",
      "tensor(3.9182)\n",
      "tensor(0.1204)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.148148\n",
      "Epoch 2380\n",
      "-------------------------------\n",
      "tensor(16.5248)\n",
      "tensor(6.0833)\n",
      "tensor(3.8155)\n",
      "tensor(0.1176)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 121.137878\n",
      "Epoch 2381\n",
      "-------------------------------\n",
      "tensor(23.7252)\n",
      "tensor(8.0423)\n",
      "tensor(3.7494)\n",
      "tensor(0.1168)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 121.139641\n",
      "Epoch 2382\n",
      "-------------------------------\n",
      "tensor(23.7340)\n",
      "tensor(8.0435)\n",
      "tensor(3.7536)\n",
      "tensor(0.1133)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 121.135262\n",
      "Epoch 2383\n",
      "-------------------------------\n",
      "tensor(22.7472)\n",
      "tensor(7.7748)\n",
      "tensor(3.7971)\n",
      "tensor(0.1069)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 121.123322\n",
      "Epoch 2384\n",
      "-------------------------------\n",
      "tensor(15.1285)\n",
      "tensor(7.2740)\n",
      "tensor(3.8331)\n",
      "tensor(0.1026)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 121.112251\n",
      "Epoch 2385\n",
      "-------------------------------\n",
      "tensor(21.4398)\n",
      "tensor(7.4334)\n",
      "tensor(3.8934)\n",
      "tensor(0.1095)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 121.103348\n",
      "Epoch 2386\n",
      "-------------------------------\n",
      "tensor(30.1209)\n",
      "tensor(9.7837)\n",
      "tensor(3.7936)\n",
      "tensor(0.1314)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.113167\n",
      "Epoch 2387\n",
      "-------------------------------\n",
      "tensor(26.0102)\n",
      "tensor(9.7664)\n",
      "tensor(4.5102)\n",
      "tensor(0.1264)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.118904\n",
      "Epoch 2388\n",
      "-------------------------------\n",
      "tensor(24.7674)\n",
      "tensor(8.2762)\n",
      "tensor(4.9517)\n",
      "tensor(0.0651)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.102203\n",
      "Epoch 2389\n",
      "-------------------------------\n",
      "tensor(25.2233)\n",
      "tensor(8.4935)\n",
      "tensor(4.1282)\n",
      "tensor(0.0037)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.037384\n",
      "Epoch 2390\n",
      "-------------------------------\n",
      "tensor(22.9285)\n",
      "tensor(7.7972)\n",
      "tensor(4.0752)\n",
      "tensor(0.1254)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 121.063606\n",
      "Epoch 2391\n",
      "-------------------------------\n",
      "tensor(26.7067)\n",
      "tensor(8.9452)\n",
      "tensor(4.3584)\n",
      "tensor(0.2541)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 121.069534\n",
      "Epoch 2392\n",
      "-------------------------------\n",
      "tensor(20.1638)\n",
      "tensor(6.9094)\n",
      "tensor(4.7929)\n",
      "tensor(0.1411)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 121.021317\n",
      "Epoch 2393\n",
      "-------------------------------\n",
      "tensor(18.8330)\n",
      "tensor(8.0195)\n",
      "tensor(4.3222)\n",
      "tensor(0.0059)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 121.018341\n",
      "Epoch 2394\n",
      "-------------------------------\n",
      "tensor(22.2631)\n",
      "tensor(7.6191)\n",
      "tensor(5.0874)\n",
      "tensor(0.0384)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 121.008820\n",
      "Epoch 2395\n",
      "-------------------------------\n",
      "tensor(17.8037)\n",
      "tensor(6.5115)\n",
      "tensor(3.9897)\n",
      "tensor(0.1517)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.959770\n",
      "Epoch 2396\n",
      "-------------------------------\n",
      "tensor(18.6126)\n",
      "tensor(6.6173)\n",
      "tensor(4.3976)\n",
      "tensor(0.1573)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.913208\n",
      "Epoch 2397\n",
      "-------------------------------\n",
      "tensor(22.4585)\n",
      "tensor(7.7011)\n",
      "tensor(3.9322)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.909988\n",
      "Epoch 2398\n",
      "-------------------------------\n",
      "tensor(22.3959)\n",
      "tensor(7.7826)\n",
      "tensor(4.0980)\n",
      "tensor(0.0040)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.884987\n",
      "Epoch 2399\n",
      "-------------------------------\n",
      "tensor(26.1540)\n",
      "tensor(8.8997)\n",
      "tensor(4.2880)\n",
      "tensor(0.0291)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.856056\n",
      "Epoch 2400\n",
      "-------------------------------\n",
      "tensor(17.1079)\n",
      "tensor(7.7836)\n",
      "tensor(4.1426)\n",
      "tensor(0.0284)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 120.830818\n",
      "Epoch 2401\n",
      "-------------------------------\n",
      "tensor(26.3418)\n",
      "tensor(9.9964)\n",
      "tensor(3.9962)\n",
      "tensor(0.0073)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.829460\n",
      "Epoch 2402\n",
      "-------------------------------\n",
      "tensor(40.6819)\n",
      "tensor(18.6953)\n",
      "tensor(3.8980)\n",
      "tensor(0.0180)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.832588\n",
      "Epoch 2403\n",
      "-------------------------------\n",
      "tensor(25.1416)\n",
      "tensor(8.4728)\n",
      "tensor(3.8897)\n",
      "tensor(0.0481)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.826530\n",
      "Epoch 2404\n",
      "-------------------------------\n",
      "tensor(17.0701)\n",
      "tensor(6.1176)\n",
      "tensor(4.0196)\n",
      "tensor(0.0912)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.816711\n",
      "Epoch 2405\n",
      "-------------------------------\n",
      "tensor(19.9295)\n",
      "tensor(7.0128)\n",
      "tensor(4.0213)\n",
      "tensor(0.1494)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.813400\n",
      "Epoch 2406\n",
      "-------------------------------\n",
      "tensor(24.0614)\n",
      "tensor(8.1889)\n",
      "tensor(3.9444)\n",
      "tensor(0.1945)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.811729\n",
      "Epoch 2407\n",
      "-------------------------------\n",
      "tensor(30.8516)\n",
      "tensor(9.9066)\n",
      "tensor(4.0406)\n",
      "tensor(0.1874)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.796242\n",
      "Epoch 2408\n",
      "-------------------------------\n",
      "tensor(19.6380)\n",
      "tensor(6.8607)\n",
      "tensor(4.4708)\n",
      "tensor(0.1346)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.791275\n",
      "Epoch 2409\n",
      "-------------------------------\n",
      "tensor(25.2289)\n",
      "tensor(9.5963)\n",
      "tensor(4.4373)\n",
      "tensor(0.1035)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.800377\n",
      "Epoch 2410\n",
      "-------------------------------\n",
      "tensor(22.3471)\n",
      "tensor(8.8200)\n",
      "tensor(3.9182)\n",
      "tensor(0.1214)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 120.753448\n",
      "Epoch 2411\n",
      "-------------------------------\n",
      "tensor(22.1299)\n",
      "tensor(7.7529)\n",
      "tensor(4.0884)\n",
      "tensor(0.2018)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.736900\n",
      "Epoch 2412\n",
      "-------------------------------\n",
      "tensor(14.8809)\n",
      "tensor(5.9441)\n",
      "tensor(4.0303)\n",
      "tensor(0.1777)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.696625\n",
      "Epoch 2413\n",
      "-------------------------------\n",
      "tensor(28.2535)\n",
      "tensor(9.4029)\n",
      "tensor(3.9794)\n",
      "tensor(0.0560)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.708679\n",
      "Epoch 2414\n",
      "-------------------------------\n",
      "tensor(26.0248)\n",
      "tensor(9.1008)\n",
      "tensor(4.3489)\n",
      "tensor(0.0746)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.687263\n",
      "Epoch 2415\n",
      "-------------------------------\n",
      "tensor(24.8077)\n",
      "tensor(8.7327)\n",
      "tensor(4.4788)\n",
      "tensor(0.0888)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.634300\n",
      "Epoch 2416\n",
      "-------------------------------\n",
      "tensor(20.4717)\n",
      "tensor(7.2444)\n",
      "tensor(3.9343)\n",
      "tensor(0.0170)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.627457\n",
      "Epoch 2417\n",
      "-------------------------------\n",
      "tensor(19.2150)\n",
      "tensor(6.9633)\n",
      "tensor(4.1154)\n",
      "tensor(0.1173)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.612885\n",
      "Epoch 2418\n",
      "-------------------------------\n",
      "tensor(20.7601)\n",
      "tensor(7.3822)\n",
      "tensor(4.3354)\n",
      "tensor(0.1639)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.603233\n",
      "Epoch 2419\n",
      "-------------------------------\n",
      "tensor(20.6718)\n",
      "tensor(7.3549)\n",
      "tensor(4.1326)\n",
      "tensor(0.1635)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.592995\n",
      "Epoch 2420\n",
      "-------------------------------\n",
      "tensor(26.3751)\n",
      "tensor(8.7925)\n",
      "tensor(3.9420)\n",
      "tensor(0.1436)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 120.581245\n",
      "Epoch 2421\n",
      "-------------------------------\n",
      "tensor(24.4550)\n",
      "tensor(8.2916)\n",
      "tensor(3.8674)\n",
      "tensor(0.1219)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.575027\n",
      "Epoch 2422\n",
      "-------------------------------\n",
      "tensor(22.4651)\n",
      "tensor(7.7317)\n",
      "tensor(3.9090)\n",
      "tensor(0.1000)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.567917\n",
      "Epoch 2423\n",
      "-------------------------------\n",
      "tensor(17.6912)\n",
      "tensor(6.3384)\n",
      "tensor(4.0313)\n",
      "tensor(0.0748)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.565536\n",
      "Epoch 2424\n",
      "-------------------------------\n",
      "tensor(21.9911)\n",
      "tensor(7.5618)\n",
      "tensor(4.0704)\n",
      "tensor(0.0458)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.566193\n",
      "Epoch 2425\n",
      "-------------------------------\n",
      "tensor(21.9370)\n",
      "tensor(7.5825)\n",
      "tensor(3.9883)\n",
      "tensor(0.0178)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.554245\n",
      "Epoch 2426\n",
      "-------------------------------\n",
      "tensor(16.9273)\n",
      "tensor(6.2465)\n",
      "tensor(4.0342)\n",
      "tensor(0.0257)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.523232\n",
      "Epoch 2427\n",
      "-------------------------------\n",
      "tensor(24.9633)\n",
      "tensor(9.4154)\n",
      "tensor(4.2564)\n",
      "tensor(0.1221)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.535049\n",
      "Epoch 2428\n",
      "-------------------------------\n",
      "tensor(28.1851)\n",
      "tensor(10.2382)\n",
      "tensor(4.2425)\n",
      "tensor(0.2026)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.542648\n",
      "Epoch 2429\n",
      "-------------------------------\n",
      "tensor(43.2492)\n",
      "tensor(19.6429)\n",
      "tensor(5.4405)\n",
      "tensor(0.1160)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.534729\n",
      "Epoch 2430\n",
      "-------------------------------\n",
      "tensor(29.6886)\n",
      "tensor(10.1606)\n",
      "tensor(5.0092)\n",
      "tensor(0.1068)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 120.462898\n",
      "Epoch 2431\n",
      "-------------------------------\n",
      "tensor(18.3395)\n",
      "tensor(6.3267)\n",
      "tensor(4.6602)\n",
      "tensor(4.5776e-05)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.447113\n",
      "Epoch 2432\n",
      "-------------------------------\n",
      "tensor(19.6030)\n",
      "tensor(8.0572)\n",
      "tensor(4.9596)\n",
      "tensor(0.3744)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.459038\n",
      "Epoch 2433\n",
      "-------------------------------\n",
      "tensor(18.2818)\n",
      "tensor(8.2296)\n",
      "tensor(5.6764)\n",
      "tensor(0.4565)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.443344\n",
      "Epoch 2434\n",
      "-------------------------------\n",
      "tensor(19.0929)\n",
      "tensor(6.9925)\n",
      "tensor(3.9178)\n",
      "tensor(0.1498)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.421822\n",
      "Epoch 2435\n",
      "-------------------------------\n",
      "tensor(24.9667)\n",
      "tensor(8.9300)\n",
      "tensor(4.8025)\n",
      "tensor(0.1394)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.383072\n",
      "Epoch 2436\n",
      "-------------------------------\n",
      "tensor(22.1410)\n",
      "tensor(8.1731)\n",
      "tensor(4.7030)\n",
      "tensor(0.1495)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.367760\n",
      "Epoch 2437\n",
      "-------------------------------\n",
      "tensor(27.4982)\n",
      "tensor(9.4681)\n",
      "tensor(4.0648)\n",
      "tensor(0.0133)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.347786\n",
      "Epoch 2438\n",
      "-------------------------------\n",
      "tensor(20.0584)\n",
      "tensor(7.2215)\n",
      "tensor(4.1278)\n",
      "tensor(0.0882)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.317429\n",
      "Epoch 2439\n",
      "-------------------------------\n",
      "tensor(14.8106)\n",
      "tensor(6.0055)\n",
      "tensor(4.1878)\n",
      "tensor(0.1258)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.301689\n",
      "Epoch 2440\n",
      "-------------------------------\n",
      "tensor(17.2074)\n",
      "tensor(6.6122)\n",
      "tensor(4.2111)\n",
      "tensor(0.1243)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 120.291862\n",
      "Epoch 2441\n",
      "-------------------------------\n",
      "tensor(15.1424)\n",
      "tensor(6.1690)\n",
      "tensor(4.2546)\n",
      "tensor(0.1089)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 120.283157\n",
      "Epoch 2442\n",
      "-------------------------------\n",
      "tensor(15.0920)\n",
      "tensor(6.1593)\n",
      "tensor(4.3149)\n",
      "tensor(0.0891)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.274300\n",
      "Epoch 2443\n",
      "-------------------------------\n",
      "tensor(24.3173)\n",
      "tensor(8.3822)\n",
      "tensor(4.3830)\n",
      "tensor(0.0629)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.265244\n",
      "Epoch 2444\n",
      "-------------------------------\n",
      "tensor(30.7288)\n",
      "tensor(10.1930)\n",
      "tensor(4.2242)\n",
      "tensor(0.0256)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.253899\n",
      "Epoch 2445\n",
      "-------------------------------\n",
      "tensor(25.4931)\n",
      "tensor(8.8018)\n",
      "tensor(4.1161)\n",
      "tensor(0.0104)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.259247\n",
      "Epoch 2446\n",
      "-------------------------------\n",
      "tensor(28.5947)\n",
      "tensor(9.7700)\n",
      "tensor(5.4888)\n",
      "tensor(0.0228)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.257751\n",
      "Epoch 2447\n",
      "-------------------------------\n",
      "tensor(30.8936)\n",
      "tensor(11.5186)\n",
      "tensor(5.8629)\n",
      "tensor(0.0102)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.224236\n",
      "Epoch 2448\n",
      "-------------------------------\n",
      "tensor(23.9438)\n",
      "tensor(9.3875)\n",
      "tensor(4.6452)\n",
      "tensor(0.1268)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.225342\n",
      "Epoch 2449\n",
      "-------------------------------\n",
      "tensor(17.1238)\n",
      "tensor(8.3540)\n",
      "tensor(4.9552)\n",
      "tensor(0.2865)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.231110\n",
      "Epoch 2450\n",
      "-------------------------------\n",
      "tensor(16.2604)\n",
      "tensor(7.8256)\n",
      "tensor(5.7438)\n",
      "tensor(0.3942)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 120.214195\n",
      "Epoch 2451\n",
      "-------------------------------\n",
      "tensor(22.1802)\n",
      "tensor(8.1754)\n",
      "tensor(4.7073)\n",
      "tensor(0.2872)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 120.181824\n",
      "Epoch 2452\n",
      "-------------------------------\n",
      "tensor(24.0731)\n",
      "tensor(8.2047)\n",
      "tensor(5.2708)\n",
      "tensor(0.0300)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 120.161247\n",
      "Epoch 2453\n",
      "-------------------------------\n",
      "tensor(22.0666)\n",
      "tensor(8.3728)\n",
      "tensor(5.3289)\n",
      "tensor(0.2140)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 120.105522\n",
      "Epoch 2454\n",
      "-------------------------------\n",
      "tensor(23.0505)\n",
      "tensor(8.2751)\n",
      "tensor(5.2373)\n",
      "tensor(0.0501)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 120.063766\n",
      "Epoch 2455\n",
      "-------------------------------\n",
      "tensor(29.6890)\n",
      "tensor(9.6948)\n",
      "tensor(5.0027)\n",
      "tensor(0.1569)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 120.069008\n",
      "Epoch 2456\n",
      "-------------------------------\n",
      "tensor(42.1063)\n",
      "tensor(19.2461)\n",
      "tensor(5.7879)\n",
      "tensor(0.0998)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 120.085434\n",
      "Epoch 2457\n",
      "-------------------------------\n",
      "tensor(22.4968)\n",
      "tensor(7.9599)\n",
      "tensor(4.3247)\n",
      "tensor(0.0678)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 120.019951\n",
      "Epoch 2458\n",
      "-------------------------------\n",
      "tensor(17.8703)\n",
      "tensor(6.6631)\n",
      "tensor(5.0993)\n",
      "tensor(0.1242)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 120.003143\n",
      "Epoch 2459\n",
      "-------------------------------\n",
      "tensor(18.0356)\n",
      "tensor(6.5971)\n",
      "tensor(5.2180)\n",
      "tensor(0.0685)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.993797\n",
      "Epoch 2460\n",
      "-------------------------------\n",
      "tensor(15.0884)\n",
      "tensor(5.8626)\n",
      "tensor(4.7250)\n",
      "tensor(0.0145)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 119.982849\n",
      "Epoch 2461\n",
      "-------------------------------\n",
      "tensor(16.4648)\n",
      "tensor(6.3003)\n",
      "tensor(4.3196)\n",
      "tensor(0.0841)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.974388\n",
      "Epoch 2462\n",
      "-------------------------------\n",
      "tensor(14.7943)\n",
      "tensor(6.0719)\n",
      "tensor(4.1135)\n",
      "tensor(0.1416)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.966179\n",
      "Epoch 2463\n",
      "-------------------------------\n",
      "tensor(18.0628)\n",
      "tensor(6.9504)\n",
      "tensor(4.1532)\n",
      "tensor(0.2015)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.954430\n",
      "Epoch 2464\n",
      "-------------------------------\n",
      "tensor(26.8110)\n",
      "tensor(9.1359)\n",
      "tensor(4.6128)\n",
      "tensor(0.2652)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.955956\n",
      "Epoch 2465\n",
      "-------------------------------\n",
      "tensor(30.3706)\n",
      "tensor(10.1073)\n",
      "tensor(5.4020)\n",
      "tensor(0.2793)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.960297\n",
      "Epoch 2466\n",
      "-------------------------------\n",
      "tensor(33.7011)\n",
      "tensor(12.0862)\n",
      "tensor(5.8967)\n",
      "tensor(0.1623)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.965073\n",
      "Epoch 2467\n",
      "-------------------------------\n",
      "tensor(26.6180)\n",
      "tensor(10.3592)\n",
      "tensor(6.2457)\n",
      "tensor(0.0963)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.943695\n",
      "Epoch 2468\n",
      "-------------------------------\n",
      "tensor(23.8923)\n",
      "tensor(9.8955)\n",
      "tensor(6.0934)\n",
      "tensor(0.2439)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.919090\n",
      "Epoch 2469\n",
      "-------------------------------\n",
      "tensor(15.5923)\n",
      "tensor(6.0230)\n",
      "tensor(5.6740)\n",
      "tensor(0.0140)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.899803\n",
      "Epoch 2470\n",
      "-------------------------------\n",
      "tensor(17.1090)\n",
      "tensor(8.0897)\n",
      "tensor(6.4754)\n",
      "tensor(0.4688)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 119.887001\n",
      "Epoch 2471\n",
      "-------------------------------\n",
      "tensor(23.8544)\n",
      "tensor(8.8455)\n",
      "tensor(6.6749)\n",
      "tensor(0.4252)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.877220\n",
      "Epoch 2472\n",
      "-------------------------------\n",
      "tensor(27.0070)\n",
      "tensor(9.6412)\n",
      "tensor(5.1346)\n",
      "tensor(0.1577)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.855827\n",
      "Epoch 2473\n",
      "-------------------------------\n",
      "tensor(39.2886)\n",
      "tensor(20.3244)\n",
      "tensor(7.4596)\n",
      "tensor(0.4166)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.838303\n",
      "Epoch 2474\n",
      "-------------------------------\n",
      "tensor(21.7251)\n",
      "tensor(7.6762)\n",
      "tensor(5.7538)\n",
      "tensor(0.0219)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.788414\n",
      "Epoch 2475\n",
      "-------------------------------\n",
      "tensor(26.6401)\n",
      "tensor(9.5581)\n",
      "tensor(6.2772)\n",
      "tensor(0.4563)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.782616\n",
      "Epoch 2476\n",
      "-------------------------------\n",
      "tensor(26.1109)\n",
      "tensor(9.5955)\n",
      "tensor(7.3150)\n",
      "tensor(0.4521)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.760391\n",
      "Epoch 2477\n",
      "-------------------------------\n",
      "tensor(17.8952)\n",
      "tensor(6.5836)\n",
      "tensor(5.1493)\n",
      "tensor(0.1432)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.726280\n",
      "Epoch 2478\n",
      "-------------------------------\n",
      "tensor(15.7390)\n",
      "tensor(5.9021)\n",
      "tensor(4.8641)\n",
      "tensor(0.0855)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.711052\n",
      "Epoch 2479\n",
      "-------------------------------\n",
      "tensor(17.4307)\n",
      "tensor(6.5677)\n",
      "tensor(6.0165)\n",
      "tensor(0.1387)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.695786\n",
      "Epoch 2480\n",
      "-------------------------------\n",
      "tensor(21.3870)\n",
      "tensor(7.6329)\n",
      "tensor(5.9847)\n",
      "tensor(0.0931)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 119.683830\n",
      "Epoch 2481\n",
      "-------------------------------\n",
      "tensor(26.9785)\n",
      "tensor(9.1117)\n",
      "tensor(5.3064)\n",
      "tensor(0.0254)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.674309\n",
      "Epoch 2482\n",
      "-------------------------------\n",
      "tensor(22.2213)\n",
      "tensor(7.8430)\n",
      "tensor(4.4523)\n",
      "tensor(0.0468)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.669365\n",
      "Epoch 2483\n",
      "-------------------------------\n",
      "tensor(27.7452)\n",
      "tensor(9.3440)\n",
      "tensor(4.0313)\n",
      "tensor(0.1371)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.661530\n",
      "Epoch 2484\n",
      "-------------------------------\n",
      "tensor(22.7696)\n",
      "tensor(8.1560)\n",
      "tensor(5.3748)\n",
      "tensor(0.2478)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.657806\n",
      "Epoch 2485\n",
      "-------------------------------\n",
      "tensor(21.1081)\n",
      "tensor(7.7483)\n",
      "tensor(7.4333)\n",
      "tensor(0.3209)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.647713\n",
      "Epoch 2486\n",
      "-------------------------------\n",
      "tensor(21.3232)\n",
      "tensor(8.8713)\n",
      "tensor(6.0652)\n",
      "tensor(0.2450)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.638931\n",
      "Epoch 2487\n",
      "-------------------------------\n",
      "tensor(24.1432)\n",
      "tensor(9.5730)\n",
      "tensor(4.8115)\n",
      "tensor(0.0168)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.638252\n",
      "Epoch 2488\n",
      "-------------------------------\n",
      "tensor(33.4585)\n",
      "tensor(12.3860)\n",
      "tensor(7.9960)\n",
      "tensor(0.2352)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.632156\n",
      "Epoch 2489\n",
      "-------------------------------\n",
      "tensor(24.7284)\n",
      "tensor(8.7046)\n",
      "tensor(5.5358)\n",
      "tensor(0.1108)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.606178\n",
      "Epoch 2490\n",
      "-------------------------------\n",
      "tensor(26.1284)\n",
      "tensor(8.7426)\n",
      "tensor(8.1712)\n",
      "tensor(0.1875)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 119.576027\n",
      "Epoch 2491\n",
      "-------------------------------\n",
      "tensor(23.0011)\n",
      "tensor(8.2224)\n",
      "tensor(4.8204)\n",
      "tensor(0.2286)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.549438\n",
      "Epoch 2492\n",
      "-------------------------------\n",
      "tensor(21.2159)\n",
      "tensor(7.5362)\n",
      "tensor(6.5680)\n",
      "tensor(0.1329)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.529724\n",
      "Epoch 2493\n",
      "-------------------------------\n",
      "tensor(19.9522)\n",
      "tensor(7.1484)\n",
      "tensor(4.7235)\n",
      "tensor(0.1108)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.489906\n",
      "Epoch 2494\n",
      "-------------------------------\n",
      "tensor(26.7270)\n",
      "tensor(9.1009)\n",
      "tensor(5.7213)\n",
      "tensor(0.0567)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.456268\n",
      "Epoch 2495\n",
      "-------------------------------\n",
      "tensor(39.1066)\n",
      "tensor(19.3923)\n",
      "tensor(4.1630)\n",
      "tensor(0.0118)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.434731\n",
      "Epoch 2496\n",
      "-------------------------------\n",
      "tensor(24.5731)\n",
      "tensor(8.3083)\n",
      "tensor(6.4647)\n",
      "tensor(5.5313e-05)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.419182\n",
      "Epoch 2497\n",
      "-------------------------------\n",
      "tensor(23.7624)\n",
      "tensor(8.1947)\n",
      "tensor(4.9317)\n",
      "tensor(0.0945)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.401604\n",
      "Epoch 2498\n",
      "-------------------------------\n",
      "tensor(16.6312)\n",
      "tensor(6.5373)\n",
      "tensor(4.3371)\n",
      "tensor(0.1578)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.382858\n",
      "Epoch 2499\n",
      "-------------------------------\n",
      "tensor(17.2257)\n",
      "tensor(6.5526)\n",
      "tensor(4.9515)\n",
      "tensor(0.1596)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.365150\n",
      "Epoch 2500\n",
      "-------------------------------\n",
      "tensor(22.6687)\n",
      "tensor(7.9880)\n",
      "tensor(4.7093)\n",
      "tensor(0.1344)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 119.359695\n",
      "Epoch 2501\n",
      "-------------------------------\n",
      "tensor(22.5279)\n",
      "tensor(7.9476)\n",
      "tensor(4.3359)\n",
      "tensor(0.1033)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.352318\n",
      "Epoch 2502\n",
      "-------------------------------\n",
      "tensor(24.7453)\n",
      "tensor(8.5721)\n",
      "tensor(4.0887)\n",
      "tensor(0.0724)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.344048\n",
      "Epoch 2503\n",
      "-------------------------------\n",
      "tensor(18.1625)\n",
      "tensor(6.7976)\n",
      "tensor(4.0619)\n",
      "tensor(0.0351)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.332970\n",
      "Epoch 2504\n",
      "-------------------------------\n",
      "tensor(17.6075)\n",
      "tensor(6.7003)\n",
      "tensor(4.4557)\n",
      "tensor(0.0052)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.320320\n",
      "Epoch 2505\n",
      "-------------------------------\n",
      "tensor(30.8015)\n",
      "tensor(10.2318)\n",
      "tensor(4.8602)\n",
      "tensor(0.0097)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.352821\n",
      "Epoch 2506\n",
      "-------------------------------\n",
      "tensor(28.9131)\n",
      "tensor(10.7841)\n",
      "tensor(4.0940)\n",
      "tensor(0.0222)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.371986\n",
      "Epoch 2507\n",
      "-------------------------------\n",
      "tensor(29.9027)\n",
      "tensor(11.1950)\n",
      "tensor(5.5466)\n",
      "tensor(0.0647)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.347336\n",
      "Epoch 2508\n",
      "-------------------------------\n",
      "tensor(28.3439)\n",
      "tensor(10.7641)\n",
      "tensor(6.6720)\n",
      "tensor(0.0556)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.289612\n",
      "Epoch 2509\n",
      "-------------------------------\n",
      "tensor(15.7303)\n",
      "tensor(6.0153)\n",
      "tensor(4.2764)\n",
      "tensor(0.0369)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.281021\n",
      "Epoch 2510\n",
      "-------------------------------\n",
      "tensor(18.5210)\n",
      "tensor(7.0296)\n",
      "tensor(7.7301)\n",
      "tensor(0.1365)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 119.276360\n",
      "Epoch 2511\n",
      "-------------------------------\n",
      "tensor(22.8610)\n",
      "tensor(8.5590)\n",
      "tensor(5.2731)\n",
      "tensor(0.3324)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 119.253708\n",
      "Epoch 2512\n",
      "-------------------------------\n",
      "tensor(21.6627)\n",
      "tensor(7.3978)\n",
      "tensor(7.9026)\n",
      "tensor(0.1990)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 119.211647\n",
      "Epoch 2513\n",
      "-------------------------------\n",
      "tensor(38.3582)\n",
      "tensor(19.8544)\n",
      "tensor(5.0725)\n",
      "tensor(0.1766)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.179100\n",
      "Epoch 2514\n",
      "-------------------------------\n",
      "tensor(36.5732)\n",
      "tensor(11.7595)\n",
      "tensor(10.6480)\n",
      "tensor(0.2009)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 119.167831\n",
      "Epoch 2515\n",
      "-------------------------------\n",
      "tensor(25.5798)\n",
      "tensor(8.8554)\n",
      "tensor(4.6801)\n",
      "tensor(0.1287)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.133682\n",
      "Epoch 2516\n",
      "-------------------------------\n",
      "tensor(26.3355)\n",
      "tensor(8.9336)\n",
      "tensor(9.7264)\n",
      "tensor(0.2772)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.089516\n",
      "Epoch 2517\n",
      "-------------------------------\n",
      "tensor(20.6192)\n",
      "tensor(7.4944)\n",
      "tensor(5.9246)\n",
      "tensor(0.1781)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.087143\n",
      "Epoch 2518\n",
      "-------------------------------\n",
      "tensor(19.5966)\n",
      "tensor(7.2525)\n",
      "tensor(4.8435)\n",
      "tensor(0.0415)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.080635\n",
      "Epoch 2519\n",
      "-------------------------------\n",
      "tensor(28.4760)\n",
      "tensor(9.3492)\n",
      "tensor(6.8991)\n",
      "tensor(0.0093)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.071129\n",
      "Epoch 2520\n",
      "-------------------------------\n",
      "tensor(21.9619)\n",
      "tensor(7.7559)\n",
      "tensor(6.3446)\n",
      "tensor(0.0012)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 119.059166\n",
      "Epoch 2521\n",
      "-------------------------------\n",
      "tensor(27.4411)\n",
      "tensor(9.2261)\n",
      "tensor(5.2255)\n",
      "tensor(0.0318)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 119.047295\n",
      "Epoch 2522\n",
      "-------------------------------\n",
      "tensor(19.9765)\n",
      "tensor(7.3893)\n",
      "tensor(4.2594)\n",
      "tensor(0.0681)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 119.040329\n",
      "Epoch 2523\n",
      "-------------------------------\n",
      "tensor(20.1080)\n",
      "tensor(7.4327)\n",
      "tensor(4.2984)\n",
      "tensor(0.1161)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 119.035309\n",
      "Epoch 2524\n",
      "-------------------------------\n",
      "tensor(19.7392)\n",
      "tensor(7.2375)\n",
      "tensor(5.8831)\n",
      "tensor(0.1728)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 119.025803\n",
      "Epoch 2525\n",
      "-------------------------------\n",
      "tensor(24.4166)\n",
      "tensor(9.6778)\n",
      "tensor(6.9485)\n",
      "tensor(0.2047)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 119.018875\n",
      "Epoch 2526\n",
      "-------------------------------\n",
      "tensor(25.0257)\n",
      "tensor(9.8436)\n",
      "tensor(5.0118)\n",
      "tensor(0.1362)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.999016\n",
      "Epoch 2527\n",
      "-------------------------------\n",
      "tensor(31.4868)\n",
      "tensor(11.4300)\n",
      "tensor(4.9910)\n",
      "tensor(0.0205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 119.011024\n",
      "Epoch 2528\n",
      "-------------------------------\n",
      "tensor(25.4594)\n",
      "tensor(9.0966)\n",
      "tensor(5.5795)\n",
      "tensor(0.1324)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.989754\n",
      "Epoch 2529\n",
      "-------------------------------\n",
      "tensor(17.6890)\n",
      "tensor(6.4528)\n",
      "tensor(4.9241)\n",
      "tensor(0.0167)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.959236\n",
      "Epoch 2530\n",
      "-------------------------------\n",
      "tensor(23.3008)\n",
      "tensor(8.3118)\n",
      "tensor(4.8218)\n",
      "tensor(0.1804)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 118.939323\n",
      "Epoch 2531\n",
      "-------------------------------\n",
      "tensor(22.4567)\n",
      "tensor(8.0740)\n",
      "tensor(4.7649)\n",
      "tensor(0.1678)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.901131\n",
      "Epoch 2532\n",
      "-------------------------------\n",
      "tensor(22.5578)\n",
      "tensor(8.0134)\n",
      "tensor(4.3113)\n",
      "tensor(0.0387)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.868668\n",
      "Epoch 2533\n",
      "-------------------------------\n",
      "tensor(26.0780)\n",
      "tensor(9.0827)\n",
      "tensor(4.9010)\n",
      "tensor(0.0191)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 118.839386\n",
      "Epoch 2534\n",
      "-------------------------------\n",
      "tensor(39.7509)\n",
      "tensor(19.6653)\n",
      "tensor(4.9786)\n",
      "tensor(0.0022)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.851837\n",
      "Epoch 2535\n",
      "-------------------------------\n",
      "tensor(25.0121)\n",
      "tensor(8.4512)\n",
      "tensor(6.2792)\n",
      "tensor(0.0696)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 118.819908\n",
      "Epoch 2536\n",
      "-------------------------------\n",
      "tensor(26.4108)\n",
      "tensor(8.9556)\n",
      "tensor(5.2989)\n",
      "tensor(0.1691)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 118.789246\n",
      "Epoch 2537\n",
      "-------------------------------\n",
      "tensor(25.3290)\n",
      "tensor(8.9846)\n",
      "tensor(4.9773)\n",
      "tensor(0.2019)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 118.752594\n",
      "Epoch 2538\n",
      "-------------------------------\n",
      "tensor(23.2996)\n",
      "tensor(8.2313)\n",
      "tensor(6.4503)\n",
      "tensor(0.1502)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 118.728325\n",
      "Epoch 2539\n",
      "-------------------------------\n",
      "tensor(19.8689)\n",
      "tensor(7.1308)\n",
      "tensor(5.6460)\n",
      "tensor(0.0830)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 118.717827\n",
      "Epoch 2540\n",
      "-------------------------------\n",
      "tensor(18.8394)\n",
      "tensor(6.9681)\n",
      "tensor(4.5073)\n",
      "tensor(0.0394)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 118.706543\n",
      "Epoch 2541\n",
      "-------------------------------\n",
      "tensor(15.7667)\n",
      "tensor(6.2634)\n",
      "tensor(4.2413)\n",
      "tensor(0.0218)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 118.698868\n",
      "Epoch 2542\n",
      "-------------------------------\n",
      "tensor(17.8427)\n",
      "tensor(6.8495)\n",
      "tensor(4.5229)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 118.690964\n",
      "Epoch 2543\n",
      "-------------------------------\n",
      "tensor(18.6061)\n",
      "tensor(7.0713)\n",
      "tensor(5.0849)\n",
      "tensor(0.0322)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 118.681061\n",
      "Epoch 2544\n",
      "-------------------------------\n",
      "tensor(26.5890)\n",
      "tensor(8.9943)\n",
      "tensor(5.5023)\n",
      "tensor(0.0739)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 118.678886\n",
      "Epoch 2545\n",
      "-------------------------------\n",
      "tensor(26.9155)\n",
      "tensor(10.1239)\n",
      "tensor(4.8180)\n",
      "tensor(0.1385)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 118.673363\n",
      "Epoch 2546\n",
      "-------------------------------\n",
      "tensor(32.5255)\n",
      "tensor(11.7218)\n",
      "tensor(5.2577)\n",
      "tensor(0.1796)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.683029\n",
      "Epoch 2547\n",
      "-------------------------------\n",
      "tensor(26.2171)\n",
      "tensor(9.9735)\n",
      "tensor(8.3825)\n",
      "tensor(0.0644)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 118.664062\n",
      "Epoch 2548\n",
      "-------------------------------\n",
      "tensor(25.0509)\n",
      "tensor(9.1147)\n",
      "tensor(5.7852)\n",
      "tensor(0.1737)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.640442\n",
      "Epoch 2549\n",
      "-------------------------------\n",
      "tensor(20.0289)\n",
      "tensor(7.7052)\n",
      "tensor(8.1723)\n",
      "tensor(0.2233)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.611954\n",
      "Epoch 2550\n",
      "-------------------------------\n",
      "tensor(28.3644)\n",
      "tensor(9.3453)\n",
      "tensor(6.2124)\n",
      "tensor(0.1747)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 118.601967\n",
      "Epoch 2551\n",
      "-------------------------------\n",
      "tensor(31.3097)\n",
      "tensor(10.9875)\n",
      "tensor(10.0120)\n",
      "tensor(0.4435)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.580322\n",
      "Epoch 2552\n",
      "-------------------------------\n",
      "tensor(44.2273)\n",
      "tensor(20.2437)\n",
      "tensor(8.0502)\n",
      "tensor(0.0357)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.568695\n",
      "Epoch 2553\n",
      "-------------------------------\n",
      "tensor(21.4742)\n",
      "tensor(7.2700)\n",
      "tensor(12.3052)\n",
      "tensor(0.3190)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 118.548943\n",
      "Epoch 2554\n",
      "-------------------------------\n",
      "tensor(17.9520)\n",
      "tensor(7.1154)\n",
      "tensor(7.4527)\n",
      "tensor(0.1535)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.525665\n",
      "Epoch 2555\n",
      "-------------------------------\n",
      "tensor(17.4294)\n",
      "tensor(9.3604)\n",
      "tensor(9.7746)\n",
      "tensor(0.6017)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 118.488823\n",
      "Epoch 2556\n",
      "-------------------------------\n",
      "tensor(23.7961)\n",
      "tensor(9.1623)\n",
      "tensor(8.7308)\n",
      "tensor(0.4248)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 118.489815\n",
      "Epoch 2557\n",
      "-------------------------------\n",
      "tensor(22.2066)\n",
      "tensor(8.1558)\n",
      "tensor(4.3963)\n",
      "tensor(0.0359)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 118.457077\n",
      "Epoch 2558\n",
      "-------------------------------\n",
      "tensor(23.3077)\n",
      "tensor(9.1716)\n",
      "tensor(7.7679)\n",
      "tensor(0.3026)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 118.417984\n",
      "Epoch 2559\n",
      "-------------------------------\n",
      "tensor(28.2739)\n",
      "tensor(10.6960)\n",
      "tensor(7.7100)\n",
      "tensor(0.3179)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 118.389740\n",
      "Epoch 2560\n",
      "-------------------------------\n",
      "tensor(22.1051)\n",
      "tensor(8.6978)\n",
      "tensor(5.9709)\n",
      "tensor(0.2322)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 118.360565\n",
      "Epoch 2561\n",
      "-------------------------------\n",
      "tensor(29.7342)\n",
      "tensor(10.6292)\n",
      "tensor(4.7582)\n",
      "tensor(0.1301)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 118.358612\n",
      "Epoch 2562\n",
      "-------------------------------\n",
      "tensor(28.0428)\n",
      "tensor(10.9603)\n",
      "tensor(4.5312)\n",
      "tensor(0.0364)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 118.354568\n",
      "Epoch 2563\n",
      "-------------------------------\n",
      "tensor(25.6083)\n",
      "tensor(10.1273)\n",
      "tensor(5.3097)\n",
      "tensor(0.0700)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 118.344093\n",
      "Epoch 2564\n",
      "-------------------------------\n",
      "tensor(20.1345)\n",
      "tensor(7.4156)\n",
      "tensor(6.6618)\n",
      "tensor(0.1955)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 118.335579\n",
      "Epoch 2565\n",
      "-------------------------------\n",
      "tensor(16.4004)\n",
      "tensor(7.0311)\n",
      "tensor(6.5116)\n",
      "tensor(0.2877)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 118.329185\n",
      "Epoch 2566\n",
      "-------------------------------\n",
      "tensor(16.7724)\n",
      "tensor(7.3143)\n",
      "tensor(4.9576)\n",
      "tensor(0.2495)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.317413\n",
      "Epoch 2567\n",
      "-------------------------------\n",
      "tensor(29.1152)\n",
      "tensor(9.4253)\n",
      "tensor(7.6458)\n",
      "tensor(0.0844)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 118.336380\n",
      "Epoch 2568\n",
      "-------------------------------\n",
      "tensor(26.7202)\n",
      "tensor(9.2623)\n",
      "tensor(5.1777)\n",
      "tensor(0.0592)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.306465\n",
      "Epoch 2569\n",
      "-------------------------------\n",
      "tensor(27.9142)\n",
      "tensor(10.6740)\n",
      "tensor(9.1056)\n",
      "tensor(0.0010)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.298828\n",
      "Epoch 2570\n",
      "-------------------------------\n",
      "tensor(27.3649)\n",
      "tensor(9.5746)\n",
      "tensor(6.5792)\n",
      "tensor(0.1016)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 118.265205\n",
      "Epoch 2571\n",
      "-------------------------------\n",
      "tensor(22.6096)\n",
      "tensor(8.0454)\n",
      "tensor(6.8530)\n",
      "tensor(0.1435)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 118.214584\n",
      "Epoch 2572\n",
      "-------------------------------\n",
      "tensor(19.2404)\n",
      "tensor(7.6723)\n",
      "tensor(5.7359)\n",
      "tensor(0.2248)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 118.171852\n",
      "Epoch 2573\n",
      "-------------------------------\n",
      "tensor(44.9853)\n",
      "tensor(20.9003)\n",
      "tensor(6.4926)\n",
      "tensor(0.1565)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 118.168175\n",
      "Epoch 2574\n",
      "-------------------------------\n",
      "tensor(18.6496)\n",
      "tensor(8.5430)\n",
      "tensor(4.9342)\n",
      "tensor(0.1224)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 118.100105\n",
      "Epoch 2575\n",
      "-------------------------------\n",
      "tensor(19.0215)\n",
      "tensor(7.3579)\n",
      "tensor(6.7434)\n",
      "tensor(0.1451)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 118.064621\n",
      "Epoch 2576\n",
      "-------------------------------\n",
      "tensor(28.6335)\n",
      "tensor(9.6529)\n",
      "tensor(5.0774)\n",
      "tensor(0.0857)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 118.038559\n",
      "Epoch 2577\n",
      "-------------------------------\n",
      "tensor(27.0266)\n",
      "tensor(9.5138)\n",
      "tensor(5.7451)\n",
      "tensor(0.2385)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 118.030899\n",
      "Epoch 2578\n",
      "-------------------------------\n",
      "tensor(24.8758)\n",
      "tensor(8.8564)\n",
      "tensor(6.8947)\n",
      "tensor(0.2068)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 118.023003\n",
      "Epoch 2579\n",
      "-------------------------------\n",
      "tensor(21.7326)\n",
      "tensor(7.9024)\n",
      "tensor(5.5685)\n",
      "tensor(0.1000)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 118.004593\n",
      "Epoch 2580\n",
      "-------------------------------\n",
      "tensor(23.5752)\n",
      "tensor(8.5021)\n",
      "tensor(4.4890)\n",
      "tensor(0.0178)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 117.992905\n",
      "Epoch 2581\n",
      "-------------------------------\n",
      "tensor(24.5747)\n",
      "tensor(8.8114)\n",
      "tensor(4.4292)\n",
      "tensor(0.0257)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 117.985474\n",
      "Epoch 2582\n",
      "-------------------------------\n",
      "tensor(18.8628)\n",
      "tensor(7.2680)\n",
      "tensor(4.6851)\n",
      "tensor(0.0458)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 117.977409\n",
      "Epoch 2583\n",
      "-------------------------------\n",
      "tensor(15.5067)\n",
      "tensor(6.4692)\n",
      "tensor(4.9803)\n",
      "tensor(0.0447)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 117.972107\n",
      "Epoch 2584\n",
      "-------------------------------\n",
      "tensor(18.3513)\n",
      "tensor(8.4547)\n",
      "tensor(5.1720)\n",
      "tensor(0.0064)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 117.962418\n",
      "Epoch 2585\n",
      "-------------------------------\n",
      "tensor(25.2491)\n",
      "tensor(9.8949)\n",
      "tensor(4.8079)\n",
      "tensor(0.0936)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 117.967957\n",
      "Epoch 2586\n",
      "-------------------------------\n",
      "tensor(32.2133)\n",
      "tensor(10.7632)\n",
      "tensor(4.8224)\n",
      "tensor(0.2062)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.961853\n",
      "Epoch 2587\n",
      "-------------------------------\n",
      "tensor(27.4704)\n",
      "tensor(9.5752)\n",
      "tensor(7.3528)\n",
      "tensor(0.1890)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.933495\n",
      "Epoch 2588\n",
      "-------------------------------\n",
      "tensor(22.7806)\n",
      "tensor(8.0039)\n",
      "tensor(6.4194)\n",
      "tensor(0.0171)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.929703\n",
      "Epoch 2589\n",
      "-------------------------------\n",
      "tensor(28.0800)\n",
      "tensor(9.8439)\n",
      "tensor(6.9422)\n",
      "tensor(0.1903)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.923874\n",
      "Epoch 2590\n",
      "-------------------------------\n",
      "tensor(17.6556)\n",
      "tensor(6.9065)\n",
      "tensor(5.4923)\n",
      "tensor(0.0158)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 117.886375\n",
      "Epoch 2591\n",
      "-------------------------------\n",
      "tensor(17.3127)\n",
      "tensor(7.8325)\n",
      "tensor(6.6102)\n",
      "tensor(0.3586)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.841431\n",
      "Epoch 2592\n",
      "-------------------------------\n",
      "tensor(41.4968)\n",
      "tensor(21.2149)\n",
      "tensor(5.0528)\n",
      "tensor(0.2109)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.848221\n",
      "Epoch 2593\n",
      "-------------------------------\n",
      "tensor(29.8174)\n",
      "tensor(9.9865)\n",
      "tensor(8.8046)\n",
      "tensor(0.1668)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.786812\n",
      "Epoch 2594\n",
      "-------------------------------\n",
      "tensor(30.1626)\n",
      "tensor(10.7126)\n",
      "tensor(5.1132)\n",
      "tensor(0.1211)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.753914\n",
      "Epoch 2595\n",
      "-------------------------------\n",
      "tensor(28.8321)\n",
      "tensor(9.8388)\n",
      "tensor(9.9063)\n",
      "tensor(0.1188)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 117.737091\n",
      "Epoch 2596\n",
      "-------------------------------\n",
      "tensor(21.3266)\n",
      "tensor(8.1080)\n",
      "tensor(5.8510)\n",
      "tensor(0.1802)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 117.700066\n",
      "Epoch 2597\n",
      "-------------------------------\n",
      "tensor(18.4974)\n",
      "tensor(7.4312)\n",
      "tensor(5.9087)\n",
      "tensor(0.1399)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 117.684082\n",
      "Epoch 2598\n",
      "-------------------------------\n",
      "tensor(18.3887)\n",
      "tensor(8.4132)\n",
      "tensor(7.5082)\n",
      "tensor(0.1173)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 117.669861\n",
      "Epoch 2599\n",
      "-------------------------------\n",
      "tensor(16.3574)\n",
      "tensor(8.1891)\n",
      "tensor(6.0490)\n",
      "tensor(0.1193)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 117.650909\n",
      "Epoch 2600\n",
      "-------------------------------\n",
      "tensor(26.0467)\n",
      "tensor(9.1554)\n",
      "tensor(4.6836)\n",
      "tensor(0.1243)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 117.640923\n",
      "Epoch 2601\n",
      "-------------------------------\n",
      "tensor(25.6546)\n",
      "tensor(9.1414)\n",
      "tensor(4.3772)\n",
      "tensor(0.1180)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 117.630623\n",
      "Epoch 2602\n",
      "-------------------------------\n",
      "tensor(21.8804)\n",
      "tensor(8.1380)\n",
      "tensor(4.7852)\n",
      "tensor(0.1046)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 117.623978\n",
      "Epoch 2603\n",
      "-------------------------------\n",
      "tensor(18.9773)\n",
      "tensor(7.2004)\n",
      "tensor(5.4572)\n",
      "tensor(0.0793)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 117.619110\n",
      "Epoch 2604\n",
      "-------------------------------\n",
      "tensor(21.7913)\n",
      "tensor(7.9244)\n",
      "tensor(5.6601)\n",
      "tensor(0.0288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 117.605003\n",
      "Epoch 2605\n",
      "-------------------------------\n",
      "tensor(24.8148)\n",
      "tensor(8.9889)\n",
      "tensor(4.9061)\n",
      "tensor(0.0416)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 117.604889\n",
      "Epoch 2606\n",
      "-------------------------------\n",
      "tensor(29.4991)\n",
      "tensor(10.4469)\n",
      "tensor(4.9147)\n",
      "tensor(0.1036)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.601730\n",
      "Epoch 2607\n",
      "-------------------------------\n",
      "tensor(25.6500)\n",
      "tensor(10.3341)\n",
      "tensor(5.2814)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.578583\n",
      "Epoch 2608\n",
      "-------------------------------\n",
      "tensor(25.6711)\n",
      "tensor(9.1354)\n",
      "tensor(4.3983)\n",
      "tensor(0.0507)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.528328\n",
      "Epoch 2609\n",
      "-------------------------------\n",
      "tensor(27.0899)\n",
      "tensor(9.5396)\n",
      "tensor(6.4024)\n",
      "tensor(0.1699)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.533867\n",
      "Epoch 2610\n",
      "-------------------------------\n",
      "tensor(22.9346)\n",
      "tensor(8.3537)\n",
      "tensor(4.6919)\n",
      "tensor(0.0471)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 117.513985\n",
      "Epoch 2611\n",
      "-------------------------------\n",
      "tensor(20.3940)\n",
      "tensor(7.5238)\n",
      "tensor(6.0621)\n",
      "tensor(0.0703)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.485352\n",
      "Epoch 2612\n",
      "-------------------------------\n",
      "tensor(42.5840)\n",
      "tensor(21.9187)\n",
      "tensor(4.4819)\n",
      "tensor(0.0956)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.519455\n",
      "Epoch 2613\n",
      "-------------------------------\n",
      "tensor(23.0648)\n",
      "tensor(8.8213)\n",
      "tensor(4.7898)\n",
      "tensor(0.2511)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.444061\n",
      "Epoch 2614\n",
      "-------------------------------\n",
      "tensor(22.7058)\n",
      "tensor(8.5804)\n",
      "tensor(4.8753)\n",
      "tensor(0.1737)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.395996\n",
      "Epoch 2615\n",
      "-------------------------------\n",
      "tensor(18.9709)\n",
      "tensor(7.3135)\n",
      "tensor(4.6242)\n",
      "tensor(0.0286)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 117.359863\n",
      "Epoch 2616\n",
      "-------------------------------\n",
      "tensor(19.5958)\n",
      "tensor(7.5344)\n",
      "tensor(4.7429)\n",
      "tensor(0.0079)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 117.322571\n",
      "Epoch 2617\n",
      "-------------------------------\n",
      "tensor(21.8302)\n",
      "tensor(8.1634)\n",
      "tensor(4.6823)\n",
      "tensor(0.0497)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 117.289070\n",
      "Epoch 2618\n",
      "-------------------------------\n",
      "tensor(27.6236)\n",
      "tensor(9.6618)\n",
      "tensor(4.4912)\n",
      "tensor(0.1030)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 117.271927\n",
      "Epoch 2619\n",
      "-------------------------------\n",
      "tensor(23.7737)\n",
      "tensor(9.8283)\n",
      "tensor(4.7310)\n",
      "tensor(0.1072)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 117.267380\n",
      "Epoch 2620\n",
      "-------------------------------\n",
      "tensor(23.5191)\n",
      "tensor(8.6102)\n",
      "tensor(4.8509)\n",
      "tensor(0.0802)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 117.253967\n",
      "Epoch 2621\n",
      "-------------------------------\n",
      "tensor(22.8466)\n",
      "tensor(8.4303)\n",
      "tensor(4.7539)\n",
      "tensor(0.0487)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 117.240097\n",
      "Epoch 2622\n",
      "-------------------------------\n",
      "tensor(15.8845)\n",
      "tensor(6.6233)\n",
      "tensor(4.5961)\n",
      "tensor(0.0175)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 117.224693\n",
      "Epoch 2623\n",
      "-------------------------------\n",
      "tensor(17.8477)\n",
      "tensor(8.5609)\n",
      "tensor(4.4676)\n",
      "tensor(0.0138)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 117.218269\n",
      "Epoch 2624\n",
      "-------------------------------\n",
      "tensor(25.5740)\n",
      "tensor(9.2480)\n",
      "tensor(4.7377)\n",
      "tensor(0.0448)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 117.214386\n",
      "Epoch 2625\n",
      "-------------------------------\n",
      "tensor(31.1420)\n",
      "tensor(10.7184)\n",
      "tensor(5.1295)\n",
      "tensor(0.0517)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 117.225212\n",
      "Epoch 2626\n",
      "-------------------------------\n",
      "tensor(23.5796)\n",
      "tensor(8.6637)\n",
      "tensor(4.4820)\n",
      "tensor(0.0164)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.216835\n",
      "Epoch 2627\n",
      "-------------------------------\n",
      "tensor(22.8898)\n",
      "tensor(8.4123)\n",
      "tensor(5.1912)\n",
      "tensor(0.0674)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.196358\n",
      "Epoch 2628\n",
      "-------------------------------\n",
      "tensor(27.7751)\n",
      "tensor(9.8426)\n",
      "tensor(5.1093)\n",
      "tensor(0.1141)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.139786\n",
      "Epoch 2629\n",
      "-------------------------------\n",
      "tensor(23.9236)\n",
      "tensor(9.8585)\n",
      "tensor(4.4686)\n",
      "tensor(0.0943)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.129601\n",
      "Epoch 2630\n",
      "-------------------------------\n",
      "tensor(26.1334)\n",
      "tensor(9.2022)\n",
      "tensor(4.6223)\n",
      "tensor(0.0641)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 117.108704\n",
      "Epoch 2631\n",
      "-------------------------------\n",
      "tensor(43.1125)\n",
      "tensor(21.4696)\n",
      "tensor(4.7550)\n",
      "tensor(0.0983)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 117.140099\n",
      "Epoch 2632\n",
      "-------------------------------\n",
      "tensor(24.1367)\n",
      "tensor(8.6755)\n",
      "tensor(5.2647)\n",
      "tensor(0.1112)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 117.084183\n",
      "Epoch 2633\n",
      "-------------------------------\n",
      "tensor(15.5496)\n",
      "tensor(7.0542)\n",
      "tensor(4.5211)\n",
      "tensor(0.1605)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 117.052231\n",
      "Epoch 2634\n",
      "-------------------------------\n",
      "tensor(15.9836)\n",
      "tensor(8.3616)\n",
      "tensor(4.6945)\n",
      "tensor(0.1487)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 117.011307\n",
      "Epoch 2635\n",
      "-------------------------------\n",
      "tensor(22.3541)\n",
      "tensor(8.3792)\n",
      "tensor(4.5586)\n",
      "tensor(0.0491)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.961456\n",
      "Epoch 2636\n",
      "-------------------------------\n",
      "tensor(28.7882)\n",
      "tensor(10.1782)\n",
      "tensor(4.6991)\n",
      "tensor(0.0290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.920746\n",
      "Epoch 2637\n",
      "-------------------------------\n",
      "tensor(20.2164)\n",
      "tensor(7.8660)\n",
      "tensor(4.8122)\n",
      "tensor(0.0573)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.888763\n",
      "Epoch 2638\n",
      "-------------------------------\n",
      "tensor(23.2267)\n",
      "tensor(8.7045)\n",
      "tensor(4.8817)\n",
      "tensor(0.0484)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.875648\n",
      "Epoch 2639\n",
      "-------------------------------\n",
      "tensor(27.1898)\n",
      "tensor(9.8317)\n",
      "tensor(4.6647)\n",
      "tensor(0.0314)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.873833\n",
      "Epoch 2640\n",
      "-------------------------------\n",
      "tensor(28.0309)\n",
      "tensor(10.0376)\n",
      "tensor(4.5693)\n",
      "tensor(0.0213)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 116.866112\n",
      "Epoch 2641\n",
      "-------------------------------\n",
      "tensor(28.2816)\n",
      "tensor(11.1200)\n",
      "tensor(4.5375)\n",
      "tensor(0.0168)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.851807\n",
      "Epoch 2642\n",
      "-------------------------------\n",
      "tensor(16.8694)\n",
      "tensor(7.0491)\n",
      "tensor(4.5222)\n",
      "tensor(0.0139)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.833527\n",
      "Epoch 2643\n",
      "-------------------------------\n",
      "tensor(16.4440)\n",
      "tensor(6.9512)\n",
      "tensor(4.4925)\n",
      "tensor(0.0002)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.830254\n",
      "Epoch 2644\n",
      "-------------------------------\n",
      "tensor(14.7121)\n",
      "tensor(6.6103)\n",
      "tensor(4.4841)\n",
      "tensor(0.0250)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.820915\n",
      "Epoch 2645\n",
      "-------------------------------\n",
      "tensor(26.0475)\n",
      "tensor(10.3321)\n",
      "tensor(4.6396)\n",
      "tensor(0.0723)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.823265\n",
      "Epoch 2646\n",
      "-------------------------------\n",
      "tensor(29.1084)\n",
      "tensor(10.1045)\n",
      "tensor(4.5382)\n",
      "tensor(0.1198)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 116.822403\n",
      "Epoch 2647\n",
      "-------------------------------\n",
      "tensor(27.4713)\n",
      "tensor(9.8479)\n",
      "tensor(5.0565)\n",
      "tensor(0.1139)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 116.783760\n",
      "Epoch 2648\n",
      "-------------------------------\n",
      "tensor(25.2552)\n",
      "tensor(9.1607)\n",
      "tensor(6.3924)\n",
      "tensor(0.0544)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 116.759842\n",
      "Epoch 2649\n",
      "-------------------------------\n",
      "tensor(20.3279)\n",
      "tensor(7.7255)\n",
      "tensor(4.8943)\n",
      "tensor(0.0114)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 116.744347\n",
      "Epoch 2650\n",
      "-------------------------------\n",
      "tensor(25.3755)\n",
      "tensor(8.9188)\n",
      "tensor(6.0587)\n",
      "tensor(0.1146)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 116.717186\n",
      "Epoch 2651\n",
      "-------------------------------\n",
      "tensor(42.7738)\n",
      "tensor(22.1744)\n",
      "tensor(5.1770)\n",
      "tensor(0.2588)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 116.740723\n",
      "Epoch 2652\n",
      "-------------------------------\n",
      "tensor(15.1133)\n",
      "tensor(6.8324)\n",
      "tensor(4.6121)\n",
      "tensor(0.1058)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 116.641754\n",
      "Epoch 2653\n",
      "-------------------------------\n",
      "tensor(26.3835)\n",
      "tensor(9.4323)\n",
      "tensor(5.0925)\n",
      "tensor(0.0472)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 116.609177\n",
      "Epoch 2654\n",
      "-------------------------------\n",
      "tensor(25.2949)\n",
      "tensor(9.2639)\n",
      "tensor(5.2646)\n",
      "tensor(0.0367)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 116.547401\n",
      "Epoch 2655\n",
      "-------------------------------\n",
      "tensor(28.0755)\n",
      "tensor(10.0414)\n",
      "tensor(5.6626)\n",
      "tensor(0.0361)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.557091\n",
      "Epoch 2656\n",
      "-------------------------------\n",
      "tensor(23.2226)\n",
      "tensor(9.8617)\n",
      "tensor(4.5693)\n",
      "tensor(0.0270)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.544937\n",
      "Epoch 2657\n",
      "-------------------------------\n",
      "tensor(29.1026)\n",
      "tensor(10.1805)\n",
      "tensor(4.9031)\n",
      "tensor(0.0044)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.515518\n",
      "Epoch 2658\n",
      "-------------------------------\n",
      "tensor(20.9982)\n",
      "tensor(8.1241)\n",
      "tensor(4.6490)\n",
      "tensor(0.0040)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.475662\n",
      "Epoch 2659\n",
      "-------------------------------\n",
      "tensor(15.0724)\n",
      "tensor(6.7088)\n",
      "tensor(4.5110)\n",
      "tensor(0.0286)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.464035\n",
      "Epoch 2660\n",
      "-------------------------------\n",
      "tensor(16.6677)\n",
      "tensor(7.0874)\n",
      "tensor(4.5281)\n",
      "tensor(0.0501)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 116.456322\n",
      "Epoch 2661\n",
      "-------------------------------\n",
      "tensor(16.6046)\n",
      "tensor(7.1040)\n",
      "tensor(4.5188)\n",
      "tensor(0.0619)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.448082\n",
      "Epoch 2662\n",
      "-------------------------------\n",
      "tensor(14.9164)\n",
      "tensor(6.7733)\n",
      "tensor(4.5055)\n",
      "tensor(0.0683)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.439697\n",
      "Epoch 2663\n",
      "-------------------------------\n",
      "tensor(22.5683)\n",
      "tensor(8.5376)\n",
      "tensor(4.5362)\n",
      "tensor(0.0720)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.427368\n",
      "Epoch 2664\n",
      "-------------------------------\n",
      "tensor(29.6453)\n",
      "tensor(11.2510)\n",
      "tensor(4.6237)\n",
      "tensor(0.0715)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.415062\n",
      "Epoch 2665\n",
      "-------------------------------\n",
      "tensor(27.8382)\n",
      "tensor(9.8880)\n",
      "tensor(4.5252)\n",
      "tensor(0.0631)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.421974\n",
      "Epoch 2666\n",
      "-------------------------------\n",
      "tensor(26.5995)\n",
      "tensor(9.6342)\n",
      "tensor(4.9679)\n",
      "tensor(0.0216)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 116.406708\n",
      "Epoch 2667\n",
      "-------------------------------\n",
      "tensor(24.4490)\n",
      "tensor(10.2227)\n",
      "tensor(5.6890)\n",
      "tensor(0.0416)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 116.384285\n",
      "Epoch 2668\n",
      "-------------------------------\n",
      "tensor(23.7117)\n",
      "tensor(8.8277)\n",
      "tensor(4.8498)\n",
      "tensor(0.0675)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 116.349075\n",
      "Epoch 2669\n",
      "-------------------------------\n",
      "tensor(23.3590)\n",
      "tensor(8.6026)\n",
      "tensor(4.9836)\n",
      "tensor(0.0425)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 116.341667\n",
      "Epoch 2670\n",
      "-------------------------------\n",
      "tensor(47.4157)\n",
      "tensor(22.9202)\n",
      "tensor(4.8126)\n",
      "tensor(0.1887)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 116.322830\n",
      "Epoch 2671\n",
      "-------------------------------\n",
      "tensor(17.3039)\n",
      "tensor(7.3895)\n",
      "tensor(4.5867)\n",
      "tensor(0.1124)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 116.260605\n",
      "Epoch 2672\n",
      "-------------------------------\n",
      "tensor(22.5570)\n",
      "tensor(8.5360)\n",
      "tensor(4.6980)\n",
      "tensor(0.0576)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 116.273743\n",
      "Epoch 2673\n",
      "-------------------------------\n",
      "tensor(25.5176)\n",
      "tensor(9.3893)\n",
      "tensor(4.6810)\n",
      "tensor(0.0859)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 116.253319\n",
      "Epoch 2674\n",
      "-------------------------------\n",
      "tensor(23.2561)\n",
      "tensor(9.9695)\n",
      "tensor(4.8202)\n",
      "tensor(0.1050)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 116.193314\n",
      "Epoch 2675\n",
      "-------------------------------\n",
      "tensor(19.6669)\n",
      "tensor(9.1571)\n",
      "tensor(4.5636)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.141533\n",
      "Epoch 2676\n",
      "-------------------------------\n",
      "tensor(14.7469)\n",
      "tensor(6.8595)\n",
      "tensor(4.7359)\n",
      "tensor(0.0304)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.094765\n",
      "Epoch 2677\n",
      "-------------------------------\n",
      "tensor(22.8021)\n",
      "tensor(8.6749)\n",
      "tensor(4.9419)\n",
      "tensor(0.0265)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.091293\n",
      "Epoch 2678\n",
      "-------------------------------\n",
      "tensor(25.2002)\n",
      "tensor(9.3102)\n",
      "tensor(4.6976)\n",
      "tensor(0.0173)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.070976\n",
      "Epoch 2679\n",
      "-------------------------------\n",
      "tensor(31.2048)\n",
      "tensor(11.0173)\n",
      "tensor(4.7247)\n",
      "tensor(0.0034)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.061050\n",
      "Epoch 2680\n",
      "-------------------------------\n",
      "tensor(31.0353)\n",
      "tensor(11.0743)\n",
      "tensor(4.9964)\n",
      "tensor(0.0195)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 116.037262\n",
      "Epoch 2681\n",
      "-------------------------------\n",
      "tensor(26.1946)\n",
      "tensor(9.6691)\n",
      "tensor(5.2372)\n",
      "tensor(0.0377)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 116.018158\n",
      "Epoch 2682\n",
      "-------------------------------\n",
      "tensor(19.1585)\n",
      "tensor(7.6093)\n",
      "tensor(5.2996)\n",
      "tensor(0.0482)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 116.008209\n",
      "Epoch 2683\n",
      "-------------------------------\n",
      "tensor(21.0790)\n",
      "tensor(8.1919)\n",
      "tensor(5.0743)\n",
      "tensor(0.0491)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 116.011238\n",
      "Epoch 2684\n",
      "-------------------------------\n",
      "tensor(19.9588)\n",
      "tensor(7.9495)\n",
      "tensor(4.6950)\n",
      "tensor(0.0393)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 116.008987\n",
      "Epoch 2685\n",
      "-------------------------------\n",
      "tensor(22.4556)\n",
      "tensor(8.5370)\n",
      "tensor(5.0924)\n",
      "tensor(0.0008)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 116.007393\n",
      "Epoch 2686\n",
      "-------------------------------\n",
      "tensor(19.0108)\n",
      "tensor(7.7717)\n",
      "tensor(5.7355)\n",
      "tensor(0.0839)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 115.992584\n",
      "Epoch 2687\n",
      "-------------------------------\n",
      "tensor(19.8952)\n",
      "tensor(8.2146)\n",
      "tensor(5.0370)\n",
      "tensor(0.1867)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 115.959442\n",
      "Epoch 2688\n",
      "-------------------------------\n",
      "tensor(30.0841)\n",
      "tensor(11.6709)\n",
      "tensor(5.7755)\n",
      "tensor(0.2038)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 115.941643\n",
      "Epoch 2689\n",
      "-------------------------------\n",
      "tensor(28.7525)\n",
      "tensor(11.4786)\n",
      "tensor(7.4834)\n",
      "tensor(0.0011)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 115.899490\n",
      "Epoch 2690\n",
      "-------------------------------\n",
      "tensor(44.4538)\n",
      "tensor(22.9247)\n",
      "tensor(6.0043)\n",
      "tensor(0.1900)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 115.877899\n",
      "Epoch 2691\n",
      "-------------------------------\n",
      "tensor(29.3624)\n",
      "tensor(9.3199)\n",
      "tensor(10.8698)\n",
      "tensor(0.0085)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 115.865479\n",
      "Epoch 2692\n",
      "-------------------------------\n",
      "tensor(21.9304)\n",
      "tensor(10.1955)\n",
      "tensor(7.2419)\n",
      "tensor(0.4871)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 115.819916\n",
      "Epoch 2693\n",
      "-------------------------------\n",
      "tensor(21.2706)\n",
      "tensor(8.8623)\n",
      "tensor(10.9868)\n",
      "tensor(0.3596)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 115.782425\n",
      "Epoch 2694\n",
      "-------------------------------\n",
      "tensor(20.4637)\n",
      "tensor(8.1638)\n",
      "tensor(6.8869)\n",
      "tensor(0.2052)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 115.728683\n",
      "Epoch 2695\n",
      "-------------------------------\n",
      "tensor(33.9202)\n",
      "tensor(11.9139)\n",
      "tensor(10.1175)\n",
      "tensor(0.2971)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 115.704994\n",
      "Epoch 2696\n",
      "-------------------------------\n",
      "tensor(28.5763)\n",
      "tensor(10.3675)\n",
      "tensor(5.3888)\n",
      "tensor(0.0012)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 115.667328\n",
      "Epoch 2697\n",
      "-------------------------------\n",
      "tensor(23.0908)\n",
      "tensor(9.4987)\n",
      "tensor(9.8942)\n",
      "tensor(0.1666)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 115.639862\n",
      "Epoch 2698\n",
      "-------------------------------\n",
      "tensor(21.6757)\n",
      "tensor(8.2694)\n",
      "tensor(6.8223)\n",
      "tensor(0.1236)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 115.619301\n",
      "Epoch 2699\n",
      "-------------------------------\n",
      "tensor(17.7534)\n",
      "tensor(7.5810)\n",
      "tensor(4.7804)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 115.602821\n",
      "Epoch 2700\n",
      "-------------------------------\n",
      "tensor(22.4626)\n",
      "tensor(8.6556)\n",
      "tensor(6.0693)\n",
      "tensor(0.0448)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 115.598511\n",
      "Epoch 2701\n",
      "-------------------------------\n",
      "tensor(20.2854)\n",
      "tensor(8.1474)\n",
      "tensor(6.6930)\n",
      "tensor(0.0701)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 115.591995\n",
      "Epoch 2702\n",
      "-------------------------------\n",
      "tensor(23.1823)\n",
      "tensor(8.8116)\n",
      "tensor(6.5458)\n",
      "tensor(0.0724)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 115.583488\n",
      "Epoch 2703\n",
      "-------------------------------\n",
      "tensor(23.0608)\n",
      "tensor(9.9329)\n",
      "tensor(5.6824)\n",
      "tensor(0.0557)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 115.566872\n",
      "Epoch 2704\n",
      "-------------------------------\n",
      "tensor(29.1930)\n",
      "tensor(10.4685)\n",
      "tensor(4.6812)\n",
      "tensor(0.0015)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 115.566559\n",
      "Epoch 2705\n",
      "-------------------------------\n",
      "tensor(30.3477)\n",
      "tensor(10.8705)\n",
      "tensor(7.1442)\n",
      "tensor(0.0723)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 115.567253\n",
      "Epoch 2706\n",
      "-------------------------------\n",
      "tensor(28.5517)\n",
      "tensor(10.0818)\n",
      "tensor(9.9829)\n",
      "tensor(0.0936)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 115.548698\n",
      "Epoch 2707\n",
      "-------------------------------\n",
      "tensor(21.3965)\n",
      "tensor(9.5034)\n",
      "tensor(6.2290)\n",
      "tensor(0.0137)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 115.527420\n",
      "Epoch 2708\n",
      "-------------------------------\n",
      "tensor(28.5589)\n",
      "tensor(9.5227)\n",
      "tensor(8.5842)\n",
      "tensor(0.0415)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 115.513931\n",
      "Epoch 2709\n",
      "-------------------------------\n",
      "tensor(17.2437)\n",
      "tensor(7.6025)\n",
      "tensor(6.8173)\n",
      "tensor(0.1527)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 115.482910\n",
      "Epoch 2710\n",
      "-------------------------------\n",
      "tensor(46.3870)\n",
      "tensor(22.6076)\n",
      "tensor(8.3707)\n",
      "tensor(0.3649)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 115.514473\n",
      "Epoch 2711\n",
      "-------------------------------\n",
      "tensor(22.4483)\n",
      "tensor(8.6455)\n",
      "tensor(4.7234)\n",
      "tensor(0.0883)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 115.387924\n",
      "Epoch 2712\n",
      "-------------------------------\n",
      "tensor(28.1084)\n",
      "tensor(10.0283)\n",
      "tensor(7.0090)\n",
      "tensor(0.1656)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 115.342888\n",
      "Epoch 2713\n",
      "-------------------------------\n",
      "tensor(25.9130)\n",
      "tensor(9.5967)\n",
      "tensor(6.1808)\n",
      "tensor(0.0034)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 115.328651\n",
      "Epoch 2714\n",
      "-------------------------------\n",
      "tensor(26.3829)\n",
      "tensor(10.8438)\n",
      "tensor(6.7857)\n",
      "tensor(0.1336)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 115.298691\n",
      "Epoch 2715\n",
      "-------------------------------\n",
      "tensor(25.6798)\n",
      "tensor(10.5166)\n",
      "tensor(4.9263)\n",
      "tensor(0.0376)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 115.247864\n",
      "Epoch 2716\n",
      "-------------------------------\n",
      "tensor(19.2338)\n",
      "tensor(7.9319)\n",
      "tensor(5.6794)\n",
      "tensor(0.0434)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 115.209831\n",
      "Epoch 2717\n",
      "-------------------------------\n",
      "tensor(15.4229)\n",
      "tensor(7.0977)\n",
      "tensor(4.8548)\n",
      "tensor(0.0194)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 115.185379\n",
      "Epoch 2718\n",
      "-------------------------------\n",
      "tensor(24.3896)\n",
      "tensor(9.2291)\n",
      "tensor(4.7486)\n",
      "tensor(0.0301)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 115.162773\n",
      "Epoch 2719\n",
      "-------------------------------\n",
      "tensor(27.1936)\n",
      "tensor(9.9810)\n",
      "tensor(4.9756)\n",
      "tensor(0.0531)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 115.152039\n",
      "Epoch 2720\n",
      "-------------------------------\n",
      "tensor(21.2739)\n",
      "tensor(8.4127)\n",
      "tensor(5.0864)\n",
      "tensor(0.0488)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 115.141449\n",
      "Epoch 2721\n",
      "-------------------------------\n",
      "tensor(21.9396)\n",
      "tensor(8.5861)\n",
      "tensor(4.9733)\n",
      "tensor(0.0366)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 115.130493\n",
      "Epoch 2722\n",
      "-------------------------------\n",
      "tensor(24.4232)\n",
      "tensor(9.2404)\n",
      "tensor(4.8318)\n",
      "tensor(0.0232)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 115.123932\n",
      "Epoch 2723\n",
      "-------------------------------\n",
      "tensor(14.9278)\n",
      "tensor(6.9379)\n",
      "tensor(4.7373)\n",
      "tensor(0.0050)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 115.114693\n",
      "Epoch 2724\n",
      "-------------------------------\n",
      "tensor(23.7883)\n",
      "tensor(9.0440)\n",
      "tensor(4.8441)\n",
      "tensor(0.0102)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 115.114380\n",
      "Epoch 2725\n",
      "-------------------------------\n",
      "tensor(31.5108)\n",
      "tensor(10.9927)\n",
      "tensor(5.1603)\n",
      "tensor(0.0165)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 115.106895\n",
      "Epoch 2726\n",
      "-------------------------------\n",
      "tensor(22.0587)\n",
      "tensor(8.5924)\n",
      "tensor(4.7363)\n",
      "tensor(0.0011)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 115.082184\n",
      "Epoch 2727\n",
      "-------------------------------\n",
      "tensor(24.2396)\n",
      "tensor(9.1624)\n",
      "tensor(5.3958)\n",
      "tensor(0.0469)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 115.058609\n",
      "Epoch 2728\n",
      "-------------------------------\n",
      "tensor(18.1830)\n",
      "tensor(8.9288)\n",
      "tensor(5.4794)\n",
      "tensor(0.0640)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 115.025909\n",
      "Epoch 2729\n",
      "-------------------------------\n",
      "tensor(51.4489)\n",
      "tensor(25.2146)\n",
      "tensor(5.1527)\n",
      "tensor(0.0345)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 115.052040\n",
      "Epoch 2730\n",
      "-------------------------------\n",
      "tensor(26.9069)\n",
      "tensor(9.5156)\n",
      "tensor(6.1799)\n",
      "tensor(0.0151)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 114.971840\n",
      "Epoch 2731\n",
      "-------------------------------\n",
      "tensor(19.1662)\n",
      "tensor(8.2283)\n",
      "tensor(7.0326)\n",
      "tensor(0.1706)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 114.955948\n",
      "Epoch 2732\n",
      "-------------------------------\n",
      "tensor(16.1631)\n",
      "tensor(7.9281)\n",
      "tensor(5.5595)\n",
      "tensor(0.2029)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 114.917618\n",
      "Epoch 2733\n",
      "-------------------------------\n",
      "tensor(30.3254)\n",
      "tensor(10.3063)\n",
      "tensor(7.0451)\n",
      "tensor(0.1122)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 114.899078\n",
      "Epoch 2734\n",
      "-------------------------------\n",
      "tensor(27.2462)\n",
      "tensor(10.0638)\n",
      "tensor(4.8640)\n",
      "tensor(0.0479)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 114.859436\n",
      "Epoch 2735\n",
      "-------------------------------\n",
      "tensor(23.1148)\n",
      "tensor(8.5364)\n",
      "tensor(7.8545)\n",
      "tensor(0.0475)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 114.823631\n",
      "Epoch 2736\n",
      "-------------------------------\n",
      "tensor(21.8878)\n",
      "tensor(10.1028)\n",
      "tensor(5.2135)\n",
      "tensor(0.1374)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 114.775230\n",
      "Epoch 2737\n",
      "-------------------------------\n",
      "tensor(15.1142)\n",
      "tensor(7.3524)\n",
      "tensor(6.5513)\n",
      "tensor(0.1144)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 114.740242\n",
      "Epoch 2738\n",
      "-------------------------------\n",
      "tensor(28.2972)\n",
      "tensor(10.0981)\n",
      "tensor(6.7260)\n",
      "tensor(0.0130)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 114.730034\n",
      "Epoch 2739\n",
      "-------------------------------\n",
      "tensor(29.2023)\n",
      "tensor(10.3958)\n",
      "tensor(5.2882)\n",
      "tensor(0.0570)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 114.715355\n",
      "Epoch 2740\n",
      "-------------------------------\n",
      "tensor(25.8718)\n",
      "tensor(9.6601)\n",
      "tensor(5.0865)\n",
      "tensor(0.0728)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 114.693810\n",
      "Epoch 2741\n",
      "-------------------------------\n",
      "tensor(18.2507)\n",
      "tensor(7.7121)\n",
      "tensor(5.5229)\n",
      "tensor(0.0634)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 114.684479\n",
      "Epoch 2742\n",
      "-------------------------------\n",
      "tensor(18.0166)\n",
      "tensor(7.5757)\n",
      "tensor(5.7059)\n",
      "tensor(0.0429)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 114.679871\n",
      "Epoch 2743\n",
      "-------------------------------\n",
      "tensor(16.6181)\n",
      "tensor(7.2217)\n",
      "tensor(5.4639)\n",
      "tensor(0.0059)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 114.671242\n",
      "Epoch 2744\n",
      "-------------------------------\n",
      "tensor(18.0360)\n",
      "tensor(7.6875)\n",
      "tensor(4.9390)\n",
      "tensor(0.0532)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 114.655350\n",
      "Epoch 2745\n",
      "-------------------------------\n",
      "tensor(26.4291)\n",
      "tensor(10.9030)\n",
      "tensor(5.6449)\n",
      "tensor(0.1048)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 114.657516\n",
      "Epoch 2746\n",
      "-------------------------------\n",
      "tensor(32.4209)\n",
      "tensor(12.1612)\n",
      "tensor(6.2817)\n",
      "tensor(0.0916)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 114.649063\n",
      "Epoch 2747\n",
      "-------------------------------\n",
      "tensor(29.3206)\n",
      "tensor(10.6332)\n",
      "tensor(4.9629)\n",
      "tensor(0.0386)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 114.628540\n",
      "Epoch 2748\n",
      "-------------------------------\n",
      "tensor(29.8200)\n",
      "tensor(11.0387)\n",
      "tensor(8.8558)\n",
      "tensor(0.1660)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 114.560318\n",
      "Epoch 2749\n",
      "-------------------------------\n",
      "tensor(49.2405)\n",
      "tensor(23.7153)\n",
      "tensor(7.0989)\n",
      "tensor(0.1279)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 114.647896\n",
      "Epoch 2750\n",
      "-------------------------------\n",
      "tensor(26.0745)\n",
      "tensor(8.5628)\n",
      "tensor(11.6235)\n",
      "tensor(0.0016)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 114.560844\n",
      "Epoch 2751\n",
      "-------------------------------\n",
      "tensor(17.1844)\n",
      "tensor(8.6962)\n",
      "tensor(6.1914)\n",
      "tensor(0.2813)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 114.531067\n",
      "Epoch 2752\n",
      "-------------------------------\n",
      "tensor(26.7000)\n",
      "tensor(11.0680)\n",
      "tensor(11.9005)\n",
      "tensor(0.4448)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 114.508583\n",
      "Epoch 2753\n",
      "-------------------------------\n",
      "tensor(20.6787)\n",
      "tensor(8.2788)\n",
      "tensor(5.5759)\n",
      "tensor(0.0429)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 114.442223\n",
      "Epoch 2754\n",
      "-------------------------------\n",
      "tensor(22.8674)\n",
      "tensor(10.7336)\n",
      "tensor(11.1632)\n",
      "tensor(0.3784)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 114.387680\n",
      "Epoch 2755\n",
      "-------------------------------\n",
      "tensor(24.5355)\n",
      "tensor(9.6516)\n",
      "tensor(6.3085)\n",
      "tensor(0.1211)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 114.345177\n",
      "Epoch 2756\n",
      "-------------------------------\n",
      "tensor(29.7883)\n",
      "tensor(10.5521)\n",
      "tensor(8.2027)\n",
      "tensor(0.1440)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 114.309860\n",
      "Epoch 2757\n",
      "-------------------------------\n",
      "tensor(23.3241)\n",
      "tensor(8.6104)\n",
      "tensor(8.3325)\n",
      "tensor(0.0772)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 114.254196\n",
      "Epoch 2758\n",
      "-------------------------------\n",
      "tensor(23.3692)\n",
      "tensor(9.2707)\n",
      "tensor(5.2472)\n",
      "tensor(0.0954)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 114.232704\n",
      "Epoch 2759\n",
      "-------------------------------\n",
      "tensor(30.3311)\n",
      "tensor(12.2795)\n",
      "tensor(6.1514)\n",
      "tensor(0.1915)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 114.231316\n",
      "Epoch 2760\n",
      "-------------------------------\n",
      "tensor(24.2838)\n",
      "tensor(9.6336)\n",
      "tensor(6.6164)\n",
      "tensor(0.2070)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 114.215065\n",
      "Epoch 2761\n",
      "-------------------------------\n",
      "tensor(22.9799)\n",
      "tensor(9.2006)\n",
      "tensor(6.2341)\n",
      "tensor(0.1784)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 114.200180\n",
      "Epoch 2762\n",
      "-------------------------------\n",
      "tensor(17.8734)\n",
      "tensor(7.8528)\n",
      "tensor(5.5774)\n",
      "tensor(0.1286)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 114.187988\n",
      "Epoch 2763\n",
      "-------------------------------\n",
      "tensor(24.5075)\n",
      "tensor(9.4122)\n",
      "tensor(4.9437)\n",
      "tensor(0.0436)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 114.183922\n",
      "Epoch 2764\n",
      "-------------------------------\n",
      "tensor(19.8340)\n",
      "tensor(8.2697)\n",
      "tensor(5.3539)\n",
      "tensor(0.0871)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 114.177177\n",
      "Epoch 2765\n",
      "-------------------------------\n",
      "tensor(22.3622)\n",
      "tensor(10.2940)\n",
      "tensor(6.9316)\n",
      "tensor(0.2340)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 114.168762\n",
      "Epoch 2766\n",
      "-------------------------------\n",
      "tensor(30.1040)\n",
      "tensor(11.1826)\n",
      "tensor(6.6527)\n",
      "tensor(0.2820)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 114.148048\n",
      "Epoch 2767\n",
      "-------------------------------\n",
      "tensor(22.5226)\n",
      "tensor(9.0453)\n",
      "tensor(4.9824)\n",
      "tensor(0.1259)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 114.120697\n",
      "Epoch 2768\n",
      "-------------------------------\n",
      "tensor(48.6757)\n",
      "tensor(24.9114)\n",
      "tensor(6.1978)\n",
      "tensor(0.1016)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 114.142860\n",
      "Epoch 2769\n",
      "-------------------------------\n",
      "tensor(24.4608)\n",
      "tensor(8.8748)\n",
      "tensor(7.1909)\n",
      "tensor(0.0665)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 114.062233\n",
      "Epoch 2770\n",
      "-------------------------------\n",
      "tensor(17.0242)\n",
      "tensor(10.0513)\n",
      "tensor(6.8529)\n",
      "tensor(0.3130)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 114.033279\n",
      "Epoch 2771\n",
      "-------------------------------\n",
      "tensor(27.3266)\n",
      "tensor(11.1979)\n",
      "tensor(7.4630)\n",
      "tensor(0.4159)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 114.019302\n",
      "Epoch 2772\n",
      "-------------------------------\n",
      "tensor(25.6934)\n",
      "tensor(9.6899)\n",
      "tensor(5.1572)\n",
      "tensor(0.0186)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 113.947998\n",
      "Epoch 2773\n",
      "-------------------------------\n",
      "tensor(26.7017)\n",
      "tensor(10.6169)\n",
      "tensor(6.3364)\n",
      "tensor(0.2754)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 113.898865\n",
      "Epoch 2774\n",
      "-------------------------------\n",
      "tensor(23.9012)\n",
      "tensor(9.3897)\n",
      "tensor(6.0458)\n",
      "tensor(0.1090)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 113.882347\n",
      "Epoch 2775\n",
      "-------------------------------\n",
      "tensor(15.9691)\n",
      "tensor(8.8620)\n",
      "tensor(5.3943)\n",
      "tensor(0.0628)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 113.845032\n",
      "Epoch 2776\n",
      "-------------------------------\n",
      "tensor(17.5830)\n",
      "tensor(7.9925)\n",
      "tensor(6.3717)\n",
      "tensor(0.0726)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 113.816101\n",
      "Epoch 2777\n",
      "-------------------------------\n",
      "tensor(25.4642)\n",
      "tensor(9.5915)\n",
      "tensor(6.3194)\n",
      "tensor(0.0119)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 113.799767\n",
      "Epoch 2778\n",
      "-------------------------------\n",
      "tensor(24.0020)\n",
      "tensor(9.4162)\n",
      "tensor(5.0990)\n",
      "tensor(0.0543)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 113.758797\n",
      "Epoch 2779\n",
      "-------------------------------\n",
      "tensor(24.2835)\n",
      "tensor(9.5164)\n",
      "tensor(5.4317)\n",
      "tensor(0.0892)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 113.729126\n",
      "Epoch 2780\n",
      "-------------------------------\n",
      "tensor(23.2029)\n",
      "tensor(9.1645)\n",
      "tensor(5.8053)\n",
      "tensor(0.0969)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 113.716171\n",
      "Epoch 2781\n",
      "-------------------------------\n",
      "tensor(23.1160)\n",
      "tensor(9.1378)\n",
      "tensor(5.7198)\n",
      "tensor(0.0940)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 113.700890\n",
      "Epoch 2782\n",
      "-------------------------------\n",
      "tensor(18.5384)\n",
      "tensor(7.8875)\n",
      "tensor(5.4534)\n",
      "tensor(0.0818)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 113.693855\n",
      "Epoch 2783\n",
      "-------------------------------\n",
      "tensor(23.5026)\n",
      "tensor(9.2406)\n",
      "tensor(5.0720)\n",
      "tensor(0.0558)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 113.689026\n",
      "Epoch 2784\n",
      "-------------------------------\n",
      "tensor(31.1581)\n",
      "tensor(11.1287)\n",
      "tensor(5.0285)\n",
      "tensor(0.0042)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 113.690025\n",
      "Epoch 2785\n",
      "-------------------------------\n",
      "tensor(23.8666)\n",
      "tensor(9.2236)\n",
      "tensor(5.2086)\n",
      "tensor(0.0697)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 113.680145\n",
      "Epoch 2786\n",
      "-------------------------------\n",
      "tensor(25.5419)\n",
      "tensor(10.7280)\n",
      "tensor(5.0923)\n",
      "tensor(0.1500)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 113.660622\n",
      "Epoch 2787\n",
      "-------------------------------\n",
      "tensor(48.2149)\n",
      "tensor(24.7723)\n",
      "tensor(5.5979)\n",
      "tensor(0.1595)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 113.665215\n",
      "Epoch 2788\n",
      "-------------------------------\n",
      "tensor(24.9301)\n",
      "tensor(9.5448)\n",
      "tensor(5.2098)\n",
      "tensor(0.0603)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 113.617867\n",
      "Epoch 2789\n",
      "-------------------------------\n",
      "tensor(24.8532)\n",
      "tensor(9.4929)\n",
      "tensor(5.6131)\n",
      "tensor(0.0275)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 113.583359\n",
      "Epoch 2790\n",
      "-------------------------------\n",
      "tensor(25.9559)\n",
      "tensor(10.3949)\n",
      "tensor(6.1673)\n",
      "tensor(0.1894)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 113.558670\n",
      "Epoch 2791\n",
      "-------------------------------\n",
      "tensor(23.5267)\n",
      "tensor(9.9271)\n",
      "tensor(6.6708)\n",
      "tensor(0.2472)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 113.555649\n",
      "Epoch 2792\n",
      "-------------------------------\n",
      "tensor(21.8037)\n",
      "tensor(8.8245)\n",
      "tensor(5.4241)\n",
      "tensor(0.0285)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 113.505531\n",
      "Epoch 2793\n",
      "-------------------------------\n",
      "tensor(23.3178)\n",
      "tensor(9.3112)\n",
      "tensor(5.8732)\n",
      "tensor(0.1047)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 113.434433\n",
      "Epoch 2794\n",
      "-------------------------------\n",
      "tensor(16.9033)\n",
      "tensor(9.0392)\n",
      "tensor(5.8127)\n",
      "tensor(0.0588)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 113.384560\n",
      "Epoch 2795\n",
      "-------------------------------\n",
      "tensor(23.1406)\n",
      "tensor(9.3196)\n",
      "tensor(5.3954)\n",
      "tensor(0.0581)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 113.326431\n",
      "Epoch 2796\n",
      "-------------------------------\n",
      "tensor(18.5778)\n",
      "tensor(8.3611)\n",
      "tensor(5.7120)\n",
      "tensor(0.0940)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 113.297897\n",
      "Epoch 2797\n",
      "-------------------------------\n",
      "tensor(32.9623)\n",
      "tensor(11.8623)\n",
      "tensor(5.8321)\n",
      "tensor(0.1006)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 113.268112\n",
      "Epoch 2798\n",
      "-------------------------------\n",
      "tensor(25.7008)\n",
      "tensor(9.9888)\n",
      "tensor(5.2719)\n",
      "tensor(0.0864)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 113.246620\n",
      "Epoch 2799\n",
      "-------------------------------\n",
      "tensor(23.9334)\n",
      "tensor(9.4220)\n",
      "tensor(5.7518)\n",
      "tensor(0.0725)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 113.220032\n",
      "Epoch 2800\n",
      "-------------------------------\n",
      "tensor(21.4554)\n",
      "tensor(8.6979)\n",
      "tensor(5.7553)\n",
      "tensor(0.0622)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 113.195183\n",
      "Epoch 2801\n",
      "-------------------------------\n",
      "tensor(22.7201)\n",
      "tensor(9.0671)\n",
      "tensor(5.4742)\n",
      "tensor(0.0494)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 113.185860\n",
      "Epoch 2802\n",
      "-------------------------------\n",
      "tensor(15.3511)\n",
      "tensor(7.2601)\n",
      "tensor(5.1917)\n",
      "tensor(0.0360)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 113.178993\n",
      "Epoch 2803\n",
      "-------------------------------\n",
      "tensor(28.2615)\n",
      "tensor(10.4853)\n",
      "tensor(5.0011)\n",
      "tensor(0.0133)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 113.173302\n",
      "Epoch 2804\n",
      "-------------------------------\n",
      "tensor(27.0776)\n",
      "tensor(11.0747)\n",
      "tensor(5.1501)\n",
      "tensor(0.0250)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 113.170410\n",
      "Epoch 2805\n",
      "-------------------------------\n",
      "tensor(48.6079)\n",
      "tensor(25.6306)\n",
      "tensor(5.2146)\n",
      "tensor(0.0780)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 113.172913\n",
      "Epoch 2806\n",
      "-------------------------------\n",
      "tensor(18.3005)\n",
      "tensor(8.2757)\n",
      "tensor(5.6392)\n",
      "tensor(0.1264)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 113.144379\n",
      "Epoch 2807\n",
      "-------------------------------\n",
      "tensor(30.3815)\n",
      "tensor(11.1074)\n",
      "tensor(5.2167)\n",
      "tensor(0.1784)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 113.129044\n",
      "Epoch 2808\n",
      "-------------------------------\n",
      "tensor(24.6995)\n",
      "tensor(10.1098)\n",
      "tensor(7.7120)\n",
      "tensor(0.1703)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 113.098289\n",
      "Epoch 2809\n",
      "-------------------------------\n",
      "tensor(24.0424)\n",
      "tensor(9.6623)\n",
      "tensor(7.2263)\n",
      "tensor(0.0755)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 113.097435\n",
      "Epoch 2810\n",
      "-------------------------------\n",
      "tensor(26.1407)\n",
      "tensor(9.7025)\n",
      "tensor(6.4750)\n",
      "tensor(0.0068)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 113.054123\n",
      "Epoch 2811\n",
      "-------------------------------\n",
      "tensor(25.6064)\n",
      "tensor(9.8496)\n",
      "tensor(5.4335)\n",
      "tensor(0.1330)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 113.018822\n",
      "Epoch 2812\n",
      "-------------------------------\n",
      "tensor(24.7505)\n",
      "tensor(10.7807)\n",
      "tensor(7.7655)\n",
      "tensor(0.1506)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 112.969528\n",
      "Epoch 2813\n",
      "-------------------------------\n",
      "tensor(15.8027)\n",
      "tensor(7.5817)\n",
      "tensor(5.6196)\n",
      "tensor(0.1632)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 112.913490\n",
      "Epoch 2814\n",
      "-------------------------------\n",
      "tensor(25.4225)\n",
      "tensor(10.3647)\n",
      "tensor(8.9999)\n",
      "tensor(0.2866)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 112.893044\n",
      "Epoch 2815\n",
      "-------------------------------\n",
      "tensor(25.4839)\n",
      "tensor(9.9339)\n",
      "tensor(5.6264)\n",
      "tensor(0.0737)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 112.844536\n",
      "Epoch 2816\n",
      "-------------------------------\n",
      "tensor(22.0200)\n",
      "tensor(8.7315)\n",
      "tensor(7.2863)\n",
      "tensor(0.0792)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 112.781303\n",
      "Epoch 2817\n",
      "-------------------------------\n",
      "tensor(52.8302)\n",
      "tensor(25.8626)\n",
      "tensor(6.4046)\n",
      "tensor(0.0376)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 112.819122\n",
      "Epoch 2818\n",
      "-------------------------------\n",
      "tensor(23.1161)\n",
      "tensor(9.1796)\n",
      "tensor(5.7017)\n",
      "tensor(0.0722)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 112.751770\n",
      "Epoch 2819\n",
      "-------------------------------\n",
      "tensor(24.4322)\n",
      "tensor(10.4142)\n",
      "tensor(7.0928)\n",
      "tensor(0.1042)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 112.726868\n",
      "Epoch 2820\n",
      "-------------------------------\n",
      "tensor(25.9651)\n",
      "tensor(9.7141)\n",
      "tensor(6.6182)\n",
      "tensor(0.0721)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 112.702271\n",
      "Epoch 2821\n",
      "-------------------------------\n",
      "tensor(18.5293)\n",
      "tensor(8.1079)\n",
      "tensor(5.6519)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 112.688850\n",
      "Epoch 2822\n",
      "-------------------------------\n",
      "tensor(18.2490)\n",
      "tensor(8.1895)\n",
      "tensor(5.1066)\n",
      "tensor(0.0341)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 112.681351\n",
      "Epoch 2823\n",
      "-------------------------------\n",
      "tensor(22.6064)\n",
      "tensor(9.3458)\n",
      "tensor(5.3051)\n",
      "tensor(0.1064)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 112.680771\n",
      "Epoch 2824\n",
      "-------------------------------\n",
      "tensor(27.4879)\n",
      "tensor(11.7180)\n",
      "tensor(6.6736)\n",
      "tensor(0.1930)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 112.683289\n",
      "Epoch 2825\n",
      "-------------------------------\n",
      "tensor(27.7101)\n",
      "tensor(10.8796)\n",
      "tensor(8.0668)\n",
      "tensor(0.2360)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 112.673447\n",
      "Epoch 2826\n",
      "-------------------------------\n",
      "tensor(23.4592)\n",
      "tensor(9.5786)\n",
      "tensor(6.6431)\n",
      "tensor(0.1383)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 112.632065\n",
      "Epoch 2827\n",
      "-------------------------------\n",
      "tensor(23.2369)\n",
      "tensor(9.2390)\n",
      "tensor(5.5617)\n",
      "tensor(0.0741)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 112.613647\n",
      "Epoch 2828\n",
      "-------------------------------\n",
      "tensor(27.9902)\n",
      "tensor(10.4024)\n",
      "tensor(7.6572)\n",
      "tensor(0.1828)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 112.599228\n",
      "Epoch 2829\n",
      "-------------------------------\n",
      "tensor(52.7815)\n",
      "tensor(26.7333)\n",
      "tensor(5.4064)\n",
      "tensor(0.0289)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 112.583725\n",
      "Epoch 2830\n",
      "-------------------------------\n",
      "tensor(25.0314)\n",
      "tensor(9.8912)\n",
      "tensor(5.3333)\n",
      "tensor(0.0904)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 112.519295\n",
      "Epoch 2831\n",
      "-------------------------------\n",
      "tensor(24.7505)\n",
      "tensor(9.8722)\n",
      "tensor(5.0924)\n",
      "tensor(0.0864)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 112.498528\n",
      "Epoch 2832\n",
      "-------------------------------\n",
      "tensor(23.4117)\n",
      "tensor(9.6262)\n",
      "tensor(5.2241)\n",
      "tensor(0.0654)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 112.480507\n",
      "Epoch 2833\n",
      "-------------------------------\n",
      "tensor(21.3732)\n",
      "tensor(9.2483)\n",
      "tensor(5.3655)\n",
      "tensor(0.0845)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 112.445038\n",
      "Epoch 2834\n",
      "-------------------------------\n",
      "tensor(15.5028)\n",
      "tensor(8.1315)\n",
      "tensor(5.1113)\n",
      "tensor(0.1011)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 112.408676\n",
      "Epoch 2835\n",
      "-------------------------------\n",
      "tensor(23.6702)\n",
      "tensor(10.6514)\n",
      "tensor(5.5079)\n",
      "tensor(0.0831)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 112.367592\n",
      "Epoch 2836\n",
      "-------------------------------\n",
      "tensor(25.1163)\n",
      "tensor(11.0633)\n",
      "tensor(5.1345)\n",
      "tensor(0.0310)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 112.332558\n",
      "Epoch 2837\n",
      "-------------------------------\n",
      "tensor(18.4170)\n",
      "tensor(8.3685)\n",
      "tensor(5.5543)\n",
      "tensor(0.0459)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 112.294556\n",
      "Epoch 2838\n",
      "-------------------------------\n",
      "tensor(18.0793)\n",
      "tensor(8.3734)\n",
      "tensor(5.4074)\n",
      "tensor(0.0990)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 112.257034\n",
      "Epoch 2839\n",
      "-------------------------------\n",
      "tensor(17.6262)\n",
      "tensor(8.3785)\n",
      "tensor(5.3795)\n",
      "tensor(0.1122)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 112.221771\n",
      "Epoch 2840\n",
      "-------------------------------\n",
      "tensor(22.7499)\n",
      "tensor(9.5343)\n",
      "tensor(5.5341)\n",
      "tensor(0.0959)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 112.204910\n",
      "Epoch 2841\n",
      "-------------------------------\n",
      "tensor(22.7932)\n",
      "tensor(9.5041)\n",
      "tensor(5.5282)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 112.195663\n",
      "Epoch 2842\n",
      "-------------------------------\n",
      "tensor(24.9933)\n",
      "tensor(9.9932)\n",
      "tensor(5.4166)\n",
      "tensor(0.0500)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 112.185616\n",
      "Epoch 2843\n",
      "-------------------------------\n",
      "tensor(29.0688)\n",
      "tensor(10.9861)\n",
      "tensor(5.2388)\n",
      "tensor(0.0223)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 112.167053\n",
      "Epoch 2844\n",
      "-------------------------------\n",
      "tensor(28.9813)\n",
      "tensor(10.9992)\n",
      "tensor(5.3051)\n",
      "tensor(0.0166)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 112.158813\n",
      "Epoch 2845\n",
      "-------------------------------\n",
      "tensor(27.3902)\n",
      "tensor(10.6141)\n",
      "tensor(6.4383)\n",
      "tensor(0.0426)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 112.144135\n",
      "Epoch 2846\n",
      "-------------------------------\n",
      "tensor(20.5389)\n",
      "tensor(8.6436)\n",
      "tensor(7.1140)\n",
      "tensor(0.0194)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 112.126869\n",
      "Epoch 2847\n",
      "-------------------------------\n",
      "tensor(20.9847)\n",
      "tensor(8.8777)\n",
      "tensor(5.3022)\n",
      "tensor(0.0374)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 112.115654\n",
      "Epoch 2848\n",
      "-------------------------------\n",
      "tensor(29.2847)\n",
      "tensor(11.3697)\n",
      "tensor(7.2350)\n",
      "tensor(0.0144)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 112.081108\n",
      "Epoch 2849\n",
      "-------------------------------\n",
      "tensor(21.2995)\n",
      "tensor(9.3537)\n",
      "tensor(5.2910)\n",
      "tensor(0.1733)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 112.021317\n",
      "Epoch 2850\n",
      "-------------------------------\n",
      "tensor(51.1781)\n",
      "tensor(25.6548)\n",
      "tensor(8.2790)\n",
      "tensor(0.2183)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 112.077591\n",
      "Epoch 2851\n",
      "-------------------------------\n",
      "tensor(53.2136)\n",
      "tensor(27.0969)\n",
      "tensor(8.9506)\n",
      "tensor(0.0133)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 111.934212\n",
      "Epoch 2852\n",
      "-------------------------------\n",
      "tensor(37.1858)\n",
      "tensor(12.1380)\n",
      "tensor(9.1306)\n",
      "tensor(0.0283)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 111.908630\n",
      "Epoch 2853\n",
      "-------------------------------\n",
      "tensor(27.8393)\n",
      "tensor(11.1929)\n",
      "tensor(15.5436)\n",
      "tensor(0.3142)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 111.885239\n",
      "Epoch 2854\n",
      "-------------------------------\n",
      "tensor(19.5868)\n",
      "tensor(9.9563)\n",
      "tensor(7.4311)\n",
      "tensor(0.2562)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 111.853821\n",
      "Epoch 2855\n",
      "-------------------------------\n",
      "tensor(31.4544)\n",
      "tensor(10.7400)\n",
      "tensor(12.0431)\n",
      "tensor(0.0433)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 111.827026\n",
      "Epoch 2856\n",
      "-------------------------------\n",
      "tensor(20.2955)\n",
      "tensor(9.1990)\n",
      "tensor(7.5035)\n",
      "tensor(0.0515)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 111.778465\n",
      "Epoch 2857\n",
      "-------------------------------\n",
      "tensor(20.3743)\n",
      "tensor(9.1577)\n",
      "tensor(7.0236)\n",
      "tensor(0.0960)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 111.757683\n",
      "Epoch 2858\n",
      "-------------------------------\n",
      "tensor(31.0787)\n",
      "tensor(12.8769)\n",
      "tensor(8.6370)\n",
      "tensor(0.0253)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 111.744438\n",
      "Epoch 2859\n",
      "-------------------------------\n",
      "tensor(31.8362)\n",
      "tensor(13.3905)\n",
      "tensor(7.1296)\n",
      "tensor(0.0922)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 111.724442\n",
      "Epoch 2860\n",
      "-------------------------------\n",
      "tensor(31.1987)\n",
      "tensor(12.4476)\n",
      "tensor(5.9933)\n",
      "tensor(0.1741)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 111.705414\n",
      "Epoch 2861\n",
      "-------------------------------\n",
      "tensor(23.0606)\n",
      "tensor(10.2907)\n",
      "tensor(5.7851)\n",
      "tensor(0.2091)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 111.696228\n",
      "Epoch 2862\n",
      "-------------------------------\n",
      "tensor(22.9131)\n",
      "tensor(10.2725)\n",
      "tensor(5.8741)\n",
      "tensor(0.2139)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 111.689217\n",
      "Epoch 2863\n",
      "-------------------------------\n",
      "tensor(16.9554)\n",
      "tensor(8.8691)\n",
      "tensor(6.0637)\n",
      "tensor(0.1865)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 111.686150\n",
      "Epoch 2864\n",
      "-------------------------------\n",
      "tensor(17.2060)\n",
      "tensor(8.7775)\n",
      "tensor(6.2860)\n",
      "tensor(0.1020)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 111.679008\n",
      "Epoch 2865\n",
      "-------------------------------\n",
      "tensor(23.5231)\n",
      "tensor(9.9291)\n",
      "tensor(6.1768)\n",
      "tensor(0.0662)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 111.673370\n",
      "Epoch 2866\n",
      "-------------------------------\n",
      "tensor(25.1788)\n",
      "tensor(10.6399)\n",
      "tensor(5.9380)\n",
      "tensor(0.2573)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 111.645103\n",
      "Epoch 2867\n",
      "-------------------------------\n",
      "tensor(23.4870)\n",
      "tensor(10.6367)\n",
      "tensor(7.4861)\n",
      "tensor(0.2984)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 111.606499\n",
      "Epoch 2868\n",
      "-------------------------------\n",
      "tensor(20.9266)\n",
      "tensor(10.5381)\n",
      "tensor(6.8705)\n",
      "tensor(0.0550)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 111.567039\n",
      "Epoch 2869\n",
      "-------------------------------\n",
      "tensor(31.9999)\n",
      "tensor(11.9507)\n",
      "tensor(7.3063)\n",
      "tensor(0.2136)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 111.518188\n",
      "Epoch 2870\n",
      "-------------------------------\n",
      "tensor(16.2608)\n",
      "tensor(7.9853)\n",
      "tensor(5.6428)\n",
      "tensor(0.0475)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 111.443283\n",
      "Epoch 2871\n",
      "-------------------------------\n",
      "tensor(29.5850)\n",
      "tensor(11.7866)\n",
      "tensor(6.8817)\n",
      "tensor(0.3377)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 111.393173\n",
      "Epoch 2872\n",
      "-------------------------------\n",
      "tensor(19.1847)\n",
      "tensor(10.2153)\n",
      "tensor(6.7427)\n",
      "tensor(0.2009)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 111.324989\n",
      "Epoch 2873\n",
      "-------------------------------\n",
      "tensor(23.0620)\n",
      "tensor(9.6104)\n",
      "tensor(6.8410)\n",
      "tensor(0.2577)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 111.253746\n",
      "Epoch 2874\n",
      "-------------------------------\n",
      "tensor(51.7897)\n",
      "tensor(28.0279)\n",
      "tensor(6.6576)\n",
      "tensor(0.3105)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 111.217270\n",
      "Epoch 2875\n",
      "-------------------------------\n",
      "tensor(26.2047)\n",
      "tensor(10.0728)\n",
      "tensor(6.0886)\n",
      "tensor(0.0996)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 111.173820\n",
      "Epoch 2876\n",
      "-------------------------------\n",
      "tensor(26.6589)\n",
      "tensor(10.4501)\n",
      "tensor(5.7557)\n",
      "tensor(0.1395)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 111.134186\n",
      "Epoch 2877\n",
      "-------------------------------\n",
      "tensor(25.6027)\n",
      "tensor(10.3972)\n",
      "tensor(7.1065)\n",
      "tensor(0.1706)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 111.105316\n",
      "Epoch 2878\n",
      "-------------------------------\n",
      "tensor(16.4796)\n",
      "tensor(8.0391)\n",
      "tensor(6.2051)\n",
      "tensor(0.0674)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 111.079666\n",
      "Epoch 2879\n",
      "-------------------------------\n",
      "tensor(15.5874)\n",
      "tensor(7.7225)\n",
      "tensor(5.3012)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 111.061073\n",
      "Epoch 2880\n",
      "-------------------------------\n",
      "tensor(25.1145)\n",
      "tensor(9.8386)\n",
      "tensor(5.5854)\n",
      "tensor(0.0563)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 111.046791\n",
      "Epoch 2881\n",
      "-------------------------------\n",
      "tensor(23.1170)\n",
      "tensor(9.3227)\n",
      "tensor(5.7353)\n",
      "tensor(0.0575)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 111.038269\n",
      "Epoch 2882\n",
      "-------------------------------\n",
      "tensor(21.5011)\n",
      "tensor(8.9687)\n",
      "tensor(5.6181)\n",
      "tensor(0.0431)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 111.026688\n",
      "Epoch 2883\n",
      "-------------------------------\n",
      "tensor(15.1257)\n",
      "tensor(7.7093)\n",
      "tensor(5.3222)\n",
      "tensor(0.0096)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 111.014816\n",
      "Epoch 2884\n",
      "-------------------------------\n",
      "tensor(29.7875)\n",
      "tensor(12.1108)\n",
      "tensor(5.2343)\n",
      "tensor(0.0511)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 111.001244\n",
      "Epoch 2885\n",
      "-------------------------------\n",
      "tensor(29.0243)\n",
      "tensor(12.0931)\n",
      "tensor(6.4711)\n",
      "tensor(0.1175)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 111.008705\n",
      "Epoch 2886\n",
      "-------------------------------\n",
      "tensor(30.7799)\n",
      "tensor(11.5638)\n",
      "tensor(7.8870)\n",
      "tensor(0.0877)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 110.990242\n",
      "Epoch 2887\n",
      "-------------------------------\n",
      "tensor(24.8134)\n",
      "tensor(9.8897)\n",
      "tensor(6.6272)\n",
      "tensor(0.1100)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 110.944107\n",
      "Epoch 2888\n",
      "-------------------------------\n",
      "tensor(53.6032)\n",
      "tensor(28.5726)\n",
      "tensor(7.1724)\n",
      "tensor(0.3093)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 111.002495\n",
      "Epoch 2889\n",
      "-------------------------------\n",
      "tensor(22.6763)\n",
      "tensor(8.7302)\n",
      "tensor(10.7480)\n",
      "tensor(0.1727)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 110.905594\n",
      "Epoch 2890\n",
      "-------------------------------\n",
      "tensor(15.4352)\n",
      "tensor(9.5111)\n",
      "tensor(6.8459)\n",
      "tensor(0.3562)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 110.875053\n",
      "Epoch 2891\n",
      "-------------------------------\n",
      "tensor(21.2884)\n",
      "tensor(10.7330)\n",
      "tensor(10.3216)\n",
      "tensor(0.4222)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 110.826851\n",
      "Epoch 2892\n",
      "-------------------------------\n",
      "tensor(24.7706)\n",
      "tensor(9.8504)\n",
      "tensor(6.8118)\n",
      "tensor(0.1358)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 110.777626\n",
      "Epoch 2893\n",
      "-------------------------------\n",
      "tensor(27.7192)\n",
      "tensor(12.0196)\n",
      "tensor(8.1512)\n",
      "tensor(0.2947)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 110.728165\n",
      "Epoch 2894\n",
      "-------------------------------\n",
      "tensor(26.4837)\n",
      "tensor(11.6769)\n",
      "tensor(7.6824)\n",
      "tensor(0.0590)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 110.679947\n",
      "Epoch 2895\n",
      "-------------------------------\n",
      "tensor(31.7415)\n",
      "tensor(12.0025)\n",
      "tensor(8.0764)\n",
      "tensor(0.1954)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 110.667175\n",
      "Epoch 2896\n",
      "-------------------------------\n",
      "tensor(24.3747)\n",
      "tensor(10.0103)\n",
      "tensor(5.3314)\n",
      "tensor(0.0084)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 110.617340\n",
      "Epoch 2897\n",
      "-------------------------------\n",
      "tensor(23.4747)\n",
      "tensor(9.7679)\n",
      "tensor(6.7039)\n",
      "tensor(0.1622)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 110.576172\n",
      "Epoch 2898\n",
      "-------------------------------\n",
      "tensor(25.7320)\n",
      "tensor(10.3422)\n",
      "tensor(6.2507)\n",
      "tensor(0.1540)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 110.524582\n",
      "Epoch 2899\n",
      "-------------------------------\n",
      "tensor(15.3117)\n",
      "tensor(7.9249)\n",
      "tensor(5.4393)\n",
      "tensor(0.0687)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 110.507652\n",
      "Epoch 2900\n",
      "-------------------------------\n",
      "tensor(15.5550)\n",
      "tensor(8.0162)\n",
      "tensor(5.5224)\n",
      "tensor(0.0031)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 110.496544\n",
      "Epoch 2901\n",
      "-------------------------------\n",
      "tensor(17.1554)\n",
      "tensor(8.4095)\n",
      "tensor(5.5777)\n",
      "tensor(0.0436)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 110.487366\n",
      "Epoch 2902\n",
      "-------------------------------\n",
      "tensor(20.2256)\n",
      "tensor(9.0934)\n",
      "tensor(5.5191)\n",
      "tensor(0.0653)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 110.477837\n",
      "Epoch 2903\n",
      "-------------------------------\n",
      "tensor(22.9525)\n",
      "tensor(9.6833)\n",
      "tensor(5.4198)\n",
      "tensor(0.0749)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 110.465469\n",
      "Epoch 2904\n",
      "-------------------------------\n",
      "tensor(25.8334)\n",
      "tensor(10.2689)\n",
      "tensor(5.4044)\n",
      "tensor(0.0674)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 110.455948\n",
      "Epoch 2905\n",
      "-------------------------------\n",
      "tensor(22.8655)\n",
      "tensor(9.5409)\n",
      "tensor(5.4394)\n",
      "tensor(0.0245)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 110.440910\n",
      "Epoch 2906\n",
      "-------------------------------\n",
      "tensor(27.4722)\n",
      "tensor(10.6615)\n",
      "tensor(5.4296)\n",
      "tensor(0.0524)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 110.403114\n",
      "Epoch 2907\n",
      "-------------------------------\n",
      "tensor(57.1005)\n",
      "tensor(28.4435)\n",
      "tensor(5.9266)\n",
      "tensor(0.1313)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 110.421875\n",
      "Epoch 2908\n",
      "-------------------------------\n",
      "tensor(27.6712)\n",
      "tensor(11.8946)\n",
      "tensor(5.9205)\n",
      "tensor(0.1427)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 110.325714\n",
      "Epoch 2909\n",
      "-------------------------------\n",
      "tensor(19.6152)\n",
      "tensor(10.1797)\n",
      "tensor(5.5530)\n",
      "tensor(0.0505)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 110.298912\n",
      "Epoch 2910\n",
      "-------------------------------\n",
      "tensor(17.9431)\n",
      "tensor(10.0398)\n",
      "tensor(6.1961)\n",
      "tensor(0.3176)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 110.267113\n",
      "Epoch 2911\n",
      "-------------------------------\n",
      "tensor(26.3723)\n",
      "tensor(11.2066)\n",
      "tensor(6.0389)\n",
      "tensor(0.3045)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 110.231041\n",
      "Epoch 2912\n",
      "-------------------------------\n",
      "tensor(21.5846)\n",
      "tensor(9.4116)\n",
      "tensor(5.8330)\n",
      "tensor(0.0080)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 110.147926\n",
      "Epoch 2913\n",
      "-------------------------------\n",
      "tensor(28.7092)\n",
      "tensor(11.4631)\n",
      "tensor(6.1217)\n",
      "tensor(0.1992)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 110.092018\n",
      "Epoch 2914\n",
      "-------------------------------\n",
      "tensor(26.6586)\n",
      "tensor(10.7592)\n",
      "tensor(5.7387)\n",
      "tensor(0.1208)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 110.040886\n",
      "Epoch 2915\n",
      "-------------------------------\n",
      "tensor(24.3270)\n",
      "tensor(9.9793)\n",
      "tensor(5.7086)\n",
      "tensor(0.0024)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 110.003082\n",
      "Epoch 2916\n",
      "-------------------------------\n",
      "tensor(19.3788)\n",
      "tensor(10.0643)\n",
      "tensor(5.6577)\n",
      "tensor(0.0032)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 109.972092\n",
      "Epoch 2917\n",
      "-------------------------------\n",
      "tensor(51.2831)\n",
      "tensor(27.8389)\n",
      "tensor(5.7241)\n",
      "tensor(0.0636)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 109.952744\n",
      "Epoch 2918\n",
      "-------------------------------\n",
      "tensor(15.8066)\n",
      "tensor(8.0870)\n",
      "tensor(7.2062)\n",
      "tensor(0.1161)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 109.914429\n",
      "Epoch 2919\n",
      "-------------------------------\n",
      "tensor(17.6526)\n",
      "tensor(8.4121)\n",
      "tensor(6.9569)\n",
      "tensor(0.0914)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 109.892151\n",
      "Epoch 2920\n",
      "-------------------------------\n",
      "tensor(23.2254)\n",
      "tensor(9.6530)\n",
      "tensor(5.9699)\n",
      "tensor(0.0404)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 109.874100\n",
      "Epoch 2921\n",
      "-------------------------------\n",
      "tensor(25.3887)\n",
      "tensor(10.2565)\n",
      "tensor(5.4098)\n",
      "tensor(0.0028)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 109.862961\n",
      "Epoch 2922\n",
      "-------------------------------\n",
      "tensor(18.6726)\n",
      "tensor(8.8450)\n",
      "tensor(5.4366)\n",
      "tensor(0.0352)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 109.850510\n",
      "Epoch 2923\n",
      "-------------------------------\n",
      "tensor(19.9492)\n",
      "tensor(9.1595)\n",
      "tensor(5.9379)\n",
      "tensor(0.0665)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 109.840065\n",
      "Epoch 2924\n",
      "-------------------------------\n",
      "tensor(20.5882)\n",
      "tensor(9.3201)\n",
      "tensor(6.5660)\n",
      "tensor(0.0890)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 109.823792\n",
      "Epoch 2925\n",
      "-------------------------------\n",
      "tensor(33.7871)\n",
      "tensor(13.5636)\n",
      "tensor(6.4174)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 109.835823\n",
      "Epoch 2926\n",
      "-------------------------------\n",
      "tensor(28.9600)\n",
      "tensor(12.3617)\n",
      "tensor(5.9029)\n",
      "tensor(0.0038)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 109.830200\n",
      "Epoch 2927\n",
      "-------------------------------\n",
      "tensor(28.7326)\n",
      "tensor(11.2609)\n",
      "tensor(5.8344)\n",
      "tensor(0.1259)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 109.803741\n",
      "Epoch 2928\n",
      "-------------------------------\n",
      "tensor(30.8526)\n",
      "tensor(11.7781)\n",
      "tensor(5.9397)\n",
      "tensor(0.1293)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 109.722328\n",
      "Epoch 2929\n",
      "-------------------------------\n",
      "tensor(53.4806)\n",
      "tensor(27.0519)\n",
      "tensor(6.7195)\n",
      "tensor(0.0805)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 109.735748\n",
      "Epoch 2930\n",
      "-------------------------------\n",
      "tensor(21.6495)\n",
      "tensor(9.6148)\n",
      "tensor(8.0416)\n",
      "tensor(0.1794)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 109.697868\n",
      "Epoch 2931\n",
      "-------------------------------\n",
      "tensor(16.6350)\n",
      "tensor(9.6568)\n",
      "tensor(6.4007)\n",
      "tensor(0.2713)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 109.662415\n",
      "Epoch 2932\n",
      "-------------------------------\n",
      "tensor(24.0153)\n",
      "tensor(10.6701)\n",
      "tensor(8.1539)\n",
      "tensor(0.1912)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 109.617790\n",
      "Epoch 2933\n",
      "-------------------------------\n",
      "tensor(24.1159)\n",
      "tensor(10.2816)\n",
      "tensor(6.3380)\n",
      "tensor(0.1339)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 109.574921\n",
      "Epoch 2934\n",
      "-------------------------------\n",
      "tensor(26.4907)\n",
      "tensor(12.0394)\n",
      "tensor(7.5301)\n",
      "tensor(0.2466)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 109.519142\n",
      "Epoch 2935\n",
      "-------------------------------\n",
      "tensor(26.9040)\n",
      "tensor(10.8997)\n",
      "tensor(5.7111)\n",
      "tensor(0.0405)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 109.448700\n",
      "Epoch 2936\n",
      "-------------------------------\n",
      "tensor(19.6290)\n",
      "tensor(9.1657)\n",
      "tensor(7.0739)\n",
      "tensor(0.0895)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 109.386681\n",
      "Epoch 2937\n",
      "-------------------------------\n",
      "tensor(15.8366)\n",
      "tensor(8.4343)\n",
      "tensor(6.0990)\n",
      "tensor(0.0179)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 109.345589\n",
      "Epoch 2938\n",
      "-------------------------------\n",
      "tensor(14.6966)\n",
      "tensor(8.3788)\n",
      "tensor(5.9048)\n",
      "tensor(0.1001)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 109.304893\n",
      "Epoch 2939\n",
      "-------------------------------\n",
      "tensor(30.0716)\n",
      "tensor(12.6139)\n",
      "tensor(6.7387)\n",
      "tensor(0.1431)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 109.298767\n",
      "Epoch 2940\n",
      "-------------------------------\n",
      "tensor(30.6190)\n",
      "tensor(11.8474)\n",
      "tensor(6.3378)\n",
      "tensor(0.1411)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 109.283791\n",
      "Epoch 2941\n",
      "-------------------------------\n",
      "tensor(28.2082)\n",
      "tensor(11.2926)\n",
      "tensor(5.7659)\n",
      "tensor(0.1208)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 109.275078\n",
      "Epoch 2942\n",
      "-------------------------------\n",
      "tensor(26.7081)\n",
      "tensor(10.9327)\n",
      "tensor(5.6071)\n",
      "tensor(0.0959)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 109.262718\n",
      "Epoch 2943\n",
      "-------------------------------\n",
      "tensor(29.4633)\n",
      "tensor(11.6824)\n",
      "tensor(5.9987)\n",
      "tensor(0.0629)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 109.242073\n",
      "Epoch 2944\n",
      "-------------------------------\n",
      "tensor(21.6724)\n",
      "tensor(9.4991)\n",
      "tensor(7.2404)\n",
      "tensor(0.0121)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 109.236053\n",
      "Epoch 2945\n",
      "-------------------------------\n",
      "tensor(21.7701)\n",
      "tensor(9.5794)\n",
      "tensor(7.5526)\n",
      "tensor(0.0388)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 109.230682\n",
      "Epoch 2946\n",
      "-------------------------------\n",
      "tensor(26.8086)\n",
      "tensor(11.9053)\n",
      "tensor(5.6058)\n",
      "tensor(0.0568)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 109.228485\n",
      "Epoch 2947\n",
      "-------------------------------\n",
      "tensor(27.9568)\n",
      "tensor(10.7219)\n",
      "tensor(6.3934)\n",
      "tensor(0.0341)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 109.193329\n",
      "Epoch 2948\n",
      "-------------------------------\n",
      "tensor(19.0811)\n",
      "tensor(9.0588)\n",
      "tensor(5.6589)\n",
      "tensor(0.0752)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 109.154205\n",
      "Epoch 2949\n",
      "-------------------------------\n",
      "tensor(53.4690)\n",
      "tensor(27.3327)\n",
      "tensor(6.6600)\n",
      "tensor(0.1230)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 109.282875\n",
      "Epoch 2950\n",
      "-------------------------------\n",
      "tensor(53.8041)\n",
      "tensor(28.7863)\n",
      "tensor(7.6361)\n",
      "tensor(0.0592)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 109.120094\n",
      "Epoch 2951\n",
      "-------------------------------\n",
      "tensor(25.6852)\n",
      "tensor(9.9969)\n",
      "tensor(9.8566)\n",
      "tensor(0.0753)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 109.039330\n",
      "Epoch 2952\n",
      "-------------------------------\n",
      "tensor(21.7824)\n",
      "tensor(11.5944)\n",
      "tensor(10.8264)\n",
      "tensor(0.3620)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 108.984550\n",
      "Epoch 2953\n",
      "-------------------------------\n",
      "tensor(31.7665)\n",
      "tensor(13.1420)\n",
      "tensor(8.7308)\n",
      "tensor(0.2319)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 108.983055\n",
      "Epoch 2954\n",
      "-------------------------------\n",
      "tensor(32.6842)\n",
      "tensor(12.6518)\n",
      "tensor(6.9398)\n",
      "tensor(0.1573)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 108.956879\n",
      "Epoch 2955\n",
      "-------------------------------\n",
      "tensor(24.6649)\n",
      "tensor(10.9972)\n",
      "tensor(6.4600)\n",
      "tensor(0.2026)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 108.882309\n",
      "Epoch 2956\n",
      "-------------------------------\n",
      "tensor(21.2210)\n",
      "tensor(11.2527)\n",
      "tensor(6.2313)\n",
      "tensor(0.0051)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 108.854256\n",
      "Epoch 2957\n",
      "-------------------------------\n",
      "tensor(22.9950)\n",
      "tensor(10.5776)\n",
      "tensor(6.3653)\n",
      "tensor(0.1027)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 108.834183\n",
      "Epoch 2958\n",
      "-------------------------------\n",
      "tensor(20.6939)\n",
      "tensor(10.1279)\n",
      "tensor(5.7269)\n",
      "tensor(0.0537)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 108.808151\n",
      "Epoch 2959\n",
      "-------------------------------\n",
      "tensor(17.6606)\n",
      "tensor(9.5748)\n",
      "tensor(5.9146)\n",
      "tensor(0.0247)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 108.786812\n",
      "Epoch 2960\n",
      "-------------------------------\n",
      "tensor(17.0019)\n",
      "tensor(9.4621)\n",
      "tensor(6.1418)\n",
      "tensor(0.0736)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 108.770935\n",
      "Epoch 2961\n",
      "-------------------------------\n",
      "tensor(18.8906)\n",
      "tensor(9.7917)\n",
      "tensor(6.1192)\n",
      "tensor(0.0936)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 108.760574\n",
      "Epoch 2962\n",
      "-------------------------------\n",
      "tensor(18.7973)\n",
      "tensor(9.7659)\n",
      "tensor(5.9571)\n",
      "tensor(0.0993)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 108.748825\n",
      "Epoch 2963\n",
      "-------------------------------\n",
      "tensor(27.7421)\n",
      "tensor(11.6719)\n",
      "tensor(5.7194)\n",
      "tensor(0.0943)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 108.730927\n",
      "Epoch 2964\n",
      "-------------------------------\n",
      "tensor(28.5950)\n",
      "tensor(12.8821)\n",
      "tensor(5.6451)\n",
      "tensor(0.0717)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 108.715073\n",
      "Epoch 2965\n",
      "-------------------------------\n",
      "tensor(19.0948)\n",
      "tensor(9.4975)\n",
      "tensor(6.7211)\n",
      "tensor(0.0392)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 108.684608\n",
      "Epoch 2966\n",
      "-------------------------------\n",
      "tensor(28.2162)\n",
      "tensor(11.7829)\n",
      "tensor(6.7351)\n",
      "tensor(0.0060)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 108.659302\n",
      "Epoch 2967\n",
      "-------------------------------\n",
      "tensor(29.7669)\n",
      "tensor(12.0534)\n",
      "tensor(5.6067)\n",
      "tensor(0.0002)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 108.643265\n",
      "Epoch 2968\n",
      "-------------------------------\n",
      "tensor(26.3398)\n",
      "tensor(11.9773)\n",
      "tensor(5.7221)\n",
      "tensor(0.0018)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 108.578941\n",
      "Epoch 2969\n",
      "-------------------------------\n",
      "tensor(18.8631)\n",
      "tensor(9.6933)\n",
      "tensor(5.6982)\n",
      "tensor(0.0944)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 108.496300\n",
      "Epoch 2970\n",
      "-------------------------------\n",
      "tensor(22.8785)\n",
      "tensor(10.7198)\n",
      "tensor(6.1788)\n",
      "tensor(0.1914)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 108.435860\n",
      "Epoch 2971\n",
      "-------------------------------\n",
      "tensor(25.4211)\n",
      "tensor(10.6990)\n",
      "tensor(5.5636)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 108.371155\n",
      "Epoch 2972\n",
      "-------------------------------\n",
      "tensor(26.5057)\n",
      "tensor(10.7205)\n",
      "tensor(5.7633)\n",
      "tensor(0.1261)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 108.282364\n",
      "Epoch 2973\n",
      "-------------------------------\n",
      "tensor(56.7266)\n",
      "tensor(29.6236)\n",
      "tensor(6.4573)\n",
      "tensor(0.2195)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 108.401672\n",
      "Epoch 2974\n",
      "-------------------------------\n",
      "tensor(55.5914)\n",
      "tensor(30.5070)\n",
      "tensor(9.0146)\n",
      "tensor(0.1414)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 108.269379\n",
      "Epoch 2975\n",
      "-------------------------------\n",
      "tensor(30.5636)\n",
      "tensor(11.0632)\n",
      "tensor(8.7126)\n",
      "tensor(0.0946)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 108.168106\n",
      "Epoch 2976\n",
      "-------------------------------\n",
      "tensor(19.5943)\n",
      "tensor(10.9089)\n",
      "tensor(8.6958)\n",
      "tensor(0.3094)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 108.145866\n",
      "Epoch 2977\n",
      "-------------------------------\n",
      "tensor(20.8464)\n",
      "tensor(10.8220)\n",
      "tensor(10.6269)\n",
      "tensor(0.2648)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 108.124489\n",
      "Epoch 2978\n",
      "-------------------------------\n",
      "tensor(23.4014)\n",
      "tensor(10.7886)\n",
      "tensor(6.5404)\n",
      "tensor(0.1065)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 108.102936\n",
      "Epoch 2979\n",
      "-------------------------------\n",
      "tensor(23.6834)\n",
      "tensor(10.3216)\n",
      "tensor(6.0970)\n",
      "tensor(0.0004)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 108.080070\n",
      "Epoch 2980\n",
      "-------------------------------\n",
      "tensor(24.1548)\n",
      "tensor(10.2538)\n",
      "tensor(6.7681)\n",
      "tensor(0.0314)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 108.055130\n",
      "Epoch 2981\n",
      "-------------------------------\n",
      "tensor(19.9848)\n",
      "tensor(9.4628)\n",
      "tensor(6.5961)\n",
      "tensor(0.0217)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 108.037857\n",
      "Epoch 2982\n",
      "-------------------------------\n",
      "tensor(19.9504)\n",
      "tensor(10.6644)\n",
      "tensor(6.0634)\n",
      "tensor(0.0070)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 108.029434\n",
      "Epoch 2983\n",
      "-------------------------------\n",
      "tensor(21.0958)\n",
      "tensor(11.0218)\n",
      "tensor(5.5508)\n",
      "tensor(0.0516)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 108.016449\n",
      "Epoch 2984\n",
      "-------------------------------\n",
      "tensor(31.8047)\n",
      "tensor(12.4738)\n",
      "tensor(6.0862)\n",
      "tensor(0.1134)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 108.008530\n",
      "Epoch 2985\n",
      "-------------------------------\n",
      "tensor(30.5065)\n",
      "tensor(12.1915)\n",
      "tensor(8.7312)\n",
      "tensor(0.1381)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 108.018379\n",
      "Epoch 2986\n",
      "-------------------------------\n",
      "tensor(31.2292)\n",
      "tensor(12.2745)\n",
      "tensor(9.2867)\n",
      "tensor(0.0029)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 107.992744\n",
      "Epoch 2987\n",
      "-------------------------------\n",
      "tensor(21.7428)\n",
      "tensor(10.4572)\n",
      "tensor(6.8334)\n",
      "tensor(0.3243)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.939865\n",
      "Epoch 2988\n",
      "-------------------------------\n",
      "tensor(24.8346)\n",
      "tensor(11.7507)\n",
      "tensor(10.9785)\n",
      "tensor(0.4863)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 107.901459\n",
      "Epoch 2989\n",
      "-------------------------------\n",
      "tensor(19.3565)\n",
      "tensor(9.4716)\n",
      "tensor(7.1510)\n",
      "tensor(0.1363)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 107.843796\n",
      "Epoch 2990\n",
      "-------------------------------\n",
      "tensor(22.0549)\n",
      "tensor(10.0038)\n",
      "tensor(9.9108)\n",
      "tensor(0.2752)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 107.775909\n",
      "Epoch 2991\n",
      "-------------------------------\n",
      "tensor(26.9899)\n",
      "tensor(10.9498)\n",
      "tensor(6.1443)\n",
      "tensor(0.0130)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 107.734985\n",
      "Epoch 2992\n",
      "-------------------------------\n",
      "tensor(26.5007)\n",
      "tensor(12.1837)\n",
      "tensor(9.1401)\n",
      "tensor(0.4207)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 107.639931\n",
      "Epoch 2993\n",
      "-------------------------------\n",
      "tensor(57.1752)\n",
      "tensor(29.4894)\n",
      "tensor(6.5590)\n",
      "tensor(0.1606)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.795280\n",
      "Epoch 2994\n",
      "-------------------------------\n",
      "tensor(55.5961)\n",
      "tensor(28.8471)\n",
      "tensor(6.5146)\n",
      "tensor(0.2878)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 107.587669\n",
      "Epoch 2995\n",
      "-------------------------------\n",
      "tensor(31.6388)\n",
      "tensor(13.1242)\n",
      "tensor(9.0331)\n",
      "tensor(0.4286)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 107.551262\n",
      "Epoch 2996\n",
      "-------------------------------\n",
      "tensor(23.4266)\n",
      "tensor(13.0655)\n",
      "tensor(8.3493)\n",
      "tensor(0.4180)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 107.536591\n",
      "Epoch 2997\n",
      "-------------------------------\n",
      "tensor(26.9932)\n",
      "tensor(13.7839)\n",
      "tensor(10.6479)\n",
      "tensor(0.2589)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 107.505554\n",
      "Epoch 2998\n",
      "-------------------------------\n",
      "tensor(26.0233)\n",
      "tensor(12.7972)\n",
      "tensor(8.5499)\n",
      "tensor(0.0896)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 107.471832\n",
      "Epoch 2999\n",
      "-------------------------------\n",
      "tensor(21.7499)\n",
      "tensor(10.3915)\n",
      "tensor(6.5471)\n",
      "tensor(0.0068)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 107.451805\n",
      "Epoch 3000\n",
      "-------------------------------\n",
      "tensor(21.3600)\n",
      "tensor(10.1628)\n",
      "tensor(6.1702)\n",
      "tensor(0.0061)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 107.433998\n",
      "Epoch 3001\n",
      "-------------------------------\n",
      "tensor(18.6356)\n",
      "tensor(9.6377)\n",
      "tensor(6.2328)\n",
      "tensor(0.0103)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 107.417900\n",
      "Epoch 3002\n",
      "-------------------------------\n",
      "tensor(18.4028)\n",
      "tensor(9.6570)\n",
      "tensor(6.1998)\n",
      "tensor(0.0388)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 107.404442\n",
      "Epoch 3003\n",
      "-------------------------------\n",
      "tensor(26.1787)\n",
      "tensor(11.1664)\n",
      "tensor(6.0092)\n",
      "tensor(0.0838)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 107.398491\n",
      "Epoch 3004\n",
      "-------------------------------\n",
      "tensor(25.8253)\n",
      "tensor(11.2076)\n",
      "tensor(5.9103)\n",
      "tensor(0.1370)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 107.387939\n",
      "Epoch 3005\n",
      "-------------------------------\n",
      "tensor(27.0217)\n",
      "tensor(11.4278)\n",
      "tensor(6.9716)\n",
      "tensor(0.1422)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 107.363518\n",
      "Epoch 3006\n",
      "-------------------------------\n",
      "tensor(29.6733)\n",
      "tensor(13.0249)\n",
      "tensor(7.9759)\n",
      "tensor(0.0120)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 107.320251\n",
      "Epoch 3007\n",
      "-------------------------------\n",
      "tensor(27.9352)\n",
      "tensor(12.5596)\n",
      "tensor(7.4729)\n",
      "tensor(0.3458)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.314354\n",
      "Epoch 3008\n",
      "-------------------------------\n",
      "tensor(25.8260)\n",
      "tensor(13.6261)\n",
      "tensor(9.7915)\n",
      "tensor(0.6264)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 107.259819\n",
      "Epoch 3009\n",
      "-------------------------------\n",
      "tensor(15.0634)\n",
      "tensor(9.8679)\n",
      "tensor(8.3521)\n",
      "tensor(0.3861)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 107.183220\n",
      "Epoch 3010\n",
      "-------------------------------\n",
      "tensor(27.1344)\n",
      "tensor(11.1251)\n",
      "tensor(7.5131)\n",
      "tensor(0.2108)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 107.136795\n",
      "Epoch 3011\n",
      "-------------------------------\n",
      "tensor(24.0219)\n",
      "tensor(10.8650)\n",
      "tensor(8.2622)\n",
      "tensor(0.2430)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 107.057358\n",
      "Epoch 3012\n",
      "-------------------------------\n",
      "tensor(22.3136)\n",
      "tensor(9.6553)\n",
      "tensor(7.0610)\n",
      "tensor(0.2366)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 106.971497\n",
      "Epoch 3013\n",
      "-------------------------------\n",
      "tensor(57.3955)\n",
      "tensor(30.6416)\n",
      "tensor(7.4787)\n",
      "tensor(0.2256)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.012474\n",
      "Epoch 3014\n",
      "-------------------------------\n",
      "tensor(24.3757)\n",
      "tensor(11.0683)\n",
      "tensor(6.7150)\n",
      "tensor(0.2191)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 106.855942\n",
      "Epoch 3015\n",
      "-------------------------------\n",
      "tensor(24.8711)\n",
      "tensor(13.8334)\n",
      "tensor(9.3247)\n",
      "tensor(0.5746)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 106.830597\n",
      "Epoch 3016\n",
      "-------------------------------\n",
      "tensor(25.7499)\n",
      "tensor(12.8342)\n",
      "tensor(9.3046)\n",
      "tensor(0.3990)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 106.788406\n",
      "Epoch 3017\n",
      "-------------------------------\n",
      "tensor(28.9042)\n",
      "tensor(12.8158)\n",
      "tensor(6.3600)\n",
      "tensor(0.0082)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 106.725449\n",
      "Epoch 3018\n",
      "-------------------------------\n",
      "tensor(19.8103)\n",
      "tensor(9.1867)\n",
      "tensor(6.8352)\n",
      "tensor(0.2144)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 106.688858\n",
      "Epoch 3019\n",
      "-------------------------------\n",
      "tensor(19.8887)\n",
      "tensor(9.3123)\n",
      "tensor(6.7858)\n",
      "tensor(0.2326)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 106.671799\n",
      "Epoch 3020\n",
      "-------------------------------\n",
      "tensor(26.1005)\n",
      "tensor(10.8644)\n",
      "tensor(6.2592)\n",
      "tensor(0.1702)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 106.657654\n",
      "Epoch 3021\n",
      "-------------------------------\n",
      "tensor(23.4788)\n",
      "tensor(10.2166)\n",
      "tensor(5.8705)\n",
      "tensor(0.1056)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 106.641083\n",
      "Epoch 3022\n",
      "-------------------------------\n",
      "tensor(22.8845)\n",
      "tensor(10.0977)\n",
      "tensor(5.7395)\n",
      "tensor(0.0439)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 106.635666\n",
      "Epoch 3023\n",
      "-------------------------------\n",
      "tensor(22.9533)\n",
      "tensor(10.1435)\n",
      "tensor(5.9425)\n",
      "tensor(0.0226)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 106.628441\n",
      "Epoch 3024\n",
      "-------------------------------\n",
      "tensor(20.6845)\n",
      "tensor(10.8523)\n",
      "tensor(6.5082)\n",
      "tensor(0.0855)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 106.611198\n",
      "Epoch 3025\n",
      "-------------------------------\n",
      "tensor(15.5425)\n",
      "tensor(8.6782)\n",
      "tensor(6.7184)\n",
      "tensor(0.0920)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 106.580383\n",
      "Epoch 3026\n",
      "-------------------------------\n",
      "tensor(60.2299)\n",
      "tensor(30.8634)\n",
      "tensor(6.1619)\n",
      "tensor(0.0297)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 106.602806\n",
      "Epoch 3027\n",
      "-------------------------------\n",
      "tensor(35.3770)\n",
      "tensor(13.0908)\n",
      "tensor(9.1279)\n",
      "tensor(0.2660)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 106.561493\n",
      "Epoch 3028\n",
      "-------------------------------\n",
      "tensor(25.8083)\n",
      "tensor(11.1753)\n",
      "tensor(7.1526)\n",
      "tensor(0.3121)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 106.501122\n",
      "Epoch 3029\n",
      "-------------------------------\n",
      "tensor(29.0086)\n",
      "tensor(13.0802)\n",
      "tensor(10.4073)\n",
      "tensor(0.0147)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 106.472496\n",
      "Epoch 3030\n",
      "-------------------------------\n",
      "tensor(22.3248)\n",
      "tensor(11.3359)\n",
      "tensor(7.6573)\n",
      "tensor(0.2584)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 106.452324\n",
      "Epoch 3031\n",
      "-------------------------------\n",
      "tensor(27.3811)\n",
      "tensor(11.1359)\n",
      "tensor(9.3534)\n",
      "tensor(0.2025)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 106.416077\n",
      "Epoch 3032\n",
      "-------------------------------\n",
      "tensor(15.9642)\n",
      "tensor(9.6935)\n",
      "tensor(5.8877)\n",
      "tensor(0.1611)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 106.333099\n",
      "Epoch 3033\n",
      "-------------------------------\n",
      "tensor(22.7035)\n",
      "tensor(10.2734)\n",
      "tensor(8.5449)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 106.261406\n",
      "Epoch 3034\n",
      "-------------------------------\n",
      "tensor(26.8584)\n",
      "tensor(11.4449)\n",
      "tensor(6.2898)\n",
      "tensor(0.2085)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 106.183304\n",
      "Epoch 3035\n",
      "-------------------------------\n",
      "tensor(19.7331)\n",
      "tensor(11.0277)\n",
      "tensor(7.3464)\n",
      "tensor(0.2389)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 106.106010\n",
      "Epoch 3036\n",
      "-------------------------------\n",
      "tensor(32.3437)\n",
      "tensor(12.4814)\n",
      "tensor(6.3419)\n",
      "tensor(0.0910)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 106.046844\n",
      "Epoch 3037\n",
      "-------------------------------\n",
      "tensor(61.0028)\n",
      "tensor(30.6296)\n",
      "tensor(7.3967)\n",
      "tensor(0.0410)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 106.090981\n",
      "Epoch 3038\n",
      "-------------------------------\n",
      "tensor(19.9528)\n",
      "tensor(9.6035)\n",
      "tensor(5.9228)\n",
      "tensor(0.0947)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 105.993523\n",
      "Epoch 3039\n",
      "-------------------------------\n",
      "tensor(20.2083)\n",
      "tensor(9.6048)\n",
      "tensor(6.3734)\n",
      "tensor(0.1283)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 105.980247\n",
      "Epoch 3040\n",
      "-------------------------------\n",
      "tensor(20.4962)\n",
      "tensor(9.6122)\n",
      "tensor(6.5980)\n",
      "tensor(0.1151)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 105.963280\n",
      "Epoch 3041\n",
      "-------------------------------\n",
      "tensor(20.4285)\n",
      "tensor(9.6233)\n",
      "tensor(6.3527)\n",
      "tensor(0.0828)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 105.946793\n",
      "Epoch 3042\n",
      "-------------------------------\n",
      "tensor(19.0243)\n",
      "tensor(9.4300)\n",
      "tensor(5.9679)\n",
      "tensor(0.0399)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 105.938431\n",
      "Epoch 3043\n",
      "-------------------------------\n",
      "tensor(25.7214)\n",
      "tensor(10.9709)\n",
      "tensor(5.7235)\n",
      "tensor(0.0199)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 105.927299\n",
      "Epoch 3044\n",
      "-------------------------------\n",
      "tensor(28.7110)\n",
      "tensor(11.8924)\n",
      "tensor(6.3155)\n",
      "tensor(0.1001)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 105.912888\n",
      "Epoch 3045\n",
      "-------------------------------\n",
      "tensor(24.1487)\n",
      "tensor(12.0992)\n",
      "tensor(8.1950)\n",
      "tensor(0.1617)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 105.898743\n",
      "Epoch 3046\n",
      "-------------------------------\n",
      "tensor(28.1559)\n",
      "tensor(12.9123)\n",
      "tensor(7.9876)\n",
      "tensor(0.1171)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 105.878860\n",
      "Epoch 3047\n",
      "-------------------------------\n",
      "tensor(24.3576)\n",
      "tensor(10.6219)\n",
      "tensor(5.9758)\n",
      "tensor(0.0661)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 105.865967\n",
      "Epoch 3048\n",
      "-------------------------------\n",
      "tensor(56.6696)\n",
      "tensor(30.9973)\n",
      "tensor(7.7429)\n",
      "tensor(0.2162)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 105.909378\n",
      "Epoch 3049\n",
      "-------------------------------\n",
      "tensor(33.9501)\n",
      "tensor(11.8745)\n",
      "tensor(10.0603)\n",
      "tensor(0.0534)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 105.794586\n",
      "Epoch 3050\n",
      "-------------------------------\n",
      "tensor(26.3500)\n",
      "tensor(12.7783)\n",
      "tensor(10.7772)\n",
      "tensor(0.3719)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 105.723694\n",
      "Epoch 3051\n",
      "-------------------------------\n",
      "tensor(26.2884)\n",
      "tensor(12.2433)\n",
      "tensor(11.6986)\n",
      "tensor(0.2551)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 105.697052\n",
      "Epoch 3052\n",
      "-------------------------------\n",
      "tensor(23.2432)\n",
      "tensor(9.8951)\n",
      "tensor(10.8395)\n",
      "tensor(0.2619)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 105.644226\n",
      "Epoch 3053\n",
      "-------------------------------\n",
      "tensor(25.2540)\n",
      "tensor(10.7081)\n",
      "tensor(8.2099)\n",
      "tensor(0.0870)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 105.602692\n",
      "Epoch 3054\n",
      "-------------------------------\n",
      "tensor(28.5245)\n",
      "tensor(12.9993)\n",
      "tensor(11.3445)\n",
      "tensor(0.2577)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 105.557953\n",
      "Epoch 3055\n",
      "-------------------------------\n",
      "tensor(21.6189)\n",
      "tensor(11.2795)\n",
      "tensor(8.3264)\n",
      "tensor(0.0182)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 105.471214\n",
      "Epoch 3056\n",
      "-------------------------------\n",
      "tensor(21.2036)\n",
      "tensor(10.8088)\n",
      "tensor(9.4363)\n",
      "tensor(0.3860)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 105.405312\n",
      "Epoch 3057\n",
      "-------------------------------\n",
      "tensor(17.9099)\n",
      "tensor(9.9715)\n",
      "tensor(10.4547)\n",
      "tensor(0.3697)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 105.350121\n",
      "Epoch 3058\n",
      "-------------------------------\n",
      "tensor(27.8016)\n",
      "tensor(11.6272)\n",
      "tensor(6.5664)\n",
      "tensor(0.1349)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 105.326607\n",
      "Epoch 3059\n",
      "-------------------------------\n",
      "tensor(23.7120)\n",
      "tensor(10.6567)\n",
      "tensor(6.7586)\n",
      "tensor(0.0281)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 105.297859\n",
      "Epoch 3060\n",
      "-------------------------------\n",
      "tensor(25.4274)\n",
      "tensor(11.0542)\n",
      "tensor(7.8346)\n",
      "tensor(0.0769)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 105.272003\n",
      "Epoch 3061\n",
      "-------------------------------\n",
      "tensor(25.3276)\n",
      "tensor(11.0372)\n",
      "tensor(7.7339)\n",
      "tensor(0.0633)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 105.248550\n",
      "Epoch 3062\n",
      "-------------------------------\n",
      "tensor(24.8934)\n",
      "tensor(10.9378)\n",
      "tensor(7.0882)\n",
      "tensor(0.0238)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 105.231789\n",
      "Epoch 3063\n",
      "-------------------------------\n",
      "tensor(23.6652)\n",
      "tensor(10.6395)\n",
      "tensor(6.2167)\n",
      "tensor(0.0427)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 105.220245\n",
      "Epoch 3064\n",
      "-------------------------------\n",
      "tensor(30.7940)\n",
      "tensor(12.3356)\n",
      "tensor(6.1468)\n",
      "tensor(0.1444)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 105.207062\n",
      "Epoch 3065\n",
      "-------------------------------\n",
      "tensor(25.8912)\n",
      "tensor(10.9879)\n",
      "tensor(7.4008)\n",
      "tensor(0.2395)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 105.195129\n",
      "Epoch 3066\n",
      "-------------------------------\n",
      "tensor(57.4004)\n",
      "tensor(31.1510)\n",
      "tensor(7.0725)\n",
      "tensor(0.2199)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 105.232094\n",
      "Epoch 3067\n",
      "-------------------------------\n",
      "tensor(18.8669)\n",
      "tensor(9.5037)\n",
      "tensor(6.5679)\n",
      "tensor(0.0120)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 105.128738\n",
      "Epoch 3068\n",
      "-------------------------------\n",
      "tensor(18.1609)\n",
      "tensor(11.7957)\n",
      "tensor(7.3018)\n",
      "tensor(0.3952)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 105.113068\n",
      "Epoch 3069\n",
      "-------------------------------\n",
      "tensor(25.0810)\n",
      "tensor(14.1145)\n",
      "tensor(8.9727)\n",
      "tensor(0.4833)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 105.081596\n",
      "Epoch 3070\n",
      "-------------------------------\n",
      "tensor(28.6019)\n",
      "tensor(13.0170)\n",
      "tensor(6.3318)\n",
      "tensor(0.0367)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 105.006981\n",
      "Epoch 3071\n",
      "-------------------------------\n",
      "tensor(31.3894)\n",
      "tensor(13.9729)\n",
      "tensor(7.6031)\n",
      "tensor(0.3056)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 104.970909\n",
      "Epoch 3072\n",
      "-------------------------------\n",
      "tensor(28.2491)\n",
      "tensor(11.9268)\n",
      "tensor(7.2122)\n",
      "tensor(0.1443)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 104.890259\n",
      "Epoch 3073\n",
      "-------------------------------\n",
      "tensor(58.7332)\n",
      "tensor(30.6362)\n",
      "tensor(6.4406)\n",
      "tensor(0.0624)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 104.899521\n",
      "Epoch 3074\n",
      "-------------------------------\n",
      "tensor(27.2753)\n",
      "tensor(10.4612)\n",
      "tensor(13.6575)\n",
      "tensor(0.0016)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 104.795059\n",
      "Epoch 3075\n",
      "-------------------------------\n",
      "tensor(18.7743)\n",
      "tensor(10.0333)\n",
      "tensor(6.1666)\n",
      "tensor(0.0823)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 104.733597\n",
      "Epoch 3076\n",
      "-------------------------------\n",
      "tensor(21.5191)\n",
      "tensor(10.2985)\n",
      "tensor(10.6897)\n",
      "tensor(0.0891)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 104.693207\n",
      "Epoch 3077\n",
      "-------------------------------\n",
      "tensor(24.5818)\n",
      "tensor(11.2174)\n",
      "tensor(8.0303)\n",
      "tensor(0.0479)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 104.657669\n",
      "Epoch 3078\n",
      "-------------------------------\n",
      "tensor(29.2503)\n",
      "tensor(12.2720)\n",
      "tensor(6.3464)\n",
      "tensor(0.1433)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 104.636803\n",
      "Epoch 3079\n",
      "-------------------------------\n",
      "tensor(30.1771)\n",
      "tensor(12.2492)\n",
      "tensor(7.2362)\n",
      "tensor(0.1451)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 104.601028\n",
      "Epoch 3080\n",
      "-------------------------------\n",
      "tensor(27.8488)\n",
      "tensor(11.7231)\n",
      "tensor(6.7317)\n",
      "tensor(0.0972)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 104.573029\n",
      "Epoch 3081\n",
      "-------------------------------\n",
      "tensor(20.7926)\n",
      "tensor(10.3526)\n",
      "tensor(6.0549)\n",
      "tensor(0.0442)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 104.561569\n",
      "Epoch 3082\n",
      "-------------------------------\n",
      "tensor(20.6208)\n",
      "tensor(10.4134)\n",
      "tensor(5.8530)\n",
      "tensor(0.0038)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 104.552513\n",
      "Epoch 3083\n",
      "-------------------------------\n",
      "tensor(20.5682)\n",
      "tensor(10.4914)\n",
      "tensor(6.1837)\n",
      "tensor(0.0542)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 104.540604\n",
      "Epoch 3084\n",
      "-------------------------------\n",
      "tensor(21.0017)\n",
      "tensor(10.6261)\n",
      "tensor(7.0128)\n",
      "tensor(0.1014)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 104.528938\n",
      "Epoch 3085\n",
      "-------------------------------\n",
      "tensor(21.0561)\n",
      "tensor(10.5571)\n",
      "tensor(7.0978)\n",
      "tensor(0.0933)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 104.496048\n",
      "Epoch 3086\n",
      "-------------------------------\n",
      "tensor(23.2093)\n",
      "tensor(10.7873)\n",
      "tensor(5.9458)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 104.448288\n",
      "Epoch 3087\n",
      "-------------------------------\n",
      "tensor(33.0599)\n",
      "tensor(12.9934)\n",
      "tensor(6.9071)\n",
      "tensor(0.1820)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 104.421097\n",
      "Epoch 3088\n",
      "-------------------------------\n",
      "tensor(30.1069)\n",
      "tensor(12.5862)\n",
      "tensor(6.4763)\n",
      "tensor(0.2278)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 104.387215\n",
      "Epoch 3089\n",
      "-------------------------------\n",
      "tensor(63.2587)\n",
      "tensor(31.3780)\n",
      "tensor(8.9015)\n",
      "tensor(0.1553)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 104.488632\n",
      "Epoch 3090\n",
      "-------------------------------\n",
      "tensor(28.2920)\n",
      "tensor(11.6951)\n",
      "tensor(9.9924)\n",
      "tensor(0.0791)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 104.278366\n",
      "Epoch 3091\n",
      "-------------------------------\n",
      "tensor(19.7070)\n",
      "tensor(12.2482)\n",
      "tensor(6.7398)\n",
      "tensor(0.3136)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 104.241203\n",
      "Epoch 3092\n",
      "-------------------------------\n",
      "tensor(21.7780)\n",
      "tensor(13.9213)\n",
      "tensor(12.1821)\n",
      "tensor(0.4508)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 104.183357\n",
      "Epoch 3093\n",
      "-------------------------------\n",
      "tensor(24.0123)\n",
      "tensor(10.9635)\n",
      "tensor(6.3757)\n",
      "tensor(0.0284)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 104.099686\n",
      "Epoch 3094\n",
      "-------------------------------\n",
      "tensor(27.8552)\n",
      "tensor(11.3607)\n",
      "tensor(8.8506)\n",
      "tensor(0.2616)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 104.027466\n",
      "Epoch 3095\n",
      "-------------------------------\n",
      "tensor(25.8414)\n",
      "tensor(11.3129)\n",
      "tensor(6.3568)\n",
      "tensor(0.0262)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 103.980560\n",
      "Epoch 3096\n",
      "-------------------------------\n",
      "tensor(64.8080)\n",
      "tensor(31.4345)\n",
      "tensor(8.9027)\n",
      "tensor(0.0740)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 103.935547\n",
      "Epoch 3097\n",
      "-------------------------------\n",
      "tensor(27.9178)\n",
      "tensor(11.6877)\n",
      "tensor(6.7155)\n",
      "tensor(0.1227)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 103.888214\n",
      "Epoch 3098\n",
      "-------------------------------\n",
      "tensor(16.4598)\n",
      "tensor(9.3354)\n",
      "tensor(8.3630)\n",
      "tensor(0.2193)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 103.842567\n",
      "Epoch 3099\n",
      "-------------------------------\n",
      "tensor(17.8094)\n",
      "tensor(9.6211)\n",
      "tensor(7.2686)\n",
      "tensor(0.1701)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 103.829697\n",
      "Epoch 3100\n",
      "-------------------------------\n",
      "tensor(17.2064)\n",
      "tensor(9.6274)\n",
      "tensor(6.1556)\n",
      "tensor(0.0904)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 103.814400\n",
      "Epoch 3101\n",
      "-------------------------------\n",
      "tensor(17.1311)\n",
      "tensor(9.7173)\n",
      "tensor(5.9237)\n",
      "tensor(0.0285)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 103.800491\n",
      "Epoch 3102\n",
      "-------------------------------\n",
      "tensor(19.1833)\n",
      "tensor(10.1726)\n",
      "tensor(6.0903)\n",
      "tensor(0.0179)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 103.788696\n",
      "Epoch 3103\n",
      "-------------------------------\n",
      "tensor(21.9015)\n",
      "tensor(10.7741)\n",
      "tensor(6.4035)\n",
      "tensor(0.0590)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 103.775093\n",
      "Epoch 3104\n",
      "-------------------------------\n",
      "tensor(30.5100)\n",
      "tensor(12.6943)\n",
      "tensor(6.6803)\n",
      "tensor(0.0885)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 103.767708\n",
      "Epoch 3105\n",
      "-------------------------------\n",
      "tensor(26.3066)\n",
      "tensor(11.7026)\n",
      "tensor(6.6653)\n",
      "tensor(0.0574)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 103.747772\n",
      "Epoch 3106\n",
      "-------------------------------\n",
      "tensor(30.7876)\n",
      "tensor(12.7056)\n",
      "tensor(6.1560)\n",
      "tensor(0.0582)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 103.729340\n",
      "Epoch 3107\n",
      "-------------------------------\n",
      "tensor(25.4052)\n",
      "tensor(11.3785)\n",
      "tensor(6.5203)\n",
      "tensor(0.2044)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 103.689903\n",
      "Epoch 3108\n",
      "-------------------------------\n",
      "tensor(27.7346)\n",
      "tensor(11.8685)\n",
      "tensor(6.4931)\n",
      "tensor(0.1978)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 103.620369\n",
      "Epoch 3109\n",
      "-------------------------------\n",
      "tensor(61.6783)\n",
      "tensor(31.7801)\n",
      "tensor(6.4691)\n",
      "tensor(0.0071)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 103.693306\n",
      "Epoch 3110\n",
      "-------------------------------\n",
      "tensor(20.9907)\n",
      "tensor(11.0598)\n",
      "tensor(9.1698)\n",
      "tensor(0.1078)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 103.525085\n",
      "Epoch 3111\n",
      "-------------------------------\n",
      "tensor(18.8346)\n",
      "tensor(12.3701)\n",
      "tensor(6.6005)\n",
      "tensor(0.3009)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 103.478722\n",
      "Epoch 3112\n",
      "-------------------------------\n",
      "tensor(31.3740)\n",
      "tensor(14.0122)\n",
      "tensor(10.5341)\n",
      "tensor(0.2483)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 103.451897\n",
      "Epoch 3113\n",
      "-------------------------------\n",
      "tensor(24.8640)\n",
      "tensor(11.3677)\n",
      "tensor(7.8538)\n",
      "tensor(0.1816)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 103.362671\n",
      "Epoch 3114\n",
      "-------------------------------\n",
      "tensor(17.9210)\n",
      "tensor(9.5759)\n",
      "tensor(8.7195)\n",
      "tensor(0.2581)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 103.279533\n",
      "Epoch 3115\n",
      "-------------------------------\n",
      "tensor(19.7505)\n",
      "tensor(10.3032)\n",
      "tensor(7.0350)\n",
      "tensor(0.0902)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 103.205742\n",
      "Epoch 3116\n",
      "-------------------------------\n",
      "tensor(22.9877)\n",
      "tensor(11.0551)\n",
      "tensor(8.3253)\n",
      "tensor(0.2317)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 103.152054\n",
      "Epoch 3117\n",
      "-------------------------------\n",
      "tensor(30.8866)\n",
      "tensor(12.6025)\n",
      "tensor(7.7045)\n",
      "tensor(0.0076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 103.127052\n",
      "Epoch 3118\n",
      "-------------------------------\n",
      "tensor(27.1460)\n",
      "tensor(12.2840)\n",
      "tensor(7.0759)\n",
      "tensor(0.2874)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 103.092957\n",
      "Epoch 3119\n",
      "-------------------------------\n",
      "tensor(26.9234)\n",
      "tensor(12.7119)\n",
      "tensor(7.7396)\n",
      "tensor(0.4337)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 103.054565\n",
      "Epoch 3120\n",
      "-------------------------------\n",
      "tensor(23.2200)\n",
      "tensor(11.7058)\n",
      "tensor(7.9319)\n",
      "tensor(0.4433)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 103.019341\n",
      "Epoch 3121\n",
      "-------------------------------\n",
      "tensor(23.2046)\n",
      "tensor(11.4425)\n",
      "tensor(7.5926)\n",
      "tensor(0.3906)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 102.992424\n",
      "Epoch 3122\n",
      "-------------------------------\n",
      "tensor(59.2039)\n",
      "tensor(32.6918)\n",
      "tensor(7.0028)\n",
      "tensor(0.3079)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 103.007164\n",
      "Epoch 3123\n",
      "-------------------------------\n",
      "tensor(26.1165)\n",
      "tensor(11.4753)\n",
      "tensor(6.5044)\n",
      "tensor(0.1758)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 102.989861\n",
      "Epoch 3124\n",
      "-------------------------------\n",
      "tensor(27.8277)\n",
      "tensor(11.7041)\n",
      "tensor(6.1902)\n",
      "tensor(0.0284)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 102.986046\n",
      "Epoch 3125\n",
      "-------------------------------\n",
      "tensor(25.6008)\n",
      "tensor(11.9848)\n",
      "tensor(6.8633)\n",
      "tensor(0.2803)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 102.967232\n",
      "Epoch 3126\n",
      "-------------------------------\n",
      "tensor(24.5579)\n",
      "tensor(13.0143)\n",
      "tensor(8.6437)\n",
      "tensor(0.4430)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 102.969208\n",
      "Epoch 3127\n",
      "-------------------------------\n",
      "tensor(22.9665)\n",
      "tensor(12.3814)\n",
      "tensor(8.2625)\n",
      "tensor(0.2883)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 102.951591\n",
      "Epoch 3128\n",
      "-------------------------------\n",
      "tensor(25.6960)\n",
      "tensor(11.3312)\n",
      "tensor(7.5192)\n",
      "tensor(0.1185)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 102.893112\n",
      "Epoch 3129\n",
      "-------------------------------\n",
      "tensor(56.7400)\n",
      "tensor(31.1795)\n",
      "tensor(8.1896)\n",
      "tensor(0.2210)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 102.823235\n",
      "Epoch 3130\n",
      "-------------------------------\n",
      "tensor(23.5722)\n",
      "tensor(11.7403)\n",
      "tensor(10.4793)\n",
      "tensor(0.1592)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 102.777153\n",
      "Epoch 3131\n",
      "-------------------------------\n",
      "tensor(32.8956)\n",
      "tensor(16.3611)\n",
      "tensor(11.6345)\n",
      "tensor(0.7042)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 102.783394\n",
      "Epoch 3132\n",
      "-------------------------------\n",
      "tensor(34.9926)\n",
      "tensor(15.5697)\n",
      "tensor(13.7598)\n",
      "tensor(0.1377)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 102.719482\n",
      "Epoch 3133\n",
      "-------------------------------\n",
      "tensor(21.1321)\n",
      "tensor(12.5542)\n",
      "tensor(12.4973)\n",
      "tensor(0.7865)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 102.622734\n",
      "Epoch 3134\n",
      "-------------------------------\n",
      "tensor(22.9497)\n",
      "tensor(11.5836)\n",
      "tensor(12.5051)\n",
      "tensor(0.4496)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 102.574898\n",
      "Epoch 3135\n",
      "-------------------------------\n",
      "tensor(21.7548)\n",
      "tensor(11.8996)\n",
      "tensor(9.5460)\n",
      "tensor(0.4177)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 102.501633\n",
      "Epoch 3136\n",
      "-------------------------------\n",
      "tensor(19.7812)\n",
      "tensor(11.8079)\n",
      "tensor(12.5956)\n",
      "tensor(0.4976)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 102.446091\n",
      "Epoch 3137\n",
      "-------------------------------\n",
      "tensor(28.7447)\n",
      "tensor(12.5233)\n",
      "tensor(6.5389)\n",
      "tensor(0.0382)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 102.399460\n",
      "Epoch 3138\n",
      "-------------------------------\n",
      "tensor(25.0403)\n",
      "tensor(12.7028)\n",
      "tensor(8.8592)\n",
      "tensor(0.4817)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 102.347763\n",
      "Epoch 3139\n",
      "-------------------------------\n",
      "tensor(22.7109)\n",
      "tensor(12.4687)\n",
      "tensor(9.9851)\n",
      "tensor(0.5863)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 102.317719\n",
      "Epoch 3140\n",
      "-------------------------------\n",
      "tensor(22.5093)\n",
      "tensor(12.0014)\n",
      "tensor(8.8408)\n",
      "tensor(0.5010)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 102.293503\n",
      "Epoch 3141\n",
      "-------------------------------\n",
      "tensor(27.3336)\n",
      "tensor(12.7455)\n",
      "tensor(7.5132)\n",
      "tensor(0.3712)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 102.273331\n",
      "Epoch 3142\n",
      "-------------------------------\n",
      "tensor(23.3905)\n",
      "tensor(11.4306)\n",
      "tensor(6.6161)\n",
      "tensor(0.2293)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 102.264778\n",
      "Epoch 3143\n",
      "-------------------------------\n",
      "tensor(27.3428)\n",
      "tensor(12.2028)\n",
      "tensor(6.5232)\n",
      "tensor(0.0484)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 102.250839\n",
      "Epoch 3144\n",
      "-------------------------------\n",
      "tensor(23.7527)\n",
      "tensor(11.6969)\n",
      "tensor(7.9032)\n",
      "tensor(0.1878)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 102.240128\n",
      "Epoch 3145\n",
      "-------------------------------\n",
      "tensor(26.4498)\n",
      "tensor(12.8331)\n",
      "tensor(9.1438)\n",
      "tensor(0.3870)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 102.214478\n",
      "Epoch 3146\n",
      "-------------------------------\n",
      "tensor(25.8147)\n",
      "tensor(12.5538)\n",
      "tensor(7.6592)\n",
      "tensor(0.3515)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 102.173492\n",
      "Epoch 3147\n",
      "-------------------------------\n",
      "tensor(16.1336)\n",
      "tensor(9.6122)\n",
      "tensor(6.3017)\n",
      "tensor(0.0020)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 102.117912\n",
      "Epoch 3148\n",
      "-------------------------------\n",
      "tensor(59.8006)\n",
      "tensor(33.4310)\n",
      "tensor(8.5618)\n",
      "tensor(0.3626)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 102.258347\n",
      "Epoch 3149\n",
      "-------------------------------\n",
      "tensor(63.0954)\n",
      "tensor(33.5940)\n",
      "tensor(9.2183)\n",
      "tensor(0.2231)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 102.113426\n",
      "Epoch 3150\n",
      "-------------------------------\n",
      "tensor(23.2409)\n",
      "tensor(13.5014)\n",
      "tensor(7.4412)\n",
      "tensor(0.3798)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 102.009216\n",
      "Epoch 3151\n",
      "-------------------------------\n",
      "tensor(19.5482)\n",
      "tensor(17.1571)\n",
      "tensor(13.7633)\n",
      "tensor(0.7840)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 101.996727\n",
      "Epoch 3152\n",
      "-------------------------------\n",
      "tensor(32.2450)\n",
      "tensor(14.7224)\n",
      "tensor(7.1694)\n",
      "tensor(0.2931)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 102.012451\n",
      "Epoch 3153\n",
      "-------------------------------\n",
      "tensor(27.2669)\n",
      "tensor(12.5675)\n",
      "tensor(8.4100)\n",
      "tensor(0.2988)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 101.935951\n",
      "Epoch 3154\n",
      "-------------------------------\n",
      "tensor(24.1505)\n",
      "tensor(12.2351)\n",
      "tensor(7.2578)\n",
      "tensor(0.2419)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 101.839287\n",
      "Epoch 3155\n",
      "-------------------------------\n",
      "tensor(30.2781)\n",
      "tensor(14.2272)\n",
      "tensor(7.1619)\n",
      "tensor(0.0114)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 101.809799\n",
      "Epoch 3156\n",
      "-------------------------------\n",
      "tensor(20.5115)\n",
      "tensor(11.4026)\n",
      "tensor(7.1648)\n",
      "tensor(0.0206)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 101.766235\n",
      "Epoch 3157\n",
      "-------------------------------\n",
      "tensor(20.5649)\n",
      "tensor(11.7057)\n",
      "tensor(7.5730)\n",
      "tensor(0.1923)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 101.731995\n",
      "Epoch 3158\n",
      "-------------------------------\n",
      "tensor(21.7270)\n",
      "tensor(12.1391)\n",
      "tensor(7.6053)\n",
      "tensor(0.2977)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 101.705879\n",
      "Epoch 3159\n",
      "-------------------------------\n",
      "tensor(20.7892)\n",
      "tensor(11.8955)\n",
      "tensor(7.1485)\n",
      "tensor(0.3108)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 101.676201\n",
      "Epoch 3160\n",
      "-------------------------------\n",
      "tensor(27.5286)\n",
      "tensor(13.1884)\n",
      "tensor(6.8324)\n",
      "tensor(0.2801)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 101.647751\n",
      "Epoch 3161\n",
      "-------------------------------\n",
      "tensor(20.7849)\n",
      "tensor(11.5507)\n",
      "tensor(6.6705)\n",
      "tensor(0.2423)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 101.626389\n",
      "Epoch 3162\n",
      "-------------------------------\n",
      "tensor(16.6608)\n",
      "tensor(10.6465)\n",
      "tensor(6.5585)\n",
      "tensor(0.2012)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 101.608452\n",
      "Epoch 3163\n",
      "-------------------------------\n",
      "tensor(23.7795)\n",
      "tensor(11.9499)\n",
      "tensor(6.3830)\n",
      "tensor(0.1436)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 101.591423\n",
      "Epoch 3164\n",
      "-------------------------------\n",
      "tensor(22.7504)\n",
      "tensor(11.6484)\n",
      "tensor(6.1831)\n",
      "tensor(0.0594)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 101.568382\n",
      "Epoch 3165\n",
      "-------------------------------\n",
      "tensor(25.3817)\n",
      "tensor(13.0846)\n",
      "tensor(6.0864)\n",
      "tensor(0.0417)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 101.534637\n",
      "Epoch 3166\n",
      "-------------------------------\n",
      "tensor(28.4073)\n",
      "tensor(12.8295)\n",
      "tensor(6.0995)\n",
      "tensor(0.1141)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 101.473885\n",
      "Epoch 3167\n",
      "-------------------------------\n",
      "tensor(26.0733)\n",
      "tensor(12.6170)\n",
      "tensor(6.6024)\n",
      "tensor(0.1209)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 101.420700\n",
      "Epoch 3168\n",
      "-------------------------------\n",
      "tensor(24.4790)\n",
      "tensor(12.1633)\n",
      "tensor(7.8734)\n",
      "tensor(0.0558)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 101.383911\n",
      "Epoch 3169\n",
      "-------------------------------\n",
      "tensor(30.2384)\n",
      "tensor(13.8769)\n",
      "tensor(6.7950)\n",
      "tensor(0.0196)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 101.294044\n",
      "Epoch 3170\n",
      "-------------------------------\n",
      "tensor(64.0799)\n",
      "tensor(32.5624)\n",
      "tensor(6.3538)\n",
      "tensor(0.0077)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 101.329300\n",
      "Epoch 3171\n",
      "-------------------------------\n",
      "tensor(58.9684)\n",
      "tensor(32.2865)\n",
      "tensor(8.4173)\n",
      "tensor(0.0891)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 101.213959\n",
      "Epoch 3172\n",
      "-------------------------------\n",
      "tensor(29.0048)\n",
      "tensor(12.9127)\n",
      "tensor(9.5022)\n",
      "tensor(0.2740)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 101.130722\n",
      "Epoch 3173\n",
      "-------------------------------\n",
      "tensor(24.4163)\n",
      "tensor(14.7051)\n",
      "tensor(13.8603)\n",
      "tensor(0.4506)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 101.104309\n",
      "Epoch 3174\n",
      "-------------------------------\n",
      "tensor(27.1772)\n",
      "tensor(14.0960)\n",
      "tensor(8.8532)\n",
      "tensor(0.0582)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 101.074036\n",
      "Epoch 3175\n",
      "-------------------------------\n",
      "tensor(28.7421)\n",
      "tensor(13.5111)\n",
      "tensor(9.6092)\n",
      "tensor(0.3060)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 101.015862\n",
      "Epoch 3176\n",
      "-------------------------------\n",
      "tensor(21.1734)\n",
      "tensor(11.3382)\n",
      "tensor(7.5321)\n",
      "tensor(0.1867)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 100.950203\n",
      "Epoch 3177\n",
      "-------------------------------\n",
      "tensor(24.4966)\n",
      "tensor(12.0621)\n",
      "tensor(7.2062)\n",
      "tensor(0.0375)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 100.920700\n",
      "Epoch 3178\n",
      "-------------------------------\n",
      "tensor(22.3035)\n",
      "tensor(11.5064)\n",
      "tensor(8.4057)\n",
      "tensor(0.0526)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 100.889320\n",
      "Epoch 3179\n",
      "-------------------------------\n",
      "tensor(21.3339)\n",
      "tensor(11.4036)\n",
      "tensor(7.1762)\n",
      "tensor(0.0620)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 100.852325\n",
      "Epoch 3180\n",
      "-------------------------------\n",
      "tensor(19.8049)\n",
      "tensor(11.2974)\n",
      "tensor(6.6734)\n",
      "tensor(0.1691)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 100.832718\n",
      "Epoch 3181\n",
      "-------------------------------\n",
      "tensor(19.5691)\n",
      "tensor(11.3731)\n",
      "tensor(6.9756)\n",
      "tensor(0.2344)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 100.815765\n",
      "Epoch 3182\n",
      "-------------------------------\n",
      "tensor(19.5612)\n",
      "tensor(11.4177)\n",
      "tensor(7.3798)\n",
      "tensor(0.2707)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 100.797768\n",
      "Epoch 3183\n",
      "-------------------------------\n",
      "tensor(19.3841)\n",
      "tensor(11.3565)\n",
      "tensor(7.6239)\n",
      "tensor(0.2843)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 100.775238\n",
      "Epoch 3184\n",
      "-------------------------------\n",
      "tensor(19.2533)\n",
      "tensor(11.1799)\n",
      "tensor(7.3069)\n",
      "tensor(0.2532)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 100.743423\n",
      "Epoch 3185\n",
      "-------------------------------\n",
      "tensor(32.8457)\n",
      "tensor(13.8662)\n",
      "tensor(6.3389)\n",
      "tensor(0.1337)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 100.724815\n",
      "Epoch 3186\n",
      "-------------------------------\n",
      "tensor(32.3660)\n",
      "tensor(13.9167)\n",
      "tensor(7.9202)\n",
      "tensor(0.0361)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 100.712128\n",
      "Epoch 3187\n",
      "-------------------------------\n",
      "tensor(31.9773)\n",
      "tensor(14.1110)\n",
      "tensor(10.0267)\n",
      "tensor(0.0834)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 100.644135\n",
      "Epoch 3188\n",
      "-------------------------------\n",
      "tensor(28.7567)\n",
      "tensor(12.9141)\n",
      "tensor(7.3634)\n",
      "tensor(0.0582)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 100.534607\n",
      "Epoch 3189\n",
      "-------------------------------\n",
      "tensor(22.2616)\n",
      "tensor(10.7127)\n",
      "tensor(7.4548)\n",
      "tensor(0.0510)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 100.483208\n",
      "Epoch 3190\n",
      "-------------------------------\n",
      "tensor(61.4431)\n",
      "tensor(32.3591)\n",
      "tensor(6.5023)\n",
      "tensor(0.2660)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 100.518349\n",
      "Epoch 3191\n",
      "-------------------------------\n",
      "tensor(59.2966)\n",
      "tensor(32.3649)\n",
      "tensor(7.9144)\n",
      "tensor(0.3224)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 100.370560\n",
      "Epoch 3192\n",
      "-------------------------------\n",
      "tensor(24.3190)\n",
      "tensor(12.5574)\n",
      "tensor(9.2942)\n",
      "tensor(0.1270)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 100.344566\n",
      "Epoch 3193\n",
      "-------------------------------\n",
      "tensor(28.9790)\n",
      "tensor(14.7118)\n",
      "tensor(11.2445)\n",
      "tensor(0.2438)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 100.290321\n",
      "Epoch 3194\n",
      "-------------------------------\n",
      "tensor(24.1496)\n",
      "tensor(12.8000)\n",
      "tensor(10.9841)\n",
      "tensor(0.1215)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 100.251869\n",
      "Epoch 3195\n",
      "-------------------------------\n",
      "tensor(31.6188)\n",
      "tensor(13.2436)\n",
      "tensor(8.5185)\n",
      "tensor(0.1444)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 100.194878\n",
      "Epoch 3196\n",
      "-------------------------------\n",
      "tensor(20.6821)\n",
      "tensor(11.4486)\n",
      "tensor(8.3824)\n",
      "tensor(0.1564)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 100.124146\n",
      "Epoch 3197\n",
      "-------------------------------\n",
      "tensor(26.8656)\n",
      "tensor(12.7960)\n",
      "tensor(6.8596)\n",
      "tensor(0.0566)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 100.097244\n",
      "Epoch 3198\n",
      "-------------------------------\n",
      "tensor(31.0410)\n",
      "tensor(13.7531)\n",
      "tensor(8.4769)\n",
      "tensor(0.0745)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 100.083153\n",
      "Epoch 3199\n",
      "-------------------------------\n",
      "tensor(30.6836)\n",
      "tensor(13.9045)\n",
      "tensor(8.0624)\n",
      "tensor(0.1793)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 100.043732\n",
      "Epoch 3200\n",
      "-------------------------------\n",
      "tensor(17.1964)\n",
      "tensor(11.0736)\n",
      "tensor(7.3358)\n",
      "tensor(0.2646)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 100.016685\n",
      "Epoch 3201\n",
      "-------------------------------\n",
      "tensor(15.9239)\n",
      "tensor(11.0598)\n",
      "tensor(7.0237)\n",
      "tensor(0.3037)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 100.003731\n",
      "Epoch 3202\n",
      "-------------------------------\n",
      "tensor(19.6222)\n",
      "tensor(11.7594)\n",
      "tensor(7.1739)\n",
      "tensor(0.3133)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 99.991272\n",
      "Epoch 3203\n",
      "-------------------------------\n",
      "tensor(27.2764)\n",
      "tensor(13.1329)\n",
      "tensor(7.6121)\n",
      "tensor(0.2910)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 99.980698\n",
      "Epoch 3204\n",
      "-------------------------------\n",
      "tensor(24.6386)\n",
      "tensor(12.2356)\n",
      "tensor(7.7284)\n",
      "tensor(0.2116)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 99.958992\n",
      "Epoch 3205\n",
      "-------------------------------\n",
      "tensor(28.6061)\n",
      "tensor(12.8155)\n",
      "tensor(6.7171)\n",
      "tensor(0.0440)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 99.914780\n",
      "Epoch 3206\n",
      "-------------------------------\n",
      "tensor(31.6150)\n",
      "tensor(13.9800)\n",
      "tensor(7.2512)\n",
      "tensor(0.1741)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 99.880974\n",
      "Epoch 3207\n",
      "-------------------------------\n",
      "tensor(23.0231)\n",
      "tensor(12.7573)\n",
      "tensor(11.1583)\n",
      "tensor(0.2273)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 99.837997\n",
      "Epoch 3208\n",
      "-------------------------------\n",
      "tensor(28.8584)\n",
      "tensor(13.1938)\n",
      "tensor(7.2756)\n",
      "tensor(0.0165)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 99.771362\n",
      "Epoch 3209\n",
      "-------------------------------\n",
      "tensor(25.8969)\n",
      "tensor(11.1354)\n",
      "tensor(8.7512)\n",
      "tensor(0.1856)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 99.685486\n",
      "Epoch 3210\n",
      "-------------------------------\n",
      "tensor(60.0796)\n",
      "tensor(32.4213)\n",
      "tensor(6.3065)\n",
      "tensor(0.1507)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 99.607376\n",
      "Epoch 3211\n",
      "-------------------------------\n",
      "tensor(21.6586)\n",
      "tensor(12.6733)\n",
      "tensor(8.5983)\n",
      "tensor(0.3068)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 99.512238\n",
      "Epoch 3212\n",
      "-------------------------------\n",
      "tensor(29.1586)\n",
      "tensor(14.3556)\n",
      "tensor(7.5070)\n",
      "tensor(0.2703)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 99.431526\n",
      "Epoch 3213\n",
      "-------------------------------\n",
      "tensor(33.4103)\n",
      "tensor(14.1588)\n",
      "tensor(11.6892)\n",
      "tensor(0.1569)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 99.367516\n",
      "Epoch 3214\n",
      "-------------------------------\n",
      "tensor(64.1001)\n",
      "tensor(35.2297)\n",
      "tensor(9.1827)\n",
      "tensor(0.5389)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 99.325783\n",
      "Epoch 3215\n",
      "-------------------------------\n",
      "tensor(31.7271)\n",
      "tensor(11.3856)\n",
      "tensor(17.0763)\n",
      "tensor(0.3323)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 99.260208\n",
      "Epoch 3216\n",
      "-------------------------------\n",
      "tensor(27.0170)\n",
      "tensor(12.9602)\n",
      "tensor(8.1759)\n",
      "tensor(0.3612)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 99.223953\n",
      "Epoch 3217\n",
      "-------------------------------\n",
      "tensor(25.9827)\n",
      "tensor(14.6583)\n",
      "tensor(14.8773)\n",
      "tensor(0.5815)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 99.192642\n",
      "Epoch 3218\n",
      "-------------------------------\n",
      "tensor(22.3274)\n",
      "tensor(12.3622)\n",
      "tensor(13.1138)\n",
      "tensor(0.2786)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 99.162674\n",
      "Epoch 3219\n",
      "-------------------------------\n",
      "tensor(17.1673)\n",
      "tensor(10.4991)\n",
      "tensor(7.3286)\n",
      "tensor(0.0932)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 99.128777\n",
      "Epoch 3220\n",
      "-------------------------------\n",
      "tensor(16.1687)\n",
      "tensor(10.1953)\n",
      "tensor(7.7098)\n",
      "tensor(0.3010)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 99.105728\n",
      "Epoch 3221\n",
      "-------------------------------\n",
      "tensor(27.4105)\n",
      "tensor(12.4568)\n",
      "tensor(9.3485)\n",
      "tensor(0.3647)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 99.093758\n",
      "Epoch 3222\n",
      "-------------------------------\n",
      "tensor(27.7810)\n",
      "tensor(12.4501)\n",
      "tensor(9.6348)\n",
      "tensor(0.3520)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 99.082291\n",
      "Epoch 3223\n",
      "-------------------------------\n",
      "tensor(24.8814)\n",
      "tensor(11.7750)\n",
      "tensor(8.4803)\n",
      "tensor(0.2693)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 99.058052\n",
      "Epoch 3224\n",
      "-------------------------------\n",
      "tensor(30.3994)\n",
      "tensor(13.1057)\n",
      "tensor(6.3026)\n",
      "tensor(0.0674)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 99.055481\n",
      "Epoch 3225\n",
      "-------------------------------\n",
      "tensor(30.8514)\n",
      "tensor(13.6190)\n",
      "tensor(9.7469)\n",
      "tensor(0.2091)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 99.047600\n",
      "Epoch 3226\n",
      "-------------------------------\n",
      "tensor(30.9881)\n",
      "tensor(13.7194)\n",
      "tensor(13.7944)\n",
      "tensor(0.3123)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 98.998894\n",
      "Epoch 3227\n",
      "-------------------------------\n",
      "tensor(22.7592)\n",
      "tensor(11.3531)\n",
      "tensor(7.8410)\n",
      "tensor(0.0292)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 98.940773\n",
      "Epoch 3228\n",
      "-------------------------------\n",
      "tensor(25.7079)\n",
      "tensor(12.0607)\n",
      "tensor(13.0803)\n",
      "tensor(0.5546)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 98.884552\n",
      "Epoch 3229\n",
      "-------------------------------\n",
      "tensor(62.2411)\n",
      "tensor(36.2337)\n",
      "tensor(10.0673)\n",
      "tensor(0.4365)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 98.950943\n",
      "Epoch 3230\n",
      "-------------------------------\n",
      "tensor(24.3215)\n",
      "tensor(13.2137)\n",
      "tensor(6.8282)\n",
      "tensor(0.2554)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 98.764526\n",
      "Epoch 3231\n",
      "-------------------------------\n",
      "tensor(24.6993)\n",
      "tensor(14.8738)\n",
      "tensor(12.1642)\n",
      "tensor(0.5230)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 98.703705\n",
      "Epoch 3232\n",
      "-------------------------------\n",
      "tensor(22.5938)\n",
      "tensor(11.5174)\n",
      "tensor(7.0824)\n",
      "tensor(0.0261)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 98.616608\n",
      "Epoch 3233\n",
      "-------------------------------\n",
      "tensor(32.3298)\n",
      "tensor(12.9648)\n",
      "tensor(8.8408)\n",
      "tensor(0.2409)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 98.581741\n",
      "Epoch 3234\n",
      "-------------------------------\n",
      "tensor(24.8664)\n",
      "tensor(12.3467)\n",
      "tensor(8.7037)\n",
      "tensor(0.0887)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 98.483444\n",
      "Epoch 3235\n",
      "-------------------------------\n",
      "tensor(66.3526)\n",
      "tensor(33.0564)\n",
      "tensor(8.5878)\n",
      "tensor(0.1491)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 98.374352\n",
      "Epoch 3236\n",
      "-------------------------------\n",
      "tensor(28.3740)\n",
      "tensor(11.5152)\n",
      "tensor(11.9669)\n",
      "tensor(0.1463)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 98.357025\n",
      "Epoch 3237\n",
      "-------------------------------\n",
      "tensor(32.1867)\n",
      "tensor(12.7612)\n",
      "tensor(9.9868)\n",
      "tensor(0.1590)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 98.335838\n",
      "Epoch 3238\n",
      "-------------------------------\n",
      "tensor(22.1734)\n",
      "tensor(11.6820)\n",
      "tensor(6.8084)\n",
      "tensor(0.0339)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 98.304283\n",
      "Epoch 3239\n",
      "-------------------------------\n",
      "tensor(29.6425)\n",
      "tensor(14.5385)\n",
      "tensor(9.9816)\n",
      "tensor(0.0291)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 98.280670\n",
      "Epoch 3240\n",
      "-------------------------------\n",
      "tensor(20.7159)\n",
      "tensor(12.4627)\n",
      "tensor(10.3107)\n",
      "tensor(0.0176)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 98.262505\n",
      "Epoch 3241\n",
      "-------------------------------\n",
      "tensor(21.9597)\n",
      "tensor(12.7813)\n",
      "tensor(8.7803)\n",
      "tensor(0.0159)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 98.249168\n",
      "Epoch 3242\n",
      "-------------------------------\n",
      "tensor(21.5691)\n",
      "tensor(11.5711)\n",
      "tensor(7.1296)\n",
      "tensor(0.0536)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 98.241760\n",
      "Epoch 3243\n",
      "-------------------------------\n",
      "tensor(24.3701)\n",
      "tensor(11.9687)\n",
      "tensor(6.3622)\n",
      "tensor(0.1002)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 98.230339\n",
      "Epoch 3244\n",
      "-------------------------------\n",
      "tensor(29.7475)\n",
      "tensor(12.6371)\n",
      "tensor(8.2833)\n",
      "tensor(0.1452)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 98.214821\n",
      "Epoch 3245\n",
      "-------------------------------\n",
      "tensor(27.1527)\n",
      "tensor(11.8080)\n",
      "tensor(9.8300)\n",
      "tensor(0.1399)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 98.176842\n",
      "Epoch 3246\n",
      "-------------------------------\n",
      "tensor(19.6316)\n",
      "tensor(10.8741)\n",
      "tensor(6.8314)\n",
      "tensor(0.0156)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 98.123512\n",
      "Epoch 3247\n",
      "-------------------------------\n",
      "tensor(27.8346)\n",
      "tensor(12.7758)\n",
      "tensor(9.8364)\n",
      "tensor(0.1318)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 98.100327\n",
      "Epoch 3248\n",
      "-------------------------------\n",
      "tensor(70.7493)\n",
      "tensor(34.0540)\n",
      "tensor(10.7726)\n",
      "tensor(0.0583)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 98.129051\n",
      "Epoch 3249\n",
      "-------------------------------\n",
      "tensor(34.3592)\n",
      "tensor(13.0150)\n",
      "tensor(16.4752)\n",
      "tensor(0.5443)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 98.022346\n",
      "Epoch 3250\n",
      "-------------------------------\n",
      "tensor(30.1692)\n",
      "tensor(12.5245)\n",
      "tensor(8.3668)\n",
      "tensor(0.0694)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 97.889099\n",
      "Epoch 3251\n",
      "-------------------------------\n",
      "tensor(35.1192)\n",
      "tensor(18.3793)\n",
      "tensor(22.6634)\n",
      "tensor(0.7534)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 97.898384\n",
      "Epoch 3252\n",
      "-------------------------------\n",
      "tensor(24.7599)\n",
      "tensor(12.9097)\n",
      "tensor(7.0228)\n",
      "tensor(0.1344)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 97.799072\n",
      "Epoch 3253\n",
      "-------------------------------\n",
      "tensor(34.2783)\n",
      "tensor(13.2680)\n",
      "tensor(17.0419)\n",
      "tensor(0.4755)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 97.760689\n",
      "Epoch 3254\n",
      "-------------------------------\n",
      "tensor(25.7064)\n",
      "tensor(13.3947)\n",
      "tensor(7.6452)\n",
      "tensor(0.0039)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 97.669991\n",
      "Epoch 3255\n",
      "-------------------------------\n",
      "tensor(71.1234)\n",
      "tensor(32.3810)\n",
      "tensor(15.4395)\n",
      "tensor(0.2396)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 97.628319\n",
      "Epoch 3256\n",
      "-------------------------------\n",
      "tensor(28.9228)\n",
      "tensor(11.9036)\n",
      "tensor(12.0713)\n",
      "tensor(0.1879)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 97.565216\n",
      "Epoch 3257\n",
      "-------------------------------\n",
      "tensor(25.6273)\n",
      "tensor(11.0287)\n",
      "tensor(15.3206)\n",
      "tensor(0.2992)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 97.532722\n",
      "Epoch 3258\n",
      "-------------------------------\n",
      "tensor(20.5864)\n",
      "tensor(11.1357)\n",
      "tensor(7.1882)\n",
      "tensor(0.1027)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 97.482803\n",
      "Epoch 3259\n",
      "-------------------------------\n",
      "tensor(17.6103)\n",
      "tensor(11.1337)\n",
      "tensor(8.3390)\n",
      "tensor(0.0649)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 97.452103\n",
      "Epoch 3260\n",
      "-------------------------------\n",
      "tensor(20.4016)\n",
      "tensor(11.6175)\n",
      "tensor(10.3953)\n",
      "tensor(0.1131)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 97.429810\n",
      "Epoch 3261\n",
      "-------------------------------\n",
      "tensor(19.7646)\n",
      "tensor(11.4703)\n",
      "tensor(9.8201)\n",
      "tensor(0.0920)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 97.409470\n",
      "Epoch 3262\n",
      "-------------------------------\n",
      "tensor(24.8945)\n",
      "tensor(12.4786)\n",
      "tensor(8.2141)\n",
      "tensor(0.0422)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 97.396812\n",
      "Epoch 3263\n",
      "-------------------------------\n",
      "tensor(29.7597)\n",
      "tensor(13.3578)\n",
      "tensor(6.5095)\n",
      "tensor(0.0429)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 97.384117\n",
      "Epoch 3264\n",
      "-------------------------------\n",
      "tensor(32.8958)\n",
      "tensor(13.7192)\n",
      "tensor(7.3532)\n",
      "tensor(0.1763)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 97.356117\n",
      "Epoch 3265\n",
      "-------------------------------\n",
      "tensor(24.3123)\n",
      "tensor(11.6524)\n",
      "tensor(9.6486)\n",
      "tensor(0.2976)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 97.316734\n",
      "Epoch 3266\n",
      "-------------------------------\n",
      "tensor(27.3680)\n",
      "tensor(12.4255)\n",
      "tensor(7.8841)\n",
      "tensor(0.2502)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 97.268265\n",
      "Epoch 3267\n",
      "-------------------------------\n",
      "tensor(27.3682)\n",
      "tensor(13.7438)\n",
      "tensor(8.7417)\n",
      "tensor(0.0032)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 97.240631\n",
      "Epoch 3268\n",
      "-------------------------------\n",
      "tensor(71.1502)\n",
      "tensor(33.7760)\n",
      "tensor(11.5261)\n",
      "tensor(0.0639)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 97.348259\n",
      "Epoch 3269\n",
      "-------------------------------\n",
      "tensor(37.5884)\n",
      "tensor(13.7687)\n",
      "tensor(14.1552)\n",
      "tensor(0.2309)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 97.104942\n",
      "Epoch 3270\n",
      "-------------------------------\n",
      "tensor(28.6583)\n",
      "tensor(12.7725)\n",
      "tensor(7.4338)\n",
      "tensor(0.0488)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 97.055740\n",
      "Epoch 3271\n",
      "-------------------------------\n",
      "tensor(32.3348)\n",
      "tensor(16.8144)\n",
      "tensor(19.6817)\n",
      "tensor(0.5100)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 97.014328\n",
      "Epoch 3272\n",
      "-------------------------------\n",
      "tensor(25.2764)\n",
      "tensor(12.6907)\n",
      "tensor(6.8223)\n",
      "tensor(0.0495)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 96.949768\n",
      "Epoch 3273\n",
      "-------------------------------\n",
      "tensor(32.2595)\n",
      "tensor(12.0461)\n",
      "tensor(14.1061)\n",
      "tensor(0.2858)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 96.888443\n",
      "Epoch 3274\n",
      "-------------------------------\n",
      "tensor(66.0547)\n",
      "tensor(33.6876)\n",
      "tensor(8.1347)\n",
      "tensor(0.0641)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 96.862679\n",
      "Epoch 3275\n",
      "-------------------------------\n",
      "tensor(15.5134)\n",
      "tensor(10.6539)\n",
      "tensor(6.5415)\n",
      "tensor(0.0241)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 96.734741\n",
      "Epoch 3276\n",
      "-------------------------------\n",
      "tensor(20.9494)\n",
      "tensor(11.3185)\n",
      "tensor(7.6368)\n",
      "tensor(0.0984)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 96.690315\n",
      "Epoch 3277\n",
      "-------------------------------\n",
      "tensor(16.0656)\n",
      "tensor(11.9402)\n",
      "tensor(6.5158)\n",
      "tensor(0.1141)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 96.635010\n",
      "Epoch 3278\n",
      "-------------------------------\n",
      "tensor(16.3047)\n",
      "tensor(12.1100)\n",
      "tensor(6.6386)\n",
      "tensor(0.0844)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 96.584534\n",
      "Epoch 3279\n",
      "-------------------------------\n",
      "tensor(26.1807)\n",
      "tensor(12.8501)\n",
      "tensor(6.7748)\n",
      "tensor(0.0653)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 96.556396\n",
      "Epoch 3280\n",
      "-------------------------------\n",
      "tensor(33.9920)\n",
      "tensor(14.6387)\n",
      "tensor(6.7498)\n",
      "tensor(0.0614)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 96.556252\n",
      "Epoch 3281\n",
      "-------------------------------\n",
      "tensor(33.9509)\n",
      "tensor(14.6426)\n",
      "tensor(6.7381)\n",
      "tensor(0.0750)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 96.540596\n",
      "Epoch 3282\n",
      "-------------------------------\n",
      "tensor(31.6407)\n",
      "tensor(14.1036)\n",
      "tensor(6.7518)\n",
      "tensor(0.0955)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 96.516411\n",
      "Epoch 3283\n",
      "-------------------------------\n",
      "tensor(26.3726)\n",
      "tensor(12.8806)\n",
      "tensor(6.8132)\n",
      "tensor(0.1186)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 96.497765\n",
      "Epoch 3284\n",
      "-------------------------------\n",
      "tensor(20.1810)\n",
      "tensor(11.5287)\n",
      "tensor(6.8111)\n",
      "tensor(0.1400)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 96.485252\n",
      "Epoch 3285\n",
      "-------------------------------\n",
      "tensor(24.1066)\n",
      "tensor(12.2295)\n",
      "tensor(6.5544)\n",
      "tensor(0.1385)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 96.476562\n",
      "Epoch 3286\n",
      "-------------------------------\n",
      "tensor(21.3678)\n",
      "tensor(11.5174)\n",
      "tensor(6.4156)\n",
      "tensor(0.0875)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 96.452507\n",
      "Epoch 3287\n",
      "-------------------------------\n",
      "tensor(24.2669)\n",
      "tensor(12.0193)\n",
      "tensor(6.3519)\n",
      "tensor(0.0235)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 96.406158\n",
      "Epoch 3288\n",
      "-------------------------------\n",
      "tensor(65.9646)\n",
      "tensor(34.5440)\n",
      "tensor(6.6532)\n",
      "tensor(0.0538)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 96.480080\n",
      "Epoch 3289\n",
      "-------------------------------\n",
      "tensor(59.7607)\n",
      "tensor(34.6480)\n",
      "tensor(9.6002)\n",
      "tensor(0.0715)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 96.281319\n",
      "Epoch 3290\n",
      "-------------------------------\n",
      "tensor(37.4205)\n",
      "tensor(13.5819)\n",
      "tensor(13.7827)\n",
      "tensor(0.0309)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 96.290771\n",
      "Epoch 3291\n",
      "-------------------------------\n",
      "tensor(27.5375)\n",
      "tensor(18.7482)\n",
      "tensor(18.9980)\n",
      "tensor(0.6623)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 96.276535\n",
      "Epoch 3292\n",
      "-------------------------------\n",
      "tensor(33.9312)\n",
      "tensor(17.7569)\n",
      "tensor(11.8659)\n",
      "tensor(0.3958)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 96.270775\n",
      "Epoch 3293\n",
      "-------------------------------\n",
      "tensor(32.6430)\n",
      "tensor(13.8272)\n",
      "tensor(11.9330)\n",
      "tensor(0.2712)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 96.218307\n",
      "Epoch 3294\n",
      "-------------------------------\n",
      "tensor(24.4542)\n",
      "tensor(13.3291)\n",
      "tensor(6.6455)\n",
      "tensor(0.1381)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 96.101944\n",
      "Epoch 3295\n",
      "-------------------------------\n",
      "tensor(25.0195)\n",
      "tensor(13.3086)\n",
      "tensor(11.4801)\n",
      "tensor(0.1375)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 96.069168\n",
      "Epoch 3296\n",
      "-------------------------------\n",
      "tensor(21.8772)\n",
      "tensor(12.8221)\n",
      "tensor(7.5082)\n",
      "tensor(0.0968)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 96.012054\n",
      "Epoch 3297\n",
      "-------------------------------\n",
      "tensor(32.4132)\n",
      "tensor(15.5477)\n",
      "tensor(10.3292)\n",
      "tensor(0.4054)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 95.964470\n",
      "Epoch 3298\n",
      "-------------------------------\n",
      "tensor(25.8025)\n",
      "tensor(14.5544)\n",
      "tensor(9.8891)\n",
      "tensor(0.4846)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 95.904579\n",
      "Epoch 3299\n",
      "-------------------------------\n",
      "tensor(20.1625)\n",
      "tensor(13.1014)\n",
      "tensor(7.8641)\n",
      "tensor(0.4080)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 95.855492\n",
      "Epoch 3300\n",
      "-------------------------------\n",
      "tensor(24.4760)\n",
      "tensor(13.4862)\n",
      "tensor(7.3839)\n",
      "tensor(0.3061)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 95.837807\n",
      "Epoch 3301\n",
      "-------------------------------\n",
      "tensor(22.0857)\n",
      "tensor(12.7457)\n",
      "tensor(7.4482)\n",
      "tensor(0.2354)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 95.820435\n",
      "Epoch 3302\n",
      "-------------------------------\n",
      "tensor(24.7908)\n",
      "tensor(13.2008)\n",
      "tensor(7.3609)\n",
      "tensor(0.1842)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 95.803108\n",
      "Epoch 3303\n",
      "-------------------------------\n",
      "tensor(22.0729)\n",
      "tensor(12.5838)\n",
      "tensor(7.0215)\n",
      "tensor(0.1370)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 95.775871\n",
      "Epoch 3304\n",
      "-------------------------------\n",
      "tensor(19.8575)\n",
      "tensor(12.1322)\n",
      "tensor(6.5120)\n",
      "tensor(0.0864)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 95.741051\n",
      "Epoch 3305\n",
      "-------------------------------\n",
      "tensor(28.4590)\n",
      "tensor(14.3156)\n",
      "tensor(6.6561)\n",
      "tensor(0.0325)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 95.695610\n",
      "Epoch 3306\n",
      "-------------------------------\n",
      "tensor(25.0247)\n",
      "tensor(12.7912)\n",
      "tensor(6.5902)\n",
      "tensor(0.0258)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 95.624908\n",
      "Epoch 3307\n",
      "-------------------------------\n",
      "tensor(29.1335)\n",
      "tensor(15.0149)\n",
      "tensor(6.9981)\n",
      "tensor(0.1146)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 95.590019\n",
      "Epoch 3308\n",
      "-------------------------------\n",
      "tensor(28.1327)\n",
      "tensor(14.2987)\n",
      "tensor(10.6538)\n",
      "tensor(0.1139)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 95.543053\n",
      "Epoch 3309\n",
      "-------------------------------\n",
      "tensor(67.0594)\n",
      "tensor(34.1525)\n",
      "tensor(7.9443)\n",
      "tensor(0.0697)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 95.521965\n",
      "Epoch 3310\n",
      "-------------------------------\n",
      "tensor(63.6771)\n",
      "tensor(35.3789)\n",
      "tensor(16.3483)\n",
      "tensor(0.1243)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 95.397766\n",
      "Epoch 3311\n",
      "-------------------------------\n",
      "tensor(36.9939)\n",
      "tensor(17.2633)\n",
      "tensor(10.0985)\n",
      "tensor(0.5618)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 95.316994\n",
      "Epoch 3312\n",
      "-------------------------------\n",
      "tensor(36.5338)\n",
      "tensor(22.0260)\n",
      "tensor(27.5958)\n",
      "tensor(0.9237)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 95.327553\n",
      "Epoch 3313\n",
      "-------------------------------\n",
      "tensor(26.9927)\n",
      "tensor(14.0967)\n",
      "tensor(8.4359)\n",
      "tensor(0.1921)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 95.246994\n",
      "Epoch 3314\n",
      "-------------------------------\n",
      "tensor(40.2149)\n",
      "tensor(16.1128)\n",
      "tensor(19.4463)\n",
      "tensor(0.6388)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 95.201668\n",
      "Epoch 3315\n",
      "-------------------------------\n",
      "tensor(25.8903)\n",
      "tensor(13.1193)\n",
      "tensor(8.8817)\n",
      "tensor(0.0307)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 95.103851\n",
      "Epoch 3316\n",
      "-------------------------------\n",
      "tensor(29.4120)\n",
      "tensor(12.7756)\n",
      "tensor(16.8514)\n",
      "tensor(0.2441)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 95.082314\n",
      "Epoch 3317\n",
      "-------------------------------\n",
      "tensor(26.3853)\n",
      "tensor(13.4416)\n",
      "tensor(8.0524)\n",
      "tensor(0.2023)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 95.023956\n",
      "Epoch 3318\n",
      "-------------------------------\n",
      "tensor(30.3422)\n",
      "tensor(15.5365)\n",
      "tensor(13.1643)\n",
      "tensor(0.5841)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 94.978668\n",
      "Epoch 3319\n",
      "-------------------------------\n",
      "tensor(18.3143)\n",
      "tensor(13.5662)\n",
      "tensor(13.4909)\n",
      "tensor(0.6477)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 94.939491\n",
      "Epoch 3320\n",
      "-------------------------------\n",
      "tensor(16.3546)\n",
      "tensor(12.6512)\n",
      "tensor(10.6274)\n",
      "tensor(0.5420)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 94.908035\n",
      "Epoch 3321\n",
      "-------------------------------\n",
      "tensor(15.6782)\n",
      "tensor(11.9087)\n",
      "tensor(8.3214)\n",
      "tensor(0.4105)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 94.884422\n",
      "Epoch 3322\n",
      "-------------------------------\n",
      "tensor(16.0497)\n",
      "tensor(11.4669)\n",
      "tensor(7.2984)\n",
      "tensor(0.2815)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 94.863792\n",
      "Epoch 3323\n",
      "-------------------------------\n",
      "tensor(20.2114)\n",
      "tensor(11.8380)\n",
      "tensor(7.5766)\n",
      "tensor(0.1289)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 94.837791\n",
      "Epoch 3324\n",
      "-------------------------------\n",
      "tensor(21.6015)\n",
      "tensor(12.1144)\n",
      "tensor(8.8345)\n",
      "tensor(0.0490)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 94.797241\n",
      "Epoch 3325\n",
      "-------------------------------\n",
      "tensor(31.5716)\n",
      "tensor(14.3753)\n",
      "tensor(8.7474)\n",
      "tensor(0.1729)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 94.739479\n",
      "Epoch 3326\n",
      "-------------------------------\n",
      "tensor(34.8873)\n",
      "tensor(14.9748)\n",
      "tensor(6.8428)\n",
      "tensor(0.1229)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 94.709290\n",
      "Epoch 3327\n",
      "-------------------------------\n",
      "tensor(35.4373)\n",
      "tensor(14.8713)\n",
      "tensor(6.9926)\n",
      "tensor(0.1034)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 94.663544\n",
      "Epoch 3328\n",
      "-------------------------------\n",
      "tensor(66.0892)\n",
      "tensor(34.0631)\n",
      "tensor(9.6621)\n",
      "tensor(0.2175)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 94.594017\n",
      "Epoch 3329\n",
      "-------------------------------\n",
      "tensor(28.0293)\n",
      "tensor(12.6994)\n",
      "tensor(10.2798)\n",
      "tensor(0.0352)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 94.532173\n",
      "Epoch 3330\n",
      "-------------------------------\n",
      "tensor(24.4395)\n",
      "tensor(19.1000)\n",
      "tensor(10.6471)\n",
      "tensor(0.7984)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 94.475990\n",
      "Epoch 3331\n",
      "-------------------------------\n",
      "tensor(70.0528)\n",
      "tensor(33.2073)\n",
      "tensor(15.3971)\n",
      "tensor(0.7846)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 94.432526\n",
      "Epoch 3332\n",
      "-------------------------------\n",
      "tensor(34.4382)\n",
      "tensor(13.3932)\n",
      "tensor(15.4074)\n",
      "tensor(0.4377)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 94.343391\n",
      "Epoch 3333\n",
      "-------------------------------\n",
      "tensor(25.0284)\n",
      "tensor(12.7418)\n",
      "tensor(8.9369)\n",
      "tensor(0.4075)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 94.262657\n",
      "Epoch 3334\n",
      "-------------------------------\n",
      "tensor(26.9457)\n",
      "tensor(13.5561)\n",
      "tensor(14.8411)\n",
      "tensor(0.2272)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 94.202812\n",
      "Epoch 3335\n",
      "-------------------------------\n",
      "tensor(32.5312)\n",
      "tensor(14.1519)\n",
      "tensor(8.1758)\n",
      "tensor(0.1140)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 94.187157\n",
      "Epoch 3336\n",
      "-------------------------------\n",
      "tensor(27.8925)\n",
      "tensor(13.7597)\n",
      "tensor(11.2655)\n",
      "tensor(0.3310)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 94.143150\n",
      "Epoch 3337\n",
      "-------------------------------\n",
      "tensor(24.9302)\n",
      "tensor(14.1805)\n",
      "tensor(9.1612)\n",
      "tensor(0.5011)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 94.064110\n",
      "Epoch 3338\n",
      "-------------------------------\n",
      "tensor(20.9432)\n",
      "tensor(12.8424)\n",
      "tensor(8.4057)\n",
      "tensor(0.4322)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 94.004532\n",
      "Epoch 3339\n",
      "-------------------------------\n",
      "tensor(28.5164)\n",
      "tensor(14.1553)\n",
      "tensor(8.6468)\n",
      "tensor(0.3131)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 93.982925\n",
      "Epoch 3340\n",
      "-------------------------------\n",
      "tensor(24.0764)\n",
      "tensor(12.9121)\n",
      "tensor(8.0119)\n",
      "tensor(0.2373)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 93.956886\n",
      "Epoch 3341\n",
      "-------------------------------\n",
      "tensor(20.5865)\n",
      "tensor(12.1636)\n",
      "tensor(7.2474)\n",
      "tensor(0.1947)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 93.935234\n",
      "Epoch 3342\n",
      "-------------------------------\n",
      "tensor(25.7227)\n",
      "tensor(13.1215)\n",
      "tensor(6.7876)\n",
      "tensor(0.1642)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 93.916267\n",
      "Epoch 3343\n",
      "-------------------------------\n",
      "tensor(15.3105)\n",
      "tensor(11.2162)\n",
      "tensor(6.7746)\n",
      "tensor(0.1301)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 93.896896\n",
      "Epoch 3344\n",
      "-------------------------------\n",
      "tensor(24.6557)\n",
      "tensor(12.4906)\n",
      "tensor(7.3696)\n",
      "tensor(0.0737)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 93.876709\n",
      "Epoch 3345\n",
      "-------------------------------\n",
      "tensor(25.3807)\n",
      "tensor(12.5497)\n",
      "tensor(7.3987)\n",
      "tensor(0.0130)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 93.847321\n",
      "Epoch 3346\n",
      "-------------------------------\n",
      "tensor(30.6984)\n",
      "tensor(13.9992)\n",
      "tensor(6.5158)\n",
      "tensor(0.1057)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 93.791908\n",
      "Epoch 3347\n",
      "-------------------------------\n",
      "tensor(27.9832)\n",
      "tensor(14.0288)\n",
      "tensor(10.3537)\n",
      "tensor(0.1052)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 93.747658\n",
      "Epoch 3348\n",
      "-------------------------------\n",
      "tensor(70.2265)\n",
      "tensor(34.7927)\n",
      "tensor(10.2973)\n",
      "tensor(0.1484)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 93.904068\n",
      "Epoch 3349\n",
      "-------------------------------\n",
      "tensor(65.8263)\n",
      "tensor(36.6503)\n",
      "tensor(17.2194)\n",
      "tensor(0.3171)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 93.740334\n",
      "Epoch 3350\n",
      "-------------------------------\n",
      "tensor(39.6042)\n",
      "tensor(17.0032)\n",
      "tensor(13.8896)\n",
      "tensor(0.4423)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 93.659477\n",
      "Epoch 3351\n",
      "-------------------------------\n",
      "tensor(36.2117)\n",
      "tensor(26.7024)\n",
      "tensor(29.6661)\n",
      "tensor(1.2834)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 93.658417\n",
      "Epoch 3352\n",
      "-------------------------------\n",
      "tensor(27.4497)\n",
      "tensor(16.6430)\n",
      "tensor(11.7275)\n",
      "tensor(0.1596)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 93.587975\n",
      "Epoch 3353\n",
      "-------------------------------\n",
      "tensor(41.5460)\n",
      "tensor(15.3668)\n",
      "tensor(24.0173)\n",
      "tensor(0.8277)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 93.549515\n",
      "Epoch 3354\n",
      "-------------------------------\n",
      "tensor(22.4040)\n",
      "tensor(13.0309)\n",
      "tensor(7.3140)\n",
      "tensor(0.0191)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 93.437454\n",
      "Epoch 3355\n",
      "-------------------------------\n",
      "tensor(32.1321)\n",
      "tensor(13.8182)\n",
      "tensor(19.4423)\n",
      "tensor(0.4067)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 93.386238\n",
      "Epoch 3356\n",
      "-------------------------------\n",
      "tensor(21.5522)\n",
      "tensor(13.3563)\n",
      "tensor(9.1619)\n",
      "tensor(0.2684)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 93.300354\n",
      "Epoch 3357\n",
      "-------------------------------\n",
      "tensor(35.2123)\n",
      "tensor(18.5515)\n",
      "tensor(17.9674)\n",
      "tensor(0.8317)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 93.256058\n",
      "Epoch 3358\n",
      "-------------------------------\n",
      "tensor(30.1113)\n",
      "tensor(17.4998)\n",
      "tensor(14.1610)\n",
      "tensor(0.8196)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 93.185791\n",
      "Epoch 3359\n",
      "-------------------------------\n",
      "tensor(21.0373)\n",
      "tensor(13.9442)\n",
      "tensor(8.8707)\n",
      "tensor(0.5615)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 93.120018\n",
      "Epoch 3360\n",
      "-------------------------------\n",
      "tensor(22.9939)\n",
      "tensor(13.0046)\n",
      "tensor(9.0050)\n",
      "tensor(0.3375)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 93.080704\n",
      "Epoch 3361\n",
      "-------------------------------\n",
      "tensor(27.2798)\n",
      "tensor(13.6395)\n",
      "tensor(9.4444)\n",
      "tensor(0.2036)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 93.058037\n",
      "Epoch 3362\n",
      "-------------------------------\n",
      "tensor(26.1485)\n",
      "tensor(13.3168)\n",
      "tensor(9.0596)\n",
      "tensor(0.1228)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 93.041161\n",
      "Epoch 3363\n",
      "-------------------------------\n",
      "tensor(24.8612)\n",
      "tensor(13.0938)\n",
      "tensor(7.7767)\n",
      "tensor(0.0677)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 93.012619\n",
      "Epoch 3364\n",
      "-------------------------------\n",
      "tensor(32.1765)\n",
      "tensor(14.3854)\n",
      "tensor(6.6136)\n",
      "tensor(0.0318)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 92.990662\n",
      "Epoch 3365\n",
      "-------------------------------\n",
      "tensor(33.8969)\n",
      "tensor(14.2162)\n",
      "tensor(7.7104)\n",
      "tensor(0.0224)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 92.961624\n",
      "Epoch 3366\n",
      "-------------------------------\n",
      "tensor(24.8564)\n",
      "tensor(12.6980)\n",
      "tensor(7.1357)\n",
      "tensor(0.0111)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 92.912308\n",
      "Epoch 3367\n",
      "-------------------------------\n",
      "tensor(20.8514)\n",
      "tensor(13.2833)\n",
      "tensor(8.4711)\n",
      "tensor(0.1344)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 92.848907\n",
      "Epoch 3368\n",
      "-------------------------------\n",
      "tensor(67.9610)\n",
      "tensor(33.1858)\n",
      "tensor(11.2764)\n",
      "tensor(0.1915)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 92.955193\n",
      "Epoch 3369\n",
      "-------------------------------\n",
      "tensor(62.1319)\n",
      "tensor(34.8356)\n",
      "tensor(13.5786)\n",
      "tensor(0.0709)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 92.853210\n",
      "Epoch 3370\n",
      "-------------------------------\n",
      "tensor(47.4171)\n",
      "tensor(17.4702)\n",
      "tensor(16.6751)\n",
      "tensor(0.4248)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 92.768082\n",
      "Epoch 3371\n",
      "-------------------------------\n",
      "tensor(39.9947)\n",
      "tensor(24.4919)\n",
      "tensor(28.2821)\n",
      "tensor(0.9715)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 92.735016\n",
      "Epoch 3372\n",
      "-------------------------------\n",
      "tensor(35.1151)\n",
      "tensor(17.6668)\n",
      "tensor(15.3338)\n",
      "tensor(0.1038)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 92.608597\n",
      "Epoch 3373\n",
      "-------------------------------\n",
      "tensor(35.8644)\n",
      "tensor(15.2379)\n",
      "tensor(23.7696)\n",
      "tensor(0.9328)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 92.531479\n",
      "Epoch 3374\n",
      "-------------------------------\n",
      "tensor(19.8729)\n",
      "tensor(12.4364)\n",
      "tensor(8.5729)\n",
      "tensor(0.0519)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 92.451752\n",
      "Epoch 3375\n",
      "-------------------------------\n",
      "tensor(36.1167)\n",
      "tensor(15.1033)\n",
      "tensor(20.1788)\n",
      "tensor(0.5668)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 92.424805\n",
      "Epoch 3376\n",
      "-------------------------------\n",
      "tensor(28.7994)\n",
      "tensor(14.3065)\n",
      "tensor(9.3545)\n",
      "tensor(0.2433)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 92.335655\n",
      "Epoch 3377\n",
      "-------------------------------\n",
      "tensor(17.4110)\n",
      "tensor(16.4437)\n",
      "tensor(15.9914)\n",
      "tensor(0.9920)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 92.276794\n",
      "Epoch 3378\n",
      "-------------------------------\n",
      "tensor(20.6650)\n",
      "tensor(16.4504)\n",
      "tensor(16.8391)\n",
      "tensor(1.0047)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 92.218132\n",
      "Epoch 3379\n",
      "-------------------------------\n",
      "tensor(21.7685)\n",
      "tensor(14.2045)\n",
      "tensor(10.7133)\n",
      "tensor(0.6390)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 92.161301\n",
      "Epoch 3380\n",
      "-------------------------------\n",
      "tensor(24.7205)\n",
      "tensor(13.3691)\n",
      "tensor(7.4029)\n",
      "tensor(0.2957)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 92.130630\n",
      "Epoch 3381\n",
      "-------------------------------\n",
      "tensor(25.2027)\n",
      "tensor(13.1068)\n",
      "tensor(7.9393)\n",
      "tensor(0.0828)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 92.109596\n",
      "Epoch 3382\n",
      "-------------------------------\n",
      "tensor(33.8607)\n",
      "tensor(14.9967)\n",
      "tensor(8.9215)\n",
      "tensor(0.0476)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 92.093826\n",
      "Epoch 3383\n",
      "-------------------------------\n",
      "tensor(30.3570)\n",
      "tensor(14.3495)\n",
      "tensor(9.5231)\n",
      "tensor(0.1314)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 92.073547\n",
      "Epoch 3384\n",
      "-------------------------------\n",
      "tensor(21.6039)\n",
      "tensor(12.8560)\n",
      "tensor(8.7906)\n",
      "tensor(0.1407)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 92.048042\n",
      "Epoch 3385\n",
      "-------------------------------\n",
      "tensor(22.7193)\n",
      "tensor(12.6844)\n",
      "tensor(6.4950)\n",
      "tensor(0.0179)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 92.016273\n",
      "Epoch 3386\n",
      "-------------------------------\n",
      "tensor(27.6852)\n",
      "tensor(12.6505)\n",
      "tensor(8.3186)\n",
      "tensor(0.1855)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 91.970657\n",
      "Epoch 3387\n",
      "-------------------------------\n",
      "tensor(63.3490)\n",
      "tensor(35.2773)\n",
      "tensor(8.3298)\n",
      "tensor(0.2072)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 91.963821\n",
      "Epoch 3388\n",
      "-------------------------------\n",
      "tensor(65.6739)\n",
      "tensor(34.6640)\n",
      "tensor(8.3685)\n",
      "tensor(0.1137)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 91.891586\n",
      "Epoch 3389\n",
      "-------------------------------\n",
      "tensor(35.1223)\n",
      "tensor(18.5939)\n",
      "tensor(9.9154)\n",
      "tensor(0.5685)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 91.892105\n",
      "Epoch 3390\n",
      "-------------------------------\n",
      "tensor(30.5156)\n",
      "tensor(23.0836)\n",
      "tensor(18.0438)\n",
      "tensor(0.8672)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 91.881310\n",
      "Epoch 3391\n",
      "-------------------------------\n",
      "tensor(28.6824)\n",
      "tensor(17.6598)\n",
      "tensor(12.0922)\n",
      "tensor(0.2195)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 91.830162\n",
      "Epoch 3392\n",
      "-------------------------------\n",
      "tensor(35.0853)\n",
      "tensor(15.5180)\n",
      "tensor(13.0572)\n",
      "tensor(0.4313)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 91.734886\n",
      "Epoch 3393\n",
      "-------------------------------\n",
      "tensor(26.8494)\n",
      "tensor(14.9509)\n",
      "tensor(7.4834)\n",
      "tensor(0.0570)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 91.668137\n",
      "Epoch 3394\n",
      "-------------------------------\n",
      "tensor(26.1763)\n",
      "tensor(13.0567)\n",
      "tensor(12.2442)\n",
      "tensor(0.0171)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 91.624413\n",
      "Epoch 3395\n",
      "-------------------------------\n",
      "tensor(21.3502)\n",
      "tensor(14.8662)\n",
      "tensor(11.3400)\n",
      "tensor(0.5454)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 91.555183\n",
      "Epoch 3396\n",
      "-------------------------------\n",
      "tensor(24.1872)\n",
      "tensor(16.4719)\n",
      "tensor(14.6386)\n",
      "tensor(0.7685)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 91.474289\n",
      "Epoch 3397\n",
      "-------------------------------\n",
      "tensor(31.6758)\n",
      "tensor(16.2149)\n",
      "tensor(9.1058)\n",
      "tensor(0.5144)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 91.392021\n",
      "Epoch 3398\n",
      "-------------------------------\n",
      "tensor(27.2554)\n",
      "tensor(13.7739)\n",
      "tensor(9.4475)\n",
      "tensor(0.2439)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 91.319031\n",
      "Epoch 3399\n",
      "-------------------------------\n",
      "tensor(19.7986)\n",
      "tensor(11.9372)\n",
      "tensor(9.5590)\n",
      "tensor(0.1311)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 91.265503\n",
      "Epoch 3400\n",
      "-------------------------------\n",
      "tensor(17.0445)\n",
      "tensor(11.6879)\n",
      "tensor(7.7961)\n",
      "tensor(0.1136)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 91.230682\n",
      "Epoch 3401\n",
      "-------------------------------\n",
      "tensor(15.6291)\n",
      "tensor(11.5038)\n",
      "tensor(6.7686)\n",
      "tensor(0.1236)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 91.205933\n",
      "Epoch 3402\n",
      "-------------------------------\n",
      "tensor(15.5956)\n",
      "tensor(11.3910)\n",
      "tensor(6.7814)\n",
      "tensor(0.1354)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 91.184952\n",
      "Epoch 3403\n",
      "-------------------------------\n",
      "tensor(20.2540)\n",
      "tensor(11.8598)\n",
      "tensor(7.5254)\n",
      "tensor(0.1402)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 91.159248\n",
      "Epoch 3404\n",
      "-------------------------------\n",
      "tensor(30.3129)\n",
      "tensor(13.4345)\n",
      "tensor(8.1667)\n",
      "tensor(0.1180)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 91.120399\n",
      "Epoch 3405\n",
      "-------------------------------\n",
      "tensor(34.3895)\n",
      "tensor(14.5749)\n",
      "tensor(6.6726)\n",
      "tensor(0.0275)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 91.089310\n",
      "Epoch 3406\n",
      "-------------------------------\n",
      "tensor(73.8744)\n",
      "tensor(35.4886)\n",
      "tensor(9.4746)\n",
      "tensor(0.0967)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 91.057381\n",
      "Epoch 3407\n",
      "-------------------------------\n",
      "tensor(27.1036)\n",
      "tensor(14.1951)\n",
      "tensor(9.0755)\n",
      "tensor(0.0130)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 91.049225\n",
      "Epoch 3408\n",
      "-------------------------------\n",
      "tensor(28.6585)\n",
      "tensor(13.7720)\n",
      "tensor(8.4922)\n",
      "tensor(0.0375)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 91.018082\n",
      "Epoch 3409\n",
      "-------------------------------\n",
      "tensor(65.5921)\n",
      "tensor(34.1244)\n",
      "tensor(7.6301)\n",
      "tensor(0.2493)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 90.956100\n",
      "Epoch 3410\n",
      "-------------------------------\n",
      "tensor(29.8892)\n",
      "tensor(16.5560)\n",
      "tensor(10.0531)\n",
      "tensor(0.4788)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 90.883850\n",
      "Epoch 3411\n",
      "-------------------------------\n",
      "tensor(29.3969)\n",
      "tensor(18.3675)\n",
      "tensor(12.6576)\n",
      "tensor(0.5781)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 90.838310\n",
      "Epoch 3412\n",
      "-------------------------------\n",
      "tensor(31.1430)\n",
      "tensor(15.3967)\n",
      "tensor(11.5345)\n",
      "tensor(0.1094)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 90.761444\n",
      "Epoch 3413\n",
      "-------------------------------\n",
      "tensor(23.5635)\n",
      "tensor(15.3525)\n",
      "tensor(12.0709)\n",
      "tensor(0.6987)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 90.706383\n",
      "Epoch 3414\n",
      "-------------------------------\n",
      "tensor(23.0088)\n",
      "tensor(14.2543)\n",
      "tensor(9.3473)\n",
      "tensor(0.2764)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 90.665718\n",
      "Epoch 3415\n",
      "-------------------------------\n",
      "tensor(23.8867)\n",
      "tensor(13.7638)\n",
      "tensor(11.0075)\n",
      "tensor(0.1462)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 90.598267\n",
      "Epoch 3416\n",
      "-------------------------------\n",
      "tensor(19.9878)\n",
      "tensor(13.4099)\n",
      "tensor(8.8759)\n",
      "tensor(0.1231)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 90.497856\n",
      "Epoch 3417\n",
      "-------------------------------\n",
      "tensor(19.9807)\n",
      "tensor(13.5450)\n",
      "tensor(9.6199)\n",
      "tensor(0.5263)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 90.411575\n",
      "Epoch 3418\n",
      "-------------------------------\n",
      "tensor(20.4848)\n",
      "tensor(13.6733)\n",
      "tensor(10.6461)\n",
      "tensor(0.6192)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 90.337708\n",
      "Epoch 3419\n",
      "-------------------------------\n",
      "tensor(15.4724)\n",
      "tensor(12.0175)\n",
      "tensor(8.7751)\n",
      "tensor(0.4822)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 90.277908\n",
      "Epoch 3420\n",
      "-------------------------------\n",
      "tensor(15.3946)\n",
      "tensor(11.4583)\n",
      "tensor(7.3370)\n",
      "tensor(0.3094)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 90.238083\n",
      "Epoch 3421\n",
      "-------------------------------\n",
      "tensor(26.9650)\n",
      "tensor(13.4383)\n",
      "tensor(6.9385)\n",
      "tensor(0.1763)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 90.223808\n",
      "Epoch 3422\n",
      "-------------------------------\n",
      "tensor(27.4703)\n",
      "tensor(13.5044)\n",
      "tensor(7.0498)\n",
      "tensor(0.0791)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 90.208504\n",
      "Epoch 3423\n",
      "-------------------------------\n",
      "tensor(28.8978)\n",
      "tensor(13.8516)\n",
      "tensor(7.3756)\n",
      "tensor(0.0071)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 90.182655\n",
      "Epoch 3424\n",
      "-------------------------------\n",
      "tensor(70.5039)\n",
      "tensor(35.7546)\n",
      "tensor(7.6875)\n",
      "tensor(0.0730)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 90.150902\n",
      "Epoch 3425\n",
      "-------------------------------\n",
      "tensor(33.4846)\n",
      "tensor(14.6589)\n",
      "tensor(6.5019)\n",
      "tensor(0.0522)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 90.153046\n",
      "Epoch 3426\n",
      "-------------------------------\n",
      "tensor(33.7417)\n",
      "tensor(14.5168)\n",
      "tensor(7.5272)\n",
      "tensor(0.0184)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 90.137276\n",
      "Epoch 3427\n",
      "-------------------------------\n",
      "tensor(24.4644)\n",
      "tensor(13.7979)\n",
      "tensor(8.2539)\n",
      "tensor(0.0266)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 90.096664\n",
      "Epoch 3428\n",
      "-------------------------------\n",
      "tensor(67.1335)\n",
      "tensor(32.9385)\n",
      "tensor(11.6229)\n",
      "tensor(0.2184)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 90.062332\n",
      "Epoch 3429\n",
      "-------------------------------\n",
      "tensor(39.9784)\n",
      "tensor(15.8975)\n",
      "tensor(12.9821)\n",
      "tensor(0.2466)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 90.057541\n",
      "Epoch 3430\n",
      "-------------------------------\n",
      "tensor(27.6490)\n",
      "tensor(17.7130)\n",
      "tensor(8.9588)\n",
      "tensor(0.5632)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 89.973663\n",
      "Epoch 3431\n",
      "-------------------------------\n",
      "tensor(29.3161)\n",
      "tensor(17.7029)\n",
      "tensor(18.3981)\n",
      "tensor(0.3948)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 89.883430\n",
      "Epoch 3432\n",
      "-------------------------------\n",
      "tensor(33.2383)\n",
      "tensor(16.9944)\n",
      "tensor(10.4316)\n",
      "tensor(0.5784)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 89.836342\n",
      "Epoch 3433\n",
      "-------------------------------\n",
      "tensor(32.5651)\n",
      "tensor(16.7144)\n",
      "tensor(12.6380)\n",
      "tensor(0.6741)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 89.751503\n",
      "Epoch 3434\n",
      "-------------------------------\n",
      "tensor(25.5506)\n",
      "tensor(12.8140)\n",
      "tensor(11.9975)\n",
      "tensor(0.0500)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 89.644173\n",
      "Epoch 3435\n",
      "-------------------------------\n",
      "tensor(24.5602)\n",
      "tensor(12.9595)\n",
      "tensor(9.3964)\n",
      "tensor(0.0761)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 89.556923\n",
      "Epoch 3436\n",
      "-------------------------------\n",
      "tensor(22.0711)\n",
      "tensor(13.2777)\n",
      "tensor(11.0603)\n",
      "tensor(0.4420)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 89.473923\n",
      "Epoch 3437\n",
      "-------------------------------\n",
      "tensor(28.8066)\n",
      "tensor(14.4057)\n",
      "tensor(10.4182)\n",
      "tensor(0.4927)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 89.389198\n",
      "Epoch 3438\n",
      "-------------------------------\n",
      "tensor(69.5976)\n",
      "tensor(37.6299)\n",
      "tensor(7.4663)\n",
      "tensor(0.3145)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 89.377998\n",
      "Epoch 3439\n",
      "-------------------------------\n",
      "tensor(16.7670)\n",
      "tensor(11.5654)\n",
      "tensor(6.8121)\n",
      "tensor(0.1619)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 89.306610\n",
      "Epoch 3440\n",
      "-------------------------------\n",
      "tensor(16.9885)\n",
      "tensor(11.8546)\n",
      "tensor(6.5316)\n",
      "tensor(0.0568)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 89.290314\n",
      "Epoch 3441\n",
      "-------------------------------\n",
      "tensor(19.4319)\n",
      "tensor(12.4323)\n",
      "tensor(6.4633)\n",
      "tensor(0.0105)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 89.276726\n",
      "Epoch 3442\n",
      "-------------------------------\n",
      "tensor(18.8106)\n",
      "tensor(12.5291)\n",
      "tensor(6.4763)\n",
      "tensor(0.0590)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 89.263000\n",
      "Epoch 3443\n",
      "-------------------------------\n",
      "tensor(24.3462)\n",
      "tensor(13.5337)\n",
      "tensor(6.5389)\n",
      "tensor(0.1048)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 89.246391\n",
      "Epoch 3444\n",
      "-------------------------------\n",
      "tensor(30.4400)\n",
      "tensor(14.7947)\n",
      "tensor(6.7729)\n",
      "tensor(0.1504)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 89.241623\n",
      "Epoch 3445\n",
      "-------------------------------\n",
      "tensor(71.8424)\n",
      "tensor(35.5577)\n",
      "tensor(7.5761)\n",
      "tensor(0.1396)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 89.239189\n",
      "Epoch 3446\n",
      "-------------------------------\n",
      "tensor(35.6558)\n",
      "tensor(15.6097)\n",
      "tensor(6.8221)\n",
      "tensor(0.0265)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 89.209427\n",
      "Epoch 3447\n",
      "-------------------------------\n",
      "tensor(24.2988)\n",
      "tensor(14.1883)\n",
      "tensor(8.0073)\n",
      "tensor(0.1636)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 89.166748\n",
      "Epoch 3448\n",
      "-------------------------------\n",
      "tensor(23.3361)\n",
      "tensor(14.7697)\n",
      "tensor(8.2137)\n",
      "tensor(0.0120)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 89.130661\n",
      "Epoch 3449\n",
      "-------------------------------\n",
      "tensor(21.4528)\n",
      "tensor(14.2708)\n",
      "tensor(7.7183)\n",
      "tensor(0.2380)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 89.049828\n",
      "Epoch 3450\n",
      "-------------------------------\n",
      "tensor(23.2264)\n",
      "tensor(13.3582)\n",
      "tensor(7.2434)\n",
      "tensor(0.1750)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 88.967400\n",
      "Epoch 3451\n",
      "-------------------------------\n",
      "tensor(29.2457)\n",
      "tensor(14.0407)\n",
      "tensor(7.1052)\n",
      "tensor(0.2042)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 88.879982\n",
      "Epoch 3452\n",
      "-------------------------------\n",
      "tensor(75.4175)\n",
      "tensor(40.0579)\n",
      "tensor(10.0047)\n",
      "tensor(0.5736)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 88.983643\n",
      "Epoch 3453\n",
      "-------------------------------\n",
      "tensor(35.5023)\n",
      "tensor(14.7194)\n",
      "tensor(17.9477)\n",
      "tensor(0.6016)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 88.765862\n",
      "Epoch 3454\n",
      "-------------------------------\n",
      "tensor(21.5928)\n",
      "tensor(14.9392)\n",
      "tensor(8.8703)\n",
      "tensor(0.3594)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 88.719536\n",
      "Epoch 3455\n",
      "-------------------------------\n",
      "tensor(22.9814)\n",
      "tensor(17.7145)\n",
      "tensor(16.9762)\n",
      "tensor(0.6311)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 88.690224\n",
      "Epoch 3456\n",
      "-------------------------------\n",
      "tensor(22.1759)\n",
      "tensor(14.6921)\n",
      "tensor(6.8354)\n",
      "tensor(0.0515)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 88.631325\n",
      "Epoch 3457\n",
      "-------------------------------\n",
      "tensor(27.1859)\n",
      "tensor(13.8234)\n",
      "tensor(11.7442)\n",
      "tensor(0.3228)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 88.567299\n",
      "Epoch 3458\n",
      "-------------------------------\n",
      "tensor(27.1669)\n",
      "tensor(13.5326)\n",
      "tensor(8.6142)\n",
      "tensor(0.2450)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 88.499657\n",
      "Epoch 3459\n",
      "-------------------------------\n",
      "tensor(24.1405)\n",
      "tensor(13.6975)\n",
      "tensor(6.9410)\n",
      "tensor(0.0490)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 88.458801\n",
      "Epoch 3460\n",
      "-------------------------------\n",
      "tensor(24.5936)\n",
      "tensor(14.0105)\n",
      "tensor(8.7072)\n",
      "tensor(0.0605)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 88.425056\n",
      "Epoch 3461\n",
      "-------------------------------\n",
      "tensor(24.8092)\n",
      "tensor(14.0091)\n",
      "tensor(9.2158)\n",
      "tensor(0.0852)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 88.395508\n",
      "Epoch 3462\n",
      "-------------------------------\n",
      "tensor(17.3827)\n",
      "tensor(12.6212)\n",
      "tensor(8.7103)\n",
      "tensor(0.0646)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 88.372063\n",
      "Epoch 3463\n",
      "-------------------------------\n",
      "tensor(16.1016)\n",
      "tensor(12.1652)\n",
      "tensor(7.3678)\n",
      "tensor(1.5259e-05)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 88.346458\n",
      "Epoch 3464\n",
      "-------------------------------\n",
      "tensor(18.2543)\n",
      "tensor(12.0086)\n",
      "tensor(6.9370)\n",
      "tensor(0.1307)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 88.308624\n",
      "Epoch 3465\n",
      "-------------------------------\n",
      "tensor(71.5525)\n",
      "tensor(39.3896)\n",
      "tensor(9.6814)\n",
      "tensor(0.2889)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 88.357391\n",
      "Epoch 3466\n",
      "-------------------------------\n",
      "tensor(40.0352)\n",
      "tensor(15.5945)\n",
      "tensor(13.4220)\n",
      "tensor(0.3991)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 88.329109\n",
      "Epoch 3467\n",
      "-------------------------------\n",
      "tensor(34.2742)\n",
      "tensor(16.3276)\n",
      "tensor(6.9431)\n",
      "tensor(0.2094)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 88.264389\n",
      "Epoch 3468\n",
      "-------------------------------\n",
      "tensor(80.5405)\n",
      "tensor(34.9671)\n",
      "tensor(18.5032)\n",
      "tensor(0.0708)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 88.194328\n",
      "Epoch 3469\n",
      "-------------------------------\n",
      "tensor(27.1074)\n",
      "tensor(13.2979)\n",
      "tensor(9.9839)\n",
      "tensor(0.1163)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 88.184204\n",
      "Epoch 3470\n",
      "-------------------------------\n",
      "tensor(39.2029)\n",
      "tensor(16.3543)\n",
      "tensor(14.1605)\n",
      "tensor(0.2660)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 88.252602\n",
      "Epoch 3471\n",
      "-------------------------------\n",
      "tensor(28.9092)\n",
      "tensor(23.3556)\n",
      "tensor(21.1135)\n",
      "tensor(0.9697)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 88.211830\n",
      "Epoch 3472\n",
      "-------------------------------\n",
      "tensor(25.0455)\n",
      "tensor(15.7305)\n",
      "tensor(11.9966)\n",
      "tensor(0.1322)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 88.089630\n",
      "Epoch 3473\n",
      "-------------------------------\n",
      "tensor(29.5392)\n",
      "tensor(14.6792)\n",
      "tensor(19.7742)\n",
      "tensor(0.8216)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 88.004875\n",
      "Epoch 3474\n",
      "-------------------------------\n",
      "tensor(22.6627)\n",
      "tensor(13.5606)\n",
      "tensor(7.9276)\n",
      "tensor(0.2233)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 87.889572\n",
      "Epoch 3475\n",
      "-------------------------------\n",
      "tensor(28.7584)\n",
      "tensor(13.3384)\n",
      "tensor(16.3535)\n",
      "tensor(0.2281)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 87.798141\n",
      "Epoch 3476\n",
      "-------------------------------\n",
      "tensor(27.5660)\n",
      "tensor(14.4299)\n",
      "tensor(9.2035)\n",
      "tensor(0.2793)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 87.691872\n",
      "Epoch 3477\n",
      "-------------------------------\n",
      "tensor(30.9763)\n",
      "tensor(18.1530)\n",
      "tensor(14.3667)\n",
      "tensor(0.7760)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 87.594315\n",
      "Epoch 3478\n",
      "-------------------------------\n",
      "tensor(20.6692)\n",
      "tensor(14.9303)\n",
      "tensor(12.5553)\n",
      "tensor(0.7760)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 87.503670\n",
      "Epoch 3479\n",
      "-------------------------------\n",
      "tensor(68.3662)\n",
      "tensor(39.4505)\n",
      "tensor(8.8215)\n",
      "tensor(0.5443)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 87.500664\n",
      "Epoch 3480\n",
      "-------------------------------\n",
      "tensor(69.5215)\n",
      "tensor(38.2835)\n",
      "tensor(7.4257)\n",
      "tensor(0.3250)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 87.428795\n",
      "Epoch 3481\n",
      "-------------------------------\n",
      "tensor(24.7700)\n",
      "tensor(13.1587)\n",
      "tensor(6.9817)\n",
      "tensor(0.1884)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 87.416855\n",
      "Epoch 3482\n",
      "-------------------------------\n",
      "tensor(33.2468)\n",
      "tensor(14.8335)\n",
      "tensor(6.7995)\n",
      "tensor(0.0755)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 87.419792\n",
      "Epoch 3483\n",
      "-------------------------------\n",
      "tensor(31.7591)\n",
      "tensor(14.8395)\n",
      "tensor(6.5574)\n",
      "tensor(0.0483)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 87.413406\n",
      "Epoch 3484\n",
      "-------------------------------\n",
      "tensor(24.9809)\n",
      "tensor(14.7374)\n",
      "tensor(7.0622)\n",
      "tensor(0.2010)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 87.409561\n",
      "Epoch 3485\n",
      "-------------------------------\n",
      "tensor(23.7899)\n",
      "tensor(16.0567)\n",
      "tensor(9.5885)\n",
      "tensor(0.3304)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 87.399559\n",
      "Epoch 3486\n",
      "-------------------------------\n",
      "tensor(30.7663)\n",
      "tensor(17.7248)\n",
      "tensor(11.1000)\n",
      "tensor(0.3213)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 87.402977\n",
      "Epoch 3487\n",
      "-------------------------------\n",
      "tensor(26.9076)\n",
      "tensor(16.6127)\n",
      "tensor(9.4717)\n",
      "tensor(0.0976)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 87.375519\n",
      "Epoch 3488\n",
      "-------------------------------\n",
      "tensor(25.3660)\n",
      "tensor(14.5993)\n",
      "tensor(8.6393)\n",
      "tensor(0.0787)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 87.291496\n",
      "Epoch 3489\n",
      "-------------------------------\n",
      "tensor(29.1185)\n",
      "tensor(14.9077)\n",
      "tensor(6.7119)\n",
      "tensor(0.1493)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 87.177475\n",
      "Epoch 3490\n",
      "-------------------------------\n",
      "tensor(71.6114)\n",
      "tensor(35.0416)\n",
      "tensor(11.1997)\n",
      "tensor(0.2491)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 87.218674\n",
      "Epoch 3491\n",
      "-------------------------------\n",
      "tensor(44.0428)\n",
      "tensor(15.4146)\n",
      "tensor(19.4300)\n",
      "tensor(0.3973)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 87.027092\n",
      "Epoch 3492\n",
      "-------------------------------\n",
      "tensor(29.2821)\n",
      "tensor(14.8396)\n",
      "tensor(9.1607)\n",
      "tensor(0.1812)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 86.941383\n",
      "Epoch 3493\n",
      "-------------------------------\n",
      "tensor(35.7350)\n",
      "tensor(15.8891)\n",
      "tensor(17.6043)\n",
      "tensor(0.0649)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.899063\n",
      "Epoch 3494\n",
      "-------------------------------\n",
      "tensor(26.7720)\n",
      "tensor(13.5716)\n",
      "tensor(11.7024)\n",
      "tensor(0.4182)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 86.821167\n",
      "Epoch 3495\n",
      "-------------------------------\n",
      "tensor(23.2659)\n",
      "tensor(12.3596)\n",
      "tensor(12.2327)\n",
      "tensor(0.1321)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 86.750389\n",
      "Epoch 3496\n",
      "-------------------------------\n",
      "tensor(17.7847)\n",
      "tensor(13.7980)\n",
      "tensor(9.9943)\n",
      "tensor(0.3084)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 86.661507\n",
      "Epoch 3497\n",
      "-------------------------------\n",
      "tensor(27.1508)\n",
      "tensor(14.6459)\n",
      "tensor(12.3000)\n",
      "tensor(0.2072)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 86.593094\n",
      "Epoch 3498\n",
      "-------------------------------\n",
      "tensor(72.2910)\n",
      "tensor(37.3261)\n",
      "tensor(7.8432)\n",
      "tensor(0.1870)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 86.612877\n",
      "Epoch 3499\n",
      "-------------------------------\n",
      "tensor(21.0775)\n",
      "tensor(12.2443)\n",
      "tensor(10.4313)\n",
      "tensor(0.4375)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 86.500267\n",
      "Epoch 3500\n",
      "-------------------------------\n",
      "tensor(33.6199)\n",
      "tensor(15.3945)\n",
      "tensor(12.4539)\n",
      "tensor(0.4567)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 86.498245\n",
      "Epoch 3501\n",
      "-------------------------------\n",
      "tensor(31.2028)\n",
      "tensor(14.8919)\n",
      "tensor(10.8316)\n",
      "tensor(0.3666)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 86.486343\n",
      "Epoch 3502\n",
      "-------------------------------\n",
      "tensor(29.4463)\n",
      "tensor(13.9217)\n",
      "tensor(8.1506)\n",
      "tensor(0.2342)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 86.465965\n",
      "Epoch 3503\n",
      "-------------------------------\n",
      "tensor(19.7688)\n",
      "tensor(12.8795)\n",
      "tensor(6.7123)\n",
      "tensor(0.0374)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 86.448471\n",
      "Epoch 3504\n",
      "-------------------------------\n",
      "tensor(71.1667)\n",
      "tensor(34.4325)\n",
      "tensor(11.4821)\n",
      "tensor(0.2293)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 86.438133\n",
      "Epoch 3505\n",
      "-------------------------------\n",
      "tensor(20.2187)\n",
      "tensor(15.3679)\n",
      "tensor(11.9722)\n",
      "tensor(0.3980)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 86.420181\n",
      "Epoch 3506\n",
      "-------------------------------\n",
      "tensor(30.4759)\n",
      "tensor(15.9561)\n",
      "tensor(7.4660)\n",
      "tensor(0.3025)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 86.450439\n",
      "Epoch 3507\n",
      "-------------------------------\n",
      "tensor(34.1243)\n",
      "tensor(14.7993)\n",
      "tensor(9.0573)\n",
      "tensor(0.0685)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.438721\n",
      "Epoch 3508\n",
      "-------------------------------\n",
      "tensor(32.3933)\n",
      "tensor(16.3934)\n",
      "tensor(8.0243)\n",
      "tensor(0.2734)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 86.342094\n",
      "Epoch 3509\n",
      "-------------------------------\n",
      "tensor(35.0996)\n",
      "tensor(16.7970)\n",
      "tensor(14.8161)\n",
      "tensor(0.0980)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 86.277184\n",
      "Epoch 3510\n",
      "-------------------------------\n",
      "tensor(27.0044)\n",
      "tensor(14.3015)\n",
      "tensor(7.6109)\n",
      "tensor(0.0985)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 86.183647\n",
      "Epoch 3511\n",
      "-------------------------------\n",
      "tensor(64.8312)\n",
      "tensor(38.5688)\n",
      "tensor(11.9824)\n",
      "tensor(0.1724)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 86.243271\n",
      "Epoch 3512\n",
      "-------------------------------\n",
      "tensor(34.0999)\n",
      "tensor(13.5257)\n",
      "tensor(16.1451)\n",
      "tensor(0.0068)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 86.112869\n",
      "Epoch 3513\n",
      "-------------------------------\n",
      "tensor(29.6261)\n",
      "tensor(17.2975)\n",
      "tensor(20.0862)\n",
      "tensor(0.4517)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.070221\n",
      "Epoch 3514\n",
      "-------------------------------\n",
      "tensor(21.9008)\n",
      "tensor(14.7788)\n",
      "tensor(10.2486)\n",
      "tensor(0.1880)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 85.979233\n",
      "Epoch 3515\n",
      "-------------------------------\n",
      "tensor(30.1922)\n",
      "tensor(13.4516)\n",
      "tensor(17.5495)\n",
      "tensor(0.5771)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 85.885292\n",
      "Epoch 3516\n",
      "-------------------------------\n",
      "tensor(23.7120)\n",
      "tensor(13.3661)\n",
      "tensor(8.5030)\n",
      "tensor(0.0233)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 85.771759\n",
      "Epoch 3517\n",
      "-------------------------------\n",
      "tensor(19.8728)\n",
      "tensor(14.5894)\n",
      "tensor(13.1503)\n",
      "tensor(0.3943)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 85.673721\n",
      "Epoch 3518\n",
      "-------------------------------\n",
      "tensor(36.1809)\n",
      "tensor(16.5696)\n",
      "tensor(13.1429)\n",
      "tensor(0.2542)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 85.629395\n",
      "Epoch 3519\n",
      "-------------------------------\n",
      "tensor(37.9960)\n",
      "tensor(16.8058)\n",
      "tensor(8.7381)\n",
      "tensor(0.1025)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 85.596275\n",
      "Epoch 3520\n",
      "-------------------------------\n",
      "tensor(34.3234)\n",
      "tensor(16.2512)\n",
      "tensor(7.7694)\n",
      "tensor(0.3750)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 85.553047\n",
      "Epoch 3521\n",
      "-------------------------------\n",
      "tensor(70.6523)\n",
      "tensor(39.4736)\n",
      "tensor(8.6189)\n",
      "tensor(0.5103)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 85.525215\n",
      "Epoch 3522\n",
      "-------------------------------\n",
      "tensor(20.0313)\n",
      "tensor(13.1705)\n",
      "tensor(9.7573)\n",
      "tensor(0.5672)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 85.506538\n",
      "Epoch 3523\n",
      "-------------------------------\n",
      "tensor(23.1642)\n",
      "tensor(14.5107)\n",
      "tensor(10.3106)\n",
      "tensor(0.5352)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 85.509094\n",
      "Epoch 3524\n",
      "-------------------------------\n",
      "tensor(26.2526)\n",
      "tensor(13.4896)\n",
      "tensor(9.0053)\n",
      "tensor(0.3490)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 85.506012\n",
      "Epoch 3525\n",
      "-------------------------------\n",
      "tensor(25.6117)\n",
      "tensor(13.9982)\n",
      "tensor(6.5315)\n",
      "tensor(0.0543)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 85.480827\n",
      "Epoch 3526\n",
      "-------------------------------\n",
      "tensor(69.2713)\n",
      "tensor(34.0221)\n",
      "tensor(11.8142)\n",
      "tensor(0.5085)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 85.493217\n",
      "Epoch 3527\n",
      "-------------------------------\n",
      "tensor(29.6092)\n",
      "tensor(17.7949)\n",
      "tensor(8.5381)\n",
      "tensor(0.5072)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 85.444107\n",
      "Epoch 3528\n",
      "-------------------------------\n",
      "tensor(25.0971)\n",
      "tensor(14.8986)\n",
      "tensor(8.8033)\n",
      "tensor(0.1306)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 85.422623\n",
      "Epoch 3529\n",
      "-------------------------------\n",
      "tensor(23.4698)\n",
      "tensor(14.9841)\n",
      "tensor(10.0746)\n",
      "tensor(0.0275)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 85.349495\n",
      "Epoch 3530\n",
      "-------------------------------\n",
      "tensor(34.8961)\n",
      "tensor(17.7611)\n",
      "tensor(10.7325)\n",
      "tensor(0.1122)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 85.312569\n",
      "Epoch 3531\n",
      "-------------------------------\n",
      "tensor(31.8913)\n",
      "tensor(15.4798)\n",
      "tensor(6.6444)\n",
      "tensor(0.0165)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 85.175758\n",
      "Epoch 3532\n",
      "-------------------------------\n",
      "tensor(21.3230)\n",
      "tensor(14.1768)\n",
      "tensor(7.7106)\n",
      "tensor(0.2701)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 85.015167\n",
      "Epoch 3533\n",
      "-------------------------------\n",
      "tensor(74.9538)\n",
      "tensor(40.5092)\n",
      "tensor(8.3894)\n",
      "tensor(0.3902)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 85.138138\n",
      "Epoch 3534\n",
      "-------------------------------\n",
      "tensor(28.2350)\n",
      "tensor(13.6890)\n",
      "tensor(15.6802)\n",
      "tensor(0.4683)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 84.906502\n",
      "Epoch 3535\n",
      "-------------------------------\n",
      "tensor(16.3098)\n",
      "tensor(13.3354)\n",
      "tensor(6.9859)\n",
      "tensor(0.0712)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 84.851555\n",
      "Epoch 3536\n",
      "-------------------------------\n",
      "tensor(22.7464)\n",
      "tensor(15.9429)\n",
      "tensor(13.2519)\n",
      "tensor(0.3175)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 84.806740\n",
      "Epoch 3537\n",
      "-------------------------------\n",
      "tensor(26.7703)\n",
      "tensor(15.5023)\n",
      "tensor(7.7526)\n",
      "tensor(0.0945)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 84.748940\n",
      "Epoch 3538\n",
      "-------------------------------\n",
      "tensor(28.4419)\n",
      "tensor(14.5258)\n",
      "tensor(7.4700)\n",
      "tensor(0.1321)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 84.690872\n",
      "Epoch 3539\n",
      "-------------------------------\n",
      "tensor(26.7621)\n",
      "tensor(13.9968)\n",
      "tensor(7.8135)\n",
      "tensor(0.1773)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 84.651726\n",
      "Epoch 3540\n",
      "-------------------------------\n",
      "tensor(25.6835)\n",
      "tensor(14.0344)\n",
      "tensor(6.9468)\n",
      "tensor(0.1304)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 84.620689\n",
      "Epoch 3541\n",
      "-------------------------------\n",
      "tensor(29.1360)\n",
      "tensor(14.9443)\n",
      "tensor(6.6520)\n",
      "tensor(0.0711)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 84.605446\n",
      "Epoch 3542\n",
      "-------------------------------\n",
      "tensor(28.5696)\n",
      "tensor(15.0159)\n",
      "tensor(7.0780)\n",
      "tensor(0.0210)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 84.593033\n",
      "Epoch 3543\n",
      "-------------------------------\n",
      "tensor(27.1186)\n",
      "tensor(14.8368)\n",
      "tensor(8.0552)\n",
      "tensor(0.0218)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 84.571869\n",
      "Epoch 3544\n",
      "-------------------------------\n",
      "tensor(70.0450)\n",
      "tensor(35.4925)\n",
      "tensor(8.9974)\n",
      "tensor(0.0326)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 84.553436\n",
      "Epoch 3545\n",
      "-------------------------------\n",
      "tensor(31.4798)\n",
      "tensor(15.8272)\n",
      "tensor(6.5497)\n",
      "tensor(0.0578)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 84.528313\n",
      "Epoch 3546\n",
      "-------------------------------\n",
      "tensor(30.7712)\n",
      "tensor(14.9457)\n",
      "tensor(9.2476)\n",
      "tensor(0.1932)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 84.503006\n",
      "Epoch 3547\n",
      "-------------------------------\n",
      "tensor(64.7854)\n",
      "tensor(36.8848)\n",
      "tensor(7.0771)\n",
      "tensor(0.1763)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 84.483582\n",
      "Epoch 3548\n",
      "-------------------------------\n",
      "tensor(27.2743)\n",
      "tensor(13.9837)\n",
      "tensor(8.4125)\n",
      "tensor(0.0543)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 84.449295\n",
      "Epoch 3549\n",
      "-------------------------------\n",
      "tensor(23.1224)\n",
      "tensor(16.3232)\n",
      "tensor(9.1422)\n",
      "tensor(0.2863)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 84.429718\n",
      "Epoch 3550\n",
      "-------------------------------\n",
      "tensor(28.4432)\n",
      "tensor(17.2627)\n",
      "tensor(11.0530)\n",
      "tensor(0.2391)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 84.390373\n",
      "Epoch 3551\n",
      "-------------------------------\n",
      "tensor(29.2399)\n",
      "tensor(15.0946)\n",
      "tensor(8.1312)\n",
      "tensor(0.2491)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 84.299667\n",
      "Epoch 3552\n",
      "-------------------------------\n",
      "tensor(29.8534)\n",
      "tensor(16.3177)\n",
      "tensor(6.8034)\n",
      "tensor(0.1876)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 84.240387\n",
      "Epoch 3553\n",
      "-------------------------------\n",
      "tensor(32.0888)\n",
      "tensor(15.9935)\n",
      "tensor(10.1667)\n",
      "tensor(0.0236)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 84.146019\n",
      "Epoch 3554\n",
      "-------------------------------\n",
      "tensor(22.6723)\n",
      "tensor(13.7635)\n",
      "tensor(7.3739)\n",
      "tensor(0.2761)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 84.001724\n",
      "Epoch 3555\n",
      "-------------------------------\n",
      "tensor(17.0109)\n",
      "tensor(12.7389)\n",
      "tensor(9.8162)\n",
      "tensor(0.4634)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 83.895157\n",
      "Epoch 3556\n",
      "-------------------------------\n",
      "tensor(67.2751)\n",
      "tensor(38.8092)\n",
      "tensor(7.7426)\n",
      "tensor(0.3125)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 84.037689\n",
      "Epoch 3557\n",
      "-------------------------------\n",
      "tensor(65.6431)\n",
      "tensor(38.1304)\n",
      "tensor(10.0935)\n",
      "tensor(0.1036)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 83.844559\n",
      "Epoch 3558\n",
      "-------------------------------\n",
      "tensor(31.6895)\n",
      "tensor(14.1525)\n",
      "tensor(12.1514)\n",
      "tensor(0.0516)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 83.778595\n",
      "Epoch 3559\n",
      "-------------------------------\n",
      "tensor(23.4766)\n",
      "tensor(14.9903)\n",
      "tensor(7.7228)\n",
      "tensor(0.2051)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 83.764130\n",
      "Epoch 3560\n",
      "-------------------------------\n",
      "tensor(29.6131)\n",
      "tensor(17.2082)\n",
      "tensor(7.8256)\n",
      "tensor(0.2874)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 83.756073\n",
      "Epoch 3561\n",
      "-------------------------------\n",
      "tensor(27.9689)\n",
      "tensor(17.5451)\n",
      "tensor(10.0722)\n",
      "tensor(0.2978)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 83.750618\n",
      "Epoch 3562\n",
      "-------------------------------\n",
      "tensor(28.4834)\n",
      "tensor(17.6241)\n",
      "tensor(11.4302)\n",
      "tensor(0.2613)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 83.736778\n",
      "Epoch 3563\n",
      "-------------------------------\n",
      "tensor(21.6313)\n",
      "tensor(15.9799)\n",
      "tensor(11.6660)\n",
      "tensor(0.1709)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 83.719315\n",
      "Epoch 3564\n",
      "-------------------------------\n",
      "tensor(27.9194)\n",
      "tensor(16.1033)\n",
      "tensor(9.6187)\n",
      "tensor(0.0011)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 83.707260\n",
      "Epoch 3565\n",
      "-------------------------------\n",
      "tensor(33.6442)\n",
      "tensor(17.1069)\n",
      "tensor(7.6063)\n",
      "tensor(0.2462)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 83.689178\n",
      "Epoch 3566\n",
      "-------------------------------\n",
      "tensor(35.3237)\n",
      "tensor(16.9612)\n",
      "tensor(9.4739)\n",
      "tensor(0.4105)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 83.642921\n",
      "Epoch 3567\n",
      "-------------------------------\n",
      "tensor(22.9072)\n",
      "tensor(14.6617)\n",
      "tensor(7.3001)\n",
      "tensor(0.2832)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 83.529510\n",
      "Epoch 3568\n",
      "-------------------------------\n",
      "tensor(76.2781)\n",
      "tensor(38.0532)\n",
      "tensor(10.6071)\n",
      "tensor(0.0622)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 83.645889\n",
      "Epoch 3569\n",
      "-------------------------------\n",
      "tensor(31.2270)\n",
      "tensor(13.8449)\n",
      "tensor(15.4639)\n",
      "tensor(0.2959)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 83.435692\n",
      "Epoch 3570\n",
      "-------------------------------\n",
      "tensor(22.9170)\n",
      "tensor(13.2926)\n",
      "tensor(7.4599)\n",
      "tensor(0.1783)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 83.349480\n",
      "Epoch 3571\n",
      "-------------------------------\n",
      "tensor(30.4723)\n",
      "tensor(15.7017)\n",
      "tensor(17.4456)\n",
      "tensor(0.0875)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 83.271362\n",
      "Epoch 3572\n",
      "-------------------------------\n",
      "tensor(34.2713)\n",
      "tensor(15.4509)\n",
      "tensor(8.2966)\n",
      "tensor(0.2661)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 83.162560\n",
      "Epoch 3573\n",
      "-------------------------------\n",
      "tensor(63.1906)\n",
      "tensor(36.5825)\n",
      "tensor(8.2325)\n",
      "tensor(0.1225)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 83.203949\n",
      "Epoch 3574\n",
      "-------------------------------\n",
      "tensor(35.4902)\n",
      "tensor(16.1534)\n",
      "tensor(11.9637)\n",
      "tensor(0.3090)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 83.037766\n",
      "Epoch 3575\n",
      "-------------------------------\n",
      "tensor(34.9513)\n",
      "tensor(20.8099)\n",
      "tensor(14.8578)\n",
      "tensor(0.6343)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 82.994537\n",
      "Epoch 3576\n",
      "-------------------------------\n",
      "tensor(28.9819)\n",
      "tensor(17.0733)\n",
      "tensor(17.5071)\n",
      "tensor(0.1581)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 82.939926\n",
      "Epoch 3577\n",
      "-------------------------------\n",
      "tensor(22.8769)\n",
      "tensor(13.7344)\n",
      "tensor(9.4571)\n",
      "tensor(0.4237)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 82.876038\n",
      "Epoch 3578\n",
      "-------------------------------\n",
      "tensor(28.5476)\n",
      "tensor(13.9949)\n",
      "tensor(13.2504)\n",
      "tensor(0.5075)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 82.826118\n",
      "Epoch 3579\n",
      "-------------------------------\n",
      "tensor(26.7134)\n",
      "tensor(13.7291)\n",
      "tensor(10.4850)\n",
      "tensor(0.2773)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 82.775780\n",
      "Epoch 3580\n",
      "-------------------------------\n",
      "tensor(33.2488)\n",
      "tensor(15.4181)\n",
      "tensor(7.2232)\n",
      "tensor(0.0472)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 82.750443\n",
      "Epoch 3581\n",
      "-------------------------------\n",
      "tensor(35.7215)\n",
      "tensor(16.2916)\n",
      "tensor(7.5354)\n",
      "tensor(0.0786)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 82.733681\n",
      "Epoch 3582\n",
      "-------------------------------\n",
      "tensor(28.3885)\n",
      "tensor(15.1267)\n",
      "tensor(9.2856)\n",
      "tensor(0.1253)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 82.712509\n",
      "Epoch 3583\n",
      "-------------------------------\n",
      "tensor(19.5784)\n",
      "tensor(13.5102)\n",
      "tensor(10.9590)\n",
      "tensor(0.1056)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 82.694115\n",
      "Epoch 3584\n",
      "-------------------------------\n",
      "tensor(73.8848)\n",
      "tensor(36.5596)\n",
      "tensor(10.5960)\n",
      "tensor(0.0466)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 82.716789\n",
      "Epoch 3585\n",
      "-------------------------------\n",
      "tensor(22.0670)\n",
      "tensor(13.4269)\n",
      "tensor(8.2756)\n",
      "tensor(0.3932)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 82.651024\n",
      "Epoch 3586\n",
      "-------------------------------\n",
      "tensor(38.0653)\n",
      "tensor(16.0393)\n",
      "tensor(16.9362)\n",
      "tensor(0.6740)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 82.617508\n",
      "Epoch 3587\n",
      "-------------------------------\n",
      "tensor(67.8680)\n",
      "tensor(38.3965)\n",
      "tensor(8.9498)\n",
      "tensor(0.3349)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 82.604530\n",
      "Epoch 3588\n",
      "-------------------------------\n",
      "tensor(21.4680)\n",
      "tensor(17.4784)\n",
      "tensor(8.7486)\n",
      "tensor(0.3672)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 82.585693\n",
      "Epoch 3589\n",
      "-------------------------------\n",
      "tensor(20.9397)\n",
      "tensor(21.3360)\n",
      "tensor(13.9267)\n",
      "tensor(0.7296)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 82.581818\n",
      "Epoch 3590\n",
      "-------------------------------\n",
      "tensor(35.3680)\n",
      "tensor(18.7892)\n",
      "tensor(7.9359)\n",
      "tensor(0.2131)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 82.533073\n",
      "Epoch 3591\n",
      "-------------------------------\n",
      "tensor(29.9270)\n",
      "tensor(16.1642)\n",
      "tensor(9.0175)\n",
      "tensor(0.2146)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 82.447746\n",
      "Epoch 3592\n",
      "-------------------------------\n",
      "tensor(26.4967)\n",
      "tensor(15.4314)\n",
      "tensor(9.5683)\n",
      "tensor(0.1358)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 82.301292\n",
      "Epoch 3593\n",
      "-------------------------------\n",
      "tensor(28.4360)\n",
      "tensor(14.7744)\n",
      "tensor(7.5763)\n",
      "tensor(0.0994)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 82.188553\n",
      "Epoch 3594\n",
      "-------------------------------\n",
      "tensor(29.8176)\n",
      "tensor(14.9060)\n",
      "tensor(8.9809)\n",
      "tensor(0.2162)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 82.135910\n",
      "Epoch 3595\n",
      "-------------------------------\n",
      "tensor(75.4572)\n",
      "tensor(41.3190)\n",
      "tensor(9.3047)\n",
      "tensor(0.5055)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 82.261406\n",
      "Epoch 3596\n",
      "-------------------------------\n",
      "tensor(27.1581)\n",
      "tensor(14.6871)\n",
      "tensor(15.0273)\n",
      "tensor(0.6588)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 81.968430\n",
      "Epoch 3597\n",
      "-------------------------------\n",
      "tensor(18.8342)\n",
      "tensor(12.5828)\n",
      "tensor(8.5671)\n",
      "tensor(0.2571)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 81.927467\n",
      "Epoch 3598\n",
      "-------------------------------\n",
      "tensor(21.4993)\n",
      "tensor(15.0399)\n",
      "tensor(8.4760)\n",
      "tensor(0.1782)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 81.895576\n",
      "Epoch 3599\n",
      "-------------------------------\n",
      "tensor(21.9361)\n",
      "tensor(16.2023)\n",
      "tensor(11.4758)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 81.864334\n",
      "Epoch 3600\n",
      "-------------------------------\n",
      "tensor(27.1080)\n",
      "tensor(16.7111)\n",
      "tensor(10.5091)\n",
      "tensor(0.2870)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 81.847939\n",
      "Epoch 3601\n",
      "-------------------------------\n",
      "tensor(27.4192)\n",
      "tensor(16.1536)\n",
      "tensor(8.6813)\n",
      "tensor(0.1885)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 81.829361\n",
      "Epoch 3602\n",
      "-------------------------------\n",
      "tensor(27.0478)\n",
      "tensor(15.4222)\n",
      "tensor(7.1984)\n",
      "tensor(0.0740)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 81.805519\n",
      "Epoch 3603\n",
      "-------------------------------\n",
      "tensor(64.7377)\n",
      "tensor(35.7405)\n",
      "tensor(6.6456)\n",
      "tensor(0.0758)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 81.791153\n",
      "Epoch 3604\n",
      "-------------------------------\n",
      "tensor(25.4932)\n",
      "tensor(13.1383)\n",
      "tensor(9.9985)\n",
      "tensor(0.2629)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 81.761505\n",
      "Epoch 3605\n",
      "-------------------------------\n",
      "tensor(32.9617)\n",
      "tensor(14.0360)\n",
      "tensor(12.9690)\n",
      "tensor(0.3472)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 81.738693\n",
      "Epoch 3606\n",
      "-------------------------------\n",
      "tensor(30.3101)\n",
      "tensor(14.9466)\n",
      "tensor(6.8674)\n",
      "tensor(0.1079)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 81.709457\n",
      "Epoch 3607\n",
      "-------------------------------\n",
      "tensor(80.3289)\n",
      "tensor(35.4374)\n",
      "tensor(15.9288)\n",
      "tensor(0.2534)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 81.753418\n",
      "Epoch 3608\n",
      "-------------------------------\n",
      "tensor(27.0686)\n",
      "tensor(15.0493)\n",
      "tensor(6.4883)\n",
      "tensor(0.0239)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 81.627296\n",
      "Epoch 3609\n",
      "-------------------------------\n",
      "tensor(38.6702)\n",
      "tensor(16.5536)\n",
      "tensor(13.0344)\n",
      "tensor(0.3122)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 81.625542\n",
      "Epoch 3610\n",
      "-------------------------------\n",
      "tensor(29.3259)\n",
      "tensor(18.1126)\n",
      "tensor(11.7320)\n",
      "tensor(0.1324)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 81.564453\n",
      "Epoch 3611\n",
      "-------------------------------\n",
      "tensor(23.6253)\n",
      "tensor(17.0927)\n",
      "tensor(13.6236)\n",
      "tensor(0.1808)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 81.508514\n",
      "Epoch 3612\n",
      "-------------------------------\n",
      "tensor(30.6087)\n",
      "tensor(14.0853)\n",
      "tensor(15.4121)\n",
      "tensor(0.3501)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 81.429436\n",
      "Epoch 3613\n",
      "-------------------------------\n",
      "tensor(29.0138)\n",
      "tensor(15.0480)\n",
      "tensor(7.2013)\n",
      "tensor(0.0416)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 81.331337\n",
      "Epoch 3614\n",
      "-------------------------------\n",
      "tensor(80.7636)\n",
      "tensor(36.3690)\n",
      "tensor(15.9825)\n",
      "tensor(0.0136)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 81.205101\n",
      "Epoch 3615\n",
      "-------------------------------\n",
      "tensor(32.4933)\n",
      "tensor(14.5394)\n",
      "tensor(20.0848)\n",
      "tensor(0.6817)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 81.160042\n",
      "Epoch 3616\n",
      "-------------------------------\n",
      "tensor(30.1581)\n",
      "tensor(14.1095)\n",
      "tensor(12.6762)\n",
      "tensor(0.2597)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 81.112839\n",
      "Epoch 3617\n",
      "-------------------------------\n",
      "tensor(24.5550)\n",
      "tensor(17.5897)\n",
      "tensor(13.3865)\n",
      "tensor(0.4072)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 81.047745\n",
      "Epoch 3618\n",
      "-------------------------------\n",
      "tensor(29.7578)\n",
      "tensor(18.9609)\n",
      "tensor(17.9192)\n",
      "tensor(0.4958)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 80.997185\n",
      "Epoch 3619\n",
      "-------------------------------\n",
      "tensor(31.0255)\n",
      "tensor(17.6410)\n",
      "tensor(11.6050)\n",
      "tensor(0.2301)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 80.961273\n",
      "Epoch 3620\n",
      "-------------------------------\n",
      "tensor(30.6919)\n",
      "tensor(16.2192)\n",
      "tensor(7.0429)\n",
      "tensor(0.0371)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 80.931557\n",
      "Epoch 3621\n",
      "-------------------------------\n",
      "tensor(28.7151)\n",
      "tensor(15.1116)\n",
      "tensor(7.1817)\n",
      "tensor(0.1993)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 80.905846\n",
      "Epoch 3622\n",
      "-------------------------------\n",
      "tensor(29.4188)\n",
      "tensor(14.9198)\n",
      "tensor(8.2072)\n",
      "tensor(0.2875)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 80.879929\n",
      "Epoch 3623\n",
      "-------------------------------\n",
      "tensor(19.7892)\n",
      "tensor(13.0050)\n",
      "tensor(8.5033)\n",
      "tensor(0.3216)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 80.853355\n",
      "Epoch 3624\n",
      "-------------------------------\n",
      "tensor(17.4688)\n",
      "tensor(12.7667)\n",
      "tensor(7.4273)\n",
      "tensor(0.2638)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 80.816803\n",
      "Epoch 3625\n",
      "-------------------------------\n",
      "tensor(71.5191)\n",
      "tensor(37.4355)\n",
      "tensor(7.0525)\n",
      "tensor(0.0987)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 80.919678\n",
      "Epoch 3626\n",
      "-------------------------------\n",
      "tensor(71.2568)\n",
      "tensor(37.4355)\n",
      "tensor(6.9593)\n",
      "tensor(0.0489)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 80.894150\n",
      "Epoch 3627\n",
      "-------------------------------\n",
      "tensor(44.2775)\n",
      "tensor(16.4033)\n",
      "tensor(12.9217)\n",
      "tensor(0.0231)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.803474\n",
      "Epoch 3628\n",
      "-------------------------------\n",
      "tensor(25.5233)\n",
      "tensor(16.3094)\n",
      "tensor(7.2384)\n",
      "tensor(0.1187)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 80.765488\n",
      "Epoch 3629\n",
      "-------------------------------\n",
      "tensor(31.4304)\n",
      "tensor(19.4992)\n",
      "tensor(19.0472)\n",
      "tensor(0.1736)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 80.774025\n",
      "Epoch 3630\n",
      "-------------------------------\n",
      "tensor(23.1030)\n",
      "tensor(15.4829)\n",
      "tensor(8.6940)\n",
      "tensor(0.2082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 80.692558\n",
      "Epoch 3631\n",
      "-------------------------------\n",
      "tensor(37.1794)\n",
      "tensor(16.4705)\n",
      "tensor(12.8002)\n",
      "tensor(0.0236)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 80.623932\n",
      "Epoch 3632\n",
      "-------------------------------\n",
      "tensor(34.2219)\n",
      "tensor(18.1598)\n",
      "tensor(17.6955)\n",
      "tensor(0.4750)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 80.496971\n",
      "Epoch 3633\n",
      "-------------------------------\n",
      "tensor(26.4354)\n",
      "tensor(15.5121)\n",
      "tensor(10.0877)\n",
      "tensor(0.5119)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.337654\n",
      "Epoch 3634\n",
      "-------------------------------\n",
      "tensor(35.3575)\n",
      "tensor(20.2442)\n",
      "tensor(19.7815)\n",
      "tensor(1.1431)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 80.252876\n",
      "Epoch 3635\n",
      "-------------------------------\n",
      "tensor(74.5726)\n",
      "tensor(40.7340)\n",
      "tensor(8.2761)\n",
      "tensor(0.4063)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 80.435875\n",
      "Epoch 3636\n",
      "-------------------------------\n",
      "tensor(68.1045)\n",
      "tensor(36.7576)\n",
      "tensor(8.1545)\n",
      "tensor(0.2172)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 80.160248\n",
      "Epoch 3637\n",
      "-------------------------------\n",
      "tensor(30.8788)\n",
      "tensor(16.1983)\n",
      "tensor(12.4348)\n",
      "tensor(0.3314)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 80.113983\n",
      "Epoch 3638\n",
      "-------------------------------\n",
      "tensor(26.0012)\n",
      "tensor(17.8792)\n",
      "tensor(8.4663)\n",
      "tensor(0.3792)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 80.129448\n",
      "Epoch 3639\n",
      "-------------------------------\n",
      "tensor(22.7884)\n",
      "tensor(18.9731)\n",
      "tensor(10.0266)\n",
      "tensor(0.3753)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 80.122589\n",
      "Epoch 3640\n",
      "-------------------------------\n",
      "tensor(23.9879)\n",
      "tensor(19.0619)\n",
      "tensor(12.3689)\n",
      "tensor(0.3192)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 80.105934\n",
      "Epoch 3641\n",
      "-------------------------------\n",
      "tensor(31.2266)\n",
      "tensor(19.6646)\n",
      "tensor(12.3739)\n",
      "tensor(0.2502)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 80.091766\n",
      "Epoch 3642\n",
      "-------------------------------\n",
      "tensor(30.9148)\n",
      "tensor(18.9532)\n",
      "tensor(11.3384)\n",
      "tensor(0.1727)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 80.073364\n",
      "Epoch 3643\n",
      "-------------------------------\n",
      "tensor(29.1902)\n",
      "tensor(17.6658)\n",
      "tensor(9.3572)\n",
      "tensor(0.0663)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 80.038986\n",
      "Epoch 3644\n",
      "-------------------------------\n",
      "tensor(27.1283)\n",
      "tensor(15.9462)\n",
      "tensor(7.2495)\n",
      "tensor(0.0759)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 79.991089\n",
      "Epoch 3645\n",
      "-------------------------------\n",
      "tensor(31.9293)\n",
      "tensor(15.6983)\n",
      "tensor(7.8932)\n",
      "tensor(0.1975)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 79.949707\n",
      "Epoch 3646\n",
      "-------------------------------\n",
      "tensor(36.6036)\n",
      "tensor(16.5102)\n",
      "tensor(8.0658)\n",
      "tensor(0.2121)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.916229\n",
      "Epoch 3647\n",
      "-------------------------------\n",
      "tensor(78.9491)\n",
      "tensor(40.1519)\n",
      "tensor(10.4238)\n",
      "tensor(0.2257)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.922729\n",
      "Epoch 3648\n",
      "-------------------------------\n",
      "tensor(26.8869)\n",
      "tensor(16.3451)\n",
      "tensor(12.5565)\n",
      "tensor(0.6344)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 79.827934\n",
      "Epoch 3649\n",
      "-------------------------------\n",
      "tensor(35.2702)\n",
      "tensor(18.8946)\n",
      "tensor(12.0867)\n",
      "tensor(0.6795)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 79.719574\n",
      "Epoch 3650\n",
      "-------------------------------\n",
      "tensor(77.3843)\n",
      "tensor(36.7152)\n",
      "tensor(14.4901)\n",
      "tensor(0.1210)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 79.685822\n",
      "Epoch 3651\n",
      "-------------------------------\n",
      "tensor(43.0406)\n",
      "tensor(14.6877)\n",
      "tensor(20.8909)\n",
      "tensor(0.1715)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 79.711250\n",
      "Epoch 3652\n",
      "-------------------------------\n",
      "tensor(23.9555)\n",
      "tensor(23.2499)\n",
      "tensor(12.4067)\n",
      "tensor(0.8189)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 79.703667\n",
      "Epoch 3653\n",
      "-------------------------------\n",
      "tensor(36.1021)\n",
      "tensor(26.5142)\n",
      "tensor(24.9938)\n",
      "tensor(0.8652)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.652458\n",
      "Epoch 3654\n",
      "-------------------------------\n",
      "tensor(33.2138)\n",
      "tensor(16.2400)\n",
      "tensor(11.6267)\n",
      "tensor(0.3413)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.524391\n",
      "Epoch 3655\n",
      "-------------------------------\n",
      "tensor(37.2359)\n",
      "tensor(16.3756)\n",
      "tensor(13.4902)\n",
      "tensor(0.4830)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 79.403580\n",
      "Epoch 3656\n",
      "-------------------------------\n",
      "tensor(38.4772)\n",
      "tensor(18.1460)\n",
      "tensor(10.7287)\n",
      "tensor(0.1168)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 79.272514\n",
      "Epoch 3657\n",
      "-------------------------------\n",
      "tensor(28.9020)\n",
      "tensor(16.0849)\n",
      "tensor(16.8499)\n",
      "tensor(0.1647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 79.218666\n",
      "Epoch 3658\n",
      "-------------------------------\n",
      "tensor(20.2510)\n",
      "tensor(14.8683)\n",
      "tensor(8.8817)\n",
      "tensor(0.2602)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 79.154114\n",
      "Epoch 3659\n",
      "-------------------------------\n",
      "tensor(18.2636)\n",
      "tensor(14.4996)\n",
      "tensor(11.5752)\n",
      "tensor(0.5962)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 79.107452\n",
      "Epoch 3660\n",
      "-------------------------------\n",
      "tensor(65.2576)\n",
      "tensor(42.9601)\n",
      "tensor(14.1478)\n",
      "tensor(0.7044)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 79.118225\n",
      "Epoch 3661\n",
      "-------------------------------\n",
      "tensor(64.2356)\n",
      "tensor(42.7162)\n",
      "tensor(15.1627)\n",
      "tensor(0.6765)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 79.079498\n",
      "Epoch 3662\n",
      "-------------------------------\n",
      "tensor(25.3878)\n",
      "tensor(14.5846)\n",
      "tensor(15.1018)\n",
      "tensor(0.5824)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 79.049141\n",
      "Epoch 3663\n",
      "-------------------------------\n",
      "tensor(30.0821)\n",
      "tensor(14.8357)\n",
      "tensor(11.8793)\n",
      "tensor(0.3861)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 79.039345\n",
      "Epoch 3664\n",
      "-------------------------------\n",
      "tensor(26.7795)\n",
      "tensor(14.8711)\n",
      "tensor(6.7804)\n",
      "tensor(0.0309)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 79.013252\n",
      "Epoch 3665\n",
      "-------------------------------\n",
      "tensor(27.9332)\n",
      "tensor(17.9260)\n",
      "tensor(15.6179)\n",
      "tensor(0.4102)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 79.004005\n",
      "Epoch 3666\n",
      "-------------------------------\n",
      "tensor(34.5071)\n",
      "tensor(21.1467)\n",
      "tensor(20.9122)\n",
      "tensor(0.5475)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.990768\n",
      "Epoch 3667\n",
      "-------------------------------\n",
      "tensor(36.1151)\n",
      "tensor(18.8684)\n",
      "tensor(9.4900)\n",
      "tensor(0.0482)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 78.965088\n",
      "Epoch 3668\n",
      "-------------------------------\n",
      "tensor(45.6748)\n",
      "tensor(16.7259)\n",
      "tensor(16.1510)\n",
      "tensor(0.4189)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 78.937912\n",
      "Epoch 3669\n",
      "-------------------------------\n",
      "tensor(71.2417)\n",
      "tensor(34.3290)\n",
      "tensor(9.9601)\n",
      "tensor(0.2097)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 78.786247\n",
      "Epoch 3670\n",
      "-------------------------------\n",
      "tensor(29.0368)\n",
      "tensor(21.5499)\n",
      "tensor(10.3480)\n",
      "tensor(0.6466)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 78.773987\n",
      "Epoch 3671\n",
      "-------------------------------\n",
      "tensor(20.8644)\n",
      "tensor(18.6922)\n",
      "tensor(8.4695)\n",
      "tensor(0.4319)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 78.738167\n",
      "Epoch 3672\n",
      "-------------------------------\n",
      "tensor(21.8299)\n",
      "tensor(15.3256)\n",
      "tensor(7.0270)\n",
      "tensor(0.2284)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 78.650238\n",
      "Epoch 3673\n",
      "-------------------------------\n",
      "tensor(18.5578)\n",
      "tensor(14.2324)\n",
      "tensor(8.5964)\n",
      "tensor(0.5300)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 78.553955\n",
      "Epoch 3674\n",
      "-------------------------------\n",
      "tensor(70.7920)\n",
      "tensor(41.2343)\n",
      "tensor(9.5334)\n",
      "tensor(0.4338)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.514999\n",
      "Epoch 3675\n",
      "-------------------------------\n",
      "tensor(38.7097)\n",
      "tensor(16.0676)\n",
      "tensor(21.5657)\n",
      "tensor(0.3946)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 78.438919\n",
      "Epoch 3676\n",
      "-------------------------------\n",
      "tensor(27.5936)\n",
      "tensor(15.5040)\n",
      "tensor(7.8783)\n",
      "tensor(0.0720)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 78.353958\n",
      "Epoch 3677\n",
      "-------------------------------\n",
      "tensor(28.9418)\n",
      "tensor(16.6787)\n",
      "tensor(16.6965)\n",
      "tensor(0.1472)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 78.287849\n",
      "Epoch 3678\n",
      "-------------------------------\n",
      "tensor(30.1722)\n",
      "tensor(16.9224)\n",
      "tensor(12.1780)\n",
      "tensor(0.1303)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 78.251968\n",
      "Epoch 3679\n",
      "-------------------------------\n",
      "tensor(32.4154)\n",
      "tensor(16.9354)\n",
      "tensor(8.1597)\n",
      "tensor(0.3531)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 78.212128\n",
      "Epoch 3680\n",
      "-------------------------------\n",
      "tensor(33.9609)\n",
      "tensor(16.6034)\n",
      "tensor(9.1386)\n",
      "tensor(0.3958)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 78.182800\n",
      "Epoch 3681\n",
      "-------------------------------\n",
      "tensor(37.6792)\n",
      "tensor(17.2002)\n",
      "tensor(9.0126)\n",
      "tensor(0.3393)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 78.149719\n",
      "Epoch 3682\n",
      "-------------------------------\n",
      "tensor(33.7398)\n",
      "tensor(16.4295)\n",
      "tensor(7.7289)\n",
      "tensor(0.2345)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 78.122078\n",
      "Epoch 3683\n",
      "-------------------------------\n",
      "tensor(24.6153)\n",
      "tensor(15.2793)\n",
      "tensor(6.5730)\n",
      "tensor(0.0624)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 78.102875\n",
      "Epoch 3684\n",
      "-------------------------------\n",
      "tensor(25.6143)\n",
      "tensor(16.4802)\n",
      "tensor(9.3485)\n",
      "tensor(0.1893)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 78.084175\n",
      "Epoch 3685\n",
      "-------------------------------\n",
      "tensor(24.2652)\n",
      "tensor(16.9631)\n",
      "tensor(13.5709)\n",
      "tensor(0.3885)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 78.041000\n",
      "Epoch 3686\n",
      "-------------------------------\n",
      "tensor(72.1758)\n",
      "tensor(35.7532)\n",
      "tensor(9.9450)\n",
      "tensor(0.1736)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.244690\n",
      "Epoch 3687\n",
      "-------------------------------\n",
      "tensor(63.8254)\n",
      "tensor(40.6112)\n",
      "tensor(18.7719)\n",
      "tensor(0.4261)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 78.229027\n",
      "Epoch 3688\n",
      "-------------------------------\n",
      "tensor(55.2437)\n",
      "tensor(15.7376)\n",
      "tensor(27.1369)\n",
      "tensor(0.4168)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 78.024712\n",
      "Epoch 3689\n",
      "-------------------------------\n",
      "tensor(38.3916)\n",
      "tensor(23.9236)\n",
      "tensor(26.6464)\n",
      "tensor(0.7305)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 77.980721\n",
      "Epoch 3690\n",
      "-------------------------------\n",
      "tensor(41.9827)\n",
      "tensor(22.7356)\n",
      "tensor(28.4873)\n",
      "tensor(0.4626)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 77.956139\n",
      "Epoch 3691\n",
      "-------------------------------\n",
      "tensor(59.2522)\n",
      "tensor(18.2409)\n",
      "tensor(34.6914)\n",
      "tensor(0.9671)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 77.898041\n",
      "Epoch 3692\n",
      "-------------------------------\n",
      "tensor(20.6816)\n",
      "tensor(16.3850)\n",
      "tensor(7.9166)\n",
      "tensor(0.2490)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 77.697533\n",
      "Epoch 3693\n",
      "-------------------------------\n",
      "tensor(49.3318)\n",
      "tensor(21.5815)\n",
      "tensor(32.6497)\n",
      "tensor(0.7614)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.655579\n",
      "Epoch 3694\n",
      "-------------------------------\n",
      "tensor(67.8298)\n",
      "tensor(46.3925)\n",
      "tensor(19.1839)\n",
      "tensor(1.0958)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 77.588974\n",
      "Epoch 3695\n",
      "-------------------------------\n",
      "tensor(56.6213)\n",
      "tensor(22.0645)\n",
      "tensor(40.2217)\n",
      "tensor(1.4221)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 77.516937\n",
      "Epoch 3696\n",
      "-------------------------------\n",
      "tensor(31.4201)\n",
      "tensor(17.9856)\n",
      "tensor(13.9781)\n",
      "tensor(0.3264)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 77.394157\n",
      "Epoch 3697\n",
      "-------------------------------\n",
      "tensor(46.3309)\n",
      "tensor(25.0414)\n",
      "tensor(35.7790)\n",
      "tensor(1.0308)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 77.393723\n",
      "Epoch 3698\n",
      "-------------------------------\n",
      "tensor(31.0853)\n",
      "tensor(19.6115)\n",
      "tensor(16.8074)\n",
      "tensor(0.4527)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 77.289551\n",
      "Epoch 3699\n",
      "-------------------------------\n",
      "tensor(25.5981)\n",
      "tensor(14.3228)\n",
      "tensor(8.9880)\n",
      "tensor(0.2168)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 77.245705\n",
      "Epoch 3700\n",
      "-------------------------------\n",
      "tensor(33.0102)\n",
      "tensor(13.9824)\n",
      "tensor(16.5269)\n",
      "tensor(0.5061)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 77.224609\n",
      "Epoch 3701\n",
      "-------------------------------\n",
      "tensor(30.2029)\n",
      "tensor(13.3698)\n",
      "tensor(16.6466)\n",
      "tensor(0.5256)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 77.200478\n",
      "Epoch 3702\n",
      "-------------------------------\n",
      "tensor(28.7563)\n",
      "tensor(13.8127)\n",
      "tensor(12.9548)\n",
      "tensor(0.4138)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 77.175072\n",
      "Epoch 3703\n",
      "-------------------------------\n",
      "tensor(29.6431)\n",
      "tensor(15.3368)\n",
      "tensor(7.0151)\n",
      "tensor(0.1746)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 77.149902\n",
      "Epoch 3704\n",
      "-------------------------------\n",
      "tensor(77.6517)\n",
      "tensor(35.5773)\n",
      "tensor(12.8887)\n",
      "tensor(0.1802)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 77.227341\n",
      "Epoch 3705\n",
      "-------------------------------\n",
      "tensor(81.4706)\n",
      "tensor(34.2630)\n",
      "tensor(19.2147)\n",
      "tensor(0.4501)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 77.198631\n",
      "Epoch 3706\n",
      "-------------------------------\n",
      "tensor(27.2709)\n",
      "tensor(16.6607)\n",
      "tensor(7.3263)\n",
      "tensor(0.2397)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 77.110542\n",
      "Epoch 3707\n",
      "-------------------------------\n",
      "tensor(43.1395)\n",
      "tensor(15.8324)\n",
      "tensor(17.1366)\n",
      "tensor(0.2873)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.159485\n",
      "Epoch 3708\n",
      "-------------------------------\n",
      "tensor(26.0903)\n",
      "tensor(15.2809)\n",
      "tensor(8.2687)\n",
      "tensor(0.2164)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 77.118835\n",
      "Epoch 3709\n",
      "-------------------------------\n",
      "tensor(34.2431)\n",
      "tensor(20.4656)\n",
      "tensor(22.0338)\n",
      "tensor(0.3104)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 77.069214\n",
      "Epoch 3710\n",
      "-------------------------------\n",
      "tensor(25.3301)\n",
      "tensor(16.4506)\n",
      "tensor(6.4025)\n",
      "tensor(0.0754)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 76.943443\n",
      "Epoch 3711\n",
      "-------------------------------\n",
      "tensor(37.3305)\n",
      "tensor(15.6854)\n",
      "tensor(17.4724)\n",
      "tensor(0.4125)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 76.857613\n",
      "Epoch 3712\n",
      "-------------------------------\n",
      "tensor(81.6356)\n",
      "tensor(36.7837)\n",
      "tensor(15.2796)\n",
      "tensor(0.0673)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 76.709793\n",
      "Epoch 3713\n",
      "-------------------------------\n",
      "tensor(35.1022)\n",
      "tensor(16.1228)\n",
      "tensor(14.5652)\n",
      "tensor(0.5510)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 76.714539\n",
      "Epoch 3714\n",
      "-------------------------------\n",
      "tensor(27.3689)\n",
      "tensor(15.8474)\n",
      "tensor(7.0091)\n",
      "tensor(0.1744)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 76.668732\n",
      "Epoch 3715\n",
      "-------------------------------\n",
      "tensor(34.8668)\n",
      "tensor(20.6776)\n",
      "tensor(16.0729)\n",
      "tensor(0.3810)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 76.603172\n",
      "Epoch 3716\n",
      "-------------------------------\n",
      "tensor(25.4579)\n",
      "tensor(17.7972)\n",
      "tensor(10.4799)\n",
      "tensor(0.1891)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 76.537186\n",
      "Epoch 3717\n",
      "-------------------------------\n",
      "tensor(25.1964)\n",
      "tensor(14.7166)\n",
      "tensor(8.2539)\n",
      "tensor(0.1654)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 76.455856\n",
      "Epoch 3718\n",
      "-------------------------------\n",
      "tensor(22.3584)\n",
      "tensor(13.7185)\n",
      "tensor(9.0675)\n",
      "tensor(0.2091)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 76.392151\n",
      "Epoch 3719\n",
      "-------------------------------\n",
      "tensor(22.2279)\n",
      "tensor(14.2969)\n",
      "tensor(6.5743)\n",
      "tensor(0.0943)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 76.342400\n",
      "Epoch 3720\n",
      "-------------------------------\n",
      "tensor(67.9838)\n",
      "tensor(36.6133)\n",
      "tensor(6.8374)\n",
      "tensor(0.0214)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 76.349457\n",
      "Epoch 3721\n",
      "-------------------------------\n",
      "tensor(67.5767)\n",
      "tensor(36.6365)\n",
      "tensor(6.6760)\n",
      "tensor(0.0099)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 76.319382\n",
      "Epoch 3722\n",
      "-------------------------------\n",
      "tensor(23.3421)\n",
      "tensor(14.5956)\n",
      "tensor(6.4893)\n",
      "tensor(0.0288)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 76.284874\n",
      "Epoch 3723\n",
      "-------------------------------\n",
      "tensor(24.5668)\n",
      "tensor(14.5583)\n",
      "tensor(7.1293)\n",
      "tensor(0.0640)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 76.280609\n",
      "Epoch 3724\n",
      "-------------------------------\n",
      "tensor(19.8771)\n",
      "tensor(13.8505)\n",
      "tensor(7.5083)\n",
      "tensor(0.0988)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 76.261490\n",
      "Epoch 3725\n",
      "-------------------------------\n",
      "tensor(31.1900)\n",
      "tensor(16.1668)\n",
      "tensor(6.4143)\n",
      "tensor(0.0775)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 76.271194\n",
      "Epoch 3726\n",
      "-------------------------------\n",
      "tensor(30.3136)\n",
      "tensor(16.9485)\n",
      "tensor(9.2272)\n",
      "tensor(0.0320)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 76.253090\n",
      "Epoch 3727\n",
      "-------------------------------\n",
      "tensor(35.9375)\n",
      "tensor(18.2827)\n",
      "tensor(11.9376)\n",
      "tensor(0.1178)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 76.183411\n",
      "Epoch 3728\n",
      "-------------------------------\n",
      "tensor(39.6183)\n",
      "tensor(18.6670)\n",
      "tensor(8.1594)\n",
      "tensor(0.3743)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 76.087067\n",
      "Epoch 3729\n",
      "-------------------------------\n",
      "tensor(69.8227)\n",
      "tensor(39.3781)\n",
      "tensor(8.1077)\n",
      "tensor(0.4249)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 76.220856\n",
      "Epoch 3730\n",
      "-------------------------------\n",
      "tensor(38.7814)\n",
      "tensor(15.0422)\n",
      "tensor(16.9099)\n",
      "tensor(0.0145)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 76.022202\n",
      "Epoch 3731\n",
      "-------------------------------\n",
      "tensor(24.6000)\n",
      "tensor(23.3502)\n",
      "tensor(19.9014)\n",
      "tensor(0.9358)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 76.006187\n",
      "Epoch 3732\n",
      "-------------------------------\n",
      "tensor(25.6712)\n",
      "tensor(17.4054)\n",
      "tensor(12.9780)\n",
      "tensor(0.1038)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 75.935768\n",
      "Epoch 3733\n",
      "-------------------------------\n",
      "tensor(31.1588)\n",
      "tensor(14.6899)\n",
      "tensor(20.7545)\n",
      "tensor(0.9319)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.832794\n",
      "Epoch 3734\n",
      "-------------------------------\n",
      "tensor(69.2127)\n",
      "tensor(37.9730)\n",
      "tensor(7.0105)\n",
      "tensor(0.1251)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.917503\n",
      "Epoch 3735\n",
      "-------------------------------\n",
      "tensor(26.1580)\n",
      "tensor(15.8090)\n",
      "tensor(10.9280)\n",
      "tensor(0.3843)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 75.641052\n",
      "Epoch 3736\n",
      "-------------------------------\n",
      "tensor(22.0370)\n",
      "tensor(15.7740)\n",
      "tensor(8.1622)\n",
      "tensor(0.2653)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 75.575996\n",
      "Epoch 3737\n",
      "-------------------------------\n",
      "tensor(30.3207)\n",
      "tensor(16.7340)\n",
      "tensor(8.1787)\n",
      "tensor(0.1581)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 75.511101\n",
      "Epoch 3738\n",
      "-------------------------------\n",
      "tensor(31.8298)\n",
      "tensor(17.3231)\n",
      "tensor(9.9172)\n",
      "tensor(0.4685)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 75.481049\n",
      "Epoch 3739\n",
      "-------------------------------\n",
      "tensor(24.4460)\n",
      "tensor(15.4326)\n",
      "tensor(9.7558)\n",
      "tensor(0.5507)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 75.436546\n",
      "Epoch 3740\n",
      "-------------------------------\n",
      "tensor(25.3017)\n",
      "tensor(15.4170)\n",
      "tensor(8.6016)\n",
      "tensor(0.4838)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 75.405106\n",
      "Epoch 3741\n",
      "-------------------------------\n",
      "tensor(25.6673)\n",
      "tensor(15.2225)\n",
      "tensor(7.6248)\n",
      "tensor(0.3773)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 75.375961\n",
      "Epoch 3742\n",
      "-------------------------------\n",
      "tensor(25.6487)\n",
      "tensor(15.0349)\n",
      "tensor(6.9067)\n",
      "tensor(0.2577)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 75.346519\n",
      "Epoch 3743\n",
      "-------------------------------\n",
      "tensor(66.1325)\n",
      "tensor(36.7293)\n",
      "tensor(6.4995)\n",
      "tensor(0.1088)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 75.377434\n",
      "Epoch 3744\n",
      "-------------------------------\n",
      "tensor(69.9256)\n",
      "tensor(37.0092)\n",
      "tensor(7.6097)\n",
      "tensor(0.1063)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 75.363724\n",
      "Epoch 3745\n",
      "-------------------------------\n",
      "tensor(36.8733)\n",
      "tensor(16.8990)\n",
      "tensor(11.6346)\n",
      "tensor(0.2977)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 75.368889\n",
      "Epoch 3746\n",
      "-------------------------------\n",
      "tensor(43.7968)\n",
      "tensor(20.2077)\n",
      "tensor(9.0697)\n",
      "tensor(0.4441)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.384460\n",
      "Epoch 3747\n",
      "-------------------------------\n",
      "tensor(33.8035)\n",
      "tensor(22.2305)\n",
      "tensor(16.6474)\n",
      "tensor(0.3674)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.376198\n",
      "Epoch 3748\n",
      "-------------------------------\n",
      "tensor(36.4835)\n",
      "tensor(18.9684)\n",
      "tensor(20.3978)\n",
      "tensor(0.2108)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 75.360817\n",
      "Epoch 3749\n",
      "-------------------------------\n",
      "tensor(34.3627)\n",
      "tensor(15.1407)\n",
      "tensor(16.7974)\n",
      "tensor(0.6728)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 75.275719\n",
      "Epoch 3750\n",
      "-------------------------------\n",
      "tensor(29.6361)\n",
      "tensor(16.3993)\n",
      "tensor(11.2351)\n",
      "tensor(0.3314)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 75.145744\n",
      "Epoch 3751\n",
      "-------------------------------\n",
      "tensor(84.6660)\n",
      "tensor(33.6764)\n",
      "tensor(25.4760)\n",
      "tensor(0.9430)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 75.143761\n",
      "Epoch 3752\n",
      "-------------------------------\n",
      "tensor(48.1601)\n",
      "tensor(19.8398)\n",
      "tensor(28.5656)\n",
      "tensor(1.0668)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 75.025986\n",
      "Epoch 3753\n",
      "-------------------------------\n",
      "tensor(28.9589)\n",
      "tensor(17.6096)\n",
      "tensor(12.4072)\n",
      "tensor(0.8324)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.941353\n",
      "Epoch 3754\n",
      "-------------------------------\n",
      "tensor(38.4154)\n",
      "tensor(19.1083)\n",
      "tensor(23.6500)\n",
      "tensor(0.2301)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.875809\n",
      "Epoch 3755\n",
      "-------------------------------\n",
      "tensor(28.0051)\n",
      "tensor(16.1142)\n",
      "tensor(9.7803)\n",
      "tensor(0.1150)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 74.753868\n",
      "Epoch 3756\n",
      "-------------------------------\n",
      "tensor(35.9275)\n",
      "tensor(15.5734)\n",
      "tensor(15.6753)\n",
      "tensor(0.0237)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 74.664848\n",
      "Epoch 3757\n",
      "-------------------------------\n",
      "tensor(70.6685)\n",
      "tensor(36.0567)\n",
      "tensor(7.3220)\n",
      "tensor(0.0548)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 74.672501\n",
      "Epoch 3758\n",
      "-------------------------------\n",
      "tensor(28.7852)\n",
      "tensor(16.5947)\n",
      "tensor(8.1501)\n",
      "tensor(0.0522)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 74.561134\n",
      "Epoch 3759\n",
      "-------------------------------\n",
      "tensor(27.4882)\n",
      "tensor(16.1661)\n",
      "tensor(8.1819)\n",
      "tensor(0.1770)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 74.526886\n",
      "Epoch 3760\n",
      "-------------------------------\n",
      "tensor(22.5264)\n",
      "tensor(14.9762)\n",
      "tensor(7.8492)\n",
      "tensor(0.2257)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 74.494484\n",
      "Epoch 3761\n",
      "-------------------------------\n",
      "tensor(22.3212)\n",
      "tensor(14.8295)\n",
      "tensor(7.3847)\n",
      "tensor(0.2180)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 74.477341\n",
      "Epoch 3762\n",
      "-------------------------------\n",
      "tensor(65.8349)\n",
      "tensor(35.8410)\n",
      "tensor(6.9245)\n",
      "tensor(0.1848)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 74.464600\n",
      "Epoch 3763\n",
      "-------------------------------\n",
      "tensor(20.1443)\n",
      "tensor(14.2272)\n",
      "tensor(6.9142)\n",
      "tensor(0.1302)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 74.449913\n",
      "Epoch 3764\n",
      "-------------------------------\n",
      "tensor(27.0186)\n",
      "tensor(15.2068)\n",
      "tensor(7.8635)\n",
      "tensor(0.0017)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 74.441719\n",
      "Epoch 3765\n",
      "-------------------------------\n",
      "tensor(39.4816)\n",
      "tensor(17.8335)\n",
      "tensor(7.7071)\n",
      "tensor(0.2193)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 74.416679\n",
      "Epoch 3766\n",
      "-------------------------------\n",
      "tensor(76.4233)\n",
      "tensor(35.1807)\n",
      "tensor(10.7748)\n",
      "tensor(0.3878)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.400749\n",
      "Epoch 3767\n",
      "-------------------------------\n",
      "tensor(33.9310)\n",
      "tensor(17.9413)\n",
      "tensor(7.1252)\n",
      "tensor(0.0107)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.384697\n",
      "Epoch 3768\n",
      "-------------------------------\n",
      "tensor(23.8895)\n",
      "tensor(16.2856)\n",
      "tensor(10.3445)\n",
      "tensor(0.5106)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 74.386169\n",
      "Epoch 3769\n",
      "-------------------------------\n",
      "tensor(22.5369)\n",
      "tensor(16.3640)\n",
      "tensor(8.1748)\n",
      "tensor(0.3070)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 74.353821\n",
      "Epoch 3770\n",
      "-------------------------------\n",
      "tensor(23.9067)\n",
      "tensor(17.8944)\n",
      "tensor(9.2610)\n",
      "tensor(0.4114)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 74.294548\n",
      "Epoch 3771\n",
      "-------------------------------\n",
      "tensor(27.8144)\n",
      "tensor(16.7335)\n",
      "tensor(9.0530)\n",
      "tensor(0.3009)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 74.172211\n",
      "Epoch 3772\n",
      "-------------------------------\n",
      "tensor(67.1086)\n",
      "tensor(39.9155)\n",
      "tensor(9.0577)\n",
      "tensor(0.6033)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 74.059349\n",
      "Epoch 3773\n",
      "-------------------------------\n",
      "tensor(49.0452)\n",
      "tensor(19.0763)\n",
      "tensor(26.5922)\n",
      "tensor(0.9533)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.016548\n",
      "Epoch 3774\n",
      "-------------------------------\n",
      "tensor(37.0836)\n",
      "tensor(21.6212)\n",
      "tensor(20.8903)\n",
      "tensor(0.5481)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 73.934273\n",
      "Epoch 3775\n",
      "-------------------------------\n",
      "tensor(29.6213)\n",
      "tensor(21.1514)\n",
      "tensor(21.8591)\n",
      "tensor(0.5639)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 73.881416\n",
      "Epoch 3776\n",
      "-------------------------------\n",
      "tensor(43.3715)\n",
      "tensor(17.1268)\n",
      "tensor(14.5529)\n",
      "tensor(0.3792)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 73.814171\n",
      "Epoch 3777\n",
      "-------------------------------\n",
      "tensor(29.9340)\n",
      "tensor(14.0865)\n",
      "tensor(15.3278)\n",
      "tensor(0.5075)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 73.722748\n",
      "Epoch 3778\n",
      "-------------------------------\n",
      "tensor(19.1102)\n",
      "tensor(14.7435)\n",
      "tensor(6.7930)\n",
      "tensor(0.0985)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 73.641640\n",
      "Epoch 3779\n",
      "-------------------------------\n",
      "tensor(72.7977)\n",
      "tensor(34.2506)\n",
      "tensor(12.7129)\n",
      "tensor(0.1314)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 73.728889\n",
      "Epoch 3780\n",
      "-------------------------------\n",
      "tensor(69.3865)\n",
      "tensor(34.5927)\n",
      "tensor(9.5810)\n",
      "tensor(0.1404)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 73.675789\n",
      "Epoch 3781\n",
      "-------------------------------\n",
      "tensor(68.6803)\n",
      "tensor(35.9701)\n",
      "tensor(6.4349)\n",
      "tensor(0.0800)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 73.580734\n",
      "Epoch 3782\n",
      "-------------------------------\n",
      "tensor(35.6282)\n",
      "tensor(16.3979)\n",
      "tensor(9.0207)\n",
      "tensor(0.0164)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 73.591408\n",
      "Epoch 3783\n",
      "-------------------------------\n",
      "tensor(30.4790)\n",
      "tensor(14.8562)\n",
      "tensor(12.3986)\n",
      "tensor(0.1037)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 73.604828\n",
      "Epoch 3784\n",
      "-------------------------------\n",
      "tensor(37.9391)\n",
      "tensor(16.4236)\n",
      "tensor(11.9810)\n",
      "tensor(0.1213)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 73.631226\n",
      "Epoch 3785\n",
      "-------------------------------\n",
      "tensor(34.5226)\n",
      "tensor(18.3196)\n",
      "tensor(6.5786)\n",
      "tensor(0.0107)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 73.635643\n",
      "Epoch 3786\n",
      "-------------------------------\n",
      "tensor(34.2542)\n",
      "tensor(21.3879)\n",
      "tensor(19.2072)\n",
      "tensor(0.2050)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 73.640732\n",
      "Epoch 3787\n",
      "-------------------------------\n",
      "tensor(34.7749)\n",
      "tensor(20.7148)\n",
      "tensor(18.6682)\n",
      "tensor(0.0357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 73.599998\n",
      "Epoch 3788\n",
      "-------------------------------\n",
      "tensor(36.1810)\n",
      "tensor(17.7218)\n",
      "tensor(12.8337)\n",
      "tensor(0.4882)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 73.539185\n",
      "Epoch 3789\n",
      "-------------------------------\n",
      "tensor(41.7206)\n",
      "tensor(17.0555)\n",
      "tensor(13.7144)\n",
      "tensor(0.2278)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 73.455307\n",
      "Epoch 3790\n",
      "-------------------------------\n",
      "tensor(35.2007)\n",
      "tensor(18.3784)\n",
      "tensor(22.9917)\n",
      "tensor(0.4782)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 73.349556\n",
      "Epoch 3791\n",
      "-------------------------------\n",
      "tensor(70.2735)\n",
      "tensor(40.7587)\n",
      "tensor(9.2047)\n",
      "tensor(0.6198)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 73.451248\n",
      "Epoch 3792\n",
      "-------------------------------\n",
      "tensor(69.0327)\n",
      "tensor(21.0627)\n",
      "tensor(45.1446)\n",
      "tensor(1.2209)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 73.282249\n",
      "Epoch 3793\n",
      "-------------------------------\n",
      "tensor(45.3098)\n",
      "tensor(26.7062)\n",
      "tensor(35.2832)\n",
      "tensor(1.1246)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 73.143066\n",
      "Epoch 3794\n",
      "-------------------------------\n",
      "tensor(36.6100)\n",
      "tensor(22.4697)\n",
      "tensor(27.2217)\n",
      "tensor(0.5968)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 73.039726\n",
      "Epoch 3795\n",
      "-------------------------------\n",
      "tensor(66.0855)\n",
      "tensor(43.9812)\n",
      "tensor(33.7245)\n",
      "tensor(1.1196)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 73.044327\n",
      "Epoch 3796\n",
      "-------------------------------\n",
      "tensor(57.1174)\n",
      "tensor(15.8512)\n",
      "tensor(29.7622)\n",
      "tensor(0.5909)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 72.970764\n",
      "Epoch 3797\n",
      "-------------------------------\n",
      "tensor(27.7716)\n",
      "tensor(24.0277)\n",
      "tensor(21.7236)\n",
      "tensor(0.8779)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 72.870117\n",
      "Epoch 3798\n",
      "-------------------------------\n",
      "tensor(47.1441)\n",
      "tensor(28.3833)\n",
      "tensor(36.3525)\n",
      "tensor(1.1678)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 72.890366\n",
      "Epoch 3799\n",
      "-------------------------------\n",
      "tensor(40.3039)\n",
      "tensor(22.8723)\n",
      "tensor(22.5202)\n",
      "tensor(0.5656)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 72.823486\n",
      "Epoch 3800\n",
      "-------------------------------\n",
      "tensor(29.6990)\n",
      "tensor(17.3333)\n",
      "tensor(8.3544)\n",
      "tensor(0.0709)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 72.767014\n",
      "Epoch 3801\n",
      "-------------------------------\n",
      "tensor(30.5259)\n",
      "tensor(15.9219)\n",
      "tensor(9.6130)\n",
      "tensor(0.4563)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 72.742104\n",
      "Epoch 3802\n",
      "-------------------------------\n",
      "tensor(28.9115)\n",
      "tensor(14.9849)\n",
      "tensor(14.2177)\n",
      "tensor(0.6615)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 72.724213\n",
      "Epoch 3803\n",
      "-------------------------------\n",
      "tensor(28.2340)\n",
      "tensor(14.7502)\n",
      "tensor(16.1197)\n",
      "tensor(0.7317)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 72.704422\n",
      "Epoch 3804\n",
      "-------------------------------\n",
      "tensor(60.8521)\n",
      "tensor(39.5599)\n",
      "tensor(12.4823)\n",
      "tensor(0.5970)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 72.775635\n",
      "Epoch 3805\n",
      "-------------------------------\n",
      "tensor(66.8828)\n",
      "tensor(37.6578)\n",
      "tensor(7.4372)\n",
      "tensor(0.1330)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 72.756340\n",
      "Epoch 3806\n",
      "-------------------------------\n",
      "tensor(27.9490)\n",
      "tensor(17.5733)\n",
      "tensor(9.7760)\n",
      "tensor(0.4672)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.668381\n",
      "Epoch 3807\n",
      "-------------------------------\n",
      "tensor(28.4900)\n",
      "tensor(20.2499)\n",
      "tensor(12.9854)\n",
      "tensor(0.6820)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 72.698654\n",
      "Epoch 3808\n",
      "-------------------------------\n",
      "tensor(33.1331)\n",
      "tensor(19.6726)\n",
      "tensor(9.5248)\n",
      "tensor(0.0040)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 72.668747\n",
      "Epoch 3809\n",
      "-------------------------------\n",
      "tensor(32.7382)\n",
      "tensor(19.4084)\n",
      "tensor(14.6251)\n",
      "tensor(0.9095)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 72.588943\n",
      "Epoch 3810\n",
      "-------------------------------\n",
      "tensor(30.2744)\n",
      "tensor(18.0115)\n",
      "tensor(8.9243)\n",
      "tensor(0.4524)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 72.512428\n",
      "Epoch 3811\n",
      "-------------------------------\n",
      "tensor(29.4428)\n",
      "tensor(19.5106)\n",
      "tensor(13.2009)\n",
      "tensor(0.6465)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 72.398438\n",
      "Epoch 3812\n",
      "-------------------------------\n",
      "tensor(67.3738)\n",
      "tensor(35.8005)\n",
      "tensor(8.5918)\n",
      "tensor(0.2324)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 72.484764\n",
      "Epoch 3813\n",
      "-------------------------------\n",
      "tensor(52.1700)\n",
      "tensor(17.1586)\n",
      "tensor(29.5229)\n",
      "tensor(0.8302)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 72.321312\n",
      "Epoch 3814\n",
      "-------------------------------\n",
      "tensor(37.5627)\n",
      "tensor(19.9614)\n",
      "tensor(20.4971)\n",
      "tensor(0.0947)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.230614\n",
      "Epoch 3815\n",
      "-------------------------------\n",
      "tensor(36.5463)\n",
      "tensor(20.0478)\n",
      "tensor(22.5067)\n",
      "tensor(0.2273)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 72.160843\n",
      "Epoch 3816\n",
      "-------------------------------\n",
      "tensor(59.5254)\n",
      "tensor(37.8637)\n",
      "tensor(14.1224)\n",
      "tensor(0.3670)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 72.109520\n",
      "Epoch 3817\n",
      "-------------------------------\n",
      "tensor(53.7307)\n",
      "tensor(15.4014)\n",
      "tensor(29.6121)\n",
      "tensor(0.3396)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 72.107048\n",
      "Epoch 3818\n",
      "-------------------------------\n",
      "tensor(26.7475)\n",
      "tensor(16.8071)\n",
      "tensor(9.9032)\n",
      "tensor(0.2878)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 72.021301\n",
      "Epoch 3819\n",
      "-------------------------------\n",
      "tensor(21.4622)\n",
      "tensor(20.3930)\n",
      "tensor(16.6082)\n",
      "tensor(0.6322)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 71.982979\n",
      "Epoch 3820\n",
      "-------------------------------\n",
      "tensor(28.6537)\n",
      "tensor(20.6600)\n",
      "tensor(21.5219)\n",
      "tensor(0.5773)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 71.958580\n",
      "Epoch 3821\n",
      "-------------------------------\n",
      "tensor(32.2144)\n",
      "tensor(19.7587)\n",
      "tensor(18.7961)\n",
      "tensor(0.3605)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 71.932281\n",
      "Epoch 3822\n",
      "-------------------------------\n",
      "tensor(29.0015)\n",
      "tensor(17.8011)\n",
      "tensor(13.2066)\n",
      "tensor(0.0835)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 71.903267\n",
      "Epoch 3823\n",
      "-------------------------------\n",
      "tensor(30.2466)\n",
      "tensor(16.8226)\n",
      "tensor(7.4351)\n",
      "tensor(0.2919)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 71.873558\n",
      "Epoch 3824\n",
      "-------------------------------\n",
      "tensor(62.4501)\n",
      "tensor(40.7391)\n",
      "tensor(14.8203)\n",
      "tensor(0.7723)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 71.871346\n",
      "Epoch 3825\n",
      "-------------------------------\n",
      "tensor(46.4017)\n",
      "tensor(18.3943)\n",
      "tensor(28.8380)\n",
      "tensor(1.0810)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 71.847420\n",
      "Epoch 3826\n",
      "-------------------------------\n",
      "tensor(70.3224)\n",
      "tensor(41.7603)\n",
      "tensor(17.4051)\n",
      "tensor(0.5480)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.862686\n",
      "Epoch 3827\n",
      "-------------------------------\n",
      "tensor(34.4616)\n",
      "tensor(21.2133)\n",
      "tensor(15.5934)\n",
      "tensor(0.6264)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 71.874863\n",
      "Epoch 3828\n",
      "-------------------------------\n",
      "tensor(41.8610)\n",
      "tensor(26.4799)\n",
      "tensor(29.6597)\n",
      "tensor(0.9435)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 71.883278\n",
      "Epoch 3829\n",
      "-------------------------------\n",
      "tensor(31.2882)\n",
      "tensor(16.9791)\n",
      "tensor(12.8348)\n",
      "tensor(0.5257)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 71.771095\n",
      "Epoch 3830\n",
      "-------------------------------\n",
      "tensor(43.0204)\n",
      "tensor(17.5716)\n",
      "tensor(21.7121)\n",
      "tensor(0.8177)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 71.756714\n",
      "Epoch 3831\n",
      "-------------------------------\n",
      "tensor(37.3201)\n",
      "tensor(24.2568)\n",
      "tensor(27.1196)\n",
      "tensor(0.8370)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 71.634727\n",
      "Epoch 3832\n",
      "-------------------------------\n",
      "tensor(69.0663)\n",
      "tensor(35.0025)\n",
      "tensor(9.4640)\n",
      "tensor(0.2244)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 71.531998\n",
      "Epoch 3833\n",
      "-------------------------------\n",
      "tensor(71.9618)\n",
      "tensor(21.1059)\n",
      "tensor(47.5459)\n",
      "tensor(1.1916)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 71.565201\n",
      "Epoch 3834\n",
      "-------------------------------\n",
      "tensor(38.2512)\n",
      "tensor(20.5278)\n",
      "tensor(26.4085)\n",
      "tensor(0.4477)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.400772\n",
      "Epoch 3835\n",
      "-------------------------------\n",
      "tensor(47.0014)\n",
      "tensor(22.0809)\n",
      "tensor(30.5240)\n",
      "tensor(0.4404)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 71.355080\n",
      "Epoch 3836\n",
      "-------------------------------\n",
      "tensor(45.4937)\n",
      "tensor(18.3937)\n",
      "tensor(20.7610)\n",
      "tensor(0.8000)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 71.244858\n",
      "Epoch 3837\n",
      "-------------------------------\n",
      "tensor(63.3376)\n",
      "tensor(42.9564)\n",
      "tensor(24.3170)\n",
      "tensor(0.8266)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 71.179504\n",
      "Epoch 3838\n",
      "-------------------------------\n",
      "tensor(31.3974)\n",
      "tensor(15.5099)\n",
      "tensor(10.1489)\n",
      "tensor(0.1622)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 71.095001\n",
      "Epoch 3839\n",
      "-------------------------------\n",
      "tensor(21.1609)\n",
      "tensor(18.0975)\n",
      "tensor(11.2702)\n",
      "tensor(0.3652)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 71.061752\n",
      "Epoch 3840\n",
      "-------------------------------\n",
      "tensor(23.0264)\n",
      "tensor(19.5956)\n",
      "tensor(17.0411)\n",
      "tensor(0.5147)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 71.048447\n",
      "Epoch 3841\n",
      "-------------------------------\n",
      "tensor(29.4469)\n",
      "tensor(19.9380)\n",
      "tensor(16.3233)\n",
      "tensor(0.4463)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 71.030472\n",
      "Epoch 3842\n",
      "-------------------------------\n",
      "tensor(28.0679)\n",
      "tensor(18.6514)\n",
      "tensor(12.8074)\n",
      "tensor(0.2785)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 71.011070\n",
      "Epoch 3843\n",
      "-------------------------------\n",
      "tensor(67.7738)\n",
      "tensor(34.6053)\n",
      "tensor(7.4651)\n",
      "tensor(0.0090)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 70.981911\n",
      "Epoch 3844\n",
      "-------------------------------\n",
      "tensor(36.1173)\n",
      "tensor(16.1652)\n",
      "tensor(12.7257)\n",
      "tensor(0.4572)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 70.979744\n",
      "Epoch 3845\n",
      "-------------------------------\n",
      "tensor(47.1057)\n",
      "tensor(17.5047)\n",
      "tensor(22.5261)\n",
      "tensor(0.7967)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 70.990143\n",
      "Epoch 3846\n",
      "-------------------------------\n",
      "tensor(32.4826)\n",
      "tensor(15.9184)\n",
      "tensor(11.1956)\n",
      "tensor(0.4884)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.945816\n",
      "Epoch 3847\n",
      "-------------------------------\n",
      "tensor(83.8148)\n",
      "tensor(32.6887)\n",
      "tensor(23.0826)\n",
      "tensor(0.3514)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 71.086082\n",
      "Epoch 3848\n",
      "-------------------------------\n",
      "tensor(27.1530)\n",
      "tensor(18.3112)\n",
      "tensor(7.8690)\n",
      "tensor(0.3337)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 70.873085\n",
      "Epoch 3849\n",
      "-------------------------------\n",
      "tensor(41.1710)\n",
      "tensor(16.1488)\n",
      "tensor(18.6873)\n",
      "tensor(0.2134)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 70.847855\n",
      "Epoch 3850\n",
      "-------------------------------\n",
      "tensor(35.6029)\n",
      "tensor(20.9455)\n",
      "tensor(12.6852)\n",
      "tensor(0.1599)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 70.790810\n",
      "Epoch 3851\n",
      "-------------------------------\n",
      "tensor(38.5821)\n",
      "tensor(19.1920)\n",
      "tensor(21.3065)\n",
      "tensor(0.0726)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 70.684654\n",
      "Epoch 3852\n",
      "-------------------------------\n",
      "tensor(62.1126)\n",
      "tensor(43.5602)\n",
      "tensor(22.7787)\n",
      "tensor(0.9474)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 70.790321\n",
      "Epoch 3853\n",
      "-------------------------------\n",
      "tensor(50.1844)\n",
      "tensor(16.4612)\n",
      "tensor(23.9729)\n",
      "tensor(0.0475)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.607445\n",
      "Epoch 3854\n",
      "-------------------------------\n",
      "tensor(45.4839)\n",
      "tensor(29.2251)\n",
      "tensor(39.2183)\n",
      "tensor(1.3778)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.599205\n",
      "Epoch 3855\n",
      "-------------------------------\n",
      "tensor(26.5084)\n",
      "tensor(16.8740)\n",
      "tensor(12.2213)\n",
      "tensor(0.2201)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 70.468216\n",
      "Epoch 3856\n",
      "-------------------------------\n",
      "tensor(45.0514)\n",
      "tensor(19.9155)\n",
      "tensor(30.7429)\n",
      "tensor(1.4208)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 70.420197\n",
      "Epoch 3857\n",
      "-------------------------------\n",
      "tensor(61.3603)\n",
      "tensor(40.6176)\n",
      "tensor(14.7236)\n",
      "tensor(0.7128)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 70.377052\n",
      "Epoch 3858\n",
      "-------------------------------\n",
      "tensor(19.1397)\n",
      "tensor(16.5330)\n",
      "tensor(7.8880)\n",
      "tensor(0.2699)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 70.245636\n",
      "Epoch 3859\n",
      "-------------------------------\n",
      "tensor(19.7676)\n",
      "tensor(19.7524)\n",
      "tensor(14.8819)\n",
      "tensor(0.6916)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 70.225021\n",
      "Epoch 3860\n",
      "-------------------------------\n",
      "tensor(24.9279)\n",
      "tensor(19.9428)\n",
      "tensor(14.7292)\n",
      "tensor(0.6605)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 70.198051\n",
      "Epoch 3861\n",
      "-------------------------------\n",
      "tensor(22.6513)\n",
      "tensor(18.3621)\n",
      "tensor(11.6217)\n",
      "tensor(0.4629)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 70.171059\n",
      "Epoch 3862\n",
      "-------------------------------\n",
      "tensor(69.1437)\n",
      "tensor(34.3596)\n",
      "tensor(8.0924)\n",
      "tensor(0.2076)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 70.165726\n",
      "Epoch 3863\n",
      "-------------------------------\n",
      "tensor(31.5048)\n",
      "tensor(16.5535)\n",
      "tensor(6.8682)\n",
      "tensor(0.1547)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 70.146774\n",
      "Epoch 3864\n",
      "-------------------------------\n",
      "tensor(43.9310)\n",
      "tensor(19.6637)\n",
      "tensor(13.4732)\n",
      "tensor(0.6100)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 70.157959\n",
      "Epoch 3865\n",
      "-------------------------------\n",
      "tensor(34.0199)\n",
      "tensor(17.5863)\n",
      "tensor(15.6283)\n",
      "tensor(0.9122)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 70.151009\n",
      "Epoch 3866\n",
      "-------------------------------\n",
      "tensor(70.1507)\n",
      "tensor(37.0595)\n",
      "tensor(12.2372)\n",
      "tensor(0.6474)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.148018\n",
      "Epoch 3867\n",
      "-------------------------------\n",
      "tensor(24.7844)\n",
      "tensor(17.6493)\n",
      "tensor(7.7151)\n",
      "tensor(0.0525)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.099648\n",
      "Epoch 3868\n",
      "-------------------------------\n",
      "tensor(32.3862)\n",
      "tensor(22.5783)\n",
      "tensor(11.0788)\n",
      "tensor(0.7748)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 70.115082\n",
      "Epoch 3869\n",
      "-------------------------------\n",
      "tensor(30.2106)\n",
      "tensor(21.7357)\n",
      "tensor(10.6936)\n",
      "tensor(0.6898)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 70.022644\n",
      "Epoch 3870\n",
      "-------------------------------\n",
      "tensor(26.6655)\n",
      "tensor(16.8440)\n",
      "tensor(10.9376)\n",
      "tensor(0.3291)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 69.919846\n",
      "Epoch 3871\n",
      "-------------------------------\n",
      "tensor(66.9526)\n",
      "tensor(41.9136)\n",
      "tensor(14.4977)\n",
      "tensor(1.0739)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 69.932526\n",
      "Epoch 3872\n",
      "-------------------------------\n",
      "tensor(59.0196)\n",
      "tensor(18.2939)\n",
      "tensor(25.6862)\n",
      "tensor(0.3531)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.866043\n",
      "Epoch 3873\n",
      "-------------------------------\n",
      "tensor(41.9179)\n",
      "tensor(28.2861)\n",
      "tensor(34.9458)\n",
      "tensor(1.3753)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.814850\n",
      "Epoch 3874\n",
      "-------------------------------\n",
      "tensor(27.6384)\n",
      "tensor(17.7762)\n",
      "tensor(11.6711)\n",
      "tensor(0.0906)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.713737\n",
      "Epoch 3875\n",
      "-------------------------------\n",
      "tensor(41.7611)\n",
      "tensor(20.4831)\n",
      "tensor(30.0461)\n",
      "tensor(1.4542)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 69.663383\n",
      "Epoch 3876\n",
      "-------------------------------\n",
      "tensor(61.9715)\n",
      "tensor(37.6439)\n",
      "tensor(8.5417)\n",
      "tensor(0.4836)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 69.591118\n",
      "Epoch 3877\n",
      "-------------------------------\n",
      "tensor(22.4434)\n",
      "tensor(18.8792)\n",
      "tensor(10.5520)\n",
      "tensor(0.5349)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 69.505989\n",
      "Epoch 3878\n",
      "-------------------------------\n",
      "tensor(18.6429)\n",
      "tensor(20.6323)\n",
      "tensor(13.5536)\n",
      "tensor(0.7722)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 69.473434\n",
      "Epoch 3879\n",
      "-------------------------------\n",
      "tensor(24.2423)\n",
      "tensor(19.0285)\n",
      "tensor(10.0089)\n",
      "tensor(0.5075)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 69.435272\n",
      "Epoch 3880\n",
      "-------------------------------\n",
      "tensor(27.2700)\n",
      "tensor(17.4323)\n",
      "tensor(6.7566)\n",
      "tensor(0.1704)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 69.402130\n",
      "Epoch 3881\n",
      "-------------------------------\n",
      "tensor(24.3925)\n",
      "tensor(16.0779)\n",
      "tensor(6.0655)\n",
      "tensor(0.0789)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 69.373154\n",
      "Epoch 3882\n",
      "-------------------------------\n",
      "tensor(64.3790)\n",
      "tensor(35.7206)\n",
      "tensor(6.7493)\n",
      "tensor(0.2677)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 69.368340\n",
      "Epoch 3883\n",
      "-------------------------------\n",
      "tensor(60.9995)\n",
      "tensor(36.3647)\n",
      "tensor(8.4978)\n",
      "tensor(0.4419)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 69.336739\n",
      "Epoch 3884\n",
      "-------------------------------\n",
      "tensor(36.0042)\n",
      "tensor(16.9131)\n",
      "tensor(12.9746)\n",
      "tensor(0.5805)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 69.358658\n",
      "Epoch 3885\n",
      "-------------------------------\n",
      "tensor(35.2866)\n",
      "tensor(16.7203)\n",
      "tensor(11.8672)\n",
      "tensor(0.4449)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 69.371613\n",
      "Epoch 3886\n",
      "-------------------------------\n",
      "tensor(38.3313)\n",
      "tensor(20.5094)\n",
      "tensor(8.1672)\n",
      "tensor(0.1461)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.398003\n",
      "Epoch 3887\n",
      "-------------------------------\n",
      "tensor(37.5313)\n",
      "tensor(23.9835)\n",
      "tensor(22.9543)\n",
      "tensor(0.6496)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.409698\n",
      "Epoch 3888\n",
      "-------------------------------\n",
      "tensor(35.0619)\n",
      "tensor(20.5457)\n",
      "tensor(9.1676)\n",
      "tensor(0.0723)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.310005\n",
      "Epoch 3889\n",
      "-------------------------------\n",
      "tensor(36.9632)\n",
      "tensor(17.8483)\n",
      "tensor(18.4936)\n",
      "tensor(0.8268)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 69.180435\n",
      "Epoch 3890\n",
      "-------------------------------\n",
      "tensor(68.5122)\n",
      "tensor(35.6562)\n",
      "tensor(8.7458)\n",
      "tensor(0.2429)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 69.283371\n",
      "Epoch 3891\n",
      "-------------------------------\n",
      "tensor(37.8392)\n",
      "tensor(16.7827)\n",
      "tensor(14.5183)\n",
      "tensor(0.2308)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 69.075912\n",
      "Epoch 3892\n",
      "-------------------------------\n",
      "tensor(33.8079)\n",
      "tensor(21.0078)\n",
      "tensor(13.5183)\n",
      "tensor(0.5452)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.014404\n",
      "Epoch 3893\n",
      "-------------------------------\n",
      "tensor(73.8192)\n",
      "tensor(33.7552)\n",
      "tensor(15.9544)\n",
      "tensor(0.3924)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.971916\n",
      "Epoch 3894\n",
      "-------------------------------\n",
      "tensor(61.8105)\n",
      "tensor(17.4633)\n",
      "tensor(38.9347)\n",
      "tensor(1.1404)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.055740\n",
      "Epoch 3895\n",
      "-------------------------------\n",
      "tensor(23.4737)\n",
      "tensor(23.8439)\n",
      "tensor(14.0949)\n",
      "tensor(0.8321)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 68.944321\n",
      "Epoch 3896\n",
      "-------------------------------\n",
      "tensor(43.0257)\n",
      "tensor(30.7557)\n",
      "tensor(36.5760)\n",
      "tensor(1.4100)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 68.952995\n",
      "Epoch 3897\n",
      "-------------------------------\n",
      "tensor(32.1613)\n",
      "tensor(19.3635)\n",
      "tensor(8.8491)\n",
      "tensor(0.0760)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 68.823128\n",
      "Epoch 3898\n",
      "-------------------------------\n",
      "tensor(33.9967)\n",
      "tensor(17.7738)\n",
      "tensor(19.5639)\n",
      "tensor(0.9870)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 68.754784\n",
      "Epoch 3899\n",
      "-------------------------------\n",
      "tensor(31.4365)\n",
      "tensor(18.1843)\n",
      "tensor(20.9177)\n",
      "tensor(1.1208)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 68.694115\n",
      "Epoch 3900\n",
      "-------------------------------\n",
      "tensor(25.1119)\n",
      "tensor(16.7167)\n",
      "tensor(13.3711)\n",
      "tensor(0.8272)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 68.633774\n",
      "Epoch 3901\n",
      "-------------------------------\n",
      "tensor(19.7215)\n",
      "tensor(15.3451)\n",
      "tensor(8.0179)\n",
      "tensor(0.5034)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 68.593987\n",
      "Epoch 3902\n",
      "-------------------------------\n",
      "tensor(20.7484)\n",
      "tensor(15.3321)\n",
      "tensor(8.4364)\n",
      "tensor(0.2176)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 68.567810\n",
      "Epoch 3903\n",
      "-------------------------------\n",
      "tensor(74.0379)\n",
      "tensor(34.8606)\n",
      "tensor(12.1879)\n",
      "tensor(0.0563)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 68.611298\n",
      "Epoch 3904\n",
      "-------------------------------\n",
      "tensor(72.7771)\n",
      "tensor(34.5404)\n",
      "tensor(11.4460)\n",
      "tensor(0.2773)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 68.606560\n",
      "Epoch 3905\n",
      "-------------------------------\n",
      "tensor(34.6006)\n",
      "tensor(17.6406)\n",
      "tensor(11.4565)\n",
      "tensor(0.2648)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 68.524452\n",
      "Epoch 3906\n",
      "-------------------------------\n",
      "tensor(53.4938)\n",
      "tensor(19.0673)\n",
      "tensor(16.9692)\n",
      "tensor(0.0804)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.542480\n",
      "Epoch 3907\n",
      "-------------------------------\n",
      "tensor(35.8141)\n",
      "tensor(20.3561)\n",
      "tensor(8.5430)\n",
      "tensor(0.0696)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.524841\n",
      "Epoch 3908\n",
      "-------------------------------\n",
      "tensor(48.6746)\n",
      "tensor(24.1354)\n",
      "tensor(29.7752)\n",
      "tensor(0.0121)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 68.504135\n",
      "Epoch 3909\n",
      "-------------------------------\n",
      "tensor(62.6114)\n",
      "tensor(34.2111)\n",
      "tensor(12.6278)\n",
      "tensor(0.5465)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 68.344200\n",
      "Epoch 3910\n",
      "-------------------------------\n",
      "tensor(82.1950)\n",
      "tensor(18.8936)\n",
      "tensor(41.3038)\n",
      "tensor(0.1257)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 68.466507\n",
      "Epoch 3911\n",
      "-------------------------------\n",
      "tensor(46.8408)\n",
      "tensor(38.7382)\n",
      "tensor(47.9409)\n",
      "tensor(2.1869)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 68.353310\n",
      "Epoch 3912\n",
      "-------------------------------\n",
      "tensor(35.5730)\n",
      "tensor(18.5178)\n",
      "tensor(19.5537)\n",
      "tensor(0.2308)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 68.171211\n",
      "Epoch 3913\n",
      "-------------------------------\n",
      "tensor(66.6249)\n",
      "tensor(61.1732)\n",
      "tensor(54.9507)\n",
      "tensor(2.6876)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.266876\n",
      "Epoch 3914\n",
      "-------------------------------\n",
      "tensor(31.7802)\n",
      "tensor(17.1979)\n",
      "tensor(10.1485)\n",
      "tensor(0.1081)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.082558\n",
      "Epoch 3915\n",
      "-------------------------------\n",
      "tensor(53.6683)\n",
      "tensor(33.4992)\n",
      "tensor(49.2143)\n",
      "tensor(1.9617)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 68.117134\n",
      "Epoch 3916\n",
      "-------------------------------\n",
      "tensor(35.0528)\n",
      "tensor(19.3344)\n",
      "tensor(8.6303)\n",
      "tensor(0.2954)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 67.949646\n",
      "Epoch 3917\n",
      "-------------------------------\n",
      "tensor(40.4578)\n",
      "tensor(18.6711)\n",
      "tensor(29.6326)\n",
      "tensor(1.3285)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 67.929161\n",
      "Epoch 3918\n",
      "-------------------------------\n",
      "tensor(32.5221)\n",
      "tensor(18.4754)\n",
      "tensor(22.3563)\n",
      "tensor(1.2796)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 67.856117\n",
      "Epoch 3919\n",
      "-------------------------------\n",
      "tensor(24.9447)\n",
      "tensor(16.3364)\n",
      "tensor(10.1086)\n",
      "tensor(0.5752)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 67.784065\n",
      "Epoch 3920\n",
      "-------------------------------\n",
      "tensor(74.5002)\n",
      "tensor(33.3666)\n",
      "tensor(14.5683)\n",
      "tensor(0.0571)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 67.789360\n",
      "Epoch 3921\n",
      "-------------------------------\n",
      "tensor(73.9040)\n",
      "tensor(32.9209)\n",
      "tensor(13.7472)\n",
      "tensor(0.1780)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 67.761826\n",
      "Epoch 3922\n",
      "-------------------------------\n",
      "tensor(30.5360)\n",
      "tensor(18.8604)\n",
      "tensor(9.3555)\n",
      "tensor(0.2574)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 67.729218\n",
      "Epoch 3923\n",
      "-------------------------------\n",
      "tensor(27.2204)\n",
      "tensor(17.6699)\n",
      "tensor(7.0783)\n",
      "tensor(0.2495)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 67.732460\n",
      "Epoch 3924\n",
      "-------------------------------\n",
      "tensor(41.4301)\n",
      "tensor(18.0449)\n",
      "tensor(11.5616)\n",
      "tensor(0.1675)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 67.749199\n",
      "Epoch 3925\n",
      "-------------------------------\n",
      "tensor(42.0695)\n",
      "tensor(17.7078)\n",
      "tensor(12.4885)\n",
      "tensor(0.0616)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 67.729713\n",
      "Epoch 3926\n",
      "-------------------------------\n",
      "tensor(24.6426)\n",
      "tensor(17.6329)\n",
      "tensor(7.0787)\n",
      "tensor(0.0542)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 67.686813\n",
      "Epoch 3927\n",
      "-------------------------------\n",
      "tensor(79.7816)\n",
      "tensor(32.0974)\n",
      "tensor(20.8838)\n",
      "tensor(0.0591)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.737411\n",
      "Epoch 3928\n",
      "-------------------------------\n",
      "tensor(40.2789)\n",
      "tensor(17.2006)\n",
      "tensor(15.5539)\n",
      "tensor(0.6332)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.649712\n",
      "Epoch 3929\n",
      "-------------------------------\n",
      "tensor(48.2782)\n",
      "tensor(19.3182)\n",
      "tensor(12.8662)\n",
      "tensor(0.0822)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 67.593361\n",
      "Epoch 3930\n",
      "-------------------------------\n",
      "tensor(39.5756)\n",
      "tensor(27.2881)\n",
      "tensor(32.6030)\n",
      "tensor(1.0320)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 67.551674\n",
      "Epoch 3931\n",
      "-------------------------------\n",
      "tensor(62.1239)\n",
      "tensor(37.1727)\n",
      "tensor(8.2203)\n",
      "tensor(0.4393)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 67.528717\n",
      "Epoch 3932\n",
      "-------------------------------\n",
      "tensor(74.5855)\n",
      "tensor(20.7988)\n",
      "tensor(47.9872)\n",
      "tensor(1.2627)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.553513\n",
      "Epoch 3933\n",
      "-------------------------------\n",
      "tensor(58.1921)\n",
      "tensor(31.7254)\n",
      "tensor(47.1489)\n",
      "tensor(1.3955)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.490265\n",
      "Epoch 3934\n",
      "-------------------------------\n",
      "tensor(35.1112)\n",
      "tensor(20.7764)\n",
      "tensor(17.0081)\n",
      "tensor(0.2643)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 67.351799\n",
      "Epoch 3935\n",
      "-------------------------------\n",
      "tensor(58.9242)\n",
      "tensor(22.5896)\n",
      "tensor(40.0921)\n",
      "tensor(1.5259)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 67.304138\n",
      "Epoch 3936\n",
      "-------------------------------\n",
      "tensor(61.5066)\n",
      "tensor(38.2578)\n",
      "tensor(9.6844)\n",
      "tensor(0.5830)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 67.189087\n",
      "Epoch 3937\n",
      "-------------------------------\n",
      "tensor(24.0193)\n",
      "tensor(19.2765)\n",
      "tensor(15.2866)\n",
      "tensor(0.4828)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 67.083252\n",
      "Epoch 3938\n",
      "-------------------------------\n",
      "tensor(28.6638)\n",
      "tensor(20.9430)\n",
      "tensor(15.6644)\n",
      "tensor(0.6362)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 67.077423\n",
      "Epoch 3939\n",
      "-------------------------------\n",
      "tensor(28.5835)\n",
      "tensor(18.6180)\n",
      "tensor(8.3555)\n",
      "tensor(0.2943)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 67.042572\n",
      "Epoch 3940\n",
      "-------------------------------\n",
      "tensor(29.8212)\n",
      "tensor(17.0713)\n",
      "tensor(5.9955)\n",
      "tensor(0.0412)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 67.004333\n",
      "Epoch 3941\n",
      "-------------------------------\n",
      "tensor(63.7542)\n",
      "tensor(35.7435)\n",
      "tensor(7.0924)\n",
      "tensor(0.2437)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 66.988457\n",
      "Epoch 3942\n",
      "-------------------------------\n",
      "tensor(26.2216)\n",
      "tensor(15.4561)\n",
      "tensor(8.9520)\n",
      "tensor(0.3643)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 66.963211\n",
      "Epoch 3943\n",
      "-------------------------------\n",
      "tensor(26.3736)\n",
      "tensor(15.4792)\n",
      "tensor(9.3490)\n",
      "tensor(0.4152)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 66.952156\n",
      "Epoch 3944\n",
      "-------------------------------\n",
      "tensor(59.5214)\n",
      "tensor(34.7730)\n",
      "tensor(7.4883)\n",
      "tensor(0.3454)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 66.932831\n",
      "Epoch 3945\n",
      "-------------------------------\n",
      "tensor(34.5873)\n",
      "tensor(18.2047)\n",
      "tensor(6.2351)\n",
      "tensor(0.1184)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 66.935913\n",
      "Epoch 3946\n",
      "-------------------------------\n",
      "tensor(37.0633)\n",
      "tensor(20.9268)\n",
      "tensor(8.5896)\n",
      "tensor(0.2734)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.970726\n",
      "Epoch 3947\n",
      "-------------------------------\n",
      "tensor(29.9377)\n",
      "tensor(21.5048)\n",
      "tensor(13.9603)\n",
      "tensor(0.4205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.938332\n",
      "Epoch 3948\n",
      "-------------------------------\n",
      "tensor(26.1003)\n",
      "tensor(17.5276)\n",
      "tensor(7.0793)\n",
      "tensor(0.0776)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.857376\n",
      "Epoch 3949\n",
      "-------------------------------\n",
      "tensor(61.4259)\n",
      "tensor(38.1388)\n",
      "tensor(11.8850)\n",
      "tensor(0.6120)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 66.840141\n",
      "Epoch 3950\n",
      "-------------------------------\n",
      "tensor(48.7337)\n",
      "tensor(18.3227)\n",
      "tensor(17.0082)\n",
      "tensor(0.2133)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 66.761337\n",
      "Epoch 3951\n",
      "-------------------------------\n",
      "tensor(38.7743)\n",
      "tensor(26.5695)\n",
      "tensor(31.2032)\n",
      "tensor(0.9315)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 66.721252\n",
      "Epoch 3952\n",
      "-------------------------------\n",
      "tensor(30.0516)\n",
      "tensor(17.9218)\n",
      "tensor(8.3074)\n",
      "tensor(0.4316)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.599335\n",
      "Epoch 3953\n",
      "-------------------------------\n",
      "tensor(60.3607)\n",
      "tensor(42.0873)\n",
      "tensor(20.8307)\n",
      "tensor(0.9893)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.607613\n",
      "Epoch 3954\n",
      "-------------------------------\n",
      "tensor(27.6162)\n",
      "tensor(17.5604)\n",
      "tensor(7.7684)\n",
      "tensor(0.2002)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.495811\n",
      "Epoch 3955\n",
      "-------------------------------\n",
      "tensor(29.7158)\n",
      "tensor(25.5221)\n",
      "tensor(24.1369)\n",
      "tensor(0.9859)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 66.499863\n",
      "Epoch 3956\n",
      "-------------------------------\n",
      "tensor(27.9628)\n",
      "tensor(18.7406)\n",
      "tensor(8.0854)\n",
      "tensor(0.0673)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 66.413185\n",
      "Epoch 3957\n",
      "-------------------------------\n",
      "tensor(27.2638)\n",
      "tensor(15.6795)\n",
      "tensor(14.4934)\n",
      "tensor(0.7608)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 66.324265\n",
      "Epoch 3958\n",
      "-------------------------------\n",
      "tensor(57.4200)\n",
      "tensor(37.9215)\n",
      "tensor(12.5325)\n",
      "tensor(0.7278)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 66.269585\n",
      "Epoch 3959\n",
      "-------------------------------\n",
      "tensor(31.4628)\n",
      "tensor(16.4833)\n",
      "tensor(9.5572)\n",
      "tensor(0.3851)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 66.234436\n",
      "Epoch 3960\n",
      "-------------------------------\n",
      "tensor(29.3560)\n",
      "tensor(16.9591)\n",
      "tensor(6.1398)\n",
      "tensor(0.0670)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 66.211700\n",
      "Epoch 3961\n",
      "-------------------------------\n",
      "tensor(34.6229)\n",
      "tensor(18.6799)\n",
      "tensor(6.8776)\n",
      "tensor(0.1209)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 66.192596\n",
      "Epoch 3962\n",
      "-------------------------------\n",
      "tensor(67.9390)\n",
      "tensor(32.8937)\n",
      "tensor(8.9889)\n",
      "tensor(0.2123)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 66.185173\n",
      "Epoch 3963\n",
      "-------------------------------\n",
      "tensor(29.8491)\n",
      "tensor(18.6841)\n",
      "tensor(8.5666)\n",
      "tensor(0.2148)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 66.169044\n",
      "Epoch 3964\n",
      "-------------------------------\n",
      "tensor(27.4536)\n",
      "tensor(17.9414)\n",
      "tensor(6.9605)\n",
      "tensor(0.0992)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 66.170975\n",
      "Epoch 3965\n",
      "-------------------------------\n",
      "tensor(30.6783)\n",
      "tensor(17.5107)\n",
      "tensor(6.1287)\n",
      "tensor(0.1681)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 66.153511\n",
      "Epoch 3966\n",
      "-------------------------------\n",
      "tensor(61.6562)\n",
      "tensor(35.2749)\n",
      "tensor(8.3135)\n",
      "tensor(0.4474)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.154114\n",
      "Epoch 3967\n",
      "-------------------------------\n",
      "tensor(35.8196)\n",
      "tensor(16.1924)\n",
      "tensor(14.1106)\n",
      "tensor(0.4566)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.149628\n",
      "Epoch 3968\n",
      "-------------------------------\n",
      "tensor(26.9105)\n",
      "tensor(20.2901)\n",
      "tensor(7.9309)\n",
      "tensor(0.3427)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.110062\n",
      "Epoch 3969\n",
      "-------------------------------\n",
      "tensor(34.5196)\n",
      "tensor(24.9342)\n",
      "tensor(21.5644)\n",
      "tensor(0.7877)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 66.048347\n",
      "Epoch 3970\n",
      "-------------------------------\n",
      "tensor(65.3696)\n",
      "tensor(36.7764)\n",
      "tensor(8.0072)\n",
      "tensor(0.4024)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 65.957565\n",
      "Epoch 3971\n",
      "-------------------------------\n",
      "tensor(59.9020)\n",
      "tensor(19.9359)\n",
      "tensor(33.5673)\n",
      "tensor(1.0685)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 66.036057\n",
      "Epoch 3972\n",
      "-------------------------------\n",
      "tensor(50.5595)\n",
      "tensor(30.1590)\n",
      "tensor(39.3643)\n",
      "tensor(1.0769)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.011360\n",
      "Epoch 3973\n",
      "-------------------------------\n",
      "tensor(27.7736)\n",
      "tensor(18.9061)\n",
      "tensor(7.8811)\n",
      "tensor(0.1163)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 65.855415\n",
      "Epoch 3974\n",
      "-------------------------------\n",
      "tensor(47.6768)\n",
      "tensor(18.2183)\n",
      "tensor(30.9169)\n",
      "tensor(1.0641)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.799774\n",
      "Epoch 3975\n",
      "-------------------------------\n",
      "tensor(72.4367)\n",
      "tensor(34.0003)\n",
      "tensor(12.7297)\n",
      "tensor(0.0863)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 65.840874\n",
      "Epoch 3976\n",
      "-------------------------------\n",
      "tensor(62.8582)\n",
      "tensor(33.0507)\n",
      "tensor(8.1291)\n",
      "tensor(0.2095)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 65.629684\n",
      "Epoch 3977\n",
      "-------------------------------\n",
      "tensor(43.4043)\n",
      "tensor(16.6368)\n",
      "tensor(20.4867)\n",
      "tensor(0.0526)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 65.665993\n",
      "Epoch 3978\n",
      "-------------------------------\n",
      "tensor(32.9631)\n",
      "tensor(17.9134)\n",
      "tensor(11.6510)\n",
      "tensor(0.0835)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 65.649529\n",
      "Epoch 3979\n",
      "-------------------------------\n",
      "tensor(31.7653)\n",
      "tensor(21.5840)\n",
      "tensor(9.0158)\n",
      "tensor(0.2797)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 65.646561\n",
      "Epoch 3980\n",
      "-------------------------------\n",
      "tensor(35.9024)\n",
      "tensor(23.2603)\n",
      "tensor(15.8838)\n",
      "tensor(0.2998)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 65.635208\n",
      "Epoch 3981\n",
      "-------------------------------\n",
      "tensor(36.6647)\n",
      "tensor(22.9782)\n",
      "tensor(17.1246)\n",
      "tensor(0.2076)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 65.610619\n",
      "Epoch 3982\n",
      "-------------------------------\n",
      "tensor(37.6077)\n",
      "tensor(22.3679)\n",
      "tensor(15.0961)\n",
      "tensor(0.0622)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 65.576050\n",
      "Epoch 3983\n",
      "-------------------------------\n",
      "tensor(30.2583)\n",
      "tensor(19.5186)\n",
      "tensor(10.7873)\n",
      "tensor(0.1528)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 65.543404\n",
      "Epoch 3984\n",
      "-------------------------------\n",
      "tensor(32.4157)\n",
      "tensor(18.1732)\n",
      "tensor(8.4041)\n",
      "tensor(0.4454)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 65.498260\n",
      "Epoch 3985\n",
      "-------------------------------\n",
      "tensor(37.1839)\n",
      "tensor(17.5178)\n",
      "tensor(14.8552)\n",
      "tensor(0.6413)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 65.449020\n",
      "Epoch 3986\n",
      "-------------------------------\n",
      "tensor(62.6033)\n",
      "tensor(38.6371)\n",
      "tensor(11.2457)\n",
      "tensor(0.4248)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.531143\n",
      "Epoch 3987\n",
      "-------------------------------\n",
      "tensor(64.4669)\n",
      "tensor(35.9801)\n",
      "tensor(9.1253)\n",
      "tensor(0.0730)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 65.598335\n",
      "Epoch 3988\n",
      "-------------------------------\n",
      "tensor(31.0452)\n",
      "tensor(17.4950)\n",
      "tensor(11.0448)\n",
      "tensor(0.3361)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 65.354454\n",
      "Epoch 3989\n",
      "-------------------------------\n",
      "tensor(25.8476)\n",
      "tensor(20.1212)\n",
      "tensor(11.1014)\n",
      "tensor(0.1562)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 65.313789\n",
      "Epoch 3990\n",
      "-------------------------------\n",
      "tensor(31.7623)\n",
      "tensor(19.5405)\n",
      "tensor(14.2217)\n",
      "tensor(0.5099)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 65.245422\n",
      "Epoch 3991\n",
      "-------------------------------\n",
      "tensor(58.3530)\n",
      "tensor(36.0928)\n",
      "tensor(12.1650)\n",
      "tensor(0.5375)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 65.110603\n",
      "Epoch 3992\n",
      "-------------------------------\n",
      "tensor(55.3776)\n",
      "tensor(21.3173)\n",
      "tensor(18.8037)\n",
      "tensor(0.5102)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 65.223030\n",
      "Epoch 3993\n",
      "-------------------------------\n",
      "tensor(44.9151)\n",
      "tensor(32.6235)\n",
      "tensor(37.5582)\n",
      "tensor(1.3987)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 65.190353\n",
      "Epoch 3994\n",
      "-------------------------------\n",
      "tensor(33.8374)\n",
      "tensor(19.2722)\n",
      "tensor(13.2863)\n",
      "tensor(0.7820)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.014450\n",
      "Epoch 3995\n",
      "-------------------------------\n",
      "tensor(39.9210)\n",
      "tensor(21.5114)\n",
      "tensor(28.9008)\n",
      "tensor(1.5774)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 64.942558\n",
      "Epoch 3996\n",
      "-------------------------------\n",
      "tensor(66.3673)\n",
      "tensor(34.9856)\n",
      "tensor(7.7312)\n",
      "tensor(0.1376)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 64.947777\n",
      "Epoch 3997\n",
      "-------------------------------\n",
      "tensor(63.5362)\n",
      "tensor(32.3324)\n",
      "tensor(13.9168)\n",
      "tensor(0.6972)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 64.812683\n",
      "Epoch 3998\n",
      "-------------------------------\n",
      "tensor(34.4170)\n",
      "tensor(19.0836)\n",
      "tensor(15.5031)\n",
      "tensor(0.5594)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 64.840706\n",
      "Epoch 3999\n",
      "-------------------------------\n",
      "tensor(38.0089)\n",
      "tensor(18.8242)\n",
      "tensor(11.7211)\n",
      "tensor(0.2826)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 64.838608\n",
      "Epoch 4000\n",
      "-------------------------------\n",
      "tensor(35.2608)\n",
      "tensor(19.2800)\n",
      "tensor(6.1434)\n",
      "tensor(0.0850)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 64.823669\n",
      "Epoch 4001\n",
      "-------------------------------\n",
      "tensor(32.7898)\n",
      "tensor(19.6459)\n",
      "tensor(7.0875)\n",
      "tensor(0.0443)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 64.810028\n",
      "Epoch 4002\n",
      "-------------------------------\n",
      "tensor(24.1025)\n",
      "tensor(18.1767)\n",
      "tensor(10.4786)\n",
      "tensor(0.1487)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 64.796585\n",
      "Epoch 4003\n",
      "-------------------------------\n",
      "tensor(33.6136)\n",
      "tensor(19.9020)\n",
      "tensor(13.0734)\n",
      "tensor(0.2647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 64.786919\n",
      "Epoch 4004\n",
      "-------------------------------\n",
      "tensor(30.7761)\n",
      "tensor(18.6190)\n",
      "tensor(13.3174)\n",
      "tensor(0.4240)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 64.748383\n",
      "Epoch 4005\n",
      "-------------------------------\n",
      "tensor(33.2577)\n",
      "tensor(18.8530)\n",
      "tensor(9.6428)\n",
      "tensor(0.5671)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 64.679169\n",
      "Epoch 4006\n",
      "-------------------------------\n",
      "tensor(65.2598)\n",
      "tensor(38.2599)\n",
      "tensor(9.9179)\n",
      "tensor(0.5139)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.727852\n",
      "Epoch 4007\n",
      "-------------------------------\n",
      "tensor(58.4707)\n",
      "tensor(37.0659)\n",
      "tensor(18.5384)\n",
      "tensor(0.1031)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 64.719025\n",
      "Epoch 4008\n",
      "-------------------------------\n",
      "tensor(44.0711)\n",
      "tensor(21.4049)\n",
      "tensor(15.2610)\n",
      "tensor(0.6951)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.622696\n",
      "Epoch 4009\n",
      "-------------------------------\n",
      "tensor(38.1046)\n",
      "tensor(28.8729)\n",
      "tensor(31.6076)\n",
      "tensor(0.9599)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 64.651482\n",
      "Epoch 4010\n",
      "-------------------------------\n",
      "tensor(39.8470)\n",
      "tensor(22.0025)\n",
      "tensor(16.1687)\n",
      "tensor(0.8562)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 64.577301\n",
      "Epoch 4011\n",
      "-------------------------------\n",
      "tensor(44.2650)\n",
      "tensor(19.8641)\n",
      "tensor(24.2200)\n",
      "tensor(1.1852)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 64.473572\n",
      "Epoch 4012\n",
      "-------------------------------\n",
      "tensor(79.2372)\n",
      "tensor(30.3232)\n",
      "tensor(26.1198)\n",
      "tensor(0.9989)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.505096\n",
      "Epoch 4013\n",
      "-------------------------------\n",
      "tensor(53.7158)\n",
      "tensor(18.6602)\n",
      "tensor(22.8775)\n",
      "tensor(0.3689)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 64.347572\n",
      "Epoch 4014\n",
      "-------------------------------\n",
      "tensor(32.5077)\n",
      "tensor(18.4809)\n",
      "tensor(7.2350)\n",
      "tensor(0.2207)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.257065\n",
      "Epoch 4015\n",
      "-------------------------------\n",
      "tensor(84.2795)\n",
      "tensor(33.5012)\n",
      "tensor(24.5172)\n",
      "tensor(0.6552)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 64.275574\n",
      "Epoch 4016\n",
      "-------------------------------\n",
      "tensor(43.4577)\n",
      "tensor(16.6660)\n",
      "tensor(26.0888)\n",
      "tensor(0.9823)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 64.251633\n",
      "Epoch 4017\n",
      "-------------------------------\n",
      "tensor(43.5800)\n",
      "tensor(17.7697)\n",
      "tensor(17.7166)\n",
      "tensor(0.0756)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 64.229996\n",
      "Epoch 4018\n",
      "-------------------------------\n",
      "tensor(21.1548)\n",
      "tensor(24.8401)\n",
      "tensor(16.9303)\n",
      "tensor(1.0092)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 64.166542\n",
      "Epoch 4019\n",
      "-------------------------------\n",
      "tensor(32.4467)\n",
      "tensor(26.4326)\n",
      "tensor(24.8724)\n",
      "tensor(1.0595)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 64.132309\n",
      "Epoch 4020\n",
      "-------------------------------\n",
      "tensor(35.1354)\n",
      "tensor(23.4374)\n",
      "tensor(20.2559)\n",
      "tensor(0.6391)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 64.082047\n",
      "Epoch 4021\n",
      "-------------------------------\n",
      "tensor(27.1328)\n",
      "tensor(19.4070)\n",
      "tensor(12.9296)\n",
      "tensor(0.1816)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 64.043526\n",
      "Epoch 4022\n",
      "-------------------------------\n",
      "tensor(20.9836)\n",
      "tensor(16.4913)\n",
      "tensor(7.4538)\n",
      "tensor(0.2359)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 64.012573\n",
      "Epoch 4023\n",
      "-------------------------------\n",
      "tensor(22.4120)\n",
      "tensor(15.7236)\n",
      "tensor(10.8433)\n",
      "tensor(0.6835)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 63.979450\n",
      "Epoch 4024\n",
      "-------------------------------\n",
      "tensor(56.6188)\n",
      "tensor(41.7728)\n",
      "tensor(19.8375)\n",
      "tensor(1.1246)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 64.040649\n",
      "Epoch 4025\n",
      "-------------------------------\n",
      "tensor(60.2012)\n",
      "tensor(44.1461)\n",
      "tensor(25.8953)\n",
      "tensor(1.1781)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 64.079262\n",
      "Epoch 4026\n",
      "-------------------------------\n",
      "tensor(53.6973)\n",
      "tensor(19.2952)\n",
      "tensor(15.8135)\n",
      "tensor(0.2979)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.952019\n",
      "Epoch 4027\n",
      "-------------------------------\n",
      "tensor(36.0574)\n",
      "tensor(27.3876)\n",
      "tensor(27.8007)\n",
      "tensor(1.1287)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.930756\n",
      "Epoch 4028\n",
      "-------------------------------\n",
      "tensor(38.8668)\n",
      "tensor(27.5739)\n",
      "tensor(30.9518)\n",
      "tensor(0.9368)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.890060\n",
      "Epoch 4029\n",
      "-------------------------------\n",
      "tensor(57.9115)\n",
      "tensor(41.4858)\n",
      "tensor(28.6045)\n",
      "tensor(1.2091)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 63.867081\n",
      "Epoch 4030\n",
      "-------------------------------\n",
      "tensor(63.2352)\n",
      "tensor(18.4125)\n",
      "tensor(32.8227)\n",
      "tensor(0.8822)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 63.894997\n",
      "Epoch 4031\n",
      "-------------------------------\n",
      "tensor(68.4544)\n",
      "tensor(42.7339)\n",
      "tensor(61.9396)\n",
      "tensor(2.0962)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 63.929482\n",
      "Epoch 4032\n",
      "-------------------------------\n",
      "tensor(35.9021)\n",
      "tensor(16.4801)\n",
      "tensor(13.5903)\n",
      "tensor(0.2079)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.667278\n",
      "Epoch 4033\n",
      "-------------------------------\n",
      "tensor(60.4920)\n",
      "tensor(47.9022)\n",
      "tensor(37.1764)\n",
      "tensor(1.4940)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.657959\n",
      "Epoch 4034\n",
      "-------------------------------\n",
      "tensor(38.4890)\n",
      "tensor(20.9385)\n",
      "tensor(16.8621)\n",
      "tensor(0.2552)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.548546\n",
      "Epoch 4035\n",
      "-------------------------------\n",
      "tensor(41.3791)\n",
      "tensor(23.3909)\n",
      "tensor(27.3540)\n",
      "tensor(0.6067)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 63.521824\n",
      "Epoch 4036\n",
      "-------------------------------\n",
      "tensor(31.7926)\n",
      "tensor(17.3187)\n",
      "tensor(14.3481)\n",
      "tensor(0.5735)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 63.431202\n",
      "Epoch 4037\n",
      "-------------------------------\n",
      "tensor(43.6444)\n",
      "tensor(18.1921)\n",
      "tensor(20.6319)\n",
      "tensor(0.7957)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 63.378826\n",
      "Epoch 4038\n",
      "-------------------------------\n",
      "tensor(61.9797)\n",
      "tensor(34.1692)\n",
      "tensor(6.5312)\n",
      "tensor(0.2934)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 63.419430\n",
      "Epoch 4039\n",
      "-------------------------------\n",
      "tensor(62.6205)\n",
      "tensor(31.2966)\n",
      "tensor(8.4699)\n",
      "tensor(0.0732)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 63.357849\n",
      "Epoch 4040\n",
      "-------------------------------\n",
      "tensor(28.0950)\n",
      "tensor(18.7622)\n",
      "tensor(6.3759)\n",
      "tensor(0.1903)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 63.295448\n",
      "Epoch 4041\n",
      "-------------------------------\n",
      "tensor(29.6111)\n",
      "tensor(18.8808)\n",
      "tensor(6.1205)\n",
      "tensor(0.1970)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 63.300564\n",
      "Epoch 4042\n",
      "-------------------------------\n",
      "tensor(28.2766)\n",
      "tensor(18.6207)\n",
      "tensor(6.0572)\n",
      "tensor(0.1723)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 63.293686\n",
      "Epoch 4043\n",
      "-------------------------------\n",
      "tensor(22.9304)\n",
      "tensor(17.9423)\n",
      "tensor(5.7523)\n",
      "tensor(0.1231)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 63.280708\n",
      "Epoch 4044\n",
      "-------------------------------\n",
      "tensor(29.7264)\n",
      "tensor(18.7548)\n",
      "tensor(5.8926)\n",
      "tensor(0.0336)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 63.253693\n",
      "Epoch 4045\n",
      "-------------------------------\n",
      "tensor(68.6266)\n",
      "tensor(32.7648)\n",
      "tensor(8.7380)\n",
      "tensor(0.1151)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 63.228054\n",
      "Epoch 4046\n",
      "-------------------------------\n",
      "tensor(34.9976)\n",
      "tensor(19.6991)\n",
      "tensor(8.0478)\n",
      "tensor(0.4043)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.230541\n",
      "Epoch 4047\n",
      "-------------------------------\n",
      "tensor(32.5996)\n",
      "tensor(18.7966)\n",
      "tensor(8.1127)\n",
      "tensor(0.3849)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.194759\n",
      "Epoch 4048\n",
      "-------------------------------\n",
      "tensor(63.9139)\n",
      "tensor(30.9717)\n",
      "tensor(8.6976)\n",
      "tensor(0.1343)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.150299\n",
      "Epoch 4049\n",
      "-------------------------------\n",
      "tensor(39.8916)\n",
      "tensor(18.1745)\n",
      "tensor(14.9901)\n",
      "tensor(0.1707)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 63.165340\n",
      "Epoch 4050\n",
      "-------------------------------\n",
      "tensor(35.3528)\n",
      "tensor(22.9654)\n",
      "tensor(11.7323)\n",
      "tensor(0.4796)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 63.133537\n",
      "Epoch 4051\n",
      "-------------------------------\n",
      "tensor(40.9019)\n",
      "tensor(21.8452)\n",
      "tensor(20.2821)\n",
      "tensor(0.2173)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 63.089317\n",
      "Epoch 4052\n",
      "-------------------------------\n",
      "tensor(37.5512)\n",
      "tensor(19.9906)\n",
      "tensor(22.3969)\n",
      "tensor(1.2550)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.960102\n",
      "Epoch 4053\n",
      "-------------------------------\n",
      "tensor(63.6780)\n",
      "tensor(34.0421)\n",
      "tensor(7.9018)\n",
      "tensor(0.0093)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.110481\n",
      "Epoch 4054\n",
      "-------------------------------\n",
      "tensor(67.0559)\n",
      "tensor(34.2074)\n",
      "tensor(16.7004)\n",
      "tensor(0.5995)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.937077\n",
      "Epoch 4055\n",
      "-------------------------------\n",
      "tensor(37.6598)\n",
      "tensor(19.9500)\n",
      "tensor(11.1783)\n",
      "tensor(0.2723)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 62.896511\n",
      "Epoch 4056\n",
      "-------------------------------\n",
      "tensor(34.7884)\n",
      "tensor(23.8097)\n",
      "tensor(19.3440)\n",
      "tensor(0.0859)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 62.960434\n",
      "Epoch 4057\n",
      "-------------------------------\n",
      "tensor(36.1686)\n",
      "tensor(23.2074)\n",
      "tensor(18.5321)\n",
      "tensor(0.2292)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 62.939617\n",
      "Epoch 4058\n",
      "-------------------------------\n",
      "tensor(33.3240)\n",
      "tensor(20.5633)\n",
      "tensor(9.4886)\n",
      "tensor(0.3441)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 62.864704\n",
      "Epoch 4059\n",
      "-------------------------------\n",
      "tensor(34.7224)\n",
      "tensor(19.4254)\n",
      "tensor(9.0141)\n",
      "tensor(0.1887)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 62.794903\n",
      "Epoch 4060\n",
      "-------------------------------\n",
      "tensor(34.6851)\n",
      "tensor(19.1065)\n",
      "tensor(7.3400)\n",
      "tensor(0.0265)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 62.752510\n",
      "Epoch 4061\n",
      "-------------------------------\n",
      "tensor(32.0201)\n",
      "tensor(19.4183)\n",
      "tensor(6.3453)\n",
      "tensor(0.1685)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 62.713951\n",
      "Epoch 4062\n",
      "-------------------------------\n",
      "tensor(30.1005)\n",
      "tensor(19.6168)\n",
      "tensor(7.3683)\n",
      "tensor(0.2416)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 62.674046\n",
      "Epoch 4063\n",
      "-------------------------------\n",
      "tensor(25.8349)\n",
      "tensor(19.1136)\n",
      "tensor(9.9165)\n",
      "tensor(0.2476)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 62.631592\n",
      "Epoch 4064\n",
      "-------------------------------\n",
      "tensor(34.2818)\n",
      "tensor(19.4435)\n",
      "tensor(12.2345)\n",
      "tensor(0.1012)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 62.584690\n",
      "Epoch 4065\n",
      "-------------------------------\n",
      "tensor(75.5713)\n",
      "tensor(36.6466)\n",
      "tensor(11.4348)\n",
      "tensor(0.4074)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 62.713943\n",
      "Epoch 4066\n",
      "-------------------------------\n",
      "tensor(62.8672)\n",
      "tensor(44.1692)\n",
      "tensor(18.1556)\n",
      "tensor(1.1741)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.792770\n",
      "Epoch 4067\n",
      "-------------------------------\n",
      "tensor(58.9485)\n",
      "tensor(44.1959)\n",
      "tensor(33.0903)\n",
      "tensor(1.1346)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 62.681698\n",
      "Epoch 4068\n",
      "-------------------------------\n",
      "tensor(38.0613)\n",
      "tensor(22.1805)\n",
      "tensor(10.9895)\n",
      "tensor(0.5981)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.594212\n",
      "Epoch 4069\n",
      "-------------------------------\n",
      "tensor(47.2331)\n",
      "tensor(43.3433)\n",
      "tensor(48.8040)\n",
      "tensor(2.0274)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 62.760983\n",
      "Epoch 4070\n",
      "-------------------------------\n",
      "tensor(44.8942)\n",
      "tensor(21.9464)\n",
      "tensor(13.3642)\n",
      "tensor(0.3056)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 62.672661\n",
      "Epoch 4071\n",
      "-------------------------------\n",
      "tensor(53.6843)\n",
      "tensor(20.4284)\n",
      "tensor(29.7710)\n",
      "tensor(1.2770)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 62.558701\n",
      "Epoch 4072\n",
      "-------------------------------\n",
      "tensor(50.5231)\n",
      "tensor(28.5724)\n",
      "tensor(38.6169)\n",
      "tensor(1.0064)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.410515\n",
      "Epoch 4073\n",
      "-------------------------------\n",
      "tensor(70.3113)\n",
      "tensor(37.9890)\n",
      "tensor(12.5330)\n",
      "tensor(0.0347)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 62.290985\n",
      "Epoch 4074\n",
      "-------------------------------\n",
      "tensor(70.4000)\n",
      "tensor(25.6872)\n",
      "tensor(47.8526)\n",
      "tensor(1.4093)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.294621\n",
      "Epoch 4075\n",
      "-------------------------------\n",
      "tensor(90.9190)\n",
      "tensor(30.1630)\n",
      "tensor(29.5661)\n",
      "tensor(0.1014)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 62.174644\n",
      "Epoch 4076\n",
      "-------------------------------\n",
      "tensor(34.9341)\n",
      "tensor(20.8832)\n",
      "tensor(12.3756)\n",
      "tensor(0.0132)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 62.163395\n",
      "Epoch 4077\n",
      "-------------------------------\n",
      "tensor(39.8266)\n",
      "tensor(17.3671)\n",
      "tensor(15.5671)\n",
      "tensor(0.3765)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 62.187908\n",
      "Epoch 4078\n",
      "-------------------------------\n",
      "tensor(36.4486)\n",
      "tensor(18.1516)\n",
      "tensor(12.4148)\n",
      "tensor(0.1119)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 62.161644\n",
      "Epoch 4079\n",
      "-------------------------------\n",
      "tensor(25.3013)\n",
      "tensor(21.1109)\n",
      "tensor(6.8255)\n",
      "tensor(0.2861)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 62.109344\n",
      "Epoch 4080\n",
      "-------------------------------\n",
      "tensor(23.1176)\n",
      "tensor(22.7400)\n",
      "tensor(12.8076)\n",
      "tensor(0.4619)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 62.067867\n",
      "Epoch 4081\n",
      "-------------------------------\n",
      "tensor(23.5702)\n",
      "tensor(22.5201)\n",
      "tensor(14.6031)\n",
      "tensor(0.4522)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 62.032066\n",
      "Epoch 4082\n",
      "-------------------------------\n",
      "tensor(23.0453)\n",
      "tensor(21.2139)\n",
      "tensor(13.1830)\n",
      "tensor(0.3399)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 61.994080\n",
      "Epoch 4083\n",
      "-------------------------------\n",
      "tensor(34.8531)\n",
      "tensor(20.8154)\n",
      "tensor(9.0812)\n",
      "tensor(0.1140)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 61.961849\n",
      "Epoch 4084\n",
      "-------------------------------\n",
      "tensor(39.3992)\n",
      "tensor(19.5256)\n",
      "tensor(6.2102)\n",
      "tensor(0.2884)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 61.907772\n",
      "Epoch 4085\n",
      "-------------------------------\n",
      "tensor(62.1281)\n",
      "tensor(39.5593)\n",
      "tensor(12.7403)\n",
      "tensor(0.8363)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 62.038658\n",
      "Epoch 4086\n",
      "-------------------------------\n",
      "tensor(57.5055)\n",
      "tensor(42.6073)\n",
      "tensor(20.0049)\n",
      "tensor(1.0614)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.131939\n",
      "Epoch 4087\n",
      "-------------------------------\n",
      "tensor(60.3376)\n",
      "tensor(37.5658)\n",
      "tensor(15.4468)\n",
      "tensor(0.3333)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 62.042587\n",
      "Epoch 4088\n",
      "-------------------------------\n",
      "tensor(36.2828)\n",
      "tensor(24.5613)\n",
      "tensor(12.6884)\n",
      "tensor(0.8762)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.892075\n",
      "Epoch 4089\n",
      "-------------------------------\n",
      "tensor(37.3448)\n",
      "tensor(31.0633)\n",
      "tensor(26.9939)\n",
      "tensor(0.9561)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 61.944126\n",
      "Epoch 4090\n",
      "-------------------------------\n",
      "tensor(39.3375)\n",
      "tensor(19.8097)\n",
      "tensor(17.0604)\n",
      "tensor(0.8023)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 61.933056\n",
      "Epoch 4091\n",
      "-------------------------------\n",
      "tensor(39.2582)\n",
      "tensor(17.9767)\n",
      "tensor(16.9048)\n",
      "tensor(0.7077)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 61.823540\n",
      "Epoch 4092\n",
      "-------------------------------\n",
      "tensor(36.7808)\n",
      "tensor(27.2233)\n",
      "tensor(27.3082)\n",
      "tensor(1.1912)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.696869\n",
      "Epoch 4093\n",
      "-------------------------------\n",
      "tensor(71.4903)\n",
      "tensor(35.8374)\n",
      "tensor(9.8049)\n",
      "tensor(0.0484)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 61.602516\n",
      "Epoch 4094\n",
      "-------------------------------\n",
      "tensor(59.2346)\n",
      "tensor(26.1382)\n",
      "tensor(41.0655)\n",
      "tensor(1.6779)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 61.549477\n",
      "Epoch 4095\n",
      "-------------------------------\n",
      "tensor(43.4584)\n",
      "tensor(20.5147)\n",
      "tensor(23.3933)\n",
      "tensor(0.1999)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 61.483566\n",
      "Epoch 4096\n",
      "-------------------------------\n",
      "tensor(83.7588)\n",
      "tensor(28.8271)\n",
      "tensor(26.5700)\n",
      "tensor(0.3082)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 61.501175\n",
      "Epoch 4097\n",
      "-------------------------------\n",
      "tensor(49.7334)\n",
      "tensor(16.8408)\n",
      "tensor(24.2976)\n",
      "tensor(0.2809)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 61.443359\n",
      "Epoch 4098\n",
      "-------------------------------\n",
      "tensor(55.5360)\n",
      "tensor(16.9601)\n",
      "tensor(26.5638)\n",
      "tensor(0.1248)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 61.451176\n",
      "Epoch 4099\n",
      "-------------------------------\n",
      "tensor(33.7215)\n",
      "tensor(20.4778)\n",
      "tensor(7.7764)\n",
      "tensor(0.3586)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 61.372841\n",
      "Epoch 4100\n",
      "-------------------------------\n",
      "tensor(22.8582)\n",
      "tensor(23.3254)\n",
      "tensor(15.6345)\n",
      "tensor(0.5988)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 61.337437\n",
      "Epoch 4101\n",
      "-------------------------------\n",
      "tensor(28.3329)\n",
      "tensor(23.9306)\n",
      "tensor(21.2872)\n",
      "tensor(0.5868)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 61.315746\n",
      "Epoch 4102\n",
      "-------------------------------\n",
      "tensor(70.7935)\n",
      "tensor(27.0964)\n",
      "tensor(20.7573)\n",
      "tensor(0.4206)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 61.303814\n",
      "Epoch 4103\n",
      "-------------------------------\n",
      "tensor(63.9535)\n",
      "tensor(29.6810)\n",
      "tensor(11.7475)\n",
      "tensor(0.0675)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 61.273247\n",
      "Epoch 4104\n",
      "-------------------------------\n",
      "tensor(36.7346)\n",
      "tensor(16.7632)\n",
      "tensor(14.0316)\n",
      "tensor(0.5236)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 61.259144\n",
      "Epoch 4105\n",
      "-------------------------------\n",
      "tensor(62.4072)\n",
      "tensor(19.8640)\n",
      "tensor(31.9137)\n",
      "tensor(0.9804)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 61.288937\n",
      "Epoch 4106\n",
      "-------------------------------\n",
      "tensor(46.4628)\n",
      "tensor(18.7797)\n",
      "tensor(15.2370)\n",
      "tensor(0.4915)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 61.243797\n",
      "Epoch 4107\n",
      "-------------------------------\n",
      "tensor(93.1846)\n",
      "tensor(28.9414)\n",
      "tensor(34.9227)\n",
      "tensor(0.7173)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 61.281837\n",
      "Epoch 4108\n",
      "-------------------------------\n",
      "tensor(34.2062)\n",
      "tensor(21.0675)\n",
      "tensor(11.8874)\n",
      "tensor(0.1675)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.258957\n",
      "Epoch 4109\n",
      "-------------------------------\n",
      "tensor(60.5857)\n",
      "tensor(20.0984)\n",
      "tensor(33.4040)\n",
      "tensor(1.0806)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 61.280209\n",
      "Epoch 4110\n",
      "-------------------------------\n",
      "tensor(28.5481)\n",
      "tensor(21.0833)\n",
      "tensor(17.3929)\n",
      "tensor(0.2065)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 61.118488\n",
      "Epoch 4111\n",
      "-------------------------------\n",
      "tensor(36.6488)\n",
      "tensor(23.0553)\n",
      "tensor(23.7286)\n",
      "tensor(0.5396)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 61.055786\n",
      "Epoch 4112\n",
      "-------------------------------\n",
      "tensor(59.7062)\n",
      "tensor(44.1810)\n",
      "tensor(31.4264)\n",
      "tensor(1.0939)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.253944\n",
      "Epoch 4113\n",
      "-------------------------------\n",
      "tensor(53.3942)\n",
      "tensor(34.4192)\n",
      "tensor(14.7589)\n",
      "tensor(0.1888)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 61.037262\n",
      "Epoch 4114\n",
      "-------------------------------\n",
      "tensor(23.6227)\n",
      "tensor(29.1089)\n",
      "tensor(21.2568)\n",
      "tensor(1.0734)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 60.976845\n",
      "Epoch 4115\n",
      "-------------------------------\n",
      "tensor(33.1245)\n",
      "tensor(26.4608)\n",
      "tensor(13.9379)\n",
      "tensor(0.4362)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 61.043198\n",
      "Epoch 4116\n",
      "-------------------------------\n",
      "tensor(39.7137)\n",
      "tensor(19.8862)\n",
      "tensor(12.8117)\n",
      "tensor(0.5109)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 60.997448\n",
      "Epoch 4117\n",
      "-------------------------------\n",
      "tensor(40.7807)\n",
      "tensor(21.0252)\n",
      "tensor(9.3134)\n",
      "tensor(0.4349)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 60.908772\n",
      "Epoch 4118\n",
      "-------------------------------\n",
      "tensor(32.6064)\n",
      "tensor(21.6279)\n",
      "tensor(10.0113)\n",
      "tensor(0.0185)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 60.826878\n",
      "Epoch 4119\n",
      "-------------------------------\n",
      "tensor(30.8438)\n",
      "tensor(20.2730)\n",
      "tensor(12.1404)\n",
      "tensor(0.1393)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 60.751938\n",
      "Epoch 4120\n",
      "-------------------------------\n",
      "tensor(30.0717)\n",
      "tensor(19.1223)\n",
      "tensor(9.6978)\n",
      "tensor(0.0760)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 60.686657\n",
      "Epoch 4121\n",
      "-------------------------------\n",
      "tensor(20.7003)\n",
      "tensor(16.8945)\n",
      "tensor(7.3123)\n",
      "tensor(0.0437)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 60.652229\n",
      "Epoch 4122\n",
      "-------------------------------\n",
      "tensor(23.2618)\n",
      "tensor(16.5055)\n",
      "tensor(6.4387)\n",
      "tensor(0.1731)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 60.625271\n",
      "Epoch 4123\n",
      "-------------------------------\n",
      "tensor(59.9743)\n",
      "tensor(35.5310)\n",
      "tensor(7.7850)\n",
      "tensor(0.3601)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 60.699112\n",
      "Epoch 4124\n",
      "-------------------------------\n",
      "tensor(57.8500)\n",
      "tensor(38.0164)\n",
      "tensor(13.5660)\n",
      "tensor(0.5707)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 60.725430\n",
      "Epoch 4125\n",
      "-------------------------------\n",
      "tensor(52.3372)\n",
      "tensor(38.0231)\n",
      "tensor(20.6215)\n",
      "tensor(0.6180)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 60.640530\n",
      "Epoch 4126\n",
      "-------------------------------\n",
      "tensor(43.5981)\n",
      "tensor(17.4347)\n",
      "tensor(16.6882)\n",
      "tensor(0.1659)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 60.636425\n",
      "Epoch 4127\n",
      "-------------------------------\n",
      "tensor(36.5916)\n",
      "tensor(29.0437)\n",
      "tensor(21.0007)\n",
      "tensor(0.8347)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 60.729362\n",
      "Epoch 4128\n",
      "-------------------------------\n",
      "tensor(47.3771)\n",
      "tensor(32.5010)\n",
      "tensor(33.6198)\n",
      "tensor(0.7699)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.786198\n",
      "Epoch 4129\n",
      "-------------------------------\n",
      "tensor(50.3833)\n",
      "tensor(19.6805)\n",
      "tensor(21.1991)\n",
      "tensor(0.8711)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 60.663719\n",
      "Epoch 4130\n",
      "-------------------------------\n",
      "tensor(43.9273)\n",
      "tensor(19.9724)\n",
      "tensor(12.0186)\n",
      "tensor(0.3588)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 60.504963\n",
      "Epoch 4131\n",
      "-------------------------------\n",
      "tensor(90.6629)\n",
      "tensor(27.2164)\n",
      "tensor(37.5074)\n",
      "tensor(1.0911)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 60.639702\n",
      "Epoch 4132\n",
      "-------------------------------\n",
      "tensor(67.7031)\n",
      "tensor(44.6959)\n",
      "tensor(44.7499)\n",
      "tensor(0.8891)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.536079\n",
      "Epoch 4133\n",
      "-------------------------------\n",
      "tensor(42.2721)\n",
      "tensor(18.8373)\n",
      "tensor(11.6329)\n",
      "tensor(0.1330)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 60.409264\n",
      "Epoch 4134\n",
      "-------------------------------\n",
      "tensor(75.7829)\n",
      "tensor(34.9587)\n",
      "tensor(55.4387)\n",
      "tensor(0.9766)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 60.584011\n",
      "Epoch 4135\n",
      "-------------------------------\n",
      "tensor(38.2937)\n",
      "tensor(17.8267)\n",
      "tensor(20.3458)\n",
      "tensor(0.9261)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 60.414238\n",
      "Epoch 4136\n",
      "-------------------------------\n",
      "tensor(59.1950)\n",
      "tensor(18.3113)\n",
      "tensor(35.2949)\n",
      "tensor(0.8889)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 60.369473\n",
      "Epoch 4137\n",
      "-------------------------------\n",
      "tensor(30.3957)\n",
      "tensor(21.9191)\n",
      "tensor(14.3056)\n",
      "tensor(0.5843)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 60.203964\n",
      "Epoch 4138\n",
      "-------------------------------\n",
      "tensor(42.2611)\n",
      "tensor(25.1536)\n",
      "tensor(33.2527)\n",
      "tensor(0.9390)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 60.138664\n",
      "Epoch 4139\n",
      "-------------------------------\n",
      "tensor(76.6921)\n",
      "tensor(29.2057)\n",
      "tensor(20.7827)\n",
      "tensor(0.3017)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 60.082687\n",
      "Epoch 4140\n",
      "-------------------------------\n",
      "tensor(62.9834)\n",
      "tensor(36.0087)\n",
      "tensor(7.2403)\n",
      "tensor(0.3912)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 60.042969\n",
      "Epoch 4141\n",
      "-------------------------------\n",
      "tensor(49.6411)\n",
      "tensor(21.4695)\n",
      "tensor(18.9360)\n",
      "tensor(0.7835)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 60.034752\n",
      "Epoch 4142\n",
      "-------------------------------\n",
      "tensor(53.7780)\n",
      "tensor(21.1749)\n",
      "tensor(23.9925)\n",
      "tensor(0.9353)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 60.029800\n",
      "Epoch 4143\n",
      "-------------------------------\n",
      "tensor(37.0044)\n",
      "tensor(17.1277)\n",
      "tensor(20.8232)\n",
      "tensor(0.8600)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 60.003574\n",
      "Epoch 4144\n",
      "-------------------------------\n",
      "tensor(25.6263)\n",
      "tensor(16.6835)\n",
      "tensor(7.4268)\n",
      "tensor(0.4115)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 59.970509\n",
      "Epoch 4145\n",
      "-------------------------------\n",
      "tensor(73.8766)\n",
      "tensor(26.6242)\n",
      "tensor(22.8916)\n",
      "tensor(0.3689)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 59.999378\n",
      "Epoch 4146\n",
      "-------------------------------\n",
      "tensor(28.8931)\n",
      "tensor(25.8569)\n",
      "tensor(21.8838)\n",
      "tensor(0.7594)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 59.963085\n",
      "Epoch 4147\n",
      "-------------------------------\n",
      "tensor(43.9605)\n",
      "tensor(20.7868)\n",
      "tensor(10.2870)\n",
      "tensor(0.2456)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 59.989605\n",
      "Epoch 4148\n",
      "-------------------------------\n",
      "tensor(54.7283)\n",
      "tensor(31.5862)\n",
      "tensor(14.9254)\n",
      "tensor(0.2069)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.985317\n",
      "Epoch 4149\n",
      "-------------------------------\n",
      "tensor(44.1150)\n",
      "tensor(23.6903)\n",
      "tensor(7.9191)\n",
      "tensor(0.1107)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 59.925835\n",
      "Epoch 4150\n",
      "-------------------------------\n",
      "tensor(44.8514)\n",
      "tensor(28.7424)\n",
      "tensor(31.5210)\n",
      "tensor(0.4966)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 59.871483\n",
      "Epoch 4151\n",
      "-------------------------------\n",
      "tensor(54.0424)\n",
      "tensor(36.5736)\n",
      "tensor(19.3177)\n",
      "tensor(0.7142)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 59.746334\n",
      "Epoch 4152\n",
      "-------------------------------\n",
      "tensor(65.9620)\n",
      "tensor(19.1835)\n",
      "tensor(28.4473)\n",
      "tensor(0.2025)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.812366\n",
      "Epoch 4153\n",
      "-------------------------------\n",
      "tensor(70.3795)\n",
      "tensor(35.0173)\n",
      "tensor(55.8503)\n",
      "tensor(1.4963)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 59.875557\n",
      "Epoch 4154\n",
      "-------------------------------\n",
      "tensor(28.4765)\n",
      "tensor(19.1104)\n",
      "tensor(20.1108)\n",
      "tensor(1.2204)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 59.676373\n",
      "Epoch 4155\n",
      "-------------------------------\n",
      "tensor(51.7680)\n",
      "tensor(48.2746)\n",
      "tensor(38.7179)\n",
      "tensor(1.6756)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 59.671898\n",
      "Epoch 4156\n",
      "-------------------------------\n",
      "tensor(22.7792)\n",
      "tensor(18.1367)\n",
      "tensor(8.6704)\n",
      "tensor(0.2854)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 59.547813\n",
      "Epoch 4157\n",
      "-------------------------------\n",
      "tensor(33.2653)\n",
      "tensor(28.0339)\n",
      "tensor(31.4543)\n",
      "tensor(1.2660)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 59.535507\n",
      "Epoch 4158\n",
      "-------------------------------\n",
      "tensor(36.9289)\n",
      "tensor(23.5230)\n",
      "tensor(17.8897)\n",
      "tensor(0.6239)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 59.481503\n",
      "Epoch 4159\n",
      "-------------------------------\n",
      "tensor(61.1325)\n",
      "tensor(32.8676)\n",
      "tensor(6.1225)\n",
      "tensor(0.2847)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 59.440948\n",
      "Epoch 4160\n",
      "-------------------------------\n",
      "tensor(44.5708)\n",
      "tensor(19.1904)\n",
      "tensor(17.0515)\n",
      "tensor(0.7772)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 59.404667\n",
      "Epoch 4161\n",
      "-------------------------------\n",
      "tensor(33.8978)\n",
      "tensor(16.3139)\n",
      "tensor(18.4757)\n",
      "tensor(0.8661)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 59.397873\n",
      "Epoch 4162\n",
      "-------------------------------\n",
      "tensor(30.3389)\n",
      "tensor(16.1429)\n",
      "tensor(14.7022)\n",
      "tensor(0.7425)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 59.380470\n",
      "Epoch 4163\n",
      "-------------------------------\n",
      "tensor(53.2894)\n",
      "tensor(31.3651)\n",
      "tensor(7.8066)\n",
      "tensor(0.4317)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 59.394852\n",
      "Epoch 4164\n",
      "-------------------------------\n",
      "tensor(60.4068)\n",
      "tensor(28.3121)\n",
      "tensor(9.2391)\n",
      "tensor(0.1245)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 59.372871\n",
      "Epoch 4165\n",
      "-------------------------------\n",
      "tensor(27.1917)\n",
      "tensor(24.2271)\n",
      "tensor(13.0471)\n",
      "tensor(0.7101)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 59.380203\n",
      "Epoch 4166\n",
      "-------------------------------\n",
      "tensor(37.4033)\n",
      "tensor(26.3685)\n",
      "tensor(13.0534)\n",
      "tensor(0.9180)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 59.426807\n",
      "Epoch 4167\n",
      "-------------------------------\n",
      "tensor(35.5013)\n",
      "tensor(22.4768)\n",
      "tensor(6.6983)\n",
      "tensor(0.3684)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 59.429749\n",
      "Epoch 4168\n",
      "-------------------------------\n",
      "tensor(43.1228)\n",
      "tensor(23.5393)\n",
      "tensor(11.0667)\n",
      "tensor(0.5705)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.366024\n",
      "Epoch 4169\n",
      "-------------------------------\n",
      "tensor(73.2036)\n",
      "tensor(36.9721)\n",
      "tensor(18.1369)\n",
      "tensor(0.9941)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 59.269825\n",
      "Epoch 4170\n",
      "-------------------------------\n",
      "tensor(55.3730)\n",
      "tensor(18.5721)\n",
      "tensor(26.3225)\n",
      "tensor(0.5776)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 59.247910\n",
      "Epoch 4171\n",
      "-------------------------------\n",
      "tensor(25.7218)\n",
      "tensor(31.3395)\n",
      "tensor(30.1737)\n",
      "tensor(1.5891)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 59.234940\n",
      "Epoch 4172\n",
      "-------------------------------\n",
      "tensor(62.2042)\n",
      "tensor(28.3109)\n",
      "tensor(11.5427)\n",
      "tensor(0.1679)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.226128\n",
      "Epoch 4173\n",
      "-------------------------------\n",
      "tensor(78.9106)\n",
      "tensor(25.2242)\n",
      "tensor(55.4449)\n",
      "tensor(1.9658)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 59.298759\n",
      "Epoch 4174\n",
      "-------------------------------\n",
      "tensor(53.2252)\n",
      "tensor(29.5224)\n",
      "tensor(39.8382)\n",
      "tensor(0.8271)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 59.186497\n",
      "Epoch 4175\n",
      "-------------------------------\n",
      "tensor(36.2532)\n",
      "tensor(25.8125)\n",
      "tensor(27.0582)\n",
      "tensor(0.8122)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 59.047413\n",
      "Epoch 4176\n",
      "-------------------------------\n",
      "tensor(53.2513)\n",
      "tensor(40.0904)\n",
      "tensor(31.7327)\n",
      "tensor(0.8428)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 59.036903\n",
      "Epoch 4177\n",
      "-------------------------------\n",
      "tensor(60.7705)\n",
      "tensor(40.4195)\n",
      "tensor(34.5009)\n",
      "tensor(0.8192)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 58.944977\n",
      "Epoch 4178\n",
      "-------------------------------\n",
      "tensor(41.8043)\n",
      "tensor(20.8254)\n",
      "tensor(5.9746)\n",
      "tensor(0.1018)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 58.938686\n",
      "Epoch 4179\n",
      "-------------------------------\n",
      "tensor(40.9725)\n",
      "tensor(27.1222)\n",
      "tensor(23.8366)\n",
      "tensor(0.6454)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 58.946712\n",
      "Epoch 4180\n",
      "-------------------------------\n",
      "tensor(41.4316)\n",
      "tensor(27.5224)\n",
      "tensor(28.7864)\n",
      "tensor(0.6293)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 58.947346\n",
      "Epoch 4181\n",
      "-------------------------------\n",
      "tensor(37.7466)\n",
      "tensor(25.2798)\n",
      "tensor(23.1517)\n",
      "tensor(0.3817)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 58.913597\n",
      "Epoch 4182\n",
      "-------------------------------\n",
      "tensor(27.6344)\n",
      "tensor(21.3280)\n",
      "tensor(14.1240)\n",
      "tensor(0.0673)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 58.884140\n",
      "Epoch 4183\n",
      "-------------------------------\n",
      "tensor(26.0364)\n",
      "tensor(17.8410)\n",
      "tensor(7.1899)\n",
      "tensor(0.3396)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 58.851414\n",
      "Epoch 4184\n",
      "-------------------------------\n",
      "tensor(41.1938)\n",
      "tensor(17.0972)\n",
      "tensor(20.9357)\n",
      "tensor(0.7790)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 58.821384\n",
      "Epoch 4185\n",
      "-------------------------------\n",
      "tensor(52.4451)\n",
      "tensor(39.1121)\n",
      "tensor(27.0444)\n",
      "tensor(0.8686)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 58.828632\n",
      "Epoch 4186\n",
      "-------------------------------\n",
      "tensor(56.4155)\n",
      "tensor(34.4140)\n",
      "tensor(13.6421)\n",
      "tensor(0.1681)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 58.873264\n",
      "Epoch 4187\n",
      "-------------------------------\n",
      "tensor(69.7074)\n",
      "tensor(27.8770)\n",
      "tensor(18.5237)\n",
      "tensor(0.8998)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 58.789589\n",
      "Epoch 4188\n",
      "-------------------------------\n",
      "tensor(33.9279)\n",
      "tensor(23.9645)\n",
      "tensor(9.1026)\n",
      "tensor(0.5022)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.836739\n",
      "Epoch 4189\n",
      "-------------------------------\n",
      "tensor(40.9520)\n",
      "tensor(21.8544)\n",
      "tensor(14.0789)\n",
      "tensor(0.6338)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.885918\n",
      "Epoch 4190\n",
      "-------------------------------\n",
      "tensor(40.9142)\n",
      "tensor(25.3517)\n",
      "tensor(15.7717)\n",
      "tensor(0.3346)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 58.882915\n",
      "Epoch 4191\n",
      "-------------------------------\n",
      "tensor(33.5883)\n",
      "tensor(25.3854)\n",
      "tensor(12.1990)\n",
      "tensor(0.4511)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.786705\n",
      "Epoch 4192\n",
      "-------------------------------\n",
      "tensor(39.4436)\n",
      "tensor(19.5787)\n",
      "tensor(10.4891)\n",
      "tensor(0.1931)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.648575\n",
      "Epoch 4193\n",
      "-------------------------------\n",
      "tensor(65.0385)\n",
      "tensor(34.0929)\n",
      "tensor(9.5023)\n",
      "tensor(0.3984)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 58.555122\n",
      "Epoch 4194\n",
      "-------------------------------\n",
      "tensor(52.3211)\n",
      "tensor(42.9259)\n",
      "tensor(27.4152)\n",
      "tensor(1.1617)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 58.488060\n",
      "Epoch 4195\n",
      "-------------------------------\n",
      "tensor(44.7566)\n",
      "tensor(19.5551)\n",
      "tensor(11.5933)\n",
      "tensor(0.0767)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 58.524456\n",
      "Epoch 4196\n",
      "-------------------------------\n",
      "tensor(39.5232)\n",
      "tensor(31.3758)\n",
      "tensor(31.2715)\n",
      "tensor(1.0281)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 58.555866\n",
      "Epoch 4197\n",
      "-------------------------------\n",
      "tensor(30.9590)\n",
      "tensor(26.0323)\n",
      "tensor(19.7270)\n",
      "tensor(0.4298)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 58.521046\n",
      "Epoch 4198\n",
      "-------------------------------\n",
      "tensor(33.9989)\n",
      "tensor(18.3824)\n",
      "tensor(11.1477)\n",
      "tensor(0.4212)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 58.455746\n",
      "Epoch 4199\n",
      "-------------------------------\n",
      "tensor(39.9508)\n",
      "tensor(17.4008)\n",
      "tensor(16.8873)\n",
      "tensor(0.6211)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 58.397709\n",
      "Epoch 4200\n",
      "-------------------------------\n",
      "tensor(33.6830)\n",
      "tensor(17.3499)\n",
      "tensor(11.5151)\n",
      "tensor(0.4320)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 58.330643\n",
      "Epoch 4201\n",
      "-------------------------------\n",
      "tensor(27.4097)\n",
      "tensor(17.9046)\n",
      "tensor(5.6962)\n",
      "tensor(0.1917)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 58.278023\n",
      "Epoch 4202\n",
      "-------------------------------\n",
      "tensor(29.5297)\n",
      "tensor(19.4753)\n",
      "tensor(6.8895)\n",
      "tensor(0.0202)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 58.242310\n",
      "Epoch 4203\n",
      "-------------------------------\n",
      "tensor(65.2485)\n",
      "tensor(28.1934)\n",
      "tensor(12.8186)\n",
      "tensor(0.2025)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 58.231850\n",
      "Epoch 4204\n",
      "-------------------------------\n",
      "tensor(65.6821)\n",
      "tensor(28.7187)\n",
      "tensor(12.5888)\n",
      "tensor(0.2304)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 58.260265\n",
      "Epoch 4205\n",
      "-------------------------------\n",
      "tensor(54.6070)\n",
      "tensor(31.4561)\n",
      "tensor(7.5983)\n",
      "tensor(0.0266)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 58.198021\n",
      "Epoch 4206\n",
      "-------------------------------\n",
      "tensor(66.7077)\n",
      "tensor(19.9677)\n",
      "tensor(27.1945)\n",
      "tensor(0.4194)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 58.277756\n",
      "Epoch 4207\n",
      "-------------------------------\n",
      "tensor(44.0641)\n",
      "tensor(21.3957)\n",
      "tensor(5.6137)\n",
      "tensor(0.0803)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 58.276825\n",
      "Epoch 4208\n",
      "-------------------------------\n",
      "tensor(59.3971)\n",
      "tensor(29.8553)\n",
      "tensor(40.6445)\n",
      "tensor(0.4337)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.296944\n",
      "Epoch 4209\n",
      "-------------------------------\n",
      "tensor(33.2357)\n",
      "tensor(20.1451)\n",
      "tensor(11.9593)\n",
      "tensor(0.7106)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.140541\n",
      "Epoch 4210\n",
      "-------------------------------\n",
      "tensor(54.1624)\n",
      "tensor(38.1052)\n",
      "tensor(32.3026)\n",
      "tensor(0.7124)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 58.070251\n",
      "Epoch 4211\n",
      "-------------------------------\n",
      "tensor(32.4553)\n",
      "tensor(28.3679)\n",
      "tensor(21.2897)\n",
      "tensor(1.3445)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.027714\n",
      "Epoch 4212\n",
      "-------------------------------\n",
      "tensor(82.6050)\n",
      "tensor(24.9267)\n",
      "tensor(31.6987)\n",
      "tensor(0.6762)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.011852\n",
      "Epoch 4213\n",
      "-------------------------------\n",
      "tensor(92.9406)\n",
      "tensor(29.9467)\n",
      "tensor(63.6890)\n",
      "tensor(2.3476)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 58.225086\n",
      "Epoch 4214\n",
      "-------------------------------\n",
      "tensor(45.3245)\n",
      "tensor(28.9978)\n",
      "tensor(33.3859)\n",
      "tensor(0.6469)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 58.045639\n",
      "Epoch 4215\n",
      "-------------------------------\n",
      "tensor(40.2992)\n",
      "tensor(32.2810)\n",
      "tensor(37.3092)\n",
      "tensor(1.3424)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 57.962471\n",
      "Epoch 4216\n",
      "-------------------------------\n",
      "tensor(62.3044)\n",
      "tensor(19.8069)\n",
      "tensor(28.6922)\n",
      "tensor(0.4659)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 57.874920\n",
      "Epoch 4217\n",
      "-------------------------------\n",
      "tensor(56.8465)\n",
      "tensor(40.2829)\n",
      "tensor(24.1014)\n",
      "tensor(0.9025)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 57.771957\n",
      "Epoch 4218\n",
      "-------------------------------\n",
      "tensor(58.3529)\n",
      "tensor(33.0357)\n",
      "tensor(7.1156)\n",
      "tensor(0.4784)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 57.716988\n",
      "Epoch 4219\n",
      "-------------------------------\n",
      "tensor(25.6656)\n",
      "tensor(18.6593)\n",
      "tensor(9.1051)\n",
      "tensor(0.1241)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 57.695793\n",
      "Epoch 4220\n",
      "-------------------------------\n",
      "tensor(25.6525)\n",
      "tensor(19.5259)\n",
      "tensor(9.8594)\n",
      "tensor(0.0232)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 57.699093\n",
      "Epoch 4221\n",
      "-------------------------------\n",
      "tensor(25.0550)\n",
      "tensor(19.4969)\n",
      "tensor(8.4033)\n",
      "tensor(0.0492)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 57.687531\n",
      "Epoch 4222\n",
      "-------------------------------\n",
      "tensor(27.0029)\n",
      "tensor(19.3579)\n",
      "tensor(6.4395)\n",
      "tensor(0.0241)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 57.670078\n",
      "Epoch 4223\n",
      "-------------------------------\n",
      "tensor(25.3570)\n",
      "tensor(18.3407)\n",
      "tensor(4.9732)\n",
      "tensor(0.0382)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 57.643936\n",
      "Epoch 4224\n",
      "-------------------------------\n",
      "tensor(39.3368)\n",
      "tensor(19.5264)\n",
      "tensor(5.8670)\n",
      "tensor(0.1265)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 57.619072\n",
      "Epoch 4225\n",
      "-------------------------------\n",
      "tensor(57.3066)\n",
      "tensor(31.6716)\n",
      "tensor(5.8974)\n",
      "tensor(0.2435)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 57.698799\n",
      "Epoch 4226\n",
      "-------------------------------\n",
      "tensor(59.8198)\n",
      "tensor(33.3250)\n",
      "tensor(8.1740)\n",
      "tensor(0.2915)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 57.703068\n",
      "Epoch 4227\n",
      "-------------------------------\n",
      "tensor(46.2862)\n",
      "tensor(20.8112)\n",
      "tensor(9.5386)\n",
      "tensor(0.0498)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 57.594391\n",
      "Epoch 4228\n",
      "-------------------------------\n",
      "tensor(30.1735)\n",
      "tensor(26.0127)\n",
      "tensor(18.2979)\n",
      "tensor(0.5146)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.587326\n",
      "Epoch 4229\n",
      "-------------------------------\n",
      "tensor(60.3078)\n",
      "tensor(24.5472)\n",
      "tensor(15.4420)\n",
      "tensor(0.2049)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 57.524708\n",
      "Epoch 4230\n",
      "-------------------------------\n",
      "tensor(77.5671)\n",
      "tensor(17.9932)\n",
      "tensor(41.0780)\n",
      "tensor(0.7465)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 57.679821\n",
      "Epoch 4231\n",
      "-------------------------------\n",
      "tensor(45.1031)\n",
      "tensor(34.0531)\n",
      "tensor(39.1288)\n",
      "tensor(1.3593)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 57.544220\n",
      "Epoch 4232\n",
      "-------------------------------\n",
      "tensor(36.8390)\n",
      "tensor(21.0478)\n",
      "tensor(14.3153)\n",
      "tensor(0.1224)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.409054\n",
      "Epoch 4233\n",
      "-------------------------------\n",
      "tensor(55.6744)\n",
      "tensor(53.1130)\n",
      "tensor(43.6428)\n",
      "tensor(2.0887)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 57.523312\n",
      "Epoch 4234\n",
      "-------------------------------\n",
      "tensor(60.8348)\n",
      "tensor(32.1440)\n",
      "tensor(8.2828)\n",
      "tensor(0.0187)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 57.348145\n",
      "Epoch 4235\n",
      "-------------------------------\n",
      "tensor(33.9047)\n",
      "tensor(29.0183)\n",
      "tensor(21.3392)\n",
      "tensor(1.2075)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 57.416958\n",
      "Epoch 4236\n",
      "-------------------------------\n",
      "tensor(31.4289)\n",
      "tensor(24.9697)\n",
      "tensor(9.6097)\n",
      "tensor(0.3988)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 57.455803\n",
      "Epoch 4237\n",
      "-------------------------------\n",
      "tensor(35.0069)\n",
      "tensor(21.1622)\n",
      "tensor(11.7818)\n",
      "tensor(0.5177)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 57.466904\n",
      "Epoch 4238\n",
      "-------------------------------\n",
      "tensor(34.9116)\n",
      "tensor(21.0852)\n",
      "tensor(12.2742)\n",
      "tensor(0.6212)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 57.410507\n",
      "Epoch 4239\n",
      "-------------------------------\n",
      "tensor(28.8481)\n",
      "tensor(20.9279)\n",
      "tensor(8.6410)\n",
      "tensor(0.3146)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 57.332916\n",
      "Epoch 4240\n",
      "-------------------------------\n",
      "tensor(31.5429)\n",
      "tensor(21.2721)\n",
      "tensor(7.6713)\n",
      "tensor(0.0348)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 57.273163\n",
      "Epoch 4241\n",
      "-------------------------------\n",
      "tensor(28.7992)\n",
      "tensor(20.9684)\n",
      "tensor(8.0106)\n",
      "tensor(0.1134)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 57.226780\n",
      "Epoch 4242\n",
      "-------------------------------\n",
      "tensor(22.5557)\n",
      "tensor(19.9282)\n",
      "tensor(8.0149)\n",
      "tensor(0.1763)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 57.188362\n",
      "Epoch 4243\n",
      "-------------------------------\n",
      "tensor(21.9593)\n",
      "tensor(18.9621)\n",
      "tensor(7.3219)\n",
      "tensor(0.1703)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 57.137016\n",
      "Epoch 4244\n",
      "-------------------------------\n",
      "tensor(56.5157)\n",
      "tensor(30.2365)\n",
      "tensor(6.6110)\n",
      "tensor(0.0250)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 57.124401\n",
      "Epoch 4245\n",
      "-------------------------------\n",
      "tensor(58.7337)\n",
      "tensor(35.4524)\n",
      "tensor(13.0522)\n",
      "tensor(0.3365)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 57.233261\n",
      "Epoch 4246\n",
      "-------------------------------\n",
      "tensor(59.6253)\n",
      "tensor(38.3959)\n",
      "tensor(21.9997)\n",
      "tensor(0.6124)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 57.217262\n",
      "Epoch 4247\n",
      "-------------------------------\n",
      "tensor(48.0055)\n",
      "tensor(20.1664)\n",
      "tensor(9.8735)\n",
      "tensor(0.2176)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 57.068836\n",
      "Epoch 4248\n",
      "-------------------------------\n",
      "tensor(49.3828)\n",
      "tensor(29.5512)\n",
      "tensor(33.6726)\n",
      "tensor(0.5387)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.170113\n",
      "Epoch 4249\n",
      "-------------------------------\n",
      "tensor(40.3322)\n",
      "tensor(23.5580)\n",
      "tensor(14.1787)\n",
      "tensor(0.2667)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 57.103714\n",
      "Epoch 4250\n",
      "-------------------------------\n",
      "tensor(59.9452)\n",
      "tensor(18.1574)\n",
      "tensor(26.8555)\n",
      "tensor(0.7409)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 56.986969\n",
      "Epoch 4251\n",
      "-------------------------------\n",
      "tensor(76.7470)\n",
      "tensor(25.4461)\n",
      "tensor(32.1531)\n",
      "tensor(1.0533)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 57.298298\n",
      "Epoch 4252\n",
      "-------------------------------\n",
      "tensor(55.2891)\n",
      "tensor(34.6586)\n",
      "tensor(27.8918)\n",
      "tensor(0.1393)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.201447\n",
      "Epoch 4253\n",
      "-------------------------------\n",
      "tensor(43.1971)\n",
      "tensor(17.8693)\n",
      "tensor(16.6839)\n",
      "tensor(0.0515)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 56.906906\n",
      "Epoch 4254\n",
      "-------------------------------\n",
      "tensor(65.0361)\n",
      "tensor(32.3060)\n",
      "tensor(46.8001)\n",
      "tensor(0.7304)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 57.015022\n",
      "Epoch 4255\n",
      "-------------------------------\n",
      "tensor(34.3613)\n",
      "tensor(19.2970)\n",
      "tensor(15.4572)\n",
      "tensor(0.9883)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 56.846500\n",
      "Epoch 4256\n",
      "-------------------------------\n",
      "tensor(55.3405)\n",
      "tensor(20.6091)\n",
      "tensor(30.2996)\n",
      "tensor(1.0418)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 56.804379\n",
      "Epoch 4257\n",
      "-------------------------------\n",
      "tensor(65.4877)\n",
      "tensor(29.4738)\n",
      "tensor(11.0852)\n",
      "tensor(0.2387)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 56.825298\n",
      "Epoch 4258\n",
      "-------------------------------\n",
      "tensor(71.8268)\n",
      "tensor(27.6203)\n",
      "tensor(17.6009)\n",
      "tensor(0.6260)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 56.757729\n",
      "Epoch 4259\n",
      "-------------------------------\n",
      "tensor(31.8432)\n",
      "tensor(20.1559)\n",
      "tensor(7.8823)\n",
      "tensor(0.3093)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 56.664192\n",
      "Epoch 4260\n",
      "-------------------------------\n",
      "tensor(32.8540)\n",
      "tensor(18.6497)\n",
      "tensor(7.1047)\n",
      "tensor(0.0351)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 56.665295\n",
      "Epoch 4261\n",
      "-------------------------------\n",
      "tensor(27.9434)\n",
      "tensor(17.4745)\n",
      "tensor(7.8621)\n",
      "tensor(0.2231)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 56.653782\n",
      "Epoch 4262\n",
      "-------------------------------\n",
      "tensor(26.7517)\n",
      "tensor(17.4842)\n",
      "tensor(7.1868)\n",
      "tensor(0.3077)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 56.642361\n",
      "Epoch 4263\n",
      "-------------------------------\n",
      "tensor(24.9181)\n",
      "tensor(17.7906)\n",
      "tensor(6.2968)\n",
      "tensor(0.3275)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 56.619106\n",
      "Epoch 4264\n",
      "-------------------------------\n",
      "tensor(56.8269)\n",
      "tensor(27.8132)\n",
      "tensor(9.1359)\n",
      "tensor(0.2731)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 56.591507\n",
      "Epoch 4265\n",
      "-------------------------------\n",
      "tensor(56.9610)\n",
      "tensor(28.3258)\n",
      "tensor(7.0743)\n",
      "tensor(0.2020)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 56.569855\n",
      "Epoch 4266\n",
      "-------------------------------\n",
      "tensor(41.1884)\n",
      "tensor(18.7468)\n",
      "tensor(12.5544)\n",
      "tensor(0.1290)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 56.587326\n",
      "Epoch 4267\n",
      "-------------------------------\n",
      "tensor(48.0440)\n",
      "tensor(23.1893)\n",
      "tensor(8.8121)\n",
      "tensor(0.3797)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 56.561356\n",
      "Epoch 4268\n",
      "-------------------------------\n",
      "tensor(42.8526)\n",
      "tensor(30.5280)\n",
      "tensor(27.9862)\n",
      "tensor(0.8562)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.618896\n",
      "Epoch 4269\n",
      "-------------------------------\n",
      "tensor(65.8465)\n",
      "tensor(30.2649)\n",
      "tensor(14.7183)\n",
      "tensor(0.5058)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 56.620701\n",
      "Epoch 4270\n",
      "-------------------------------\n",
      "tensor(84.6193)\n",
      "tensor(24.6469)\n",
      "tensor(52.6119)\n",
      "tensor(1.6847)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 56.695965\n",
      "Epoch 4271\n",
      "-------------------------------\n",
      "tensor(95.1036)\n",
      "tensor(26.3214)\n",
      "tensor(51.2627)\n",
      "tensor(1.7497)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 56.559784\n",
      "Epoch 4272\n",
      "-------------------------------\n",
      "tensor(47.5925)\n",
      "tensor(19.7772)\n",
      "tensor(18.0729)\n",
      "tensor(0.0964)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.475594\n",
      "Epoch 4273\n",
      "-------------------------------\n",
      "tensor(37.0297)\n",
      "tensor(18.0486)\n",
      "tensor(14.8811)\n",
      "tensor(0.5794)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 56.450249\n",
      "Epoch 4274\n",
      "-------------------------------\n",
      "tensor(50.1631)\n",
      "tensor(25.3668)\n",
      "tensor(32.9755)\n",
      "tensor(0.2480)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 56.390263\n",
      "Epoch 4275\n",
      "-------------------------------\n",
      "tensor(40.3145)\n",
      "tensor(20.0095)\n",
      "tensor(9.7977)\n",
      "tensor(0.5499)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 56.276352\n",
      "Epoch 4276\n",
      "-------------------------------\n",
      "tensor(63.4323)\n",
      "tensor(41.0521)\n",
      "tensor(20.4218)\n",
      "tensor(0.7951)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 56.439743\n",
      "Epoch 4277\n",
      "-------------------------------\n",
      "tensor(58.7208)\n",
      "tensor(34.1876)\n",
      "tensor(10.6437)\n",
      "tensor(0.2736)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 56.339149\n",
      "Epoch 4278\n",
      "-------------------------------\n",
      "tensor(51.6929)\n",
      "tensor(27.5508)\n",
      "tensor(6.3012)\n",
      "tensor(0.1887)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 56.158340\n",
      "Epoch 4279\n",
      "-------------------------------\n",
      "tensor(27.2161)\n",
      "tensor(20.9644)\n",
      "tensor(5.7752)\n",
      "tensor(0.2797)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 56.191200\n",
      "Epoch 4280\n",
      "-------------------------------\n",
      "tensor(28.0151)\n",
      "tensor(22.0017)\n",
      "tensor(5.7675)\n",
      "tensor(0.2629)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 56.223793\n",
      "Epoch 4281\n",
      "-------------------------------\n",
      "tensor(27.9000)\n",
      "tensor(22.3314)\n",
      "tensor(6.4259)\n",
      "tensor(0.2216)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 56.229252\n",
      "Epoch 4282\n",
      "-------------------------------\n",
      "tensor(27.6464)\n",
      "tensor(22.2878)\n",
      "tensor(7.2820)\n",
      "tensor(0.1693)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 56.220558\n",
      "Epoch 4283\n",
      "-------------------------------\n",
      "tensor(27.3154)\n",
      "tensor(21.8410)\n",
      "tensor(8.1682)\n",
      "tensor(0.0877)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 56.195942\n",
      "Epoch 4284\n",
      "-------------------------------\n",
      "tensor(30.6605)\n",
      "tensor(21.3056)\n",
      "tensor(8.5335)\n",
      "tensor(0.0507)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 56.144669\n",
      "Epoch 4285\n",
      "-------------------------------\n",
      "tensor(29.2915)\n",
      "tensor(19.3801)\n",
      "tensor(7.6559)\n",
      "tensor(0.2547)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 56.050259\n",
      "Epoch 4286\n",
      "-------------------------------\n",
      "tensor(56.9149)\n",
      "tensor(32.3870)\n",
      "tensor(7.4622)\n",
      "tensor(0.5308)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 56.176846\n",
      "Epoch 4287\n",
      "-------------------------------\n",
      "tensor(61.4178)\n",
      "tensor(38.2167)\n",
      "tensor(18.5369)\n",
      "tensor(0.5856)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 56.332333\n",
      "Epoch 4288\n",
      "-------------------------------\n",
      "tensor(67.3997)\n",
      "tensor(33.5756)\n",
      "tensor(16.8264)\n",
      "tensor(0.0801)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.255512\n",
      "Epoch 4289\n",
      "-------------------------------\n",
      "tensor(40.9194)\n",
      "tensor(28.2636)\n",
      "tensor(15.4563)\n",
      "tensor(0.7061)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 56.163315\n",
      "Epoch 4290\n",
      "-------------------------------\n",
      "tensor(48.9613)\n",
      "tensor(30.4204)\n",
      "tensor(26.4818)\n",
      "tensor(0.0626)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 56.221745\n",
      "Epoch 4291\n",
      "-------------------------------\n",
      "tensor(52.2413)\n",
      "tensor(20.6367)\n",
      "tensor(23.7825)\n",
      "tensor(1.0349)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 56.224213\n",
      "Epoch 4292\n",
      "-------------------------------\n",
      "tensor(25.4713)\n",
      "tensor(27.9456)\n",
      "tensor(14.2393)\n",
      "tensor(0.9025)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.077663\n",
      "Epoch 4293\n",
      "-------------------------------\n",
      "tensor(32.9554)\n",
      "tensor(26.2078)\n",
      "tensor(23.1275)\n",
      "tensor(1.1272)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 55.938049\n",
      "Epoch 4294\n",
      "-------------------------------\n",
      "tensor(55.0525)\n",
      "tensor(44.3919)\n",
      "tensor(20.1816)\n",
      "tensor(1.3131)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 56.211418\n",
      "Epoch 4295\n",
      "-------------------------------\n",
      "tensor(46.3310)\n",
      "tensor(49.6770)\n",
      "tensor(34.7069)\n",
      "tensor(1.8429)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 56.178944\n",
      "Epoch 4296\n",
      "-------------------------------\n",
      "tensor(50.6500)\n",
      "tensor(29.4862)\n",
      "tensor(7.3590)\n",
      "tensor(0.0168)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 55.840950\n",
      "Epoch 4297\n",
      "-------------------------------\n",
      "tensor(25.3197)\n",
      "tensor(28.3641)\n",
      "tensor(16.2507)\n",
      "tensor(1.1019)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 55.834435\n",
      "Epoch 4298\n",
      "-------------------------------\n",
      "tensor(28.7395)\n",
      "tensor(30.4127)\n",
      "tensor(15.8533)\n",
      "tensor(1.0295)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 55.884270\n",
      "Epoch 4299\n",
      "-------------------------------\n",
      "tensor(32.2506)\n",
      "tensor(27.1062)\n",
      "tensor(10.6311)\n",
      "tensor(0.5160)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 55.876701\n",
      "Epoch 4300\n",
      "-------------------------------\n",
      "tensor(34.2031)\n",
      "tensor(24.1677)\n",
      "tensor(8.5757)\n",
      "tensor(0.0921)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 55.847672\n",
      "Epoch 4301\n",
      "-------------------------------\n",
      "tensor(39.1609)\n",
      "tensor(23.4489)\n",
      "tensor(8.7805)\n",
      "tensor(0.1465)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 55.813133\n",
      "Epoch 4302\n",
      "-------------------------------\n",
      "tensor(37.2926)\n",
      "tensor(22.3004)\n",
      "tensor(9.1364)\n",
      "tensor(0.2759)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 55.775146\n",
      "Epoch 4303\n",
      "-------------------------------\n",
      "tensor(33.8320)\n",
      "tensor(21.0797)\n",
      "tensor(8.9517)\n",
      "tensor(0.3416)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 55.717789\n",
      "Epoch 4304\n",
      "-------------------------------\n",
      "tensor(29.0727)\n",
      "tensor(19.7148)\n",
      "tensor(7.8144)\n",
      "tensor(0.3168)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 55.627312\n",
      "Epoch 4305\n",
      "-------------------------------\n",
      "tensor(58.7350)\n",
      "tensor(29.2023)\n",
      "tensor(7.0128)\n",
      "tensor(0.1937)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 55.566422\n",
      "Epoch 4306\n",
      "-------------------------------\n",
      "tensor(59.5372)\n",
      "tensor(34.0743)\n",
      "tensor(11.4117)\n",
      "tensor(0.1987)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 55.730831\n",
      "Epoch 4307\n",
      "-------------------------------\n",
      "tensor(65.5818)\n",
      "tensor(37.1260)\n",
      "tensor(23.6852)\n",
      "tensor(0.1680)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 55.796661\n",
      "Epoch 4308\n",
      "-------------------------------\n",
      "tensor(44.8647)\n",
      "tensor(20.9118)\n",
      "tensor(9.4552)\n",
      "tensor(0.1516)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.629276\n",
      "Epoch 4309\n",
      "-------------------------------\n",
      "tensor(55.0938)\n",
      "tensor(29.1013)\n",
      "tensor(34.1755)\n",
      "tensor(0.1245)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 55.639065\n",
      "Epoch 4310\n",
      "-------------------------------\n",
      "tensor(46.5400)\n",
      "tensor(23.0077)\n",
      "tensor(21.3408)\n",
      "tensor(1.2964)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 55.554218\n",
      "Epoch 4311\n",
      "-------------------------------\n",
      "tensor(47.4650)\n",
      "tensor(19.9941)\n",
      "tensor(12.8443)\n",
      "tensor(0.1796)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 55.435238\n",
      "Epoch 4312\n",
      "-------------------------------\n",
      "tensor(76.0318)\n",
      "tensor(26.2380)\n",
      "tensor(40.0636)\n",
      "tensor(1.7334)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.718380\n",
      "Epoch 4313\n",
      "-------------------------------\n",
      "tensor(65.1200)\n",
      "tensor(37.8361)\n",
      "tensor(43.0797)\n",
      "tensor(0.6250)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 55.688519\n",
      "Epoch 4314\n",
      "-------------------------------\n",
      "tensor(34.9632)\n",
      "tensor(18.9842)\n",
      "tensor(8.0238)\n",
      "tensor(0.1900)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 55.367836\n",
      "Epoch 4315\n",
      "-------------------------------\n",
      "tensor(66.4913)\n",
      "tensor(32.3364)\n",
      "tensor(47.4614)\n",
      "tensor(0.6415)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 55.467350\n",
      "Epoch 4316\n",
      "-------------------------------\n",
      "tensor(30.1337)\n",
      "tensor(18.0615)\n",
      "tensor(10.2933)\n",
      "tensor(0.6558)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 55.292786\n",
      "Epoch 4317\n",
      "-------------------------------\n",
      "tensor(48.3956)\n",
      "tensor(38.8651)\n",
      "tensor(31.9417)\n",
      "tensor(0.9857)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 55.335499\n",
      "Epoch 4318\n",
      "-------------------------------\n",
      "tensor(48.2236)\n",
      "tensor(32.7114)\n",
      "tensor(20.8062)\n",
      "tensor(0.1932)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 55.234936\n",
      "Epoch 4319\n",
      "-------------------------------\n",
      "tensor(30.9286)\n",
      "tensor(21.1467)\n",
      "tensor(10.2799)\n",
      "tensor(0.5039)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 55.192123\n",
      "Epoch 4320\n",
      "-------------------------------\n",
      "tensor(23.3026)\n",
      "tensor(24.1243)\n",
      "tensor(15.5749)\n",
      "tensor(0.7475)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 55.182640\n",
      "Epoch 4321\n",
      "-------------------------------\n",
      "tensor(28.3611)\n",
      "tensor(24.6294)\n",
      "tensor(18.2665)\n",
      "tensor(0.6799)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 55.172890\n",
      "Epoch 4322\n",
      "-------------------------------\n",
      "tensor(33.3249)\n",
      "tensor(23.7867)\n",
      "tensor(16.6620)\n",
      "tensor(0.4491)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 55.158031\n",
      "Epoch 4323\n",
      "-------------------------------\n",
      "tensor(30.2567)\n",
      "tensor(20.9862)\n",
      "tensor(11.2324)\n",
      "tensor(0.0160)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 55.124210\n",
      "Epoch 4324\n",
      "-------------------------------\n",
      "tensor(55.5597)\n",
      "tensor(31.6788)\n",
      "tensor(9.5023)\n",
      "tensor(0.6889)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 55.072491\n",
      "Epoch 4325\n",
      "-------------------------------\n",
      "tensor(42.9635)\n",
      "tensor(21.1256)\n",
      "tensor(26.8920)\n",
      "tensor(1.4390)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 55.065937\n",
      "Epoch 4326\n",
      "-------------------------------\n",
      "tensor(48.4048)\n",
      "tensor(37.4353)\n",
      "tensor(23.4688)\n",
      "tensor(1.2210)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 55.087296\n",
      "Epoch 4327\n",
      "-------------------------------\n",
      "tensor(58.9425)\n",
      "tensor(25.4630)\n",
      "tensor(8.7333)\n",
      "tensor(0.4425)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 55.058044\n",
      "Epoch 4328\n",
      "-------------------------------\n",
      "tensor(39.8259)\n",
      "tensor(35.6732)\n",
      "tensor(22.8480)\n",
      "tensor(1.5527)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.240646\n",
      "Epoch 4329\n",
      "-------------------------------\n",
      "tensor(39.3814)\n",
      "tensor(29.3493)\n",
      "tensor(11.2017)\n",
      "tensor(0.5834)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 55.246315\n",
      "Epoch 4330\n",
      "-------------------------------\n",
      "tensor(46.2225)\n",
      "tensor(23.6205)\n",
      "tensor(18.4137)\n",
      "tensor(0.9702)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 55.139164\n",
      "Epoch 4331\n",
      "-------------------------------\n",
      "tensor(35.2671)\n",
      "tensor(22.4984)\n",
      "tensor(14.1416)\n",
      "tensor(0.5405)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.936478\n",
      "Epoch 4332\n",
      "-------------------------------\n",
      "tensor(59.9921)\n",
      "tensor(30.2888)\n",
      "tensor(10.7157)\n",
      "tensor(0.2190)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.076931\n",
      "Epoch 4333\n",
      "-------------------------------\n",
      "tensor(69.2936)\n",
      "tensor(37.6736)\n",
      "tensor(32.1756)\n",
      "tensor(0.0492)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 55.113934\n",
      "Epoch 4334\n",
      "-------------------------------\n",
      "tensor(26.8011)\n",
      "tensor(21.5885)\n",
      "tensor(12.0476)\n",
      "tensor(0.2659)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 54.847240\n",
      "Epoch 4335\n",
      "-------------------------------\n",
      "tensor(45.1273)\n",
      "tensor(23.7725)\n",
      "tensor(25.5772)\n",
      "tensor(0.3532)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 54.919197\n",
      "Epoch 4336\n",
      "-------------------------------\n",
      "tensor(37.6814)\n",
      "tensor(18.5265)\n",
      "tensor(18.4237)\n",
      "tensor(1.0282)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 54.851707\n",
      "Epoch 4337\n",
      "-------------------------------\n",
      "tensor(39.8438)\n",
      "tensor(18.0483)\n",
      "tensor(13.7771)\n",
      "tensor(0.2974)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 54.751179\n",
      "Epoch 4338\n",
      "-------------------------------\n",
      "tensor(51.5541)\n",
      "tensor(24.4834)\n",
      "tensor(10.9401)\n",
      "tensor(0.5896)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 54.757446\n",
      "Epoch 4339\n",
      "-------------------------------\n",
      "tensor(55.9095)\n",
      "tensor(24.7618)\n",
      "tensor(13.6856)\n",
      "tensor(0.7681)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 54.749367\n",
      "Epoch 4340\n",
      "-------------------------------\n",
      "tensor(52.0397)\n",
      "tensor(25.3422)\n",
      "tensor(9.4865)\n",
      "tensor(0.5671)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 54.681942\n",
      "Epoch 4341\n",
      "-------------------------------\n",
      "tensor(31.1450)\n",
      "tensor(20.4618)\n",
      "tensor(8.1863)\n",
      "tensor(0.3128)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 54.629044\n",
      "Epoch 4342\n",
      "-------------------------------\n",
      "tensor(37.3230)\n",
      "tensor(19.9748)\n",
      "tensor(8.1739)\n",
      "tensor(0.0824)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 54.634247\n",
      "Epoch 4343\n",
      "-------------------------------\n",
      "tensor(34.8822)\n",
      "tensor(19.4082)\n",
      "tensor(6.8323)\n",
      "tensor(0.1521)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 54.624424\n",
      "Epoch 4344\n",
      "-------------------------------\n",
      "tensor(27.0077)\n",
      "tensor(19.8627)\n",
      "tensor(7.6927)\n",
      "tensor(0.3890)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 54.595158\n",
      "Epoch 4345\n",
      "-------------------------------\n",
      "tensor(63.2171)\n",
      "tensor(28.1653)\n",
      "tensor(16.0736)\n",
      "tensor(0.5798)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 54.593273\n",
      "Epoch 4346\n",
      "-------------------------------\n",
      "tensor(38.2437)\n",
      "tensor(21.4419)\n",
      "tensor(10.8177)\n",
      "tensor(0.7096)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 54.557045\n",
      "Epoch 4347\n",
      "-------------------------------\n",
      "tensor(59.6667)\n",
      "tensor(31.4570)\n",
      "tensor(9.0368)\n",
      "tensor(0.3580)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 54.576065\n",
      "Epoch 4348\n",
      "-------------------------------\n",
      "tensor(54.3396)\n",
      "tensor(23.2649)\n",
      "tensor(12.5520)\n",
      "tensor(0.4117)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 54.726368\n",
      "Epoch 4349\n",
      "-------------------------------\n",
      "tensor(39.9326)\n",
      "tensor(31.3351)\n",
      "tensor(27.5612)\n",
      "tensor(1.0020)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.690861\n",
      "Epoch 4350\n",
      "-------------------------------\n",
      "tensor(38.2949)\n",
      "tensor(21.9781)\n",
      "tensor(13.4445)\n",
      "tensor(0.6885)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 54.521324\n",
      "Epoch 4351\n",
      "-------------------------------\n",
      "tensor(45.5490)\n",
      "tensor(38.4067)\n",
      "tensor(25.0087)\n",
      "tensor(1.4563)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.590481\n",
      "Epoch 4352\n",
      "-------------------------------\n",
      "tensor(47.8790)\n",
      "tensor(26.8641)\n",
      "tensor(13.8377)\n",
      "tensor(0.6499)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 54.520603\n",
      "Epoch 4353\n",
      "-------------------------------\n",
      "tensor(29.8089)\n",
      "tensor(33.9119)\n",
      "tensor(21.9203)\n",
      "tensor(1.6103)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 54.556526\n",
      "Epoch 4354\n",
      "-------------------------------\n",
      "tensor(32.2867)\n",
      "tensor(24.1082)\n",
      "tensor(14.3905)\n",
      "tensor(0.0174)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 54.541946\n",
      "Epoch 4355\n",
      "-------------------------------\n",
      "tensor(40.6859)\n",
      "tensor(23.1492)\n",
      "tensor(21.9237)\n",
      "tensor(1.4939)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 54.476524\n",
      "Epoch 4356\n",
      "-------------------------------\n",
      "tensor(38.6306)\n",
      "tensor(21.6158)\n",
      "tensor(10.8777)\n",
      "tensor(0.8243)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 54.335552\n",
      "Epoch 4357\n",
      "-------------------------------\n",
      "tensor(61.7188)\n",
      "tensor(27.3400)\n",
      "tensor(12.2068)\n",
      "tensor(0.2249)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 54.393082\n",
      "Epoch 4358\n",
      "-------------------------------\n",
      "tensor(53.1573)\n",
      "tensor(28.2330)\n",
      "tensor(12.4359)\n",
      "tensor(0.4209)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 54.354820\n",
      "Epoch 4359\n",
      "-------------------------------\n",
      "tensor(46.7067)\n",
      "tensor(29.2135)\n",
      "tensor(18.1843)\n",
      "tensor(0.2775)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 54.251148\n",
      "Epoch 4360\n",
      "-------------------------------\n",
      "tensor(48.8219)\n",
      "tensor(19.4803)\n",
      "tensor(18.3472)\n",
      "tensor(0.1415)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 54.241180\n",
      "Epoch 4361\n",
      "-------------------------------\n",
      "tensor(35.8612)\n",
      "tensor(19.1402)\n",
      "tensor(11.7959)\n",
      "tensor(0.0926)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 54.231312\n",
      "Epoch 4362\n",
      "-------------------------------\n",
      "tensor(27.4749)\n",
      "tensor(19.9159)\n",
      "tensor(5.0373)\n",
      "tensor(0.0770)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 54.215607\n",
      "Epoch 4363\n",
      "-------------------------------\n",
      "tensor(26.0685)\n",
      "tensor(21.3282)\n",
      "tensor(10.5113)\n",
      "tensor(0.0483)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 54.200211\n",
      "Epoch 4364\n",
      "-------------------------------\n",
      "tensor(37.4309)\n",
      "tensor(22.8286)\n",
      "tensor(21.6123)\n",
      "tensor(0.0830)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 54.179798\n",
      "Epoch 4365\n",
      "-------------------------------\n",
      "tensor(74.6473)\n",
      "tensor(27.2052)\n",
      "tensor(22.8370)\n",
      "tensor(0.4935)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 54.185848\n",
      "Epoch 4366\n",
      "-------------------------------\n",
      "tensor(50.5187)\n",
      "tensor(35.3151)\n",
      "tensor(19.4536)\n",
      "tensor(1.1221)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 54.096828\n",
      "Epoch 4367\n",
      "-------------------------------\n",
      "tensor(84.1099)\n",
      "tensor(21.5410)\n",
      "tensor(39.6184)\n",
      "tensor(0.7029)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 54.261246\n",
      "Epoch 4368\n",
      "-------------------------------\n",
      "tensor(39.3034)\n",
      "tensor(38.3199)\n",
      "tensor(33.2159)\n",
      "tensor(1.7549)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 54.274971\n",
      "Epoch 4369\n",
      "-------------------------------\n",
      "tensor(59.6168)\n",
      "tensor(38.9865)\n",
      "tensor(49.8990)\n",
      "tensor(1.4186)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.248104\n",
      "Epoch 4370\n",
      "-------------------------------\n",
      "tensor(58.4262)\n",
      "tensor(52.1453)\n",
      "tensor(56.8480)\n",
      "tensor(2.6780)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 54.223793\n",
      "Epoch 4371\n",
      "-------------------------------\n",
      "tensor(46.1988)\n",
      "tensor(31.8502)\n",
      "tensor(18.1219)\n",
      "tensor(0.6846)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.045052\n",
      "Epoch 4372\n",
      "-------------------------------\n",
      "tensor(45.1939)\n",
      "tensor(48.4355)\n",
      "tensor(54.7673)\n",
      "tensor(2.7265)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 54.122288\n",
      "Epoch 4373\n",
      "-------------------------------\n",
      "tensor(51.2591)\n",
      "tensor(20.5963)\n",
      "tensor(17.0269)\n",
      "tensor(0.1386)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 54.091026\n",
      "Epoch 4374\n",
      "-------------------------------\n",
      "tensor(41.6950)\n",
      "tensor(23.8991)\n",
      "tensor(23.1236)\n",
      "tensor(1.5755)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 54.020485\n",
      "Epoch 4375\n",
      "-------------------------------\n",
      "tensor(49.4847)\n",
      "tensor(22.2834)\n",
      "tensor(27.6566)\n",
      "tensor(0.4218)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 53.932983\n",
      "Epoch 4376\n",
      "-------------------------------\n",
      "tensor(55.6962)\n",
      "tensor(31.0106)\n",
      "tensor(7.5123)\n",
      "tensor(0.1454)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 54.065384\n",
      "Epoch 4377\n",
      "-------------------------------\n",
      "tensor(61.7693)\n",
      "tensor(36.5370)\n",
      "tensor(34.7515)\n",
      "tensor(0.1347)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 54.076897\n",
      "Epoch 4378\n",
      "-------------------------------\n",
      "tensor(53.2086)\n",
      "tensor(30.6706)\n",
      "tensor(25.1228)\n",
      "tensor(0.2998)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 53.892647\n",
      "Epoch 4379\n",
      "-------------------------------\n",
      "tensor(30.0292)\n",
      "tensor(22.8390)\n",
      "tensor(9.9335)\n",
      "tensor(0.5883)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 53.798008\n",
      "Epoch 4380\n",
      "-------------------------------\n",
      "tensor(25.3981)\n",
      "tensor(25.3977)\n",
      "tensor(15.4984)\n",
      "tensor(0.5738)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 53.808086\n",
      "Epoch 4381\n",
      "-------------------------------\n",
      "tensor(30.6012)\n",
      "tensor(25.5221)\n",
      "tensor(19.4554)\n",
      "tensor(0.3941)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 53.810047\n",
      "Epoch 4382\n",
      "-------------------------------\n",
      "tensor(32.4856)\n",
      "tensor(24.1337)\n",
      "tensor(18.4470)\n",
      "tensor(0.1323)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 53.795761\n",
      "Epoch 4383\n",
      "-------------------------------\n",
      "tensor(31.0758)\n",
      "tensor(21.4628)\n",
      "tensor(13.4989)\n",
      "tensor(0.2570)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 53.759789\n",
      "Epoch 4384\n",
      "-------------------------------\n",
      "tensor(32.2651)\n",
      "tensor(19.0529)\n",
      "tensor(12.0478)\n",
      "tensor(0.7847)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 53.695744\n",
      "Epoch 4385\n",
      "-------------------------------\n",
      "tensor(42.1455)\n",
      "tensor(35.5396)\n",
      "tensor(21.9171)\n",
      "tensor(1.1669)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 53.726868\n",
      "Epoch 4386\n",
      "-------------------------------\n",
      "tensor(56.5446)\n",
      "tensor(35.7969)\n",
      "tensor(23.4557)\n",
      "tensor(0.6807)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 53.796761\n",
      "Epoch 4387\n",
      "-------------------------------\n",
      "tensor(66.3937)\n",
      "tensor(26.3018)\n",
      "tensor(13.9607)\n",
      "tensor(0.8682)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 53.797562\n",
      "Epoch 4388\n",
      "-------------------------------\n",
      "tensor(45.1454)\n",
      "tensor(34.5152)\n",
      "tensor(25.9352)\n",
      "tensor(1.3891)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 53.803608\n",
      "Epoch 4389\n",
      "-------------------------------\n",
      "tensor(42.7193)\n",
      "tensor(24.9939)\n",
      "tensor(16.5724)\n",
      "tensor(0.4524)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.795502\n",
      "Epoch 4390\n",
      "-------------------------------\n",
      "tensor(52.1729)\n",
      "tensor(23.6148)\n",
      "tensor(27.1379)\n",
      "tensor(1.5709)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 53.747631\n",
      "Epoch 4391\n",
      "-------------------------------\n",
      "tensor(31.5274)\n",
      "tensor(26.4918)\n",
      "tensor(19.4809)\n",
      "tensor(0.6558)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.563068\n",
      "Epoch 4392\n",
      "-------------------------------\n",
      "tensor(57.1890)\n",
      "tensor(26.7348)\n",
      "tensor(16.9899)\n",
      "tensor(0.8154)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 53.739849\n",
      "Epoch 4393\n",
      "-------------------------------\n",
      "tensor(55.5802)\n",
      "tensor(42.4043)\n",
      "tensor(42.2511)\n",
      "tensor(0.9923)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 53.763775\n",
      "Epoch 4394\n",
      "-------------------------------\n",
      "tensor(60.4489)\n",
      "tensor(25.0712)\n",
      "tensor(13.7775)\n",
      "tensor(0.0554)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 53.452965\n",
      "Epoch 4395\n",
      "-------------------------------\n",
      "tensor(37.2765)\n",
      "tensor(25.1693)\n",
      "tensor(10.3452)\n",
      "tensor(0.0302)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 53.615742\n",
      "Epoch 4396\n",
      "-------------------------------\n",
      "tensor(44.1461)\n",
      "tensor(26.1017)\n",
      "tensor(8.3364)\n",
      "tensor(0.0459)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 53.719322\n",
      "Epoch 4397\n",
      "-------------------------------\n",
      "tensor(37.7983)\n",
      "tensor(28.3071)\n",
      "tensor(9.0283)\n",
      "tensor(0.3931)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 53.687248\n",
      "Epoch 4398\n",
      "-------------------------------\n",
      "tensor(38.3211)\n",
      "tensor(28.5729)\n",
      "tensor(12.6124)\n",
      "tensor(0.4850)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 53.608551\n",
      "Epoch 4399\n",
      "-------------------------------\n",
      "tensor(30.6895)\n",
      "tensor(25.6881)\n",
      "tensor(11.0535)\n",
      "tensor(0.2727)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 53.510502\n",
      "Epoch 4400\n",
      "-------------------------------\n",
      "tensor(27.1368)\n",
      "tensor(22.7242)\n",
      "tensor(7.3738)\n",
      "tensor(0.0135)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 53.440880\n",
      "Epoch 4401\n",
      "-------------------------------\n",
      "tensor(33.2500)\n",
      "tensor(21.2990)\n",
      "tensor(5.2884)\n",
      "tensor(0.1772)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 53.391174\n",
      "Epoch 4402\n",
      "-------------------------------\n",
      "tensor(25.8555)\n",
      "tensor(19.1634)\n",
      "tensor(5.3426)\n",
      "tensor(0.3166)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 53.346390\n",
      "Epoch 4403\n",
      "-------------------------------\n",
      "tensor(27.6711)\n",
      "tensor(18.7520)\n",
      "tensor(6.4671)\n",
      "tensor(0.4373)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 53.290661\n",
      "Epoch 4404\n",
      "-------------------------------\n",
      "tensor(50.2397)\n",
      "tensor(31.1864)\n",
      "tensor(7.7553)\n",
      "tensor(0.5421)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 53.253265\n",
      "Epoch 4405\n",
      "-------------------------------\n",
      "tensor(52.7996)\n",
      "tensor(34.5184)\n",
      "tensor(11.7562)\n",
      "tensor(0.6226)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 53.367996\n",
      "Epoch 4406\n",
      "-------------------------------\n",
      "tensor(61.1840)\n",
      "tensor(35.3717)\n",
      "tensor(16.7414)\n",
      "tensor(0.4276)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 53.402618\n",
      "Epoch 4407\n",
      "-------------------------------\n",
      "tensor(53.3211)\n",
      "tensor(27.5882)\n",
      "tensor(9.7944)\n",
      "tensor(0.1923)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 53.273594\n",
      "Epoch 4408\n",
      "-------------------------------\n",
      "tensor(38.9668)\n",
      "tensor(28.7961)\n",
      "tensor(10.7128)\n",
      "tensor(0.6368)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 53.302505\n",
      "Epoch 4409\n",
      "-------------------------------\n",
      "tensor(45.2492)\n",
      "tensor(32.8618)\n",
      "tensor(24.3611)\n",
      "tensor(0.3383)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.415867\n",
      "Epoch 4410\n",
      "-------------------------------\n",
      "tensor(47.9540)\n",
      "tensor(24.5180)\n",
      "tensor(15.9377)\n",
      "tensor(0.5148)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 53.296219\n",
      "Epoch 4411\n",
      "-------------------------------\n",
      "tensor(38.0660)\n",
      "tensor(24.3997)\n",
      "tensor(6.1204)\n",
      "tensor(0.3318)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.084465\n",
      "Epoch 4412\n",
      "-------------------------------\n",
      "tensor(73.0691)\n",
      "tensor(26.7365)\n",
      "tensor(20.0339)\n",
      "tensor(0.5388)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 53.349945\n",
      "Epoch 4413\n",
      "-------------------------------\n",
      "tensor(57.2342)\n",
      "tensor(46.9669)\n",
      "tensor(42.1001)\n",
      "tensor(1.4179)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 53.360298\n",
      "Epoch 4414\n",
      "-------------------------------\n",
      "tensor(49.6174)\n",
      "tensor(27.4909)\n",
      "tensor(5.5568)\n",
      "tensor(0.1791)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 53.023052\n",
      "Epoch 4415\n",
      "-------------------------------\n",
      "tensor(37.4222)\n",
      "tensor(27.8238)\n",
      "tensor(14.3648)\n",
      "tensor(0.5664)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 53.089195\n",
      "Epoch 4416\n",
      "-------------------------------\n",
      "tensor(37.4305)\n",
      "tensor(27.1197)\n",
      "tensor(9.8131)\n",
      "tensor(0.2063)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 53.195400\n",
      "Epoch 4417\n",
      "-------------------------------\n",
      "tensor(37.6338)\n",
      "tensor(24.3156)\n",
      "tensor(8.7322)\n",
      "tensor(0.1284)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 53.191845\n",
      "Epoch 4418\n",
      "-------------------------------\n",
      "tensor(33.4808)\n",
      "tensor(23.4164)\n",
      "tensor(7.0314)\n",
      "tensor(0.0858)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 53.115204\n",
      "Epoch 4419\n",
      "-------------------------------\n",
      "tensor(35.4802)\n",
      "tensor(23.8300)\n",
      "tensor(6.4645)\n",
      "tensor(0.0456)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 53.030693\n",
      "Epoch 4420\n",
      "-------------------------------\n",
      "tensor(26.4116)\n",
      "tensor(22.1747)\n",
      "tensor(7.3447)\n",
      "tensor(0.0936)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 52.959316\n",
      "Epoch 4421\n",
      "-------------------------------\n",
      "tensor(25.3395)\n",
      "tensor(21.2422)\n",
      "tensor(7.2357)\n",
      "tensor(0.0713)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 52.908386\n",
      "Epoch 4422\n",
      "-------------------------------\n",
      "tensor(33.0703)\n",
      "tensor(21.3462)\n",
      "tensor(6.5036)\n",
      "tensor(0.0070)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 52.864021\n",
      "Epoch 4423\n",
      "-------------------------------\n",
      "tensor(24.1691)\n",
      "tensor(18.9184)\n",
      "tensor(5.7179)\n",
      "tensor(0.1178)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 52.804581\n",
      "Epoch 4424\n",
      "-------------------------------\n",
      "tensor(51.4014)\n",
      "tensor(29.6334)\n",
      "tensor(6.3917)\n",
      "tensor(0.3774)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 52.844833\n",
      "Epoch 4425\n",
      "-------------------------------\n",
      "tensor(51.0986)\n",
      "tensor(35.5611)\n",
      "tensor(14.3727)\n",
      "tensor(0.7411)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 52.943398\n",
      "Epoch 4426\n",
      "-------------------------------\n",
      "tensor(61.6357)\n",
      "tensor(38.5108)\n",
      "tensor(22.1259)\n",
      "tensor(0.7654)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 53.030769\n",
      "Epoch 4427\n",
      "-------------------------------\n",
      "tensor(59.6387)\n",
      "tensor(29.6262)\n",
      "tensor(8.2847)\n",
      "tensor(0.0126)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 52.930817\n",
      "Epoch 4428\n",
      "-------------------------------\n",
      "tensor(39.5737)\n",
      "tensor(31.0710)\n",
      "tensor(16.8321)\n",
      "tensor(0.7525)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.834057\n",
      "Epoch 4429\n",
      "-------------------------------\n",
      "tensor(42.6708)\n",
      "tensor(32.7932)\n",
      "tensor(23.1700)\n",
      "tensor(0.3851)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.930202\n",
      "Epoch 4430\n",
      "-------------------------------\n",
      "tensor(52.2118)\n",
      "tensor(23.3685)\n",
      "tensor(17.3015)\n",
      "tensor(0.4569)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 52.889454\n",
      "Epoch 4431\n",
      "-------------------------------\n",
      "tensor(35.1282)\n",
      "tensor(27.7289)\n",
      "tensor(13.1115)\n",
      "tensor(0.6357)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.638168\n",
      "Epoch 4432\n",
      "-------------------------------\n",
      "tensor(66.5005)\n",
      "tensor(26.3257)\n",
      "tensor(16.2526)\n",
      "tensor(0.1880)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.922836\n",
      "Epoch 4433\n",
      "-------------------------------\n",
      "tensor(61.2916)\n",
      "tensor(51.2992)\n",
      "tensor(52.6368)\n",
      "tensor(1.6817)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 53.005955\n",
      "Epoch 4434\n",
      "-------------------------------\n",
      "tensor(60.3956)\n",
      "tensor(25.0787)\n",
      "tensor(13.9874)\n",
      "tensor(0.3240)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 52.625252\n",
      "Epoch 4435\n",
      "-------------------------------\n",
      "tensor(34.3895)\n",
      "tensor(27.5749)\n",
      "tensor(16.1204)\n",
      "tensor(0.6193)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 52.582760\n",
      "Epoch 4436\n",
      "-------------------------------\n",
      "tensor(45.7396)\n",
      "tensor(24.6092)\n",
      "tensor(9.0888)\n",
      "tensor(0.2429)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 52.703709\n",
      "Epoch 4437\n",
      "-------------------------------\n",
      "tensor(47.3145)\n",
      "tensor(25.6145)\n",
      "tensor(11.2673)\n",
      "tensor(0.3854)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 52.696716\n",
      "Epoch 4438\n",
      "-------------------------------\n",
      "tensor(41.2183)\n",
      "tensor(27.0225)\n",
      "tensor(13.2093)\n",
      "tensor(0.0457)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 52.640511\n",
      "Epoch 4439\n",
      "-------------------------------\n",
      "tensor(34.4929)\n",
      "tensor(26.1656)\n",
      "tensor(14.1101)\n",
      "tensor(0.1593)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 52.560783\n",
      "Epoch 4440\n",
      "-------------------------------\n",
      "tensor(30.2842)\n",
      "tensor(24.2461)\n",
      "tensor(10.7033)\n",
      "tensor(0.1716)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 52.493210\n",
      "Epoch 4441\n",
      "-------------------------------\n",
      "tensor(29.2965)\n",
      "tensor(22.4894)\n",
      "tensor(7.0305)\n",
      "tensor(0.1075)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 52.444260\n",
      "Epoch 4442\n",
      "-------------------------------\n",
      "tensor(29.4143)\n",
      "tensor(20.9493)\n",
      "tensor(4.6933)\n",
      "tensor(0.0171)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 52.397747\n",
      "Epoch 4443\n",
      "-------------------------------\n",
      "tensor(36.1881)\n",
      "tensor(20.3579)\n",
      "tensor(4.9985)\n",
      "tensor(0.1064)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 52.343628\n",
      "Epoch 4444\n",
      "-------------------------------\n",
      "tensor(47.3303)\n",
      "tensor(28.8585)\n",
      "tensor(6.8481)\n",
      "tensor(0.3288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 52.420586\n",
      "Epoch 4445\n",
      "-------------------------------\n",
      "tensor(47.1125)\n",
      "tensor(33.0266)\n",
      "tensor(12.2455)\n",
      "tensor(0.5904)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 52.538094\n",
      "Epoch 4446\n",
      "-------------------------------\n",
      "tensor(59.9274)\n",
      "tensor(36.2059)\n",
      "tensor(17.0024)\n",
      "tensor(0.5978)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 52.609924\n",
      "Epoch 4447\n",
      "-------------------------------\n",
      "tensor(55.3013)\n",
      "tensor(28.9420)\n",
      "tensor(8.0114)\n",
      "tensor(0.0738)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 52.509609\n",
      "Epoch 4448\n",
      "-------------------------------\n",
      "tensor(31.7895)\n",
      "tensor(26.5865)\n",
      "tensor(9.0270)\n",
      "tensor(0.5014)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.297325\n",
      "Epoch 4449\n",
      "-------------------------------\n",
      "tensor(36.4860)\n",
      "tensor(30.9219)\n",
      "tensor(19.5737)\n",
      "tensor(0.4110)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.403759\n",
      "Epoch 4450\n",
      "-------------------------------\n",
      "tensor(44.7612)\n",
      "tensor(23.2895)\n",
      "tensor(11.8512)\n",
      "tensor(0.3330)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 52.335495\n",
      "Epoch 4451\n",
      "-------------------------------\n",
      "tensor(48.1693)\n",
      "tensor(22.4661)\n",
      "tensor(6.5701)\n",
      "tensor(0.1239)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.226437\n",
      "Epoch 4452\n",
      "-------------------------------\n",
      "tensor(54.3344)\n",
      "tensor(28.5747)\n",
      "tensor(19.4627)\n",
      "tensor(0.1422)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.201023\n",
      "Epoch 4453\n",
      "-------------------------------\n",
      "tensor(43.4488)\n",
      "tensor(24.1054)\n",
      "tensor(7.1177)\n",
      "tensor(0.2997)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 52.246632\n",
      "Epoch 4454\n",
      "-------------------------------\n",
      "tensor(50.9807)\n",
      "tensor(27.4571)\n",
      "tensor(27.5196)\n",
      "tensor(0.2636)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 52.259472\n",
      "Epoch 4455\n",
      "-------------------------------\n",
      "tensor(38.3177)\n",
      "tensor(22.8026)\n",
      "tensor(21.2505)\n",
      "tensor(1.4152)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 52.198406\n",
      "Epoch 4456\n",
      "-------------------------------\n",
      "tensor(35.8766)\n",
      "tensor(18.7182)\n",
      "tensor(13.3615)\n",
      "tensor(0.5368)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 52.092186\n",
      "Epoch 4457\n",
      "-------------------------------\n",
      "tensor(57.5355)\n",
      "tensor(22.9001)\n",
      "tensor(16.8488)\n",
      "tensor(0.6113)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 52.107433\n",
      "Epoch 4458\n",
      "-------------------------------\n",
      "tensor(51.3368)\n",
      "tensor(24.2194)\n",
      "tensor(11.9802)\n",
      "tensor(0.5567)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 52.083397\n",
      "Epoch 4459\n",
      "-------------------------------\n",
      "tensor(45.8077)\n",
      "tensor(26.5689)\n",
      "tensor(12.8068)\n",
      "tensor(0.1243)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.999725\n",
      "Epoch 4460\n",
      "-------------------------------\n",
      "tensor(43.0593)\n",
      "tensor(18.4942)\n",
      "tensor(16.5760)\n",
      "tensor(0.1541)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 52.000977\n",
      "Epoch 4461\n",
      "-------------------------------\n",
      "tensor(38.2487)\n",
      "tensor(18.8672)\n",
      "tensor(12.6901)\n",
      "tensor(0.2207)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.998085\n",
      "Epoch 4462\n",
      "-------------------------------\n",
      "tensor(31.4630)\n",
      "tensor(19.7363)\n",
      "tensor(6.4219)\n",
      "tensor(0.1834)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 51.981998\n",
      "Epoch 4463\n",
      "-------------------------------\n",
      "tensor(27.6466)\n",
      "tensor(21.4917)\n",
      "tensor(8.4487)\n",
      "tensor(0.0791)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 51.959225\n",
      "Epoch 4464\n",
      "-------------------------------\n",
      "tensor(37.8050)\n",
      "tensor(24.5050)\n",
      "tensor(20.4809)\n",
      "tensor(0.0428)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.924965\n",
      "Epoch 4465\n",
      "-------------------------------\n",
      "tensor(72.9503)\n",
      "tensor(22.6272)\n",
      "tensor(24.4417)\n",
      "tensor(0.0945)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 52.025650\n",
      "Epoch 4466\n",
      "-------------------------------\n",
      "tensor(52.5774)\n",
      "tensor(31.3778)\n",
      "tensor(11.6382)\n",
      "tensor(0.6556)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 52.037663\n",
      "Epoch 4467\n",
      "-------------------------------\n",
      "tensor(76.3796)\n",
      "tensor(36.5601)\n",
      "tensor(40.5697)\n",
      "tensor(0.6608)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 52.086594\n",
      "Epoch 4468\n",
      "-------------------------------\n",
      "tensor(46.8275)\n",
      "tensor(31.4251)\n",
      "tensor(13.6087)\n",
      "tensor(1.0257)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.088013\n",
      "Epoch 4469\n",
      "-------------------------------\n",
      "tensor(72.2649)\n",
      "tensor(46.2485)\n",
      "tensor(61.6273)\n",
      "tensor(1.5870)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.219017\n",
      "Epoch 4470\n",
      "-------------------------------\n",
      "tensor(64.7353)\n",
      "tensor(23.8280)\n",
      "tensor(43.9536)\n",
      "tensor(2.0745)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 52.056820\n",
      "Epoch 4471\n",
      "-------------------------------\n",
      "tensor(32.3078)\n",
      "tensor(18.1489)\n",
      "tensor(12.9458)\n",
      "tensor(0.6635)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.814198\n",
      "Epoch 4472\n",
      "-------------------------------\n",
      "tensor(107.3358)\n",
      "tensor(23.3310)\n",
      "tensor(56.3807)\n",
      "tensor(1.7584)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.243443\n",
      "Epoch 4473\n",
      "-------------------------------\n",
      "tensor(95.3750)\n",
      "tensor(55.5043)\n",
      "tensor(75.1825)\n",
      "tensor(1.7318)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 52.255386\n",
      "Epoch 4474\n",
      "-------------------------------\n",
      "tensor(54.1014)\n",
      "tensor(23.3314)\n",
      "tensor(10.1943)\n",
      "tensor(0.0176)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 51.751045\n",
      "Epoch 4475\n",
      "-------------------------------\n",
      "tensor(50.2308)\n",
      "tensor(35.6519)\n",
      "tensor(37.5641)\n",
      "tensor(0.7369)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 51.889782\n",
      "Epoch 4476\n",
      "-------------------------------\n",
      "tensor(55.7113)\n",
      "tensor(26.1550)\n",
      "tensor(14.5986)\n",
      "tensor(0.3103)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.974590\n",
      "Epoch 4477\n",
      "-------------------------------\n",
      "tensor(58.0750)\n",
      "tensor(25.0823)\n",
      "tensor(16.5260)\n",
      "tensor(0.2255)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 51.928288\n",
      "Epoch 4478\n",
      "-------------------------------\n",
      "tensor(43.1934)\n",
      "tensor(29.8903)\n",
      "tensor(12.0677)\n",
      "tensor(0.4696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 51.785118\n",
      "Epoch 4479\n",
      "-------------------------------\n",
      "tensor(37.1060)\n",
      "tensor(30.6317)\n",
      "tensor(23.7033)\n",
      "tensor(0.6913)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.708645\n",
      "Epoch 4480\n",
      "-------------------------------\n",
      "tensor(30.9358)\n",
      "tensor(26.6946)\n",
      "tensor(20.7496)\n",
      "tensor(0.4711)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 51.630272\n",
      "Epoch 4481\n",
      "-------------------------------\n",
      "tensor(26.9728)\n",
      "tensor(22.7060)\n",
      "tensor(12.8612)\n",
      "tensor(0.1494)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.571053\n",
      "Epoch 4482\n",
      "-------------------------------\n",
      "tensor(27.8383)\n",
      "tensor(19.9836)\n",
      "tensor(5.6428)\n",
      "tensor(0.1742)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 51.522015\n",
      "Epoch 4483\n",
      "-------------------------------\n",
      "tensor(42.2173)\n",
      "tensor(29.1544)\n",
      "tensor(9.5048)\n",
      "tensor(0.5805)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 51.583858\n",
      "Epoch 4484\n",
      "-------------------------------\n",
      "tensor(51.1223)\n",
      "tensor(37.6618)\n",
      "tensor(22.8749)\n",
      "tensor(0.9847)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.673847\n",
      "Epoch 4485\n",
      "-------------------------------\n",
      "tensor(49.6213)\n",
      "tensor(38.4167)\n",
      "tensor(29.3299)\n",
      "tensor(0.9993)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 51.707047\n",
      "Epoch 4486\n",
      "-------------------------------\n",
      "tensor(44.8378)\n",
      "tensor(28.3938)\n",
      "tensor(12.4829)\n",
      "tensor(0.1065)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 51.606422\n",
      "Epoch 4487\n",
      "-------------------------------\n",
      "tensor(41.2236)\n",
      "tensor(30.3112)\n",
      "tensor(21.7293)\n",
      "tensor(1.0812)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 51.480762\n",
      "Epoch 4488\n",
      "-------------------------------\n",
      "tensor(38.1100)\n",
      "tensor(30.7776)\n",
      "tensor(25.4516)\n",
      "tensor(0.6025)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 51.566036\n",
      "Epoch 4489\n",
      "-------------------------------\n",
      "tensor(55.9106)\n",
      "tensor(22.3860)\n",
      "tensor(25.3276)\n",
      "tensor(1.2900)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.576115\n",
      "Epoch 4490\n",
      "-------------------------------\n",
      "tensor(53.7335)\n",
      "tensor(24.4495)\n",
      "tensor(9.8070)\n",
      "tensor(0.3802)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 51.427574\n",
      "Epoch 4491\n",
      "-------------------------------\n",
      "tensor(40.3833)\n",
      "tensor(28.4004)\n",
      "tensor(13.4514)\n",
      "tensor(0.9642)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.387291\n",
      "Epoch 4492\n",
      "-------------------------------\n",
      "tensor(53.8090)\n",
      "tensor(22.0219)\n",
      "tensor(15.9344)\n",
      "tensor(0.6919)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 51.444763\n",
      "Epoch 4493\n",
      "-------------------------------\n",
      "tensor(55.9379)\n",
      "tensor(19.7380)\n",
      "tensor(34.8831)\n",
      "tensor(1.3148)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 51.370365\n",
      "Epoch 4494\n",
      "-------------------------------\n",
      "tensor(78.8903)\n",
      "tensor(22.3250)\n",
      "tensor(28.3739)\n",
      "tensor(0.0205)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 51.384567\n",
      "Epoch 4495\n",
      "-------------------------------\n",
      "tensor(36.9069)\n",
      "tensor(19.1155)\n",
      "tensor(11.7287)\n",
      "tensor(0.2365)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 51.333168\n",
      "Epoch 4496\n",
      "-------------------------------\n",
      "tensor(41.5646)\n",
      "tensor(20.8820)\n",
      "tensor(12.2150)\n",
      "tensor(0.1238)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.316635\n",
      "Epoch 4497\n",
      "-------------------------------\n",
      "tensor(29.1259)\n",
      "tensor(24.9993)\n",
      "tensor(15.9687)\n",
      "tensor(0.5101)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 51.236439\n",
      "Epoch 4498\n",
      "-------------------------------\n",
      "tensor(63.7856)\n",
      "tensor(22.1952)\n",
      "tensor(17.5799)\n",
      "tensor(0.1210)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 51.216934\n",
      "Epoch 4499\n",
      "-------------------------------\n",
      "tensor(50.1300)\n",
      "tensor(27.5912)\n",
      "tensor(6.8673)\n",
      "tensor(0.4949)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.164673\n",
      "Epoch 4500\n",
      "-------------------------------\n",
      "tensor(44.5596)\n",
      "tensor(19.5431)\n",
      "tensor(17.4141)\n",
      "tensor(0.7622)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 51.142437\n",
      "Epoch 4501\n",
      "-------------------------------\n",
      "tensor(49.1837)\n",
      "tensor(20.2538)\n",
      "tensor(17.9712)\n",
      "tensor(0.7151)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 51.134079\n",
      "Epoch 4502\n",
      "-------------------------------\n",
      "tensor(40.6861)\n",
      "tensor(19.4004)\n",
      "tensor(12.2552)\n",
      "tensor(0.5089)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 51.111610\n",
      "Epoch 4503\n",
      "-------------------------------\n",
      "tensor(49.5771)\n",
      "tensor(24.5390)\n",
      "tensor(4.3760)\n",
      "tensor(0.1417)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 51.093807\n",
      "Epoch 4504\n",
      "-------------------------------\n",
      "tensor(57.3578)\n",
      "tensor(21.2449)\n",
      "tensor(14.4619)\n",
      "tensor(0.3510)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.108433\n",
      "Epoch 4505\n",
      "-------------------------------\n",
      "tensor(35.0496)\n",
      "tensor(27.5262)\n",
      "tensor(16.4411)\n",
      "tensor(0.6416)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 51.124985\n",
      "Epoch 4506\n",
      "-------------------------------\n",
      "tensor(33.4994)\n",
      "tensor(24.5221)\n",
      "tensor(6.9228)\n",
      "tensor(0.3549)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 51.133526\n",
      "Epoch 4507\n",
      "-------------------------------\n",
      "tensor(37.4145)\n",
      "tensor(19.7717)\n",
      "tensor(9.2214)\n",
      "tensor(0.3261)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 51.102402\n",
      "Epoch 4508\n",
      "-------------------------------\n",
      "tensor(54.2808)\n",
      "tensor(27.4482)\n",
      "tensor(9.6208)\n",
      "tensor(0.5410)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 51.207577\n",
      "Epoch 4509\n",
      "-------------------------------\n",
      "tensor(45.9239)\n",
      "tensor(27.9341)\n",
      "tensor(10.0794)\n",
      "tensor(0.4030)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.143837\n",
      "Epoch 4510\n",
      "-------------------------------\n",
      "tensor(47.3005)\n",
      "tensor(23.9235)\n",
      "tensor(13.6826)\n",
      "tensor(0.4780)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 51.082771\n",
      "Epoch 4511\n",
      "-------------------------------\n",
      "tensor(45.4687)\n",
      "tensor(35.6806)\n",
      "tensor(37.8725)\n",
      "tensor(1.1600)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.093922\n",
      "Epoch 4512\n",
      "-------------------------------\n",
      "tensor(40.8845)\n",
      "tensor(34.5830)\n",
      "tensor(24.6202)\n",
      "tensor(1.4777)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.957310\n",
      "Epoch 4513\n",
      "-------------------------------\n",
      "tensor(65.6310)\n",
      "tensor(21.2217)\n",
      "tensor(32.3461)\n",
      "tensor(1.0185)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 51.059242\n",
      "Epoch 4514\n",
      "-------------------------------\n",
      "tensor(69.0080)\n",
      "tensor(42.3417)\n",
      "tensor(60.7606)\n",
      "tensor(1.9642)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 51.070133\n",
      "Epoch 4515\n",
      "-------------------------------\n",
      "tensor(40.7440)\n",
      "tensor(27.6702)\n",
      "tensor(9.0659)\n",
      "tensor(0.4069)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 50.946735\n",
      "Epoch 4516\n",
      "-------------------------------\n",
      "tensor(69.2537)\n",
      "tensor(46.1754)\n",
      "tensor(58.2736)\n",
      "tensor(1.7863)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 51.007275\n",
      "Epoch 4517\n",
      "-------------------------------\n",
      "tensor(35.5407)\n",
      "tensor(20.7618)\n",
      "tensor(6.4948)\n",
      "tensor(0.0988)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.836929\n",
      "Epoch 4518\n",
      "-------------------------------\n",
      "tensor(48.5508)\n",
      "tensor(35.7726)\n",
      "tensor(41.8795)\n",
      "tensor(1.1767)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.895863\n",
      "Epoch 4519\n",
      "-------------------------------\n",
      "tensor(44.8320)\n",
      "tensor(33.3566)\n",
      "tensor(37.2021)\n",
      "tensor(0.9727)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.845421\n",
      "Epoch 4520\n",
      "-------------------------------\n",
      "tensor(34.1893)\n",
      "tensor(25.6480)\n",
      "tensor(15.5608)\n",
      "tensor(0.2989)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 50.771076\n",
      "Epoch 4521\n",
      "-------------------------------\n",
      "tensor(36.0234)\n",
      "tensor(20.9273)\n",
      "tensor(5.2154)\n",
      "tensor(0.2317)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.732197\n",
      "Epoch 4522\n",
      "-------------------------------\n",
      "tensor(44.4397)\n",
      "tensor(29.0699)\n",
      "tensor(14.2850)\n",
      "tensor(0.6028)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.760902\n",
      "Epoch 4523\n",
      "-------------------------------\n",
      "tensor(44.9787)\n",
      "tensor(31.7586)\n",
      "tensor(23.0088)\n",
      "tensor(0.8460)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.770348\n",
      "Epoch 4524\n",
      "-------------------------------\n",
      "tensor(42.4839)\n",
      "tensor(31.3359)\n",
      "tensor(25.2297)\n",
      "tensor(0.7824)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 50.760418\n",
      "Epoch 4525\n",
      "-------------------------------\n",
      "tensor(55.1584)\n",
      "tensor(22.2477)\n",
      "tensor(12.1067)\n",
      "tensor(0.0836)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 50.711014\n",
      "Epoch 4526\n",
      "-------------------------------\n",
      "tensor(45.2910)\n",
      "tensor(32.4760)\n",
      "tensor(29.3613)\n",
      "tensor(1.0104)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 50.746849\n",
      "Epoch 4527\n",
      "-------------------------------\n",
      "tensor(79.9332)\n",
      "tensor(18.2452)\n",
      "tensor(37.7263)\n",
      "tensor(0.7365)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 50.774555\n",
      "Epoch 4528\n",
      "-------------------------------\n",
      "tensor(77.0665)\n",
      "tensor(21.9586)\n",
      "tensor(46.6736)\n",
      "tensor(1.5482)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.816246\n",
      "Epoch 4529\n",
      "-------------------------------\n",
      "tensor(37.6386)\n",
      "tensor(19.4997)\n",
      "tensor(10.3548)\n",
      "tensor(0.3781)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.676556\n",
      "Epoch 4530\n",
      "-------------------------------\n",
      "tensor(112.4124)\n",
      "tensor(23.5283)\n",
      "tensor(64.0616)\n",
      "tensor(1.6361)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 50.972351\n",
      "Epoch 4531\n",
      "-------------------------------\n",
      "tensor(96.0387)\n",
      "tensor(49.2243)\n",
      "tensor(78.0119)\n",
      "tensor(1.8425)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.898495\n",
      "Epoch 4532\n",
      "-------------------------------\n",
      "tensor(31.9336)\n",
      "tensor(30.6967)\n",
      "tensor(21.0827)\n",
      "tensor(0.8626)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.666843\n",
      "Epoch 4533\n",
      "-------------------------------\n",
      "tensor(78.5587)\n",
      "tensor(42.5677)\n",
      "tensor(61.8186)\n",
      "tensor(1.2419)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 50.807091\n",
      "Epoch 4534\n",
      "-------------------------------\n",
      "tensor(102.0495)\n",
      "tensor(33.2443)\n",
      "tensor(69.9868)\n",
      "tensor(2.5012)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 50.818138\n",
      "Epoch 4535\n",
      "-------------------------------\n",
      "tensor(60.0410)\n",
      "tensor(24.7079)\n",
      "tensor(10.1476)\n",
      "tensor(0.0403)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 50.545166\n",
      "Epoch 4536\n",
      "-------------------------------\n",
      "tensor(83.3000)\n",
      "tensor(21.6105)\n",
      "tensor(37.9531)\n",
      "tensor(1.2634)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 50.564388\n",
      "Epoch 4537\n",
      "-------------------------------\n",
      "tensor(48.4851)\n",
      "tensor(21.1460)\n",
      "tensor(13.1355)\n",
      "tensor(0.0351)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.444839\n",
      "Epoch 4538\n",
      "-------------------------------\n",
      "tensor(51.6757)\n",
      "tensor(18.6979)\n",
      "tensor(22.8429)\n",
      "tensor(0.6402)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.457760\n",
      "Epoch 4539\n",
      "-------------------------------\n",
      "tensor(35.0460)\n",
      "tensor(20.0158)\n",
      "tensor(8.9808)\n",
      "tensor(0.4827)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.407394\n",
      "Epoch 4540\n",
      "-------------------------------\n",
      "tensor(30.1700)\n",
      "tensor(21.8941)\n",
      "tensor(11.7971)\n",
      "tensor(0.2048)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 50.374039\n",
      "Epoch 4541\n",
      "-------------------------------\n",
      "tensor(57.4496)\n",
      "tensor(19.7824)\n",
      "tensor(16.8315)\n",
      "tensor(0.0727)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.408848\n",
      "Epoch 4542\n",
      "-------------------------------\n",
      "tensor(55.3830)\n",
      "tensor(20.5789)\n",
      "tensor(14.1978)\n",
      "tensor(0.0399)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.407253\n",
      "Epoch 4543\n",
      "-------------------------------\n",
      "tensor(44.9696)\n",
      "tensor(21.9238)\n",
      "tensor(5.2576)\n",
      "tensor(0.0759)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.369652\n",
      "Epoch 4544\n",
      "-------------------------------\n",
      "tensor(47.5960)\n",
      "tensor(19.5146)\n",
      "tensor(16.3458)\n",
      "tensor(0.1602)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 50.359779\n",
      "Epoch 4545\n",
      "-------------------------------\n",
      "tensor(67.3880)\n",
      "tensor(21.1147)\n",
      "tensor(24.8867)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 50.392838\n",
      "Epoch 4546\n",
      "-------------------------------\n",
      "tensor(36.8581)\n",
      "tensor(25.2428)\n",
      "tensor(10.0105)\n",
      "tensor(0.5905)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 50.323700\n",
      "Epoch 4547\n",
      "-------------------------------\n",
      "tensor(89.6657)\n",
      "tensor(21.1940)\n",
      "tensor(39.4608)\n",
      "tensor(0.6372)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 50.457947\n",
      "Epoch 4548\n",
      "-------------------------------\n",
      "tensor(42.6396)\n",
      "tensor(33.4363)\n",
      "tensor(21.4423)\n",
      "tensor(1.3544)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.314804\n",
      "Epoch 4549\n",
      "-------------------------------\n",
      "tensor(89.4330)\n",
      "tensor(23.1072)\n",
      "tensor(52.7890)\n",
      "tensor(1.4689)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.490025\n",
      "Epoch 4550\n",
      "-------------------------------\n",
      "tensor(76.2761)\n",
      "tensor(54.5029)\n",
      "tensor(75.1487)\n",
      "tensor(2.6708)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 50.572514\n",
      "Epoch 4551\n",
      "-------------------------------\n",
      "tensor(39.0545)\n",
      "tensor(20.1072)\n",
      "tensor(12.3731)\n",
      "tensor(0.3060)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.259777\n",
      "Epoch 4552\n",
      "-------------------------------\n",
      "tensor(45.3049)\n",
      "tensor(46.3014)\n",
      "tensor(45.4103)\n",
      "tensor(2.1223)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.338764\n",
      "Epoch 4553\n",
      "-------------------------------\n",
      "tensor(83.1302)\n",
      "tensor(20.2389)\n",
      "tensor(35.7875)\n",
      "tensor(0.7840)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 50.338821\n",
      "Epoch 4554\n",
      "-------------------------------\n",
      "tensor(65.5958)\n",
      "tensor(23.4718)\n",
      "tensor(20.4319)\n",
      "tensor(0.3193)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 50.171185\n",
      "Epoch 4555\n",
      "-------------------------------\n",
      "tensor(39.1202)\n",
      "tensor(24.6668)\n",
      "tensor(5.4127)\n",
      "tensor(0.1248)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 50.121075\n",
      "Epoch 4556\n",
      "-------------------------------\n",
      "tensor(45.0003)\n",
      "tensor(26.3131)\n",
      "tensor(26.5393)\n",
      "tensor(0.1140)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 50.100182\n",
      "Epoch 4557\n",
      "-------------------------------\n",
      "tensor(52.2511)\n",
      "tensor(26.1361)\n",
      "tensor(12.5302)\n",
      "tensor(0.7796)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.164158\n",
      "Epoch 4558\n",
      "-------------------------------\n",
      "tensor(50.1599)\n",
      "tensor(32.1191)\n",
      "tensor(24.7800)\n",
      "tensor(0.8428)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.150349\n",
      "Epoch 4559\n",
      "-------------------------------\n",
      "tensor(48.0313)\n",
      "tensor(27.6914)\n",
      "tensor(23.7738)\n",
      "tensor(0.2726)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.077599\n",
      "Epoch 4560\n",
      "-------------------------------\n",
      "tensor(43.8624)\n",
      "tensor(21.7118)\n",
      "tensor(14.0334)\n",
      "tensor(0.3100)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 50.028770\n",
      "Epoch 4561\n",
      "-------------------------------\n",
      "tensor(31.1099)\n",
      "tensor(25.3280)\n",
      "tensor(9.7461)\n",
      "tensor(0.6511)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 50.023643\n",
      "Epoch 4562\n",
      "-------------------------------\n",
      "tensor(25.1242)\n",
      "tensor(27.6716)\n",
      "tensor(15.5408)\n",
      "tensor(0.8008)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 50.015160\n",
      "Epoch 4563\n",
      "-------------------------------\n",
      "tensor(56.4443)\n",
      "tensor(18.4036)\n",
      "tensor(22.2492)\n",
      "tensor(0.7502)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.010128\n",
      "Epoch 4564\n",
      "-------------------------------\n",
      "tensor(30.3892)\n",
      "tensor(25.0604)\n",
      "tensor(18.3021)\n",
      "tensor(0.2713)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.985359\n",
      "Epoch 4565\n",
      "-------------------------------\n",
      "tensor(46.5964)\n",
      "tensor(25.6687)\n",
      "tensor(10.7995)\n",
      "tensor(0.7563)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.972549\n",
      "Epoch 4566\n",
      "-------------------------------\n",
      "tensor(66.9965)\n",
      "tensor(26.9837)\n",
      "tensor(35.4948)\n",
      "tensor(1.6807)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 50.006840\n",
      "Epoch 4567\n",
      "-------------------------------\n",
      "tensor(45.3301)\n",
      "tensor(22.2567)\n",
      "tensor(10.5526)\n",
      "tensor(0.6977)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.947788\n",
      "Epoch 4568\n",
      "-------------------------------\n",
      "tensor(91.8039)\n",
      "tensor(20.3795)\n",
      "tensor(44.9943)\n",
      "tensor(1.1152)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.084194\n",
      "Epoch 4569\n",
      "-------------------------------\n",
      "tensor(46.4916)\n",
      "tensor(28.3701)\n",
      "tensor(27.1402)\n",
      "tensor(0.1885)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.928677\n",
      "Epoch 4570\n",
      "-------------------------------\n",
      "tensor(78.6029)\n",
      "tensor(21.2987)\n",
      "tensor(35.7145)\n",
      "tensor(0.4383)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 50.087105\n",
      "Epoch 4571\n",
      "-------------------------------\n",
      "tensor(105.3617)\n",
      "tensor(52.6828)\n",
      "tensor(84.1816)\n",
      "tensor(1.7714)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.346977\n",
      "Epoch 4572\n",
      "-------------------------------\n",
      "tensor(82.4488)\n",
      "tensor(28.6443)\n",
      "tensor(57.7722)\n",
      "tensor(2.3553)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 50.076786\n",
      "Epoch 4573\n",
      "-------------------------------\n",
      "tensor(42.0820)\n",
      "tensor(24.5448)\n",
      "tensor(5.5430)\n",
      "tensor(0.1195)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.908253\n",
      "Epoch 4574\n",
      "-------------------------------\n",
      "tensor(66.8529)\n",
      "tensor(22.2445)\n",
      "tensor(34.3267)\n",
      "tensor(1.6174)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.964855\n",
      "Epoch 4575\n",
      "-------------------------------\n",
      "tensor(71.6185)\n",
      "tensor(19.7490)\n",
      "tensor(33.2253)\n",
      "tensor(0.3576)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.794487\n",
      "Epoch 4576\n",
      "-------------------------------\n",
      "tensor(38.3243)\n",
      "tensor(22.0888)\n",
      "tensor(10.6847)\n",
      "tensor(0.6541)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.698170\n",
      "Epoch 4577\n",
      "-------------------------------\n",
      "tensor(80.9698)\n",
      "tensor(21.0723)\n",
      "tensor(31.7803)\n",
      "tensor(0.4558)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.778076\n",
      "Epoch 4578\n",
      "-------------------------------\n",
      "tensor(35.5439)\n",
      "tensor(20.8479)\n",
      "tensor(9.3470)\n",
      "tensor(0.5896)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 49.644531\n",
      "Epoch 4579\n",
      "-------------------------------\n",
      "tensor(46.2460)\n",
      "tensor(26.6037)\n",
      "tensor(15.9651)\n",
      "tensor(0.4254)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 49.636093\n",
      "Epoch 4580\n",
      "-------------------------------\n",
      "tensor(51.3725)\n",
      "tensor(20.2287)\n",
      "tensor(18.0788)\n",
      "tensor(0.1117)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 49.637245\n",
      "Epoch 4581\n",
      "-------------------------------\n",
      "tensor(47.0441)\n",
      "tensor(22.2855)\n",
      "tensor(12.1180)\n",
      "tensor(0.1994)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 49.617733\n",
      "Epoch 4582\n",
      "-------------------------------\n",
      "tensor(45.4643)\n",
      "tensor(21.3422)\n",
      "tensor(7.7092)\n",
      "tensor(0.4479)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 49.599144\n",
      "Epoch 4583\n",
      "-------------------------------\n",
      "tensor(51.9972)\n",
      "tensor(20.2825)\n",
      "tensor(12.6928)\n",
      "tensor(0.6160)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.595173\n",
      "Epoch 4584\n",
      "-------------------------------\n",
      "tensor(36.4050)\n",
      "tensor(27.7322)\n",
      "tensor(17.0624)\n",
      "tensor(0.5660)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.604988\n",
      "Epoch 4585\n",
      "-------------------------------\n",
      "tensor(30.3940)\n",
      "tensor(23.9140)\n",
      "tensor(15.3036)\n",
      "tensor(0.0402)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.604660\n",
      "Epoch 4586\n",
      "-------------------------------\n",
      "tensor(43.9665)\n",
      "tensor(26.8696)\n",
      "tensor(12.8472)\n",
      "tensor(0.9369)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.581963\n",
      "Epoch 4587\n",
      "-------------------------------\n",
      "tensor(62.3208)\n",
      "tensor(23.3115)\n",
      "tensor(32.9454)\n",
      "tensor(1.4020)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.642456\n",
      "Epoch 4588\n",
      "-------------------------------\n",
      "tensor(33.4923)\n",
      "tensor(24.5955)\n",
      "tensor(13.8941)\n",
      "tensor(0.3994)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 49.555218\n",
      "Epoch 4589\n",
      "-------------------------------\n",
      "tensor(79.2810)\n",
      "tensor(20.2434)\n",
      "tensor(37.6181)\n",
      "tensor(1.1005)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.732021\n",
      "Epoch 4590\n",
      "-------------------------------\n",
      "tensor(81.8311)\n",
      "tensor(44.4833)\n",
      "tensor(61.0522)\n",
      "tensor(1.5401)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 49.730438\n",
      "Epoch 4591\n",
      "-------------------------------\n",
      "tensor(34.7515)\n",
      "tensor(30.6269)\n",
      "tensor(22.0431)\n",
      "tensor(0.5228)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.561874\n",
      "Epoch 4592\n",
      "-------------------------------\n",
      "tensor(59.2138)\n",
      "tensor(38.4413)\n",
      "tensor(45.9448)\n",
      "tensor(0.8734)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 49.653404\n",
      "Epoch 4593\n",
      "-------------------------------\n",
      "tensor(100.1627)\n",
      "tensor(26.1998)\n",
      "tensor(58.4018)\n",
      "tensor(1.7512)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.654266\n",
      "Epoch 4594\n",
      "-------------------------------\n",
      "tensor(84.3198)\n",
      "tensor(19.4473)\n",
      "tensor(38.2806)\n",
      "tensor(0.7918)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.682400\n",
      "Epoch 4595\n",
      "-------------------------------\n",
      "tensor(44.6489)\n",
      "tensor(22.7495)\n",
      "tensor(7.0466)\n",
      "tensor(0.1913)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.574932\n",
      "Epoch 4596\n",
      "-------------------------------\n",
      "tensor(61.2000)\n",
      "tensor(34.2065)\n",
      "tensor(43.4679)\n",
      "tensor(0.7472)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.518608\n",
      "Epoch 4597\n",
      "-------------------------------\n",
      "tensor(39.8505)\n",
      "tensor(22.1439)\n",
      "tensor(9.9194)\n",
      "tensor(0.1130)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.399857\n",
      "Epoch 4598\n",
      "-------------------------------\n",
      "tensor(40.4597)\n",
      "tensor(33.0974)\n",
      "tensor(32.0050)\n",
      "tensor(0.8100)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 49.444981\n",
      "Epoch 4599\n",
      "-------------------------------\n",
      "tensor(44.4542)\n",
      "tensor(31.2665)\n",
      "tensor(32.1280)\n",
      "tensor(0.5467)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 49.419727\n",
      "Epoch 4600\n",
      "-------------------------------\n",
      "tensor(37.7316)\n",
      "tensor(25.6962)\n",
      "tensor(16.4952)\n",
      "tensor(0.0050)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 49.357159\n",
      "Epoch 4601\n",
      "-------------------------------\n",
      "tensor(36.8367)\n",
      "tensor(22.0217)\n",
      "tensor(7.0899)\n",
      "tensor(0.3920)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 49.312176\n",
      "Epoch 4602\n",
      "-------------------------------\n",
      "tensor(40.2637)\n",
      "tensor(20.3637)\n",
      "tensor(11.2460)\n",
      "tensor(0.6320)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 49.272430\n",
      "Epoch 4603\n",
      "-------------------------------\n",
      "tensor(38.9607)\n",
      "tensor(28.6078)\n",
      "tensor(16.3375)\n",
      "tensor(0.7638)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.290535\n",
      "Epoch 4604\n",
      "-------------------------------\n",
      "tensor(42.6748)\n",
      "tensor(29.9461)\n",
      "tensor(18.8236)\n",
      "tensor(0.6517)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.335403\n",
      "Epoch 4605\n",
      "-------------------------------\n",
      "tensor(46.1168)\n",
      "tensor(26.4233)\n",
      "tensor(13.3862)\n",
      "tensor(0.0737)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.354870\n",
      "Epoch 4606\n",
      "-------------------------------\n",
      "tensor(57.4887)\n",
      "tensor(22.0522)\n",
      "tensor(14.0476)\n",
      "tensor(0.8008)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.340412\n",
      "Epoch 4607\n",
      "-------------------------------\n",
      "tensor(41.0457)\n",
      "tensor(29.5885)\n",
      "tensor(14.9684)\n",
      "tensor(0.8049)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.348740\n",
      "Epoch 4608\n",
      "-------------------------------\n",
      "tensor(42.1596)\n",
      "tensor(25.3014)\n",
      "tensor(12.9860)\n",
      "tensor(0.4179)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 49.357872\n",
      "Epoch 4609\n",
      "-------------------------------\n",
      "tensor(41.0118)\n",
      "tensor(23.3664)\n",
      "tensor(19.3460)\n",
      "tensor(1.2035)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.334270\n",
      "Epoch 4610\n",
      "-------------------------------\n",
      "tensor(44.1838)\n",
      "tensor(21.7440)\n",
      "tensor(4.8755)\n",
      "tensor(0.0182)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 49.183571\n",
      "Epoch 4611\n",
      "-------------------------------\n",
      "tensor(66.5794)\n",
      "tensor(24.9341)\n",
      "tensor(23.8096)\n",
      "tensor(0.7853)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.273052\n",
      "Epoch 4612\n",
      "-------------------------------\n",
      "tensor(84.1202)\n",
      "tensor(18.9394)\n",
      "tensor(38.3447)\n",
      "tensor(0.7585)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 49.379246\n",
      "Epoch 4613\n",
      "-------------------------------\n",
      "tensor(51.7695)\n",
      "tensor(46.8208)\n",
      "tensor(55.2957)\n",
      "tensor(2.4467)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 49.241184\n",
      "Epoch 4614\n",
      "-------------------------------\n",
      "tensor(35.4606)\n",
      "tensor(24.6370)\n",
      "tensor(5.5059)\n",
      "tensor(0.0718)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.218082\n",
      "Epoch 4615\n",
      "-------------------------------\n",
      "tensor(51.6360)\n",
      "tensor(47.7708)\n",
      "tensor(54.7201)\n",
      "tensor(2.1068)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.336472\n",
      "Epoch 4616\n",
      "-------------------------------\n",
      "tensor(46.1090)\n",
      "tensor(22.4196)\n",
      "tensor(10.5375)\n",
      "tensor(0.0026)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 49.156990\n",
      "Epoch 4617\n",
      "-------------------------------\n",
      "tensor(54.9119)\n",
      "tensor(20.7070)\n",
      "tensor(29.9420)\n",
      "tensor(1.2265)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.037647\n",
      "Epoch 4618\n",
      "-------------------------------\n",
      "tensor(52.5533)\n",
      "tensor(29.7226)\n",
      "tensor(10.7114)\n",
      "tensor(0.8798)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 49.066200\n",
      "Epoch 4619\n",
      "-------------------------------\n",
      "tensor(61.8851)\n",
      "tensor(25.6181)\n",
      "tensor(13.3529)\n",
      "tensor(0.4165)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 49.101177\n",
      "Epoch 4620\n",
      "-------------------------------\n",
      "tensor(50.9636)\n",
      "tensor(23.1983)\n",
      "tensor(8.5642)\n",
      "tensor(0.2092)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 49.045792\n",
      "Epoch 4621\n",
      "-------------------------------\n",
      "tensor(40.3609)\n",
      "tensor(23.0715)\n",
      "tensor(4.6661)\n",
      "tensor(0.1334)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.989288\n",
      "Epoch 4622\n",
      "-------------------------------\n",
      "tensor(37.9046)\n",
      "tensor(23.8653)\n",
      "tensor(10.1042)\n",
      "tensor(0.0844)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.938637\n",
      "Epoch 4623\n",
      "-------------------------------\n",
      "tensor(45.4532)\n",
      "tensor(19.7537)\n",
      "tensor(17.1057)\n",
      "tensor(0.0207)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.937057\n",
      "Epoch 4624\n",
      "-------------------------------\n",
      "tensor(52.6525)\n",
      "tensor(22.4694)\n",
      "tensor(15.9705)\n",
      "tensor(0.1671)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.984344\n",
      "Epoch 4625\n",
      "-------------------------------\n",
      "tensor(45.3243)\n",
      "tensor(28.3336)\n",
      "tensor(9.2778)\n",
      "tensor(0.5525)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.998516\n",
      "Epoch 4626\n",
      "-------------------------------\n",
      "tensor(48.7780)\n",
      "tensor(33.7433)\n",
      "tensor(34.0979)\n",
      "tensor(0.6470)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 49.034878\n",
      "Epoch 4627\n",
      "-------------------------------\n",
      "tensor(46.3385)\n",
      "tensor(26.0057)\n",
      "tensor(22.8617)\n",
      "tensor(0.5091)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.976555\n",
      "Epoch 4628\n",
      "-------------------------------\n",
      "tensor(55.2045)\n",
      "tensor(40.9029)\n",
      "tensor(35.8976)\n",
      "tensor(1.8154)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.990932\n",
      "Epoch 4629\n",
      "-------------------------------\n",
      "tensor(44.6882)\n",
      "tensor(25.9650)\n",
      "tensor(16.5142)\n",
      "tensor(0.0489)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.927673\n",
      "Epoch 4630\n",
      "-------------------------------\n",
      "tensor(60.6188)\n",
      "tensor(27.6696)\n",
      "tensor(42.0098)\n",
      "tensor(2.2474)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 48.910053\n",
      "Epoch 4631\n",
      "-------------------------------\n",
      "tensor(69.1766)\n",
      "tensor(20.5996)\n",
      "tensor(31.1213)\n",
      "tensor(0.5861)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.087204\n",
      "Epoch 4632\n",
      "-------------------------------\n",
      "tensor(55.3247)\n",
      "tensor(29.5383)\n",
      "tensor(30.8489)\n",
      "tensor(0.5242)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 49.110870\n",
      "Epoch 4633\n",
      "-------------------------------\n",
      "tensor(37.8947)\n",
      "tensor(23.6991)\n",
      "tensor(10.7828)\n",
      "tensor(0.5096)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.923603\n",
      "Epoch 4634\n",
      "-------------------------------\n",
      "tensor(57.6956)\n",
      "tensor(20.6787)\n",
      "tensor(20.2461)\n",
      "tensor(0.0773)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.824409\n",
      "Epoch 4635\n",
      "-------------------------------\n",
      "tensor(74.9400)\n",
      "tensor(24.5744)\n",
      "tensor(22.0922)\n",
      "tensor(0.4182)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.049088\n",
      "Epoch 4636\n",
      "-------------------------------\n",
      "tensor(42.4051)\n",
      "tensor(33.2544)\n",
      "tensor(13.9308)\n",
      "tensor(0.8450)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.990528\n",
      "Epoch 4637\n",
      "-------------------------------\n",
      "tensor(45.8156)\n",
      "tensor(35.7262)\n",
      "tensor(30.0747)\n",
      "tensor(1.1585)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.886597\n",
      "Epoch 4638\n",
      "-------------------------------\n",
      "tensor(34.1883)\n",
      "tensor(22.3859)\n",
      "tensor(11.9207)\n",
      "tensor(0.2704)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.719322\n",
      "Epoch 4639\n",
      "-------------------------------\n",
      "tensor(32.3324)\n",
      "tensor(27.8252)\n",
      "tensor(8.8632)\n",
      "tensor(0.5133)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.709641\n",
      "Epoch 4640\n",
      "-------------------------------\n",
      "tensor(32.2501)\n",
      "tensor(32.6653)\n",
      "tensor(17.6649)\n",
      "tensor(0.8133)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 48.750912\n",
      "Epoch 4641\n",
      "-------------------------------\n",
      "tensor(32.6284)\n",
      "tensor(33.0837)\n",
      "tensor(19.5329)\n",
      "tensor(0.8062)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.749962\n",
      "Epoch 4642\n",
      "-------------------------------\n",
      "tensor(32.8955)\n",
      "tensor(31.2849)\n",
      "tensor(17.2688)\n",
      "tensor(0.6415)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.724445\n",
      "Epoch 4643\n",
      "-------------------------------\n",
      "tensor(35.0581)\n",
      "tensor(27.5837)\n",
      "tensor(11.3531)\n",
      "tensor(0.3054)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.674015\n",
      "Epoch 4644\n",
      "-------------------------------\n",
      "tensor(44.7465)\n",
      "tensor(21.2955)\n",
      "tensor(6.3304)\n",
      "tensor(0.2595)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.611572\n",
      "Epoch 4645\n",
      "-------------------------------\n",
      "tensor(42.8809)\n",
      "tensor(29.7833)\n",
      "tensor(20.3011)\n",
      "tensor(0.9580)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.638626\n",
      "Epoch 4646\n",
      "-------------------------------\n",
      "tensor(46.5823)\n",
      "tensor(32.8369)\n",
      "tensor(24.2594)\n",
      "tensor(0.9839)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.634434\n",
      "Epoch 4647\n",
      "-------------------------------\n",
      "tensor(41.1567)\n",
      "tensor(22.8866)\n",
      "tensor(7.1794)\n",
      "tensor(0.1969)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.597092\n",
      "Epoch 4648\n",
      "-------------------------------\n",
      "tensor(49.2247)\n",
      "tensor(30.3871)\n",
      "tensor(35.9544)\n",
      "tensor(0.9615)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.644333\n",
      "Epoch 4649\n",
      "-------------------------------\n",
      "tensor(40.3126)\n",
      "tensor(33.1084)\n",
      "tensor(16.5660)\n",
      "tensor(1.0460)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.595757\n",
      "Epoch 4650\n",
      "-------------------------------\n",
      "tensor(79.4230)\n",
      "tensor(26.3471)\n",
      "tensor(46.9747)\n",
      "tensor(1.6076)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 48.690891\n",
      "Epoch 4651\n",
      "-------------------------------\n",
      "tensor(91.4314)\n",
      "tensor(49.4480)\n",
      "tensor(77.0505)\n",
      "tensor(2.0438)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.722984\n",
      "Epoch 4652\n",
      "-------------------------------\n",
      "tensor(52.1981)\n",
      "tensor(34.6165)\n",
      "tensor(39.8152)\n",
      "tensor(1.2823)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.605785\n",
      "Epoch 4653\n",
      "-------------------------------\n",
      "tensor(53.5647)\n",
      "tensor(28.4511)\n",
      "tensor(34.1254)\n",
      "tensor(0.6461)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.550735\n",
      "Epoch 4654\n",
      "-------------------------------\n",
      "tensor(60.5066)\n",
      "tensor(51.5613)\n",
      "tensor(62.9003)\n",
      "tensor(2.2907)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.586357\n",
      "Epoch 4655\n",
      "-------------------------------\n",
      "tensor(45.6718)\n",
      "tensor(24.2829)\n",
      "tensor(6.8768)\n",
      "tensor(0.1781)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.434990\n",
      "Epoch 4656\n",
      "-------------------------------\n",
      "tensor(45.1056)\n",
      "tensor(36.2230)\n",
      "tensor(35.7731)\n",
      "tensor(1.6646)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.452168\n",
      "Epoch 4657\n",
      "-------------------------------\n",
      "tensor(40.3994)\n",
      "tensor(25.1092)\n",
      "tensor(9.5704)\n",
      "tensor(0.5859)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.394032\n",
      "Epoch 4658\n",
      "-------------------------------\n",
      "tensor(29.2996)\n",
      "tensor(25.6363)\n",
      "tensor(13.5817)\n",
      "tensor(0.5494)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.344395\n",
      "Epoch 4659\n",
      "-------------------------------\n",
      "tensor(26.2889)\n",
      "tensor(27.5209)\n",
      "tensor(18.8107)\n",
      "tensor(0.8123)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.349541\n",
      "Epoch 4660\n",
      "-------------------------------\n",
      "tensor(47.6565)\n",
      "tensor(18.3396)\n",
      "tensor(13.8985)\n",
      "tensor(0.5719)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 48.324318\n",
      "Epoch 4661\n",
      "-------------------------------\n",
      "tensor(27.9617)\n",
      "tensor(21.8532)\n",
      "tensor(6.1837)\n",
      "tensor(0.2192)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.316772\n",
      "Epoch 4662\n",
      "-------------------------------\n",
      "tensor(31.0675)\n",
      "tensor(19.8056)\n",
      "tensor(6.0091)\n",
      "tensor(0.1025)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.305122\n",
      "Epoch 4663\n",
      "-------------------------------\n",
      "tensor(35.0462)\n",
      "tensor(24.4348)\n",
      "tensor(9.8509)\n",
      "tensor(0.4449)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.302582\n",
      "Epoch 4664\n",
      "-------------------------------\n",
      "tensor(36.5739)\n",
      "tensor(27.4984)\n",
      "tensor(15.0449)\n",
      "tensor(0.7703)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.285675\n",
      "Epoch 4665\n",
      "-------------------------------\n",
      "tensor(39.4016)\n",
      "tensor(19.4359)\n",
      "tensor(15.3961)\n",
      "tensor(0.8076)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.266830\n",
      "Epoch 4666\n",
      "-------------------------------\n",
      "tensor(42.4037)\n",
      "tensor(24.6705)\n",
      "tensor(9.4877)\n",
      "tensor(0.1555)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.268150\n",
      "Epoch 4667\n",
      "-------------------------------\n",
      "tensor(71.1920)\n",
      "tensor(18.8732)\n",
      "tensor(27.8521)\n",
      "tensor(0.4680)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.310101\n",
      "Epoch 4668\n",
      "-------------------------------\n",
      "tensor(62.9319)\n",
      "tensor(22.1696)\n",
      "tensor(19.1725)\n",
      "tensor(0.3232)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.269855\n",
      "Epoch 4669\n",
      "-------------------------------\n",
      "tensor(43.3297)\n",
      "tensor(21.5011)\n",
      "tensor(4.4249)\n",
      "tensor(0.0521)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.259590\n",
      "Epoch 4670\n",
      "-------------------------------\n",
      "tensor(35.3353)\n",
      "tensor(28.3398)\n",
      "tensor(12.6539)\n",
      "tensor(0.3654)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 48.234848\n",
      "Epoch 4671\n",
      "-------------------------------\n",
      "tensor(44.8310)\n",
      "tensor(19.8302)\n",
      "tensor(8.4208)\n",
      "tensor(0.1207)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.180981\n",
      "Epoch 4672\n",
      "-------------------------------\n",
      "tensor(74.7898)\n",
      "tensor(20.7303)\n",
      "tensor(32.7066)\n",
      "tensor(0.6665)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.220310\n",
      "Epoch 4673\n",
      "-------------------------------\n",
      "tensor(97.4503)\n",
      "tensor(19.8154)\n",
      "tensor(55.4378)\n",
      "tensor(1.3182)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 48.309902\n",
      "Epoch 4674\n",
      "-------------------------------\n",
      "tensor(82.1253)\n",
      "tensor(21.7992)\n",
      "tensor(50.0548)\n",
      "tensor(1.3765)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.290733\n",
      "Epoch 4675\n",
      "-------------------------------\n",
      "tensor(28.0179)\n",
      "tensor(21.2379)\n",
      "tensor(5.7777)\n",
      "tensor(0.0679)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.100079\n",
      "Epoch 4676\n",
      "-------------------------------\n",
      "tensor(87.8406)\n",
      "tensor(16.5917)\n",
      "tensor(41.3375)\n",
      "tensor(0.6417)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 48.221626\n",
      "Epoch 4677\n",
      "-------------------------------\n",
      "tensor(51.0636)\n",
      "tensor(30.5307)\n",
      "tensor(18.7317)\n",
      "tensor(0.6484)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.098469\n",
      "Epoch 4678\n",
      "-------------------------------\n",
      "tensor(82.1464)\n",
      "tensor(22.6850)\n",
      "tensor(40.2579)\n",
      "tensor(0.9779)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.112331\n",
      "Epoch 4679\n",
      "-------------------------------\n",
      "tensor(44.8127)\n",
      "tensor(20.2958)\n",
      "tensor(12.7654)\n",
      "tensor(0.3020)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.008209\n",
      "Epoch 4680\n",
      "-------------------------------\n",
      "tensor(30.5671)\n",
      "tensor(25.4297)\n",
      "tensor(14.5802)\n",
      "tensor(0.2812)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 47.984486\n",
      "Epoch 4681\n",
      "-------------------------------\n",
      "tensor(62.1176)\n",
      "tensor(16.0785)\n",
      "tensor(25.7952)\n",
      "tensor(0.4784)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 48.004787\n",
      "Epoch 4682\n",
      "-------------------------------\n",
      "tensor(60.9427)\n",
      "tensor(16.2129)\n",
      "tensor(24.4329)\n",
      "tensor(0.4202)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 48.000027\n",
      "Epoch 4683\n",
      "-------------------------------\n",
      "tensor(28.3635)\n",
      "tensor(24.1418)\n",
      "tensor(11.8514)\n",
      "tensor(0.1340)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.951984\n",
      "Epoch 4684\n",
      "-------------------------------\n",
      "tensor(44.8253)\n",
      "tensor(20.4760)\n",
      "tensor(12.1518)\n",
      "tensor(0.4087)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.944225\n",
      "Epoch 4685\n",
      "-------------------------------\n",
      "tensor(40.1733)\n",
      "tensor(28.7309)\n",
      "tensor(28.4735)\n",
      "tensor(0.8507)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.968109\n",
      "Epoch 4686\n",
      "-------------------------------\n",
      "tensor(46.9962)\n",
      "tensor(26.0072)\n",
      "tensor(19.0566)\n",
      "tensor(0.4001)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.964085\n",
      "Epoch 4687\n",
      "-------------------------------\n",
      "tensor(44.1599)\n",
      "tensor(31.7350)\n",
      "tensor(20.8903)\n",
      "tensor(0.8612)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.939667\n",
      "Epoch 4688\n",
      "-------------------------------\n",
      "tensor(48.4354)\n",
      "tensor(33.6869)\n",
      "tensor(36.2347)\n",
      "tensor(0.7862)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.947845\n",
      "Epoch 4689\n",
      "-------------------------------\n",
      "tensor(39.6639)\n",
      "tensor(35.3492)\n",
      "tensor(31.0510)\n",
      "tensor(1.5727)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.970322\n",
      "Epoch 4690\n",
      "-------------------------------\n",
      "tensor(46.3514)\n",
      "tensor(30.1995)\n",
      "tensor(30.1346)\n",
      "tensor(0.7835)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 47.913570\n",
      "Epoch 4691\n",
      "-------------------------------\n",
      "tensor(44.7220)\n",
      "tensor(46.6838)\n",
      "tensor(51.4188)\n",
      "tensor(2.2530)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.027409\n",
      "Epoch 4692\n",
      "-------------------------------\n",
      "tensor(49.7571)\n",
      "tensor(21.8262)\n",
      "tensor(13.8866)\n",
      "tensor(0.4648)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.927113\n",
      "Epoch 4693\n",
      "-------------------------------\n",
      "tensor(35.1263)\n",
      "tensor(22.3442)\n",
      "tensor(20.8825)\n",
      "tensor(1.5432)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.807358\n",
      "Epoch 4694\n",
      "-------------------------------\n",
      "tensor(75.4524)\n",
      "tensor(23.3618)\n",
      "tensor(23.9758)\n",
      "tensor(0.1952)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.983002\n",
      "Epoch 4695\n",
      "-------------------------------\n",
      "tensor(59.9982)\n",
      "tensor(31.9502)\n",
      "tensor(32.4683)\n",
      "tensor(0.2325)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.941101\n",
      "Epoch 4696\n",
      "-------------------------------\n",
      "tensor(40.2590)\n",
      "tensor(22.1519)\n",
      "tensor(18.7609)\n",
      "tensor(0.5991)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.772060\n",
      "Epoch 4697\n",
      "-------------------------------\n",
      "tensor(34.1889)\n",
      "tensor(32.0660)\n",
      "tensor(18.7400)\n",
      "tensor(0.8801)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.736740\n",
      "Epoch 4698\n",
      "-------------------------------\n",
      "tensor(43.6575)\n",
      "tensor(31.0914)\n",
      "tensor(24.0697)\n",
      "tensor(0.2503)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.765770\n",
      "Epoch 4699\n",
      "-------------------------------\n",
      "tensor(42.9579)\n",
      "tensor(25.9128)\n",
      "tensor(17.4510)\n",
      "tensor(0.4493)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.744705\n",
      "Epoch 4700\n",
      "-------------------------------\n",
      "tensor(43.6399)\n",
      "tensor(23.4683)\n",
      "tensor(14.0276)\n",
      "tensor(0.7698)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 47.698006\n",
      "Epoch 4701\n",
      "-------------------------------\n",
      "tensor(41.7018)\n",
      "tensor(21.6364)\n",
      "tensor(12.9149)\n",
      "tensor(0.7911)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.650314\n",
      "Epoch 4702\n",
      "-------------------------------\n",
      "tensor(40.6872)\n",
      "tensor(22.9630)\n",
      "tensor(10.8002)\n",
      "tensor(0.6727)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.653542\n",
      "Epoch 4703\n",
      "-------------------------------\n",
      "tensor(36.3659)\n",
      "tensor(21.7909)\n",
      "tensor(7.8136)\n",
      "tensor(0.3911)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.664982\n",
      "Epoch 4704\n",
      "-------------------------------\n",
      "tensor(34.5651)\n",
      "tensor(20.6226)\n",
      "tensor(8.9039)\n",
      "tensor(0.1437)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.667294\n",
      "Epoch 4705\n",
      "-------------------------------\n",
      "tensor(43.4238)\n",
      "tensor(21.4989)\n",
      "tensor(17.0479)\n",
      "tensor(0.8474)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.649632\n",
      "Epoch 4706\n",
      "-------------------------------\n",
      "tensor(60.9421)\n",
      "tensor(29.8906)\n",
      "tensor(20.0668)\n",
      "tensor(1.2310)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.682304\n",
      "Epoch 4707\n",
      "-------------------------------\n",
      "tensor(44.9214)\n",
      "tensor(29.3039)\n",
      "tensor(21.6825)\n",
      "tensor(0.5367)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.679008\n",
      "Epoch 4708\n",
      "-------------------------------\n",
      "tensor(50.1703)\n",
      "tensor(26.4639)\n",
      "tensor(27.0263)\n",
      "tensor(1.6417)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.668739\n",
      "Epoch 4709\n",
      "-------------------------------\n",
      "tensor(33.3452)\n",
      "tensor(46.0365)\n",
      "tensor(40.0210)\n",
      "tensor(2.5881)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.716270\n",
      "Epoch 4710\n",
      "-------------------------------\n",
      "tensor(48.6348)\n",
      "tensor(23.5555)\n",
      "tensor(23.1903)\n",
      "tensor(0.9014)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 47.644741\n",
      "Epoch 4711\n",
      "-------------------------------\n",
      "tensor(37.6435)\n",
      "tensor(53.1721)\n",
      "tensor(50.0507)\n",
      "tensor(3.1517)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.706772\n",
      "Epoch 4712\n",
      "-------------------------------\n",
      "tensor(47.6897)\n",
      "tensor(23.8047)\n",
      "tensor(18.0469)\n",
      "tensor(1.1111)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.589638\n",
      "Epoch 4713\n",
      "-------------------------------\n",
      "tensor(59.0422)\n",
      "tensor(42.0299)\n",
      "tensor(39.7318)\n",
      "tensor(2.7936)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.621525\n",
      "Epoch 4714\n",
      "-------------------------------\n",
      "tensor(40.3831)\n",
      "tensor(21.1470)\n",
      "tensor(11.8633)\n",
      "tensor(0.1429)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.430267\n",
      "Epoch 4715\n",
      "-------------------------------\n",
      "tensor(64.8532)\n",
      "tensor(44.2815)\n",
      "tensor(36.3370)\n",
      "tensor(2.5843)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.616055\n",
      "Epoch 4716\n",
      "-------------------------------\n",
      "tensor(36.0428)\n",
      "tensor(36.5565)\n",
      "tensor(30.7008)\n",
      "tensor(1.4493)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.493446\n",
      "Epoch 4717\n",
      "-------------------------------\n",
      "tensor(39.5404)\n",
      "tensor(24.1039)\n",
      "tensor(18.4884)\n",
      "tensor(1.2630)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.391556\n",
      "Epoch 4718\n",
      "-------------------------------\n",
      "tensor(44.9231)\n",
      "tensor(42.0663)\n",
      "tensor(33.0725)\n",
      "tensor(2.4360)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.469360\n",
      "Epoch 4719\n",
      "-------------------------------\n",
      "tensor(35.6958)\n",
      "tensor(38.9698)\n",
      "tensor(30.4257)\n",
      "tensor(2.0107)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.435211\n",
      "Epoch 4720\n",
      "-------------------------------\n",
      "tensor(33.1541)\n",
      "tensor(29.7688)\n",
      "tensor(19.8350)\n",
      "tensor(1.0696)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 47.374771\n",
      "Epoch 4721\n",
      "-------------------------------\n",
      "tensor(34.5682)\n",
      "tensor(22.9867)\n",
      "tensor(11.4093)\n",
      "tensor(0.2558)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.327858\n",
      "Epoch 4722\n",
      "-------------------------------\n",
      "tensor(35.5763)\n",
      "tensor(22.6313)\n",
      "tensor(9.8532)\n",
      "tensor(0.3970)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.313564\n",
      "Epoch 4723\n",
      "-------------------------------\n",
      "tensor(35.5136)\n",
      "tensor(28.8280)\n",
      "tensor(16.0918)\n",
      "tensor(1.0305)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.332054\n",
      "Epoch 4724\n",
      "-------------------------------\n",
      "tensor(33.1540)\n",
      "tensor(35.0183)\n",
      "tensor(27.1412)\n",
      "tensor(1.5340)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.336220\n",
      "Epoch 4725\n",
      "-------------------------------\n",
      "tensor(47.2119)\n",
      "tensor(34.8635)\n",
      "tensor(29.5826)\n",
      "tensor(1.2865)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.335331\n",
      "Epoch 4726\n",
      "-------------------------------\n",
      "tensor(57.0830)\n",
      "tensor(22.6233)\n",
      "tensor(12.7732)\n",
      "tensor(0.4362)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.364235\n",
      "Epoch 4727\n",
      "-------------------------------\n",
      "tensor(47.6631)\n",
      "tensor(43.0017)\n",
      "tensor(44.6028)\n",
      "tensor(2.4744)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.396080\n",
      "Epoch 4728\n",
      "-------------------------------\n",
      "tensor(54.6926)\n",
      "tensor(21.5525)\n",
      "tensor(23.4062)\n",
      "tensor(1.0585)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.329720\n",
      "Epoch 4729\n",
      "-------------------------------\n",
      "tensor(86.3279)\n",
      "tensor(33.9045)\n",
      "tensor(65.0927)\n",
      "tensor(2.6437)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.439220\n",
      "Epoch 4730\n",
      "-------------------------------\n",
      "tensor(98.1440)\n",
      "tensor(23.0981)\n",
      "tensor(44.2731)\n",
      "tensor(0.1449)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 47.494545\n",
      "Epoch 4731\n",
      "-------------------------------\n",
      "tensor(38.7675)\n",
      "tensor(24.3093)\n",
      "tensor(19.8905)\n",
      "tensor(0.3252)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.262936\n",
      "Epoch 4732\n",
      "-------------------------------\n",
      "tensor(86.4377)\n",
      "tensor(30.1864)\n",
      "tensor(31.7226)\n",
      "tensor(1.2089)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.496548\n",
      "Epoch 4733\n",
      "-------------------------------\n",
      "tensor(99.0985)\n",
      "tensor(50.3488)\n",
      "tensor(76.2278)\n",
      "tensor(1.7965)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.547707\n",
      "Epoch 4734\n",
      "-------------------------------\n",
      "tensor(50.5274)\n",
      "tensor(50.9306)\n",
      "tensor(59.4737)\n",
      "tensor(2.9248)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.306061\n",
      "Epoch 4735\n",
      "-------------------------------\n",
      "tensor(45.1785)\n",
      "tensor(34.8776)\n",
      "tensor(38.1263)\n",
      "tensor(0.9862)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.223083\n",
      "Epoch 4736\n",
      "-------------------------------\n",
      "tensor(47.4654)\n",
      "tensor(44.9502)\n",
      "tensor(54.9543)\n",
      "tensor(2.7149)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.248482\n",
      "Epoch 4737\n",
      "-------------------------------\n",
      "tensor(63.9773)\n",
      "tensor(22.7178)\n",
      "tensor(41.4740)\n",
      "tensor(1.8994)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.163948\n",
      "Epoch 4738\n",
      "-------------------------------\n",
      "tensor(60.0050)\n",
      "tensor(16.9407)\n",
      "tensor(23.3114)\n",
      "tensor(0.5564)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.168652\n",
      "Epoch 4739\n",
      "-------------------------------\n",
      "tensor(68.0667)\n",
      "tensor(20.6797)\n",
      "tensor(38.3320)\n",
      "tensor(1.5151)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.183125\n",
      "Epoch 4740\n",
      "-------------------------------\n",
      "tensor(52.4409)\n",
      "tensor(19.6038)\n",
      "tensor(24.9237)\n",
      "tensor(1.3149)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 47.091305\n",
      "Epoch 4741\n",
      "-------------------------------\n",
      "tensor(46.0265)\n",
      "tensor(28.2614)\n",
      "tensor(11.7929)\n",
      "tensor(0.7998)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 47.097729\n",
      "Epoch 4742\n",
      "-------------------------------\n",
      "tensor(47.3538)\n",
      "tensor(23.4290)\n",
      "tensor(11.9112)\n",
      "tensor(0.2875)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 47.097610\n",
      "Epoch 4743\n",
      "-------------------------------\n",
      "tensor(46.9959)\n",
      "tensor(19.7650)\n",
      "tensor(16.5199)\n",
      "tensor(0.2643)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.089748\n",
      "Epoch 4744\n",
      "-------------------------------\n",
      "tensor(41.3817)\n",
      "tensor(19.6029)\n",
      "tensor(15.6111)\n",
      "tensor(0.7940)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 47.047684\n",
      "Epoch 4745\n",
      "-------------------------------\n",
      "tensor(52.6206)\n",
      "tensor(25.7582)\n",
      "tensor(16.9639)\n",
      "tensor(1.0552)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.092751\n",
      "Epoch 4746\n",
      "-------------------------------\n",
      "tensor(62.3139)\n",
      "tensor(26.0922)\n",
      "tensor(19.5517)\n",
      "tensor(0.9471)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.119583\n",
      "Epoch 4747\n",
      "-------------------------------\n",
      "tensor(52.3686)\n",
      "tensor(28.1741)\n",
      "tensor(15.8651)\n",
      "tensor(0.4670)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.099041\n",
      "Epoch 4748\n",
      "-------------------------------\n",
      "tensor(75.3278)\n",
      "tensor(26.4889)\n",
      "tensor(25.5064)\n",
      "tensor(0.7280)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.228668\n",
      "Epoch 4749\n",
      "-------------------------------\n",
      "tensor(53.3121)\n",
      "tensor(44.4795)\n",
      "tensor(50.3961)\n",
      "tensor(1.7940)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.225883\n",
      "Epoch 4750\n",
      "-------------------------------\n",
      "tensor(43.0867)\n",
      "tensor(22.6042)\n",
      "tensor(23.1347)\n",
      "tensor(1.5820)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 47.011429\n",
      "Epoch 4751\n",
      "-------------------------------\n",
      "tensor(42.3235)\n",
      "tensor(41.7773)\n",
      "tensor(38.7830)\n",
      "tensor(2.1072)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.087944\n",
      "Epoch 4752\n",
      "-------------------------------\n",
      "tensor(66.4942)\n",
      "tensor(22.6899)\n",
      "tensor(34.4594)\n",
      "tensor(1.7915)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 47.133087\n",
      "Epoch 4753\n",
      "-------------------------------\n",
      "tensor(41.5211)\n",
      "tensor(21.6772)\n",
      "tensor(20.8176)\n",
      "tensor(0.9722)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.960903\n",
      "Epoch 4754\n",
      "-------------------------------\n",
      "tensor(50.5670)\n",
      "tensor(20.6144)\n",
      "tensor(19.5311)\n",
      "tensor(0.8129)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 47.047287\n",
      "Epoch 4755\n",
      "-------------------------------\n",
      "tensor(62.4803)\n",
      "tensor(31.4432)\n",
      "tensor(37.6815)\n",
      "tensor(0.3521)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.114143\n",
      "Epoch 4756\n",
      "-------------------------------\n",
      "tensor(37.2269)\n",
      "tensor(23.1959)\n",
      "tensor(9.2737)\n",
      "tensor(0.4845)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.923637\n",
      "Epoch 4757\n",
      "-------------------------------\n",
      "tensor(50.8479)\n",
      "tensor(19.5532)\n",
      "tensor(18.5264)\n",
      "tensor(0.2306)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.818256\n",
      "Epoch 4758\n",
      "-------------------------------\n",
      "tensor(54.7091)\n",
      "tensor(24.0124)\n",
      "tensor(10.8250)\n",
      "tensor(0.2359)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.932339\n",
      "Epoch 4759\n",
      "-------------------------------\n",
      "tensor(58.0672)\n",
      "tensor(23.0737)\n",
      "tensor(10.9205)\n",
      "tensor(0.2732)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.946548\n",
      "Epoch 4760\n",
      "-------------------------------\n",
      "tensor(53.9834)\n",
      "tensor(23.2260)\n",
      "tensor(7.8143)\n",
      "tensor(0.0547)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 46.904694\n",
      "Epoch 4761\n",
      "-------------------------------\n",
      "tensor(39.0004)\n",
      "tensor(22.0916)\n",
      "tensor(5.1248)\n",
      "tensor(0.1752)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.847500\n",
      "Epoch 4762\n",
      "-------------------------------\n",
      "tensor(38.8784)\n",
      "tensor(23.4991)\n",
      "tensor(7.2386)\n",
      "tensor(0.3536)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.809654\n",
      "Epoch 4763\n",
      "-------------------------------\n",
      "tensor(35.5027)\n",
      "tensor(23.6499)\n",
      "tensor(12.0894)\n",
      "tensor(0.4964)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.750744\n",
      "Epoch 4764\n",
      "-------------------------------\n",
      "tensor(44.3698)\n",
      "tensor(19.3976)\n",
      "tensor(15.9851)\n",
      "tensor(0.5456)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.782650\n",
      "Epoch 4765\n",
      "-------------------------------\n",
      "tensor(39.9200)\n",
      "tensor(23.9036)\n",
      "tensor(7.4871)\n",
      "tensor(0.1945)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.862823\n",
      "Epoch 4766\n",
      "-------------------------------\n",
      "tensor(43.4679)\n",
      "tensor(35.2334)\n",
      "tensor(24.6304)\n",
      "tensor(0.5890)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.964928\n",
      "Epoch 4767\n",
      "-------------------------------\n",
      "tensor(43.6146)\n",
      "tensor(36.7829)\n",
      "tensor(28.0036)\n",
      "tensor(0.7056)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.978329\n",
      "Epoch 4768\n",
      "-------------------------------\n",
      "tensor(59.1113)\n",
      "tensor(24.6408)\n",
      "tensor(14.3329)\n",
      "tensor(0.4651)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.818874\n",
      "Epoch 4769\n",
      "-------------------------------\n",
      "tensor(43.2220)\n",
      "tensor(25.6186)\n",
      "tensor(7.1366)\n",
      "tensor(0.5898)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.755760\n",
      "Epoch 4770\n",
      "-------------------------------\n",
      "tensor(47.3607)\n",
      "tensor(25.2459)\n",
      "tensor(7.2549)\n",
      "tensor(0.1849)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 46.876690\n",
      "Epoch 4771\n",
      "-------------------------------\n",
      "tensor(52.0426)\n",
      "tensor(25.8305)\n",
      "tensor(17.4110)\n",
      "tensor(0.0113)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.772728\n",
      "Epoch 4772\n",
      "-------------------------------\n",
      "tensor(29.6821)\n",
      "tensor(27.3116)\n",
      "tensor(12.3579)\n",
      "tensor(0.3837)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.683929\n",
      "Epoch 4773\n",
      "-------------------------------\n",
      "tensor(39.9345)\n",
      "tensor(26.2929)\n",
      "tensor(17.5141)\n",
      "tensor(0.2868)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.745743\n",
      "Epoch 4774\n",
      "-------------------------------\n",
      "tensor(43.5000)\n",
      "tensor(19.7795)\n",
      "tensor(15.0391)\n",
      "tensor(0.7030)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.615818\n",
      "Epoch 4775\n",
      "-------------------------------\n",
      "tensor(47.1634)\n",
      "tensor(18.3686)\n",
      "tensor(11.4005)\n",
      "tensor(0.2972)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.694977\n",
      "Epoch 4776\n",
      "-------------------------------\n",
      "tensor(41.5022)\n",
      "tensor(22.8650)\n",
      "tensor(12.1575)\n",
      "tensor(0.1770)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.688126\n",
      "Epoch 4777\n",
      "-------------------------------\n",
      "tensor(47.6745)\n",
      "tensor(25.0514)\n",
      "tensor(16.4767)\n",
      "tensor(0.0994)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.641808\n",
      "Epoch 4778\n",
      "-------------------------------\n",
      "tensor(49.2673)\n",
      "tensor(22.2284)\n",
      "tensor(6.1559)\n",
      "tensor(0.0935)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.523514\n",
      "Epoch 4779\n",
      "-------------------------------\n",
      "tensor(40.2988)\n",
      "tensor(24.6706)\n",
      "tensor(5.8862)\n",
      "tensor(0.0837)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.498894\n",
      "Epoch 4780\n",
      "-------------------------------\n",
      "tensor(38.1223)\n",
      "tensor(25.9196)\n",
      "tensor(11.6089)\n",
      "tensor(0.0842)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 46.519470\n",
      "Epoch 4781\n",
      "-------------------------------\n",
      "tensor(36.0686)\n",
      "tensor(25.7439)\n",
      "tensor(13.4907)\n",
      "tensor(0.0988)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.523895\n",
      "Epoch 4782\n",
      "-------------------------------\n",
      "tensor(33.5102)\n",
      "tensor(24.7972)\n",
      "tensor(12.6135)\n",
      "tensor(0.1286)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.508900\n",
      "Epoch 4783\n",
      "-------------------------------\n",
      "tensor(31.7247)\n",
      "tensor(23.3084)\n",
      "tensor(9.1176)\n",
      "tensor(0.1767)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.470749\n",
      "Epoch 4784\n",
      "-------------------------------\n",
      "tensor(38.4013)\n",
      "tensor(19.3987)\n",
      "tensor(4.4264)\n",
      "tensor(0.2505)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.435455\n",
      "Epoch 4785\n",
      "-------------------------------\n",
      "tensor(48.1410)\n",
      "tensor(25.1493)\n",
      "tensor(14.6295)\n",
      "tensor(0.3083)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.488605\n",
      "Epoch 4786\n",
      "-------------------------------\n",
      "tensor(42.1737)\n",
      "tensor(23.9557)\n",
      "tensor(18.7500)\n",
      "tensor(0.0405)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.502663\n",
      "Epoch 4787\n",
      "-------------------------------\n",
      "tensor(51.2731)\n",
      "tensor(19.6689)\n",
      "tensor(10.7049)\n",
      "tensor(0.5760)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.440666\n",
      "Epoch 4788\n",
      "-------------------------------\n",
      "tensor(32.9618)\n",
      "tensor(27.9287)\n",
      "tensor(14.5895)\n",
      "tensor(0.2865)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.509911\n",
      "Epoch 4789\n",
      "-------------------------------\n",
      "tensor(46.3287)\n",
      "tensor(26.1521)\n",
      "tensor(14.8670)\n",
      "tensor(0.8927)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.554626\n",
      "Epoch 4790\n",
      "-------------------------------\n",
      "tensor(36.8224)\n",
      "tensor(23.1367)\n",
      "tensor(10.6001)\n",
      "tensor(0.7009)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 46.473774\n",
      "Epoch 4791\n",
      "-------------------------------\n",
      "tensor(52.6778)\n",
      "tensor(18.6298)\n",
      "tensor(14.1142)\n",
      "tensor(0.5476)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.450932\n",
      "Epoch 4792\n",
      "-------------------------------\n",
      "tensor(46.5825)\n",
      "tensor(26.5399)\n",
      "tensor(28.3062)\n",
      "tensor(0.1185)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.466171\n",
      "Epoch 4793\n",
      "-------------------------------\n",
      "tensor(45.6589)\n",
      "tensor(16.1797)\n",
      "tensor(12.5075)\n",
      "tensor(0.3342)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.323158\n",
      "Epoch 4794\n",
      "-------------------------------\n",
      "tensor(45.6957)\n",
      "tensor(24.3201)\n",
      "tensor(10.6740)\n",
      "tensor(0.4865)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.467945\n",
      "Epoch 4795\n",
      "-------------------------------\n",
      "tensor(37.5664)\n",
      "tensor(26.5765)\n",
      "tensor(11.6463)\n",
      "tensor(0.1394)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.468784\n",
      "Epoch 4796\n",
      "-------------------------------\n",
      "tensor(40.2699)\n",
      "tensor(27.6614)\n",
      "tensor(10.7335)\n",
      "tensor(0.2945)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.403641\n",
      "Epoch 4797\n",
      "-------------------------------\n",
      "tensor(41.3401)\n",
      "tensor(23.5329)\n",
      "tensor(5.7491)\n",
      "tensor(0.1007)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.260841\n",
      "Epoch 4798\n",
      "-------------------------------\n",
      "tensor(50.6915)\n",
      "tensor(24.8722)\n",
      "tensor(5.4856)\n",
      "tensor(0.3605)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.344604\n",
      "Epoch 4799\n",
      "-------------------------------\n",
      "tensor(36.5033)\n",
      "tensor(26.5789)\n",
      "tensor(9.7453)\n",
      "tensor(0.6602)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.351372\n",
      "Epoch 4800\n",
      "-------------------------------\n",
      "tensor(36.1115)\n",
      "tensor(27.4593)\n",
      "tensor(13.3022)\n",
      "tensor(0.6819)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 46.340866\n",
      "Epoch 4801\n",
      "-------------------------------\n",
      "tensor(35.1603)\n",
      "tensor(26.0634)\n",
      "tensor(13.6318)\n",
      "tensor(0.5625)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.310421\n",
      "Epoch 4802\n",
      "-------------------------------\n",
      "tensor(34.6871)\n",
      "tensor(23.7899)\n",
      "tensor(11.9999)\n",
      "tensor(0.3742)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.268932\n",
      "Epoch 4803\n",
      "-------------------------------\n",
      "tensor(39.0322)\n",
      "tensor(21.0837)\n",
      "tensor(8.5185)\n",
      "tensor(0.0752)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.213196\n",
      "Epoch 4804\n",
      "-------------------------------\n",
      "tensor(33.2722)\n",
      "tensor(24.3617)\n",
      "tensor(6.0005)\n",
      "tensor(0.3323)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.201046\n",
      "Epoch 4805\n",
      "-------------------------------\n",
      "tensor(29.7800)\n",
      "tensor(31.2068)\n",
      "tensor(15.6912)\n",
      "tensor(0.7193)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.275936\n",
      "Epoch 4806\n",
      "-------------------------------\n",
      "tensor(38.7991)\n",
      "tensor(33.9680)\n",
      "tensor(22.6622)\n",
      "tensor(0.6243)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.357777\n",
      "Epoch 4807\n",
      "-------------------------------\n",
      "tensor(53.8440)\n",
      "tensor(29.0956)\n",
      "tensor(11.4514)\n",
      "tensor(0.3334)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.400768\n",
      "Epoch 4808\n",
      "-------------------------------\n",
      "tensor(49.5751)\n",
      "tensor(24.0492)\n",
      "tensor(15.6535)\n",
      "tensor(1.0155)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.278324\n",
      "Epoch 4809\n",
      "-------------------------------\n",
      "tensor(60.6323)\n",
      "tensor(23.9235)\n",
      "tensor(12.8883)\n",
      "tensor(0.2811)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.281418\n",
      "Epoch 4810\n",
      "-------------------------------\n",
      "tensor(43.1683)\n",
      "tensor(26.7810)\n",
      "tensor(20.8607)\n",
      "tensor(0.0468)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 46.363808\n",
      "Epoch 4811\n",
      "-------------------------------\n",
      "tensor(47.8634)\n",
      "tensor(21.7615)\n",
      "tensor(13.5904)\n",
      "tensor(0.5733)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.335793\n",
      "Epoch 4812\n",
      "-------------------------------\n",
      "tensor(45.8094)\n",
      "tensor(14.4410)\n",
      "tensor(15.4469)\n",
      "tensor(0.1537)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.175114\n",
      "Epoch 4813\n",
      "-------------------------------\n",
      "tensor(68.5472)\n",
      "tensor(21.8582)\n",
      "tensor(30.1062)\n",
      "tensor(1.1036)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.516335\n",
      "Epoch 4814\n",
      "-------------------------------\n",
      "tensor(42.4872)\n",
      "tensor(41.7664)\n",
      "tensor(35.7400)\n",
      "tensor(1.1351)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.523956\n",
      "Epoch 4815\n",
      "-------------------------------\n",
      "tensor(32.2212)\n",
      "tensor(28.7849)\n",
      "tensor(11.0792)\n",
      "tensor(0.6222)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.334846\n",
      "Epoch 4816\n",
      "-------------------------------\n",
      "tensor(55.8765)\n",
      "tensor(22.2120)\n",
      "tensor(19.1474)\n",
      "tensor(0.6090)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.241108\n",
      "Epoch 4817\n",
      "-------------------------------\n",
      "tensor(60.3145)\n",
      "tensor(31.9891)\n",
      "tensor(10.3171)\n",
      "tensor(0.8527)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.215485\n",
      "Epoch 4818\n",
      "-------------------------------\n",
      "tensor(54.3803)\n",
      "tensor(31.1917)\n",
      "tensor(11.1485)\n",
      "tensor(0.9425)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.214199\n",
      "Epoch 4819\n",
      "-------------------------------\n",
      "tensor(38.3064)\n",
      "tensor(30.1057)\n",
      "tensor(12.5735)\n",
      "tensor(0.8599)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.136673\n",
      "Epoch 4820\n",
      "-------------------------------\n",
      "tensor(32.9183)\n",
      "tensor(27.6437)\n",
      "tensor(16.5450)\n",
      "tensor(0.6258)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 46.066139\n",
      "Epoch 4821\n",
      "-------------------------------\n",
      "tensor(32.3909)\n",
      "tensor(24.5175)\n",
      "tensor(16.7396)\n",
      "tensor(0.3552)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 46.008804\n",
      "Epoch 4822\n",
      "-------------------------------\n",
      "tensor(41.5827)\n",
      "tensor(19.4856)\n",
      "tensor(14.2119)\n",
      "tensor(0.1035)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 46.028481\n",
      "Epoch 4823\n",
      "-------------------------------\n",
      "tensor(34.8825)\n",
      "tensor(22.5053)\n",
      "tensor(7.4154)\n",
      "tensor(0.2339)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.033127\n",
      "Epoch 4824\n",
      "-------------------------------\n",
      "tensor(27.8858)\n",
      "tensor(28.5205)\n",
      "tensor(15.6526)\n",
      "tensor(0.6359)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.032295\n",
      "Epoch 4825\n",
      "-------------------------------\n",
      "tensor(41.7758)\n",
      "tensor(33.6605)\n",
      "tensor(29.6408)\n",
      "tensor(0.7228)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.022331\n",
      "Epoch 4826\n",
      "-------------------------------\n",
      "tensor(43.7588)\n",
      "tensor(28.0637)\n",
      "tensor(19.1213)\n",
      "tensor(0.1420)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.928295\n",
      "Epoch 4827\n",
      "-------------------------------\n",
      "tensor(49.6417)\n",
      "tensor(30.5639)\n",
      "tensor(24.8163)\n",
      "tensor(1.4172)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 46.053223\n",
      "Epoch 4828\n",
      "-------------------------------\n",
      "tensor(52.5252)\n",
      "tensor(26.5918)\n",
      "tensor(22.8442)\n",
      "tensor(0.7436)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.144562\n",
      "Epoch 4829\n",
      "-------------------------------\n",
      "tensor(56.3061)\n",
      "tensor(25.1133)\n",
      "tensor(33.0661)\n",
      "tensor(1.7727)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.147785\n",
      "Epoch 4830\n",
      "-------------------------------\n",
      "tensor(36.8338)\n",
      "tensor(19.2445)\n",
      "tensor(10.6586)\n",
      "tensor(0.7864)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 46.042706\n",
      "Epoch 4831\n",
      "-------------------------------\n",
      "tensor(66.4913)\n",
      "tensor(22.0874)\n",
      "tensor(28.5539)\n",
      "tensor(1.0275)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.100048\n",
      "Epoch 4832\n",
      "-------------------------------\n",
      "tensor(79.1038)\n",
      "tensor(39.2038)\n",
      "tensor(56.5472)\n",
      "tensor(0.5776)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 46.129242\n",
      "Epoch 4833\n",
      "-------------------------------\n",
      "tensor(70.6511)\n",
      "tensor(23.5517)\n",
      "tensor(42.4964)\n",
      "tensor(1.3479)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.972317\n",
      "Epoch 4834\n",
      "-------------------------------\n",
      "tensor(59.5935)\n",
      "tensor(23.9250)\n",
      "tensor(15.5021)\n",
      "tensor(0.2292)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 46.070793\n",
      "Epoch 4835\n",
      "-------------------------------\n",
      "tensor(59.0844)\n",
      "tensor(25.0308)\n",
      "tensor(13.0134)\n",
      "tensor(0.0896)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.053429\n",
      "Epoch 4836\n",
      "-------------------------------\n",
      "tensor(47.8824)\n",
      "tensor(32.4861)\n",
      "tensor(26.2757)\n",
      "tensor(0.9223)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.893387\n",
      "Epoch 4837\n",
      "-------------------------------\n",
      "tensor(45.0398)\n",
      "tensor(21.1622)\n",
      "tensor(12.8749)\n",
      "tensor(0.5735)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.849892\n",
      "Epoch 4838\n",
      "-------------------------------\n",
      "tensor(38.6659)\n",
      "tensor(30.2622)\n",
      "tensor(20.6187)\n",
      "tensor(0.2312)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.888744\n",
      "Epoch 4839\n",
      "-------------------------------\n",
      "tensor(39.4925)\n",
      "tensor(32.8135)\n",
      "tensor(25.9750)\n",
      "tensor(0.4580)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.879990\n",
      "Epoch 4840\n",
      "-------------------------------\n",
      "tensor(34.4730)\n",
      "tensor(29.6250)\n",
      "tensor(17.3504)\n",
      "tensor(0.3074)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 45.820740\n",
      "Epoch 4841\n",
      "-------------------------------\n",
      "tensor(33.4083)\n",
      "tensor(25.9653)\n",
      "tensor(8.4712)\n",
      "tensor(0.1074)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.764538\n",
      "Epoch 4842\n",
      "-------------------------------\n",
      "tensor(35.2731)\n",
      "tensor(22.9879)\n",
      "tensor(4.6568)\n",
      "tensor(0.0615)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.712288\n",
      "Epoch 4843\n",
      "-------------------------------\n",
      "tensor(28.8423)\n",
      "tensor(18.2073)\n",
      "tensor(8.1939)\n",
      "tensor(0.2496)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.740223\n",
      "Epoch 4844\n",
      "-------------------------------\n",
      "tensor(29.6673)\n",
      "tensor(21.7795)\n",
      "tensor(14.2358)\n",
      "tensor(0.3786)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.774929\n",
      "Epoch 4845\n",
      "-------------------------------\n",
      "tensor(36.0498)\n",
      "tensor(23.7150)\n",
      "tensor(15.3762)\n",
      "tensor(0.2539)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.795849\n",
      "Epoch 4846\n",
      "-------------------------------\n",
      "tensor(57.7059)\n",
      "tensor(23.1852)\n",
      "tensor(9.1567)\n",
      "tensor(0.2312)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.855888\n",
      "Epoch 4847\n",
      "-------------------------------\n",
      "tensor(62.4578)\n",
      "tensor(20.6477)\n",
      "tensor(13.4774)\n",
      "tensor(0.3727)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.832127\n",
      "Epoch 4848\n",
      "-------------------------------\n",
      "tensor(50.8408)\n",
      "tensor(23.8863)\n",
      "tensor(9.6977)\n",
      "tensor(0.5810)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.685394\n",
      "Epoch 4849\n",
      "-------------------------------\n",
      "tensor(43.8409)\n",
      "tensor(26.1024)\n",
      "tensor(16.5112)\n",
      "tensor(0.7329)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.732979\n",
      "Epoch 4850\n",
      "-------------------------------\n",
      "tensor(34.3074)\n",
      "tensor(27.5582)\n",
      "tensor(13.8060)\n",
      "tensor(0.1174)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 45.660065\n",
      "Epoch 4851\n",
      "-------------------------------\n",
      "tensor(35.3326)\n",
      "tensor(20.3520)\n",
      "tensor(11.3148)\n",
      "tensor(0.2670)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.692883\n",
      "Epoch 4852\n",
      "-------------------------------\n",
      "tensor(45.0786)\n",
      "tensor(23.3435)\n",
      "tensor(17.2137)\n",
      "tensor(0.0871)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.735027\n",
      "Epoch 4853\n",
      "-------------------------------\n",
      "tensor(49.2308)\n",
      "tensor(15.8032)\n",
      "tensor(14.5266)\n",
      "tensor(0.0900)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.644051\n",
      "Epoch 4854\n",
      "-------------------------------\n",
      "tensor(53.4937)\n",
      "tensor(20.1470)\n",
      "tensor(22.1584)\n",
      "tensor(0.8873)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.635468\n",
      "Epoch 4855\n",
      "-------------------------------\n",
      "tensor(38.0612)\n",
      "tensor(29.8761)\n",
      "tensor(18.4873)\n",
      "tensor(0.3380)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.619205\n",
      "Epoch 4856\n",
      "-------------------------------\n",
      "tensor(32.7271)\n",
      "tensor(27.9989)\n",
      "tensor(18.7783)\n",
      "tensor(0.4334)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.510891\n",
      "Epoch 4857\n",
      "-------------------------------\n",
      "tensor(47.7472)\n",
      "tensor(24.2275)\n",
      "tensor(7.9763)\n",
      "tensor(0.3968)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.488327\n",
      "Epoch 4858\n",
      "-------------------------------\n",
      "tensor(42.5332)\n",
      "tensor(28.1276)\n",
      "tensor(20.0553)\n",
      "tensor(0.7539)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.504478\n",
      "Epoch 4859\n",
      "-------------------------------\n",
      "tensor(38.9320)\n",
      "tensor(24.2764)\n",
      "tensor(13.7912)\n",
      "tensor(0.4947)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.483150\n",
      "Epoch 4860\n",
      "-------------------------------\n",
      "tensor(39.2527)\n",
      "tensor(20.0005)\n",
      "tensor(5.5336)\n",
      "tensor(0.1503)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 45.441784\n",
      "Epoch 4861\n",
      "-------------------------------\n",
      "tensor(35.1859)\n",
      "tensor(22.8298)\n",
      "tensor(3.8245)\n",
      "tensor(0.0654)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.426064\n",
      "Epoch 4862\n",
      "-------------------------------\n",
      "tensor(33.4448)\n",
      "tensor(24.4084)\n",
      "tensor(7.0955)\n",
      "tensor(0.1853)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.428551\n",
      "Epoch 4863\n",
      "-------------------------------\n",
      "tensor(28.3236)\n",
      "tensor(25.0806)\n",
      "tensor(11.4555)\n",
      "tensor(0.2410)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.426529\n",
      "Epoch 4864\n",
      "-------------------------------\n",
      "tensor(43.5186)\n",
      "tensor(13.9022)\n",
      "tensor(13.6288)\n",
      "tensor(0.1431)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.403481\n",
      "Epoch 4865\n",
      "-------------------------------\n",
      "tensor(32.7886)\n",
      "tensor(21.2874)\n",
      "tensor(4.9172)\n",
      "tensor(0.2885)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.379951\n",
      "Epoch 4866\n",
      "-------------------------------\n",
      "tensor(47.4498)\n",
      "tensor(25.3730)\n",
      "tensor(14.1115)\n",
      "tensor(0.6732)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.443035\n",
      "Epoch 4867\n",
      "-------------------------------\n",
      "tensor(41.1316)\n",
      "tensor(21.3207)\n",
      "tensor(10.0546)\n",
      "tensor(0.3425)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.429222\n",
      "Epoch 4868\n",
      "-------------------------------\n",
      "tensor(35.9133)\n",
      "tensor(28.5731)\n",
      "tensor(10.6751)\n",
      "tensor(0.5769)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.375652\n",
      "Epoch 4869\n",
      "-------------------------------\n",
      "tensor(39.2303)\n",
      "tensor(31.6085)\n",
      "tensor(22.7815)\n",
      "tensor(0.6335)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.394272\n",
      "Epoch 4870\n",
      "-------------------------------\n",
      "tensor(33.2339)\n",
      "tensor(28.3205)\n",
      "tensor(18.3136)\n",
      "tensor(1.2020)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 45.448269\n",
      "Epoch 4871\n",
      "-------------------------------\n",
      "tensor(37.5015)\n",
      "tensor(28.1202)\n",
      "tensor(26.3318)\n",
      "tensor(0.8025)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.425911\n",
      "Epoch 4872\n",
      "-------------------------------\n",
      "tensor(59.8811)\n",
      "tensor(22.0651)\n",
      "tensor(36.2910)\n",
      "tensor(1.7381)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.322586\n",
      "Epoch 4873\n",
      "-------------------------------\n",
      "tensor(64.0925)\n",
      "tensor(22.8981)\n",
      "tensor(21.8022)\n",
      "tensor(0.3417)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.521797\n",
      "Epoch 4874\n",
      "-------------------------------\n",
      "tensor(50.5357)\n",
      "tensor(30.4051)\n",
      "tensor(23.4049)\n",
      "tensor(0.3788)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.538387\n",
      "Epoch 4875\n",
      "-------------------------------\n",
      "tensor(38.6545)\n",
      "tensor(24.9955)\n",
      "tensor(17.5942)\n",
      "tensor(0.4522)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.371868\n",
      "Epoch 4876\n",
      "-------------------------------\n",
      "tensor(46.2314)\n",
      "tensor(19.5901)\n",
      "tensor(16.5203)\n",
      "tensor(0.4799)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.218536\n",
      "Epoch 4877\n",
      "-------------------------------\n",
      "tensor(48.6610)\n",
      "tensor(24.2839)\n",
      "tensor(10.2758)\n",
      "tensor(0.0158)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.376854\n",
      "Epoch 4878\n",
      "-------------------------------\n",
      "tensor(50.8171)\n",
      "tensor(23.2990)\n",
      "tensor(10.0137)\n",
      "tensor(0.0743)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.435734\n",
      "Epoch 4879\n",
      "-------------------------------\n",
      "tensor(48.2126)\n",
      "tensor(24.3138)\n",
      "tensor(7.9008)\n",
      "tensor(0.1400)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.393162\n",
      "Epoch 4880\n",
      "-------------------------------\n",
      "tensor(36.6186)\n",
      "tensor(23.2806)\n",
      "tensor(8.9890)\n",
      "tensor(0.3177)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 45.331360\n",
      "Epoch 4881\n",
      "-------------------------------\n",
      "tensor(30.2838)\n",
      "tensor(21.9984)\n",
      "tensor(10.1823)\n",
      "tensor(0.3769)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.286434\n",
      "Epoch 4882\n",
      "-------------------------------\n",
      "tensor(28.7683)\n",
      "tensor(20.5611)\n",
      "tensor(10.2112)\n",
      "tensor(0.3520)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.243675\n",
      "Epoch 4883\n",
      "-------------------------------\n",
      "tensor(27.5928)\n",
      "tensor(18.1521)\n",
      "tensor(8.8561)\n",
      "tensor(0.2358)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.187874\n",
      "Epoch 4884\n",
      "-------------------------------\n",
      "tensor(44.9277)\n",
      "tensor(24.5827)\n",
      "tensor(5.9184)\n",
      "tensor(0.0222)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.178177\n",
      "Epoch 4885\n",
      "-------------------------------\n",
      "tensor(42.3125)\n",
      "tensor(31.8463)\n",
      "tensor(13.2227)\n",
      "tensor(0.4361)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.277962\n",
      "Epoch 4886\n",
      "-------------------------------\n",
      "tensor(47.1746)\n",
      "tensor(37.9896)\n",
      "tensor(28.1079)\n",
      "tensor(0.6737)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.391003\n",
      "Epoch 4887\n",
      "-------------------------------\n",
      "tensor(48.0004)\n",
      "tensor(31.3275)\n",
      "tensor(16.0329)\n",
      "tensor(0.0024)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.368256\n",
      "Epoch 4888\n",
      "-------------------------------\n",
      "tensor(57.4676)\n",
      "tensor(23.4220)\n",
      "tensor(17.7651)\n",
      "tensor(0.8516)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.179554\n",
      "Epoch 4889\n",
      "-------------------------------\n",
      "tensor(52.9579)\n",
      "tensor(20.4538)\n",
      "tensor(13.2312)\n",
      "tensor(0.1478)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.266155\n",
      "Epoch 4890\n",
      "-------------------------------\n",
      "tensor(51.7998)\n",
      "tensor(30.5377)\n",
      "tensor(21.2809)\n",
      "tensor(0.3003)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 45.433117\n",
      "Epoch 4891\n",
      "-------------------------------\n",
      "tensor(42.4433)\n",
      "tensor(23.7089)\n",
      "tensor(12.1186)\n",
      "tensor(0.0376)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.340057\n",
      "Epoch 4892\n",
      "-------------------------------\n",
      "tensor(40.1790)\n",
      "tensor(14.0332)\n",
      "tensor(10.8896)\n",
      "tensor(0.0778)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.126793\n",
      "Epoch 4893\n",
      "-------------------------------\n",
      "tensor(63.6543)\n",
      "tensor(23.6151)\n",
      "tensor(22.6089)\n",
      "tensor(0.6446)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.423473\n",
      "Epoch 4894\n",
      "-------------------------------\n",
      "tensor(43.3843)\n",
      "tensor(43.4323)\n",
      "tensor(35.7074)\n",
      "tensor(1.1394)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.450447\n",
      "Epoch 4895\n",
      "-------------------------------\n",
      "tensor(42.9138)\n",
      "tensor(29.2686)\n",
      "tensor(7.6345)\n",
      "tensor(0.3857)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.269012\n",
      "Epoch 4896\n",
      "-------------------------------\n",
      "tensor(53.3676)\n",
      "tensor(22.6248)\n",
      "tensor(14.0438)\n",
      "tensor(0.6199)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.122749\n",
      "Epoch 4897\n",
      "-------------------------------\n",
      "tensor(57.9598)\n",
      "tensor(28.1210)\n",
      "tensor(10.9268)\n",
      "tensor(0.7625)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.192142\n",
      "Epoch 4898\n",
      "-------------------------------\n",
      "tensor(54.0299)\n",
      "tensor(30.3484)\n",
      "tensor(9.6799)\n",
      "tensor(0.8382)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 45.210686\n",
      "Epoch 4899\n",
      "-------------------------------\n",
      "tensor(38.3955)\n",
      "tensor(29.2477)\n",
      "tensor(13.8241)\n",
      "tensor(0.7609)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.135124\n",
      "Epoch 4900\n",
      "-------------------------------\n",
      "tensor(32.3072)\n",
      "tensor(26.3529)\n",
      "tensor(16.7831)\n",
      "tensor(0.5269)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 45.084118\n",
      "Epoch 4901\n",
      "-------------------------------\n",
      "tensor(31.8463)\n",
      "tensor(23.2576)\n",
      "tensor(15.6867)\n",
      "tensor(0.2709)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 45.037910\n",
      "Epoch 4902\n",
      "-------------------------------\n",
      "tensor(30.8291)\n",
      "tensor(20.0874)\n",
      "tensor(12.5817)\n",
      "tensor(0.0185)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.990952\n",
      "Epoch 4903\n",
      "-------------------------------\n",
      "tensor(36.2609)\n",
      "tensor(22.4612)\n",
      "tensor(8.3694)\n",
      "tensor(0.2801)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.936993\n",
      "Epoch 4904\n",
      "-------------------------------\n",
      "tensor(28.9280)\n",
      "tensor(27.9449)\n",
      "tensor(11.6990)\n",
      "tensor(0.5685)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.969090\n",
      "Epoch 4905\n",
      "-------------------------------\n",
      "tensor(40.2515)\n",
      "tensor(33.1569)\n",
      "tensor(24.6891)\n",
      "tensor(0.5973)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.024719\n",
      "Epoch 4906\n",
      "-------------------------------\n",
      "tensor(48.4948)\n",
      "tensor(30.8647)\n",
      "tensor(21.8323)\n",
      "tensor(0.1047)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.036411\n",
      "Epoch 4907\n",
      "-------------------------------\n",
      "tensor(55.5082)\n",
      "tensor(24.8175)\n",
      "tensor(19.8544)\n",
      "tensor(1.1621)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.940613\n",
      "Epoch 4908\n",
      "-------------------------------\n",
      "tensor(42.6414)\n",
      "tensor(22.7463)\n",
      "tensor(11.1139)\n",
      "tensor(0.7371)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.043678\n",
      "Epoch 4909\n",
      "-------------------------------\n",
      "tensor(48.1865)\n",
      "tensor(20.1025)\n",
      "tensor(13.6556)\n",
      "tensor(0.8146)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.142670\n",
      "Epoch 4910\n",
      "-------------------------------\n",
      "tensor(42.5283)\n",
      "tensor(20.8684)\n",
      "tensor(15.9116)\n",
      "tensor(0.8176)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 45.113583\n",
      "Epoch 4911\n",
      "-------------------------------\n",
      "tensor(32.2131)\n",
      "tensor(17.9757)\n",
      "tensor(7.3738)\n",
      "tensor(0.3862)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.983589\n",
      "Epoch 4912\n",
      "-------------------------------\n",
      "tensor(45.6794)\n",
      "tensor(23.5181)\n",
      "tensor(13.5779)\n",
      "tensor(0.7005)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.892990\n",
      "Epoch 4913\n",
      "-------------------------------\n",
      "tensor(39.9999)\n",
      "tensor(32.0590)\n",
      "tensor(24.8846)\n",
      "tensor(0.4692)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.937160\n",
      "Epoch 4914\n",
      "-------------------------------\n",
      "tensor(51.8638)\n",
      "tensor(21.9824)\n",
      "tensor(11.8922)\n",
      "tensor(0.1255)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.819038\n",
      "Epoch 4915\n",
      "-------------------------------\n",
      "tensor(44.7701)\n",
      "tensor(22.4685)\n",
      "tensor(7.4512)\n",
      "tensor(0.2888)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.894184\n",
      "Epoch 4916\n",
      "-------------------------------\n",
      "tensor(42.2724)\n",
      "tensor(28.5329)\n",
      "tensor(13.0767)\n",
      "tensor(0.6959)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.908554\n",
      "Epoch 4917\n",
      "-------------------------------\n",
      "tensor(49.4873)\n",
      "tensor(26.7376)\n",
      "tensor(15.1328)\n",
      "tensor(0.4879)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.823906\n",
      "Epoch 4918\n",
      "-------------------------------\n",
      "tensor(35.7313)\n",
      "tensor(17.0441)\n",
      "tensor(4.3999)\n",
      "tensor(0.0381)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.720795\n",
      "Epoch 4919\n",
      "-------------------------------\n",
      "tensor(35.5050)\n",
      "tensor(26.3146)\n",
      "tensor(6.2208)\n",
      "tensor(0.2721)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.791809\n",
      "Epoch 4920\n",
      "-------------------------------\n",
      "tensor(32.3390)\n",
      "tensor(27.8227)\n",
      "tensor(9.8867)\n",
      "tensor(0.3131)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 44.817696\n",
      "Epoch 4921\n",
      "-------------------------------\n",
      "tensor(32.5910)\n",
      "tensor(27.7985)\n",
      "tensor(10.9260)\n",
      "tensor(0.2545)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.812107\n",
      "Epoch 4922\n",
      "-------------------------------\n",
      "tensor(32.6768)\n",
      "tensor(26.8220)\n",
      "tensor(10.2625)\n",
      "tensor(0.1475)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.789059\n",
      "Epoch 4923\n",
      "-------------------------------\n",
      "tensor(32.4036)\n",
      "tensor(24.8454)\n",
      "tensor(8.1449)\n",
      "tensor(0.0258)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.741779\n",
      "Epoch 4924\n",
      "-------------------------------\n",
      "tensor(32.6257)\n",
      "tensor(15.8021)\n",
      "tensor(5.7114)\n",
      "tensor(0.3055)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.687542\n",
      "Epoch 4925\n",
      "-------------------------------\n",
      "tensor(40.3235)\n",
      "tensor(23.9412)\n",
      "tensor(12.3518)\n",
      "tensor(0.6396)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.743324\n",
      "Epoch 4926\n",
      "-------------------------------\n",
      "tensor(52.5855)\n",
      "tensor(28.0349)\n",
      "tensor(18.0927)\n",
      "tensor(0.5583)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.786495\n",
      "Epoch 4927\n",
      "-------------------------------\n",
      "tensor(47.9935)\n",
      "tensor(19.6901)\n",
      "tensor(7.0697)\n",
      "tensor(0.2561)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.765205\n",
      "Epoch 4928\n",
      "-------------------------------\n",
      "tensor(43.0016)\n",
      "tensor(14.9049)\n",
      "tensor(14.4000)\n",
      "tensor(0.6010)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.679920\n",
      "Epoch 4929\n",
      "-------------------------------\n",
      "tensor(62.8611)\n",
      "tensor(24.1151)\n",
      "tensor(18.8485)\n",
      "tensor(0.4790)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.869583\n",
      "Epoch 4930\n",
      "-------------------------------\n",
      "tensor(46.9125)\n",
      "tensor(33.8358)\n",
      "tensor(28.3027)\n",
      "tensor(0.2350)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.897701\n",
      "Epoch 4931\n",
      "-------------------------------\n",
      "tensor(36.7323)\n",
      "tensor(23.3871)\n",
      "tensor(4.8787)\n",
      "tensor(0.2390)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.723358\n",
      "Epoch 4932\n",
      "-------------------------------\n",
      "tensor(44.6507)\n",
      "tensor(25.4086)\n",
      "tensor(13.7233)\n",
      "tensor(0.3071)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.684963\n",
      "Epoch 4933\n",
      "-------------------------------\n",
      "tensor(58.9598)\n",
      "tensor(25.7614)\n",
      "tensor(9.2511)\n",
      "tensor(0.0920)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.717545\n",
      "Epoch 4934\n",
      "-------------------------------\n",
      "tensor(34.2364)\n",
      "tensor(19.8317)\n",
      "tensor(4.9563)\n",
      "tensor(0.4047)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.605591\n",
      "Epoch 4935\n",
      "-------------------------------\n",
      "tensor(49.8209)\n",
      "tensor(21.4878)\n",
      "tensor(15.9736)\n",
      "tensor(0.4922)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.646969\n",
      "Epoch 4936\n",
      "-------------------------------\n",
      "tensor(34.5346)\n",
      "tensor(32.0296)\n",
      "tensor(20.7052)\n",
      "tensor(0.5420)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.669102\n",
      "Epoch 4937\n",
      "-------------------------------\n",
      "tensor(32.5731)\n",
      "tensor(30.1866)\n",
      "tensor(19.7084)\n",
      "tensor(0.4720)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.584244\n",
      "Epoch 4938\n",
      "-------------------------------\n",
      "tensor(36.7843)\n",
      "tensor(16.9942)\n",
      "tensor(3.9912)\n",
      "tensor(0.1909)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.524864\n",
      "Epoch 4939\n",
      "-------------------------------\n",
      "tensor(30.5520)\n",
      "tensor(21.2771)\n",
      "tensor(15.8610)\n",
      "tensor(0.5517)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.534386\n",
      "Epoch 4940\n",
      "-------------------------------\n",
      "tensor(47.6493)\n",
      "tensor(24.1634)\n",
      "tensor(17.3512)\n",
      "tensor(0.4925)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 44.530243\n",
      "Epoch 4941\n",
      "-------------------------------\n",
      "tensor(39.6899)\n",
      "tensor(20.9661)\n",
      "tensor(12.3722)\n",
      "tensor(0.2907)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.509392\n",
      "Epoch 4942\n",
      "-------------------------------\n",
      "tensor(39.9285)\n",
      "tensor(19.4947)\n",
      "tensor(6.2842)\n",
      "tensor(0.0600)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.480690\n",
      "Epoch 4943\n",
      "-------------------------------\n",
      "tensor(30.2087)\n",
      "tensor(23.5795)\n",
      "tensor(5.2422)\n",
      "tensor(0.2127)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.457291\n",
      "Epoch 4944\n",
      "-------------------------------\n",
      "tensor(29.3229)\n",
      "tensor(27.4316)\n",
      "tensor(16.2886)\n",
      "tensor(0.4604)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.481167\n",
      "Epoch 4945\n",
      "-------------------------------\n",
      "tensor(33.6510)\n",
      "tensor(27.8555)\n",
      "tensor(22.3449)\n",
      "tensor(0.3742)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.477310\n",
      "Epoch 4946\n",
      "-------------------------------\n",
      "tensor(44.0138)\n",
      "tensor(20.1933)\n",
      "tensor(8.6977)\n",
      "tensor(0.4570)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.439312\n",
      "Epoch 4947\n",
      "-------------------------------\n",
      "tensor(66.7682)\n",
      "tensor(23.2363)\n",
      "tensor(34.2250)\n",
      "tensor(1.3509)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.493645\n",
      "Epoch 4948\n",
      "-------------------------------\n",
      "tensor(48.1354)\n",
      "tensor(18.1465)\n",
      "tensor(9.5505)\n",
      "tensor(0.0889)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.534081\n",
      "Epoch 4949\n",
      "-------------------------------\n",
      "tensor(50.6066)\n",
      "tensor(15.9440)\n",
      "tensor(17.6377)\n",
      "tensor(0.6819)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.480766\n",
      "Epoch 4950\n",
      "-------------------------------\n",
      "tensor(81.8289)\n",
      "tensor(22.0461)\n",
      "tensor(33.3281)\n",
      "tensor(0.4831)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.533920\n",
      "Epoch 4951\n",
      "-------------------------------\n",
      "tensor(69.2079)\n",
      "tensor(40.0161)\n",
      "tensor(52.6495)\n",
      "tensor(0.8040)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.527946\n",
      "Epoch 4952\n",
      "-------------------------------\n",
      "tensor(40.1577)\n",
      "tensor(31.7193)\n",
      "tensor(31.1279)\n",
      "tensor(1.6218)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.443203\n",
      "Epoch 4953\n",
      "-------------------------------\n",
      "tensor(44.1453)\n",
      "tensor(22.8646)\n",
      "tensor(25.7269)\n",
      "tensor(0.1783)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.436882\n",
      "Epoch 4954\n",
      "-------------------------------\n",
      "tensor(66.9064)\n",
      "tensor(27.0546)\n",
      "tensor(48.8169)\n",
      "tensor(2.2223)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.447147\n",
      "Epoch 4955\n",
      "-------------------------------\n",
      "tensor(56.9516)\n",
      "tensor(20.1583)\n",
      "tensor(22.6232)\n",
      "tensor(0.5026)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.453743\n",
      "Epoch 4956\n",
      "-------------------------------\n",
      "tensor(44.1701)\n",
      "tensor(22.2160)\n",
      "tensor(18.5046)\n",
      "tensor(1.1790)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.425121\n",
      "Epoch 4957\n",
      "-------------------------------\n",
      "tensor(44.8597)\n",
      "tensor(26.8558)\n",
      "tensor(24.6498)\n",
      "tensor(0.3320)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.341396\n",
      "Epoch 4958\n",
      "-------------------------------\n",
      "tensor(52.1243)\n",
      "tensor(15.2714)\n",
      "tensor(16.4144)\n",
      "tensor(0.0895)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.308628\n",
      "Epoch 4959\n",
      "-------------------------------\n",
      "tensor(43.0125)\n",
      "tensor(22.5173)\n",
      "tensor(10.7831)\n",
      "tensor(0.2280)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.301685\n",
      "Epoch 4960\n",
      "-------------------------------\n",
      "tensor(47.0567)\n",
      "tensor(24.5281)\n",
      "tensor(22.1410)\n",
      "tensor(0.2440)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 44.298519\n",
      "Epoch 4961\n",
      "-------------------------------\n",
      "tensor(44.8393)\n",
      "tensor(23.0899)\n",
      "tensor(22.1430)\n",
      "tensor(0.1375)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.267242\n",
      "Epoch 4962\n",
      "-------------------------------\n",
      "tensor(40.1782)\n",
      "tensor(20.3252)\n",
      "tensor(16.5664)\n",
      "tensor(0.0283)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.231853\n",
      "Epoch 4963\n",
      "-------------------------------\n",
      "tensor(48.5299)\n",
      "tensor(24.2306)\n",
      "tensor(7.4353)\n",
      "tensor(0.2451)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.226051\n",
      "Epoch 4964\n",
      "-------------------------------\n",
      "tensor(36.2916)\n",
      "tensor(28.7279)\n",
      "tensor(18.4140)\n",
      "tensor(0.4520)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.250702\n",
      "Epoch 4965\n",
      "-------------------------------\n",
      "tensor(51.3446)\n",
      "tensor(31.5710)\n",
      "tensor(34.9724)\n",
      "tensor(0.2720)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.291775\n",
      "Epoch 4966\n",
      "-------------------------------\n",
      "tensor(54.5896)\n",
      "tensor(18.7546)\n",
      "tensor(20.5400)\n",
      "tensor(0.7713)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.232994\n",
      "Epoch 4967\n",
      "-------------------------------\n",
      "tensor(81.9396)\n",
      "tensor(22.6684)\n",
      "tensor(48.0051)\n",
      "tensor(1.7317)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.330746\n",
      "Epoch 4968\n",
      "-------------------------------\n",
      "tensor(46.4451)\n",
      "tensor(18.1520)\n",
      "tensor(9.8416)\n",
      "tensor(0.6346)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.252274\n",
      "Epoch 4969\n",
      "-------------------------------\n",
      "tensor(74.2873)\n",
      "tensor(26.8025)\n",
      "tensor(49.8785)\n",
      "tensor(2.1528)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.350361\n",
      "Epoch 4970\n",
      "-------------------------------\n",
      "tensor(88.5646)\n",
      "tensor(21.6554)\n",
      "tensor(49.8497)\n",
      "tensor(1.4807)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.248131\n",
      "Epoch 4971\n",
      "-------------------------------\n",
      "tensor(87.3180)\n",
      "tensor(16.1059)\n",
      "tensor(40.9544)\n",
      "tensor(0.4084)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.329437\n",
      "Epoch 4972\n",
      "-------------------------------\n",
      "tensor(63.0383)\n",
      "tensor(18.1400)\n",
      "tensor(30.1607)\n",
      "tensor(0.6948)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.196564\n",
      "Epoch 4973\n",
      "-------------------------------\n",
      "tensor(55.1060)\n",
      "tensor(18.9046)\n",
      "tensor(29.0228)\n",
      "tensor(1.4388)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.235355\n",
      "Epoch 4974\n",
      "-------------------------------\n",
      "tensor(33.3420)\n",
      "tensor(18.5578)\n",
      "tensor(8.8987)\n",
      "tensor(0.1213)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.145592\n",
      "Epoch 4975\n",
      "-------------------------------\n",
      "tensor(52.0091)\n",
      "tensor(21.0800)\n",
      "tensor(24.6834)\n",
      "tensor(1.2122)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.219475\n",
      "Epoch 4976\n",
      "-------------------------------\n",
      "tensor(45.0988)\n",
      "tensor(27.9815)\n",
      "tensor(26.8167)\n",
      "tensor(0.0492)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.182007\n",
      "Epoch 4977\n",
      "-------------------------------\n",
      "tensor(35.1134)\n",
      "tensor(25.7526)\n",
      "tensor(17.9992)\n",
      "tensor(0.1398)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.075657\n",
      "Epoch 4978\n",
      "-------------------------------\n",
      "tensor(44.9521)\n",
      "tensor(22.8896)\n",
      "tensor(8.4107)\n",
      "tensor(0.2809)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.091518\n",
      "Epoch 4979\n",
      "-------------------------------\n",
      "tensor(52.0344)\n",
      "tensor(26.6236)\n",
      "tensor(19.8807)\n",
      "tensor(0.4453)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.103168\n",
      "Epoch 4980\n",
      "-------------------------------\n",
      "tensor(39.0204)\n",
      "tensor(23.1751)\n",
      "tensor(16.4828)\n",
      "tensor(0.3193)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 44.078308\n",
      "Epoch 4981\n",
      "-------------------------------\n",
      "tensor(33.6800)\n",
      "tensor(19.1687)\n",
      "tensor(9.8914)\n",
      "tensor(0.1323)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 44.049469\n",
      "Epoch 4982\n",
      "-------------------------------\n",
      "tensor(27.7993)\n",
      "tensor(15.7160)\n",
      "tensor(4.5640)\n",
      "tensor(0.0452)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 44.027729\n",
      "Epoch 4983\n",
      "-------------------------------\n",
      "tensor(30.5172)\n",
      "tensor(13.5503)\n",
      "tensor(6.0706)\n",
      "tensor(0.2219)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.999283\n",
      "Epoch 4984\n",
      "-------------------------------\n",
      "tensor(30.4079)\n",
      "tensor(26.4908)\n",
      "tensor(9.3586)\n",
      "tensor(0.2974)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.048241\n",
      "Epoch 4985\n",
      "-------------------------------\n",
      "tensor(39.2541)\n",
      "tensor(28.3546)\n",
      "tensor(11.3448)\n",
      "tensor(0.1774)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.099258\n",
      "Epoch 4986\n",
      "-------------------------------\n",
      "tensor(43.8579)\n",
      "tensor(27.1265)\n",
      "tensor(10.4442)\n",
      "tensor(0.1994)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 44.129059\n",
      "Epoch 4987\n",
      "-------------------------------\n",
      "tensor(49.2512)\n",
      "tensor(25.7823)\n",
      "tensor(9.2854)\n",
      "tensor(0.5454)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.062527\n",
      "Epoch 4988\n",
      "-------------------------------\n",
      "tensor(47.1514)\n",
      "tensor(19.8046)\n",
      "tensor(8.3924)\n",
      "tensor(0.4256)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.010334\n",
      "Epoch 4989\n",
      "-------------------------------\n",
      "tensor(42.1307)\n",
      "tensor(23.7033)\n",
      "tensor(17.6742)\n",
      "tensor(0.2913)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.081169\n",
      "Epoch 4990\n",
      "-------------------------------\n",
      "tensor(37.4476)\n",
      "tensor(16.8279)\n",
      "tensor(11.1507)\n",
      "tensor(0.5842)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.043503\n",
      "Epoch 4991\n",
      "-------------------------------\n",
      "tensor(33.8849)\n",
      "tensor(26.9256)\n",
      "tensor(7.8833)\n",
      "tensor(0.2267)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.009640\n",
      "Epoch 4992\n",
      "-------------------------------\n",
      "tensor(41.9556)\n",
      "tensor(24.5984)\n",
      "tensor(16.4106)\n",
      "tensor(0.7050)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.029308\n",
      "Epoch 4993\n",
      "-------------------------------\n",
      "tensor(33.7996)\n",
      "tensor(20.7250)\n",
      "tensor(10.6015)\n",
      "tensor(0.7694)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 44.007912\n",
      "Epoch 4994\n",
      "-------------------------------\n",
      "tensor(39.9627)\n",
      "tensor(19.4590)\n",
      "tensor(19.5304)\n",
      "tensor(0.3089)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.988274\n",
      "Epoch 4995\n",
      "-------------------------------\n",
      "tensor(34.7506)\n",
      "tensor(17.2733)\n",
      "tensor(20.1143)\n",
      "tensor(1.2230)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.895439\n",
      "Epoch 4996\n",
      "-------------------------------\n",
      "tensor(44.2358)\n",
      "tensor(26.2562)\n",
      "tensor(3.6069)\n",
      "tensor(0.0671)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.949162\n",
      "Epoch 4997\n",
      "-------------------------------\n",
      "tensor(47.4331)\n",
      "tensor(25.6119)\n",
      "tensor(13.7592)\n",
      "tensor(0.8458)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.968487\n",
      "Epoch 4998\n",
      "-------------------------------\n",
      "tensor(46.9238)\n",
      "tensor(25.7664)\n",
      "tensor(16.4581)\n",
      "tensor(0.9478)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.893967\n",
      "Epoch 4999\n",
      "-------------------------------\n",
      "tensor(49.1515)\n",
      "tensor(20.4305)\n",
      "tensor(12.6921)\n",
      "tensor(0.7057)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.826397\n",
      "Epoch 5000\n",
      "-------------------------------\n",
      "tensor(34.7286)\n",
      "tensor(18.0588)\n",
      "tensor(5.9409)\n",
      "tensor(0.4674)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 43.814842\n",
      "Epoch 5001\n",
      "-------------------------------\n",
      "tensor(30.2602)\n",
      "tensor(17.8280)\n",
      "tensor(6.9688)\n",
      "tensor(0.2644)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.804665\n",
      "Epoch 5002\n",
      "-------------------------------\n",
      "tensor(33.7315)\n",
      "tensor(17.9577)\n",
      "tensor(10.8525)\n",
      "tensor(0.0653)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.792549\n",
      "Epoch 5003\n",
      "-------------------------------\n",
      "tensor(48.5185)\n",
      "tensor(22.7414)\n",
      "tensor(13.4235)\n",
      "tensor(0.1868)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.811436\n",
      "Epoch 5004\n",
      "-------------------------------\n",
      "tensor(41.0393)\n",
      "tensor(25.3914)\n",
      "tensor(10.5302)\n",
      "tensor(0.5592)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.814686\n",
      "Epoch 5005\n",
      "-------------------------------\n",
      "tensor(42.9144)\n",
      "tensor(14.6331)\n",
      "tensor(19.1443)\n",
      "tensor(0.8465)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.778961\n",
      "Epoch 5006\n",
      "-------------------------------\n",
      "tensor(32.1124)\n",
      "tensor(26.5181)\n",
      "tensor(15.5301)\n",
      "tensor(0.2662)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.778694\n",
      "Epoch 5007\n",
      "-------------------------------\n",
      "tensor(41.0592)\n",
      "tensor(25.0925)\n",
      "tensor(15.3227)\n",
      "tensor(1.1118)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.791031\n",
      "Epoch 5008\n",
      "-------------------------------\n",
      "tensor(44.9365)\n",
      "tensor(30.5361)\n",
      "tensor(28.6339)\n",
      "tensor(1.3369)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.815689\n",
      "Epoch 5009\n",
      "-------------------------------\n",
      "tensor(42.0407)\n",
      "tensor(32.9338)\n",
      "tensor(18.1060)\n",
      "tensor(0.9361)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.796165\n",
      "Epoch 5010\n",
      "-------------------------------\n",
      "tensor(43.4670)\n",
      "tensor(38.3351)\n",
      "tensor(36.0218)\n",
      "tensor(1.3351)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.800251\n",
      "Epoch 5011\n",
      "-------------------------------\n",
      "tensor(34.0317)\n",
      "tensor(35.9400)\n",
      "tensor(40.6891)\n",
      "tensor(1.9685)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.847054\n",
      "Epoch 5012\n",
      "-------------------------------\n",
      "tensor(39.0150)\n",
      "tensor(19.8634)\n",
      "tensor(5.5764)\n",
      "tensor(0.4634)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.826904\n",
      "Epoch 5013\n",
      "-------------------------------\n",
      "tensor(38.0941)\n",
      "tensor(18.0661)\n",
      "tensor(22.7767)\n",
      "tensor(1.2682)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.767250\n",
      "Epoch 5014\n",
      "-------------------------------\n",
      "tensor(69.0269)\n",
      "tensor(21.8296)\n",
      "tensor(27.3635)\n",
      "tensor(0.0292)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.873676\n",
      "Epoch 5015\n",
      "-------------------------------\n",
      "tensor(47.4878)\n",
      "tensor(31.7034)\n",
      "tensor(30.1708)\n",
      "tensor(0.2067)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.821167\n",
      "Epoch 5016\n",
      "-------------------------------\n",
      "tensor(47.2801)\n",
      "tensor(26.2872)\n",
      "tensor(19.9052)\n",
      "tensor(0.7328)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.692867\n",
      "Epoch 5017\n",
      "-------------------------------\n",
      "tensor(36.3817)\n",
      "tensor(30.3660)\n",
      "tensor(20.7977)\n",
      "tensor(1.3239)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.705364\n",
      "Epoch 5018\n",
      "-------------------------------\n",
      "tensor(43.7073)\n",
      "tensor(27.9376)\n",
      "tensor(23.3385)\n",
      "tensor(0.7383)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.710396\n",
      "Epoch 5019\n",
      "-------------------------------\n",
      "tensor(41.3322)\n",
      "tensor(19.9144)\n",
      "tensor(12.3909)\n",
      "tensor(0.1330)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.687622\n",
      "Epoch 5020\n",
      "-------------------------------\n",
      "tensor(36.3938)\n",
      "tensor(15.9699)\n",
      "tensor(11.5509)\n",
      "tensor(0.6065)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 43.661774\n",
      "Epoch 5021\n",
      "-------------------------------\n",
      "tensor(36.2898)\n",
      "tensor(15.3117)\n",
      "tensor(12.9640)\n",
      "tensor(0.7150)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.635632\n",
      "Epoch 5022\n",
      "-------------------------------\n",
      "tensor(34.0414)\n",
      "tensor(14.5929)\n",
      "tensor(11.2540)\n",
      "tensor(0.6231)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.605251\n",
      "Epoch 5023\n",
      "-------------------------------\n",
      "tensor(31.7605)\n",
      "tensor(24.8085)\n",
      "tensor(6.0157)\n",
      "tensor(0.3360)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.582275\n",
      "Epoch 5024\n",
      "-------------------------------\n",
      "tensor(35.8206)\n",
      "tensor(22.3388)\n",
      "tensor(4.9485)\n",
      "tensor(0.2100)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.610107\n",
      "Epoch 5025\n",
      "-------------------------------\n",
      "tensor(41.2885)\n",
      "tensor(23.4520)\n",
      "tensor(12.6253)\n",
      "tensor(0.8005)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.632946\n",
      "Epoch 5026\n",
      "-------------------------------\n",
      "tensor(51.1915)\n",
      "tensor(28.5360)\n",
      "tensor(17.2271)\n",
      "tensor(0.9415)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.610981\n",
      "Epoch 5027\n",
      "-------------------------------\n",
      "tensor(64.4364)\n",
      "tensor(22.4673)\n",
      "tensor(18.8757)\n",
      "tensor(0.5300)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.652725\n",
      "Epoch 5028\n",
      "-------------------------------\n",
      "tensor(34.5834)\n",
      "tensor(19.7686)\n",
      "tensor(15.5803)\n",
      "tensor(0.1739)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.647881\n",
      "Epoch 5029\n",
      "-------------------------------\n",
      "tensor(48.3671)\n",
      "tensor(19.8281)\n",
      "tensor(23.5196)\n",
      "tensor(0.8291)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.653877\n",
      "Epoch 5030\n",
      "-------------------------------\n",
      "tensor(33.0446)\n",
      "tensor(37.8526)\n",
      "tensor(33.0195)\n",
      "tensor(1.2747)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.623150\n",
      "Epoch 5031\n",
      "-------------------------------\n",
      "tensor(41.9138)\n",
      "tensor(26.3354)\n",
      "tensor(23.3877)\n",
      "tensor(1.6092)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.621326\n",
      "Epoch 5032\n",
      "-------------------------------\n",
      "tensor(69.9477)\n",
      "tensor(21.3645)\n",
      "tensor(40.3108)\n",
      "tensor(1.5405)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.689045\n",
      "Epoch 5033\n",
      "-------------------------------\n",
      "tensor(102.2300)\n",
      "tensor(33.9023)\n",
      "tensor(75.7936)\n",
      "tensor(2.7468)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.749859\n",
      "Epoch 5034\n",
      "-------------------------------\n",
      "tensor(115.6286)\n",
      "tensor(24.8821)\n",
      "tensor(65.4619)\n",
      "tensor(1.0738)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.798317\n",
      "Epoch 5035\n",
      "-------------------------------\n",
      "tensor(65.6870)\n",
      "tensor(21.1091)\n",
      "tensor(19.7927)\n",
      "tensor(0.4087)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.546349\n",
      "Epoch 5036\n",
      "-------------------------------\n",
      "tensor(75.3374)\n",
      "tensor(16.5966)\n",
      "tensor(31.7775)\n",
      "tensor(0.4245)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.536980\n",
      "Epoch 5037\n",
      "-------------------------------\n",
      "tensor(66.3047)\n",
      "tensor(21.8924)\n",
      "tensor(29.6203)\n",
      "tensor(1.0687)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.570408\n",
      "Epoch 5038\n",
      "-------------------------------\n",
      "tensor(56.8755)\n",
      "tensor(20.6642)\n",
      "tensor(20.9257)\n",
      "tensor(0.2420)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.560692\n",
      "Epoch 5039\n",
      "-------------------------------\n",
      "tensor(30.9567)\n",
      "tensor(29.0265)\n",
      "tensor(12.4789)\n",
      "tensor(0.7085)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.486786\n",
      "Epoch 5040\n",
      "-------------------------------\n",
      "tensor(46.4017)\n",
      "tensor(14.6871)\n",
      "tensor(25.8502)\n",
      "tensor(0.9873)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 43.450668\n",
      "Epoch 5041\n",
      "-------------------------------\n",
      "tensor(45.3312)\n",
      "tensor(13.2602)\n",
      "tensor(23.0893)\n",
      "tensor(0.7905)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.449787\n",
      "Epoch 5042\n",
      "-------------------------------\n",
      "tensor(35.9296)\n",
      "tensor(12.1860)\n",
      "tensor(12.6568)\n",
      "tensor(0.4090)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.427341\n",
      "Epoch 5043\n",
      "-------------------------------\n",
      "tensor(24.4912)\n",
      "tensor(15.2453)\n",
      "tensor(6.4664)\n",
      "tensor(0.1678)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.401745\n",
      "Epoch 5044\n",
      "-------------------------------\n",
      "tensor(58.9332)\n",
      "tensor(19.3362)\n",
      "tensor(29.3435)\n",
      "tensor(0.8950)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.437168\n",
      "Epoch 5045\n",
      "-------------------------------\n",
      "tensor(62.9407)\n",
      "tensor(21.6419)\n",
      "tensor(30.3491)\n",
      "tensor(1.1018)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.423687\n",
      "Epoch 5046\n",
      "-------------------------------\n",
      "tensor(62.0382)\n",
      "tensor(19.2137)\n",
      "tensor(17.2055)\n",
      "tensor(0.1341)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.420109\n",
      "Epoch 5047\n",
      "-------------------------------\n",
      "tensor(79.4913)\n",
      "tensor(17.0025)\n",
      "tensor(37.1773)\n",
      "tensor(0.5346)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.517536\n",
      "Epoch 5048\n",
      "-------------------------------\n",
      "tensor(73.4219)\n",
      "tensor(21.0017)\n",
      "tensor(32.4273)\n",
      "tensor(0.8132)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.480175\n",
      "Epoch 5049\n",
      "-------------------------------\n",
      "tensor(37.2710)\n",
      "tensor(23.1242)\n",
      "tensor(3.4028)\n",
      "tensor(0.0025)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.373024\n",
      "Epoch 5050\n",
      "-------------------------------\n",
      "tensor(88.5054)\n",
      "tensor(15.4055)\n",
      "tensor(49.4472)\n",
      "tensor(0.8178)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.553791\n",
      "Epoch 5051\n",
      "-------------------------------\n",
      "tensor(97.0200)\n",
      "tensor(46.0658)\n",
      "tensor(81.7889)\n",
      "tensor(2.1922)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.677193\n",
      "Epoch 5052\n",
      "-------------------------------\n",
      "tensor(85.5479)\n",
      "tensor(28.6352)\n",
      "tensor(61.1770)\n",
      "tensor(2.1729)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.500843\n",
      "Epoch 5053\n",
      "-------------------------------\n",
      "tensor(59.9070)\n",
      "tensor(24.2329)\n",
      "tensor(15.4288)\n",
      "tensor(0.1873)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.440842\n",
      "Epoch 5054\n",
      "-------------------------------\n",
      "tensor(50.6750)\n",
      "tensor(24.5478)\n",
      "tensor(17.4580)\n",
      "tensor(1.0751)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.384888\n",
      "Epoch 5055\n",
      "-------------------------------\n",
      "tensor(74.7854)\n",
      "tensor(12.0454)\n",
      "tensor(35.2330)\n",
      "tensor(0.0353)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.351349\n",
      "Epoch 5056\n",
      "-------------------------------\n",
      "tensor(40.6229)\n",
      "tensor(23.6014)\n",
      "tensor(26.6294)\n",
      "tensor(0.4720)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.282169\n",
      "Epoch 5057\n",
      "-------------------------------\n",
      "tensor(72.8817)\n",
      "tensor(21.0358)\n",
      "tensor(29.9323)\n",
      "tensor(0.0898)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.295158\n",
      "Epoch 5058\n",
      "-------------------------------\n",
      "tensor(53.6790)\n",
      "tensor(15.5193)\n",
      "tensor(20.6701)\n",
      "tensor(0.8365)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.269066\n",
      "Epoch 5059\n",
      "-------------------------------\n",
      "tensor(62.8012)\n",
      "tensor(14.6738)\n",
      "tensor(27.2957)\n",
      "tensor(0.5774)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.246796\n",
      "Epoch 5060\n",
      "-------------------------------\n",
      "tensor(31.6922)\n",
      "tensor(24.2528)\n",
      "tensor(14.3319)\n",
      "tensor(0.0433)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 43.205173\n",
      "Epoch 5061\n",
      "-------------------------------\n",
      "tensor(32.0420)\n",
      "tensor(21.1797)\n",
      "tensor(7.5088)\n",
      "tensor(0.5053)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.203079\n",
      "Epoch 5062\n",
      "-------------------------------\n",
      "tensor(35.5047)\n",
      "tensor(19.7700)\n",
      "tensor(11.4703)\n",
      "tensor(0.7834)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.193718\n",
      "Epoch 5063\n",
      "-------------------------------\n",
      "tensor(24.2396)\n",
      "tensor(20.9911)\n",
      "tensor(15.6536)\n",
      "tensor(0.9180)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.186634\n",
      "Epoch 5064\n",
      "-------------------------------\n",
      "tensor(30.8208)\n",
      "tensor(21.8298)\n",
      "tensor(16.8653)\n",
      "tensor(0.7638)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.189167\n",
      "Epoch 5065\n",
      "-------------------------------\n",
      "tensor(38.9060)\n",
      "tensor(18.0034)\n",
      "tensor(9.7369)\n",
      "tensor(0.0443)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.178345\n",
      "Epoch 5066\n",
      "-------------------------------\n",
      "tensor(51.3685)\n",
      "tensor(31.6850)\n",
      "tensor(16.3572)\n",
      "tensor(1.0197)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.202854\n",
      "Epoch 5067\n",
      "-------------------------------\n",
      "tensor(55.7997)\n",
      "tensor(16.7773)\n",
      "tensor(30.6950)\n",
      "tensor(1.1003)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.232952\n",
      "Epoch 5068\n",
      "-------------------------------\n",
      "tensor(58.8251)\n",
      "tensor(22.9111)\n",
      "tensor(22.9020)\n",
      "tensor(1.0466)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.218578\n",
      "Epoch 5069\n",
      "-------------------------------\n",
      "tensor(41.0888)\n",
      "tensor(23.2601)\n",
      "tensor(16.5508)\n",
      "tensor(1.1480)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.190937\n",
      "Epoch 5070\n",
      "-------------------------------\n",
      "tensor(37.7673)\n",
      "tensor(13.5262)\n",
      "tensor(10.1262)\n",
      "tensor(0.4284)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.174976\n",
      "Epoch 5071\n",
      "-------------------------------\n",
      "tensor(63.3388)\n",
      "tensor(27.4150)\n",
      "tensor(21.4941)\n",
      "tensor(0.7423)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.306801\n",
      "Epoch 5072\n",
      "-------------------------------\n",
      "tensor(51.5431)\n",
      "tensor(34.3443)\n",
      "tensor(40.6768)\n",
      "tensor(0.7947)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.246830\n",
      "Epoch 5073\n",
      "-------------------------------\n",
      "tensor(30.6020)\n",
      "tensor(43.3616)\n",
      "tensor(40.7296)\n",
      "tensor(2.4927)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.305996\n",
      "Epoch 5074\n",
      "-------------------------------\n",
      "tensor(38.8623)\n",
      "tensor(27.3568)\n",
      "tensor(20.8350)\n",
      "tensor(0.7696)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.246613\n",
      "Epoch 5075\n",
      "-------------------------------\n",
      "tensor(60.3446)\n",
      "tensor(22.7335)\n",
      "tensor(38.2437)\n",
      "tensor(1.9262)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.207115\n",
      "Epoch 5076\n",
      "-------------------------------\n",
      "tensor(48.9420)\n",
      "tensor(27.7474)\n",
      "tensor(10.4365)\n",
      "tensor(0.6082)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.124001\n",
      "Epoch 5077\n",
      "-------------------------------\n",
      "tensor(43.9441)\n",
      "tensor(21.6484)\n",
      "tensor(13.3940)\n",
      "tensor(0.7083)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.129608\n",
      "Epoch 5078\n",
      "-------------------------------\n",
      "tensor(41.0755)\n",
      "tensor(23.3500)\n",
      "tensor(17.3310)\n",
      "tensor(0.8780)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.070980\n",
      "Epoch 5079\n",
      "-------------------------------\n",
      "tensor(51.5248)\n",
      "tensor(18.8539)\n",
      "tensor(17.9483)\n",
      "tensor(0.6774)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.077690\n",
      "Epoch 5080\n",
      "-------------------------------\n",
      "tensor(41.5649)\n",
      "tensor(18.2299)\n",
      "tensor(9.7275)\n",
      "tensor(0.4869)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 43.059006\n",
      "Epoch 5081\n",
      "-------------------------------\n",
      "tensor(40.4945)\n",
      "tensor(18.2462)\n",
      "tensor(5.1053)\n",
      "tensor(0.3260)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 43.039581\n",
      "Epoch 5082\n",
      "-------------------------------\n",
      "tensor(38.2820)\n",
      "tensor(17.7016)\n",
      "tensor(8.7990)\n",
      "tensor(0.1542)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 43.035233\n",
      "Epoch 5083\n",
      "-------------------------------\n",
      "tensor(41.4362)\n",
      "tensor(17.4258)\n",
      "tensor(13.2233)\n",
      "tensor(0.0957)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.021961\n",
      "Epoch 5084\n",
      "-------------------------------\n",
      "tensor(47.8227)\n",
      "tensor(24.5373)\n",
      "tensor(14.3726)\n",
      "tensor(0.4893)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.014854\n",
      "Epoch 5085\n",
      "-------------------------------\n",
      "tensor(32.4370)\n",
      "tensor(30.2072)\n",
      "tensor(14.5857)\n",
      "tensor(0.9413)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.014343\n",
      "Epoch 5086\n",
      "-------------------------------\n",
      "tensor(56.6353)\n",
      "tensor(13.5936)\n",
      "tensor(28.8934)\n",
      "tensor(0.7738)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.023872\n",
      "Epoch 5087\n",
      "-------------------------------\n",
      "tensor(46.5914)\n",
      "tensor(24.6492)\n",
      "tensor(12.6900)\n",
      "tensor(0.9451)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.036907\n",
      "Epoch 5088\n",
      "-------------------------------\n",
      "tensor(36.4279)\n",
      "tensor(33.8253)\n",
      "tensor(28.9991)\n",
      "tensor(1.8253)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.046509\n",
      "Epoch 5089\n",
      "-------------------------------\n",
      "tensor(45.1054)\n",
      "tensor(16.4931)\n",
      "tensor(6.0261)\n",
      "tensor(0.1558)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.950962\n",
      "Epoch 5090\n",
      "-------------------------------\n",
      "tensor(37.4856)\n",
      "tensor(35.2260)\n",
      "tensor(21.5120)\n",
      "tensor(1.4320)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.063271\n",
      "Epoch 5091\n",
      "-------------------------------\n",
      "tensor(39.0899)\n",
      "tensor(25.3828)\n",
      "tensor(6.1707)\n",
      "tensor(0.0263)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.055523\n",
      "Epoch 5092\n",
      "-------------------------------\n",
      "tensor(49.7663)\n",
      "tensor(29.5124)\n",
      "tensor(21.7081)\n",
      "tensor(1.6206)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.068108\n",
      "Epoch 5093\n",
      "-------------------------------\n",
      "tensor(39.1407)\n",
      "tensor(26.4668)\n",
      "tensor(25.2158)\n",
      "tensor(0.8951)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.065918\n",
      "Epoch 5094\n",
      "-------------------------------\n",
      "tensor(36.3244)\n",
      "tensor(23.1254)\n",
      "tensor(29.4746)\n",
      "tensor(1.8187)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.020050\n",
      "Epoch 5095\n",
      "-------------------------------\n",
      "tensor(42.3647)\n",
      "tensor(30.1332)\n",
      "tensor(11.0213)\n",
      "tensor(0.7987)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.064770\n",
      "Epoch 5096\n",
      "-------------------------------\n",
      "tensor(39.3788)\n",
      "tensor(22.6975)\n",
      "tensor(12.0352)\n",
      "tensor(0.6717)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.011742\n",
      "Epoch 5097\n",
      "-------------------------------\n",
      "tensor(51.0120)\n",
      "tensor(22.0475)\n",
      "tensor(18.6828)\n",
      "tensor(1.1056)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.944336\n",
      "Epoch 5098\n",
      "-------------------------------\n",
      "tensor(32.9276)\n",
      "tensor(21.4986)\n",
      "tensor(12.5127)\n",
      "tensor(0.9342)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.894009\n",
      "Epoch 5099\n",
      "-------------------------------\n",
      "tensor(44.7030)\n",
      "tensor(20.4118)\n",
      "tensor(13.5166)\n",
      "tensor(0.3963)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.873436\n",
      "Epoch 5100\n",
      "-------------------------------\n",
      "tensor(35.2648)\n",
      "tensor(17.3084)\n",
      "tensor(8.8697)\n",
      "tensor(0.1119)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 42.860237\n",
      "Epoch 5101\n",
      "-------------------------------\n",
      "tensor(35.5501)\n",
      "tensor(15.1366)\n",
      "tensor(8.5001)\n",
      "tensor(0.3906)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.847813\n",
      "Epoch 5102\n",
      "-------------------------------\n",
      "tensor(35.7936)\n",
      "tensor(24.9308)\n",
      "tensor(9.3154)\n",
      "tensor(0.5062)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.844074\n",
      "Epoch 5103\n",
      "-------------------------------\n",
      "tensor(34.2583)\n",
      "tensor(25.7425)\n",
      "tensor(10.5868)\n",
      "tensor(0.4994)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.828674\n",
      "Epoch 5104\n",
      "-------------------------------\n",
      "tensor(38.5324)\n",
      "tensor(11.6580)\n",
      "tensor(12.7998)\n",
      "tensor(0.2271)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.829140\n",
      "Epoch 5105\n",
      "-------------------------------\n",
      "tensor(35.9556)\n",
      "tensor(16.3209)\n",
      "tensor(7.5044)\n",
      "tensor(0.5006)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.808571\n",
      "Epoch 5106\n",
      "-------------------------------\n",
      "tensor(61.2616)\n",
      "tensor(24.6098)\n",
      "tensor(23.6520)\n",
      "tensor(1.2587)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.829800\n",
      "Epoch 5107\n",
      "-------------------------------\n",
      "tensor(44.4552)\n",
      "tensor(19.7844)\n",
      "tensor(9.1100)\n",
      "tensor(0.6372)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.865875\n",
      "Epoch 5108\n",
      "-------------------------------\n",
      "tensor(44.4513)\n",
      "tensor(31.0447)\n",
      "tensor(16.3197)\n",
      "tensor(0.6178)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.821007\n",
      "Epoch 5109\n",
      "-------------------------------\n",
      "tensor(33.2467)\n",
      "tensor(14.1043)\n",
      "tensor(11.8898)\n",
      "tensor(0.5531)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.797504\n",
      "Epoch 5110\n",
      "-------------------------------\n",
      "tensor(47.9313)\n",
      "tensor(22.7597)\n",
      "tensor(32.3010)\n",
      "tensor(0.7216)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.852791\n",
      "Epoch 5111\n",
      "-------------------------------\n",
      "tensor(40.2436)\n",
      "tensor(33.1481)\n",
      "tensor(26.9430)\n",
      "tensor(0.6837)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.813896\n",
      "Epoch 5112\n",
      "-------------------------------\n",
      "tensor(43.7077)\n",
      "tensor(14.1231)\n",
      "tensor(12.9745)\n",
      "tensor(0.3461)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.796906\n",
      "Epoch 5113\n",
      "-------------------------------\n",
      "tensor(90.2554)\n",
      "tensor(22.0495)\n",
      "tensor(54.6255)\n",
      "tensor(1.4213)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.896141\n",
      "Epoch 5114\n",
      "-------------------------------\n",
      "tensor(112.1299)\n",
      "tensor(25.6395)\n",
      "tensor(72.1915)\n",
      "tensor(1.9385)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 43.034290\n",
      "Epoch 5115\n",
      "-------------------------------\n",
      "tensor(44.9465)\n",
      "tensor(28.9342)\n",
      "tensor(37.4177)\n",
      "tensor(1.0290)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.765556\n",
      "Epoch 5116\n",
      "-------------------------------\n",
      "tensor(83.1409)\n",
      "tensor(22.4462)\n",
      "tensor(43.0268)\n",
      "tensor(1.2083)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.796448\n",
      "Epoch 5117\n",
      "-------------------------------\n",
      "tensor(55.9824)\n",
      "tensor(35.5982)\n",
      "tensor(42.7475)\n",
      "tensor(0.7890)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.722866\n",
      "Epoch 5118\n",
      "-------------------------------\n",
      "tensor(85.1045)\n",
      "tensor(15.8166)\n",
      "tensor(46.8461)\n",
      "tensor(0.7086)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.802433\n",
      "Epoch 5119\n",
      "-------------------------------\n",
      "tensor(42.5421)\n",
      "tensor(17.9205)\n",
      "tensor(5.2245)\n",
      "tensor(0.3545)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.692955\n",
      "Epoch 5120\n",
      "-------------------------------\n",
      "tensor(51.7433)\n",
      "tensor(25.7216)\n",
      "tensor(29.3718)\n",
      "tensor(0.9117)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 42.689568\n",
      "Epoch 5121\n",
      "-------------------------------\n",
      "tensor(73.6524)\n",
      "tensor(19.9524)\n",
      "tensor(35.0444)\n",
      "tensor(0.9152)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.681583\n",
      "Epoch 5122\n",
      "-------------------------------\n",
      "tensor(58.1647)\n",
      "tensor(18.3761)\n",
      "tensor(25.6234)\n",
      "tensor(0.6069)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.642078\n",
      "Epoch 5123\n",
      "-------------------------------\n",
      "tensor(26.7698)\n",
      "tensor(13.2841)\n",
      "tensor(3.9646)\n",
      "tensor(0.0067)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.628807\n",
      "Epoch 5124\n",
      "-------------------------------\n",
      "tensor(52.2168)\n",
      "tensor(13.3324)\n",
      "tensor(29.0207)\n",
      "tensor(0.8310)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.672493\n",
      "Epoch 5125\n",
      "-------------------------------\n",
      "tensor(67.6486)\n",
      "tensor(18.6411)\n",
      "tensor(41.4589)\n",
      "tensor(1.2118)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.709084\n",
      "Epoch 5126\n",
      "-------------------------------\n",
      "tensor(43.9689)\n",
      "tensor(24.7160)\n",
      "tensor(5.0755)\n",
      "tensor(0.0667)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.663055\n",
      "Epoch 5127\n",
      "-------------------------------\n",
      "tensor(83.5159)\n",
      "tensor(24.0426)\n",
      "tensor(40.2322)\n",
      "tensor(1.2638)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.759697\n",
      "Epoch 5128\n",
      "-------------------------------\n",
      "tensor(69.1025)\n",
      "tensor(16.1749)\n",
      "tensor(27.2708)\n",
      "tensor(0.1632)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.755959\n",
      "Epoch 5129\n",
      "-------------------------------\n",
      "tensor(42.3351)\n",
      "tensor(14.4297)\n",
      "tensor(10.5697)\n",
      "tensor(0.1391)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.645515\n",
      "Epoch 5130\n",
      "-------------------------------\n",
      "tensor(97.8316)\n",
      "tensor(19.7080)\n",
      "tensor(53.0479)\n",
      "tensor(0.6593)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.735851\n",
      "Epoch 5131\n",
      "-------------------------------\n",
      "tensor(118.1693)\n",
      "tensor(31.3697)\n",
      "tensor(81.0203)\n",
      "tensor(2.2362)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.886894\n",
      "Epoch 5132\n",
      "-------------------------------\n",
      "tensor(118.9965)\n",
      "tensor(33.5714)\n",
      "tensor(80.0156)\n",
      "tensor(2.6998)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.889191\n",
      "Epoch 5133\n",
      "-------------------------------\n",
      "tensor(97.3027)\n",
      "tensor(17.5080)\n",
      "tensor(54.9136)\n",
      "tensor(0.7473)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.806927\n",
      "Epoch 5134\n",
      "-------------------------------\n",
      "tensor(37.1278)\n",
      "tensor(17.9740)\n",
      "tensor(17.2946)\n",
      "tensor(0.0326)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.575722\n",
      "Epoch 5135\n",
      "-------------------------------\n",
      "tensor(79.4586)\n",
      "tensor(21.0398)\n",
      "tensor(33.7677)\n",
      "tensor(0.0817)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.607937\n",
      "Epoch 5136\n",
      "-------------------------------\n",
      "tensor(78.7946)\n",
      "tensor(18.1675)\n",
      "tensor(48.2325)\n",
      "tensor(1.1333)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.601425\n",
      "Epoch 5137\n",
      "-------------------------------\n",
      "tensor(44.2874)\n",
      "tensor(26.0416)\n",
      "tensor(13.2542)\n",
      "tensor(0.3054)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.498421\n",
      "Epoch 5138\n",
      "-------------------------------\n",
      "tensor(28.9687)\n",
      "tensor(25.5966)\n",
      "tensor(25.5903)\n",
      "tensor(1.3360)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.492989\n",
      "Epoch 5139\n",
      "-------------------------------\n",
      "tensor(32.6838)\n",
      "tensor(25.9916)\n",
      "tensor(27.6346)\n",
      "tensor(1.1244)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.475269\n",
      "Epoch 5140\n",
      "-------------------------------\n",
      "tensor(49.6355)\n",
      "tensor(20.2497)\n",
      "tensor(15.0937)\n",
      "tensor(0.4808)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 42.456528\n",
      "Epoch 5141\n",
      "-------------------------------\n",
      "tensor(32.7613)\n",
      "tensor(22.2638)\n",
      "tensor(3.5021)\n",
      "tensor(0.0650)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.433247\n",
      "Epoch 5142\n",
      "-------------------------------\n",
      "tensor(36.4519)\n",
      "tensor(11.4866)\n",
      "tensor(13.4531)\n",
      "tensor(0.4318)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.445332\n",
      "Epoch 5143\n",
      "-------------------------------\n",
      "tensor(44.9381)\n",
      "tensor(12.2505)\n",
      "tensor(20.3317)\n",
      "tensor(0.6508)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.454617\n",
      "Epoch 5144\n",
      "-------------------------------\n",
      "tensor(45.6928)\n",
      "tensor(13.7524)\n",
      "tensor(15.6506)\n",
      "tensor(0.5441)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.426655\n",
      "Epoch 5145\n",
      "-------------------------------\n",
      "tensor(53.0072)\n",
      "tensor(23.8881)\n",
      "tensor(8.9009)\n",
      "tensor(0.1267)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.436760\n",
      "Epoch 5146\n",
      "-------------------------------\n",
      "tensor(50.1011)\n",
      "tensor(19.9445)\n",
      "tensor(18.6827)\n",
      "tensor(0.7895)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.449753\n",
      "Epoch 5147\n",
      "-------------------------------\n",
      "tensor(51.3956)\n",
      "tensor(17.0764)\n",
      "tensor(15.0706)\n",
      "tensor(0.4924)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.491405\n",
      "Epoch 5148\n",
      "-------------------------------\n",
      "tensor(43.3100)\n",
      "tensor(15.7053)\n",
      "tensor(8.0919)\n",
      "tensor(0.2070)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.477428\n",
      "Epoch 5149\n",
      "-------------------------------\n",
      "tensor(77.1222)\n",
      "tensor(21.9503)\n",
      "tensor(27.9241)\n",
      "tensor(0.1046)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.434727\n",
      "Epoch 5150\n",
      "-------------------------------\n",
      "tensor(77.4682)\n",
      "tensor(20.2264)\n",
      "tensor(48.4474)\n",
      "tensor(1.3212)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.521156\n",
      "Epoch 5151\n",
      "-------------------------------\n",
      "tensor(53.1174)\n",
      "tensor(31.4404)\n",
      "tensor(47.1183)\n",
      "tensor(1.6829)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.487919\n",
      "Epoch 5152\n",
      "-------------------------------\n",
      "tensor(33.1553)\n",
      "tensor(29.5647)\n",
      "tensor(13.0442)\n",
      "tensor(0.3616)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.464012\n",
      "Epoch 5153\n",
      "-------------------------------\n",
      "tensor(45.2544)\n",
      "tensor(37.7535)\n",
      "tensor(41.4932)\n",
      "tensor(1.2284)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.436253\n",
      "Epoch 5154\n",
      "-------------------------------\n",
      "tensor(58.2325)\n",
      "tensor(39.2697)\n",
      "tensor(48.6834)\n",
      "tensor(1.7362)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.536476\n",
      "Epoch 5155\n",
      "-------------------------------\n",
      "tensor(45.4861)\n",
      "tensor(21.6153)\n",
      "tensor(5.4126)\n",
      "tensor(0.4247)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.482090\n",
      "Epoch 5156\n",
      "-------------------------------\n",
      "tensor(53.6269)\n",
      "tensor(12.3080)\n",
      "tensor(24.3300)\n",
      "tensor(0.6117)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.415932\n",
      "Epoch 5157\n",
      "-------------------------------\n",
      "tensor(29.4394)\n",
      "tensor(15.5066)\n",
      "tensor(13.2086)\n",
      "tensor(0.0055)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.306820\n",
      "Epoch 5158\n",
      "-------------------------------\n",
      "tensor(60.4679)\n",
      "tensor(20.8349)\n",
      "tensor(23.0766)\n",
      "tensor(0.2458)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.374557\n",
      "Epoch 5159\n",
      "-------------------------------\n",
      "tensor(44.8065)\n",
      "tensor(26.0174)\n",
      "tensor(4.4761)\n",
      "tensor(0.0939)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.347992\n",
      "Epoch 5160\n",
      "-------------------------------\n",
      "tensor(45.7237)\n",
      "tensor(30.8492)\n",
      "tensor(16.4405)\n",
      "tensor(0.3291)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 42.326115\n",
      "Epoch 5161\n",
      "-------------------------------\n",
      "tensor(43.3926)\n",
      "tensor(30.8647)\n",
      "tensor(23.0490)\n",
      "tensor(0.3281)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.300735\n",
      "Epoch 5162\n",
      "-------------------------------\n",
      "tensor(51.3750)\n",
      "tensor(10.8559)\n",
      "tensor(22.4563)\n",
      "tensor(0.1696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.278351\n",
      "Epoch 5163\n",
      "-------------------------------\n",
      "tensor(40.1906)\n",
      "tensor(11.8503)\n",
      "tensor(12.0591)\n",
      "tensor(0.1885)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.265858\n",
      "Epoch 5164\n",
      "-------------------------------\n",
      "tensor(25.6356)\n",
      "tensor(19.3703)\n",
      "tensor(15.6047)\n",
      "tensor(0.7302)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.261963\n",
      "Epoch 5165\n",
      "-------------------------------\n",
      "tensor(45.9927)\n",
      "tensor(27.0906)\n",
      "tensor(39.5414)\n",
      "tensor(1.0062)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.296448\n",
      "Epoch 5166\n",
      "-------------------------------\n",
      "tensor(49.6482)\n",
      "tensor(20.2068)\n",
      "tensor(23.1661)\n",
      "tensor(0.0029)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.291828\n",
      "Epoch 5167\n",
      "-------------------------------\n",
      "tensor(51.5804)\n",
      "tensor(39.2497)\n",
      "tensor(40.2818)\n",
      "tensor(1.5794)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.382999\n",
      "Epoch 5168\n",
      "-------------------------------\n",
      "tensor(54.5252)\n",
      "tensor(31.4347)\n",
      "tensor(33.1819)\n",
      "tensor(0.2377)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.340115\n",
      "Epoch 5169\n",
      "-------------------------------\n",
      "tensor(70.7372)\n",
      "tensor(35.2509)\n",
      "tensor(58.1381)\n",
      "tensor(2.9658)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.313492\n",
      "Epoch 5170\n",
      "-------------------------------\n",
      "tensor(64.8293)\n",
      "tensor(16.1188)\n",
      "tensor(25.4990)\n",
      "tensor(0.0109)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.395615\n",
      "Epoch 5171\n",
      "-------------------------------\n",
      "tensor(51.4201)\n",
      "tensor(20.8020)\n",
      "tensor(23.0962)\n",
      "tensor(1.1355)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.428856\n",
      "Epoch 5172\n",
      "-------------------------------\n",
      "tensor(48.5013)\n",
      "tensor(19.0237)\n",
      "tensor(19.3775)\n",
      "tensor(0.2919)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.354416\n",
      "Epoch 5173\n",
      "-------------------------------\n",
      "tensor(66.0582)\n",
      "tensor(12.5757)\n",
      "tensor(33.1763)\n",
      "tensor(0.1614)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.359760\n",
      "Epoch 5174\n",
      "-------------------------------\n",
      "tensor(80.0845)\n",
      "tensor(21.3467)\n",
      "tensor(36.7474)\n",
      "tensor(1.3143)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.319439\n",
      "Epoch 5175\n",
      "-------------------------------\n",
      "tensor(32.4707)\n",
      "tensor(39.9261)\n",
      "tensor(30.8222)\n",
      "tensor(1.2524)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.193684\n",
      "Epoch 5176\n",
      "-------------------------------\n",
      "tensor(44.5857)\n",
      "tensor(14.8493)\n",
      "tensor(26.8257)\n",
      "tensor(1.0913)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.160713\n",
      "Epoch 5177\n",
      "-------------------------------\n",
      "tensor(57.9005)\n",
      "tensor(28.7428)\n",
      "tensor(30.1798)\n",
      "tensor(0.7180)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.228691\n",
      "Epoch 5178\n",
      "-------------------------------\n",
      "tensor(55.2161)\n",
      "tensor(32.1778)\n",
      "tensor(28.9824)\n",
      "tensor(1.0883)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.189140\n",
      "Epoch 5179\n",
      "-------------------------------\n",
      "tensor(34.4976)\n",
      "tensor(20.0346)\n",
      "tensor(6.7940)\n",
      "tensor(0.6063)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.093910\n",
      "Epoch 5180\n",
      "-------------------------------\n",
      "tensor(43.4751)\n",
      "tensor(14.1444)\n",
      "tensor(11.2673)\n",
      "tensor(0.1961)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 42.069592\n",
      "Epoch 5181\n",
      "-------------------------------\n",
      "tensor(34.4550)\n",
      "tensor(24.1628)\n",
      "tensor(12.3320)\n",
      "tensor(0.0463)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 42.092979\n",
      "Epoch 5182\n",
      "-------------------------------\n",
      "tensor(37.8178)\n",
      "tensor(24.9497)\n",
      "tensor(10.3959)\n",
      "tensor(0.0126)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.107380\n",
      "Epoch 5183\n",
      "-------------------------------\n",
      "tensor(30.0224)\n",
      "tensor(22.9954)\n",
      "tensor(6.0243)\n",
      "tensor(0.0572)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.102303\n",
      "Epoch 5184\n",
      "-------------------------------\n",
      "tensor(36.1495)\n",
      "tensor(21.8122)\n",
      "tensor(3.5210)\n",
      "tensor(0.1713)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.084759\n",
      "Epoch 5185\n",
      "-------------------------------\n",
      "tensor(30.9266)\n",
      "tensor(16.3585)\n",
      "tensor(7.4208)\n",
      "tensor(0.2680)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.050751\n",
      "Epoch 5186\n",
      "-------------------------------\n",
      "tensor(43.8179)\n",
      "tensor(18.8281)\n",
      "tensor(8.2295)\n",
      "tensor(0.2127)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.096127\n",
      "Epoch 5187\n",
      "-------------------------------\n",
      "tensor(37.1771)\n",
      "tensor(14.1671)\n",
      "tensor(3.9190)\n",
      "tensor(0.1230)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.085453\n",
      "Epoch 5188\n",
      "-------------------------------\n",
      "tensor(36.9172)\n",
      "tensor(13.4920)\n",
      "tensor(6.1977)\n",
      "tensor(0.2581)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.078857\n",
      "Epoch 5189\n",
      "-------------------------------\n",
      "tensor(48.2219)\n",
      "tensor(25.9946)\n",
      "tensor(9.2726)\n",
      "tensor(0.0508)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.157669\n",
      "Epoch 5190\n",
      "-------------------------------\n",
      "tensor(37.1511)\n",
      "tensor(32.7320)\n",
      "tensor(24.8564)\n",
      "tensor(0.4693)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.100647\n",
      "Epoch 5191\n",
      "-------------------------------\n",
      "tensor(43.6883)\n",
      "tensor(24.6914)\n",
      "tensor(11.0294)\n",
      "tensor(0.7435)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 42.134193\n",
      "Epoch 5192\n",
      "-------------------------------\n",
      "tensor(34.9545)\n",
      "tensor(26.2978)\n",
      "tensor(23.2466)\n",
      "tensor(0.7184)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.146069\n",
      "Epoch 5193\n",
      "-------------------------------\n",
      "tensor(53.2857)\n",
      "tensor(13.9733)\n",
      "tensor(25.8396)\n",
      "tensor(0.9293)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.106655\n",
      "Epoch 5194\n",
      "-------------------------------\n",
      "tensor(55.9236)\n",
      "tensor(18.9701)\n",
      "tensor(22.2381)\n",
      "tensor(0.4945)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.010796\n",
      "Epoch 5195\n",
      "-------------------------------\n",
      "tensor(46.0221)\n",
      "tensor(27.9285)\n",
      "tensor(18.2648)\n",
      "tensor(0.2005)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.981285\n",
      "Epoch 5196\n",
      "-------------------------------\n",
      "tensor(56.0225)\n",
      "tensor(12.4213)\n",
      "tensor(22.7540)\n",
      "tensor(0.3366)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.019249\n",
      "Epoch 5197\n",
      "-------------------------------\n",
      "tensor(46.3949)\n",
      "tensor(22.7755)\n",
      "tensor(24.0750)\n",
      "tensor(0.7366)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.994549\n",
      "Epoch 5198\n",
      "-------------------------------\n",
      "tensor(35.6271)\n",
      "tensor(16.6160)\n",
      "tensor(22.7696)\n",
      "tensor(0.1102)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.963112\n",
      "Epoch 5199\n",
      "-------------------------------\n",
      "tensor(42.1706)\n",
      "tensor(27.0560)\n",
      "tensor(10.0394)\n",
      "tensor(0.6364)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.929688\n",
      "Epoch 5200\n",
      "-------------------------------\n",
      "tensor(35.6555)\n",
      "tensor(14.1723)\n",
      "tensor(18.4870)\n",
      "tensor(0.8944)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.928188\n",
      "Epoch 5201\n",
      "-------------------------------\n",
      "tensor(40.7690)\n",
      "tensor(13.3917)\n",
      "tensor(20.0758)\n",
      "tensor(0.7759)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.919338\n",
      "Epoch 5202\n",
      "-------------------------------\n",
      "tensor(28.9278)\n",
      "tensor(27.8446)\n",
      "tensor(14.5818)\n",
      "tensor(0.4665)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.908951\n",
      "Epoch 5203\n",
      "-------------------------------\n",
      "tensor(31.1901)\n",
      "tensor(23.2624)\n",
      "tensor(5.6705)\n",
      "tensor(0.0465)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.893524\n",
      "Epoch 5204\n",
      "-------------------------------\n",
      "tensor(33.5503)\n",
      "tensor(19.5744)\n",
      "tensor(11.9788)\n",
      "tensor(0.7859)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.906185\n",
      "Epoch 5205\n",
      "-------------------------------\n",
      "tensor(47.4396)\n",
      "tensor(30.5239)\n",
      "tensor(25.4004)\n",
      "tensor(1.3901)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.933926\n",
      "Epoch 5206\n",
      "-------------------------------\n",
      "tensor(35.0099)\n",
      "tensor(23.2640)\n",
      "tensor(15.7188)\n",
      "tensor(0.9693)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.876770\n",
      "Epoch 5207\n",
      "-------------------------------\n",
      "tensor(37.6471)\n",
      "tensor(28.5135)\n",
      "tensor(16.4049)\n",
      "tensor(0.5384)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.940723\n",
      "Epoch 5208\n",
      "-------------------------------\n",
      "tensor(35.6827)\n",
      "tensor(32.5705)\n",
      "tensor(25.5454)\n",
      "tensor(1.0350)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.970207\n",
      "Epoch 5209\n",
      "-------------------------------\n",
      "tensor(31.8671)\n",
      "tensor(23.9785)\n",
      "tensor(19.5394)\n",
      "tensor(0.8461)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.912971\n",
      "Epoch 5210\n",
      "-------------------------------\n",
      "tensor(41.7042)\n",
      "tensor(26.5744)\n",
      "tensor(18.4607)\n",
      "tensor(1.0162)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.924652\n",
      "Epoch 5211\n",
      "-------------------------------\n",
      "tensor(50.5330)\n",
      "tensor(14.9547)\n",
      "tensor(29.4864)\n",
      "tensor(0.8547)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.883812\n",
      "Epoch 5212\n",
      "-------------------------------\n",
      "tensor(84.0220)\n",
      "tensor(21.2947)\n",
      "tensor(37.5092)\n",
      "tensor(0.2943)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.086605\n",
      "Epoch 5213\n",
      "-------------------------------\n",
      "tensor(61.9716)\n",
      "tensor(44.0966)\n",
      "tensor(53.5574)\n",
      "tensor(1.3812)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.031944\n",
      "Epoch 5214\n",
      "-------------------------------\n",
      "tensor(39.3577)\n",
      "tensor(30.1696)\n",
      "tensor(23.7373)\n",
      "tensor(1.5110)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.873848\n",
      "Epoch 5215\n",
      "-------------------------------\n",
      "tensor(50.9490)\n",
      "tensor(34.6298)\n",
      "tensor(38.4458)\n",
      "tensor(1.5052)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.925903\n",
      "Epoch 5216\n",
      "-------------------------------\n",
      "tensor(49.8741)\n",
      "tensor(13.7449)\n",
      "tensor(28.6796)\n",
      "tensor(1.0563)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.860580\n",
      "Epoch 5217\n",
      "-------------------------------\n",
      "tensor(27.7726)\n",
      "tensor(31.4317)\n",
      "tensor(20.5868)\n",
      "tensor(1.0910)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.807087\n",
      "Epoch 5218\n",
      "-------------------------------\n",
      "tensor(38.2301)\n",
      "tensor(21.5721)\n",
      "tensor(7.1139)\n",
      "tensor(0.0166)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.783092\n",
      "Epoch 5219\n",
      "-------------------------------\n",
      "tensor(21.7269)\n",
      "tensor(16.4483)\n",
      "tensor(11.1887)\n",
      "tensor(0.6037)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.766907\n",
      "Epoch 5220\n",
      "-------------------------------\n",
      "tensor(34.2258)\n",
      "tensor(19.4781)\n",
      "tensor(11.3446)\n",
      "tensor(0.7370)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.770706\n",
      "Epoch 5221\n",
      "-------------------------------\n",
      "tensor(41.7860)\n",
      "tensor(20.1103)\n",
      "tensor(9.2196)\n",
      "tensor(0.6488)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.760231\n",
      "Epoch 5222\n",
      "-------------------------------\n",
      "tensor(34.9804)\n",
      "tensor(16.2944)\n",
      "tensor(6.5410)\n",
      "tensor(0.4741)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.742928\n",
      "Epoch 5223\n",
      "-------------------------------\n",
      "tensor(32.5205)\n",
      "tensor(21.9521)\n",
      "tensor(3.9748)\n",
      "tensor(0.1994)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.734283\n",
      "Epoch 5224\n",
      "-------------------------------\n",
      "tensor(33.0021)\n",
      "tensor(25.3167)\n",
      "tensor(7.7934)\n",
      "tensor(0.2120)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.737717\n",
      "Epoch 5225\n",
      "-------------------------------\n",
      "tensor(40.2194)\n",
      "tensor(12.9828)\n",
      "tensor(14.8369)\n",
      "tensor(0.5823)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.735012\n",
      "Epoch 5226\n",
      "-------------------------------\n",
      "tensor(34.2519)\n",
      "tensor(13.6927)\n",
      "tensor(6.4919)\n",
      "tensor(0.3509)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.713669\n",
      "Epoch 5227\n",
      "-------------------------------\n",
      "tensor(58.4874)\n",
      "tensor(21.0185)\n",
      "tensor(18.7954)\n",
      "tensor(0.3223)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.787594\n",
      "Epoch 5228\n",
      "-------------------------------\n",
      "tensor(57.7264)\n",
      "tensor(16.3408)\n",
      "tensor(18.4487)\n",
      "tensor(0.1121)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.780357\n",
      "Epoch 5229\n",
      "-------------------------------\n",
      "tensor(41.9775)\n",
      "tensor(24.0317)\n",
      "tensor(9.7372)\n",
      "tensor(0.6921)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.740582\n",
      "Epoch 5230\n",
      "-------------------------------\n",
      "tensor(29.5069)\n",
      "tensor(18.1361)\n",
      "tensor(7.2872)\n",
      "tensor(0.4548)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.732052\n",
      "Epoch 5231\n",
      "-------------------------------\n",
      "tensor(28.1100)\n",
      "tensor(14.9570)\n",
      "tensor(12.1753)\n",
      "tensor(0.5125)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.731651\n",
      "Epoch 5232\n",
      "-------------------------------\n",
      "tensor(42.6761)\n",
      "tensor(29.4082)\n",
      "tensor(11.5529)\n",
      "tensor(0.7013)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.732437\n",
      "Epoch 5233\n",
      "-------------------------------\n",
      "tensor(56.2581)\n",
      "tensor(14.8170)\n",
      "tensor(22.4116)\n",
      "tensor(0.6124)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.766766\n",
      "Epoch 5234\n",
      "-------------------------------\n",
      "tensor(77.8023)\n",
      "tensor(22.5836)\n",
      "tensor(46.4640)\n",
      "tensor(1.7351)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.754745\n",
      "Epoch 5235\n",
      "-------------------------------\n",
      "tensor(69.0289)\n",
      "tensor(17.9140)\n",
      "tensor(42.6565)\n",
      "tensor(1.3193)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.801289\n",
      "Epoch 5236\n",
      "-------------------------------\n",
      "tensor(35.4833)\n",
      "tensor(14.3924)\n",
      "tensor(11.3231)\n",
      "tensor(0.6488)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.715790\n",
      "Epoch 5237\n",
      "-------------------------------\n",
      "tensor(55.0873)\n",
      "tensor(23.5952)\n",
      "tensor(33.9057)\n",
      "tensor(0.5942)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.695049\n",
      "Epoch 5238\n",
      "-------------------------------\n",
      "tensor(43.0212)\n",
      "tensor(19.5810)\n",
      "tensor(12.9338)\n",
      "tensor(0.4385)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.628731\n",
      "Epoch 5239\n",
      "-------------------------------\n",
      "tensor(35.2225)\n",
      "tensor(26.0626)\n",
      "tensor(17.5974)\n",
      "tensor(0.0288)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.617382\n",
      "Epoch 5240\n",
      "-------------------------------\n",
      "tensor(56.4831)\n",
      "tensor(9.8172)\n",
      "tensor(26.0012)\n",
      "tensor(0.1190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.641953\n",
      "Epoch 5241\n",
      "-------------------------------\n",
      "tensor(46.5840)\n",
      "tensor(9.5506)\n",
      "tensor(18.6794)\n",
      "tensor(0.0148)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.614941\n",
      "Epoch 5242\n",
      "-------------------------------\n",
      "tensor(33.3418)\n",
      "tensor(12.3134)\n",
      "tensor(6.4083)\n",
      "tensor(0.2102)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.580849\n",
      "Epoch 5243\n",
      "-------------------------------\n",
      "tensor(52.3432)\n",
      "tensor(21.2535)\n",
      "tensor(13.6341)\n",
      "tensor(0.4324)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.580448\n",
      "Epoch 5244\n",
      "-------------------------------\n",
      "tensor(70.1135)\n",
      "tensor(21.0357)\n",
      "tensor(26.5283)\n",
      "tensor(0.5242)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.597088\n",
      "Epoch 5245\n",
      "-------------------------------\n",
      "tensor(42.3570)\n",
      "tensor(17.8913)\n",
      "tensor(13.1655)\n",
      "tensor(0.0804)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.590618\n",
      "Epoch 5246\n",
      "-------------------------------\n",
      "tensor(54.6077)\n",
      "tensor(14.3395)\n",
      "tensor(22.1168)\n",
      "tensor(0.6718)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.615791\n",
      "Epoch 5247\n",
      "-------------------------------\n",
      "tensor(50.0640)\n",
      "tensor(12.7554)\n",
      "tensor(18.4460)\n",
      "tensor(0.2743)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.603214\n",
      "Epoch 5248\n",
      "-------------------------------\n",
      "tensor(69.0868)\n",
      "tensor(20.0499)\n",
      "tensor(35.5579)\n",
      "tensor(1.2710)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.700211\n",
      "Epoch 5249\n",
      "-------------------------------\n",
      "tensor(37.6430)\n",
      "tensor(29.0896)\n",
      "tensor(18.5402)\n",
      "tensor(0.1913)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.611973\n",
      "Epoch 5250\n",
      "-------------------------------\n",
      "tensor(59.0531)\n",
      "tensor(13.4038)\n",
      "tensor(29.4550)\n",
      "tensor(0.5635)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.657642\n",
      "Epoch 5251\n",
      "-------------------------------\n",
      "tensor(96.4883)\n",
      "tensor(41.4748)\n",
      "tensor(71.0680)\n",
      "tensor(1.6376)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.870037\n",
      "Epoch 5252\n",
      "-------------------------------\n",
      "tensor(108.9554)\n",
      "tensor(29.2205)\n",
      "tensor(74.3892)\n",
      "tensor(2.0337)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.839802\n",
      "Epoch 5253\n",
      "-------------------------------\n",
      "tensor(94.2790)\n",
      "tensor(21.7341)\n",
      "tensor(54.4427)\n",
      "tensor(1.6421)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.736965\n",
      "Epoch 5254\n",
      "-------------------------------\n",
      "tensor(41.3907)\n",
      "tensor(29.8632)\n",
      "tensor(25.2491)\n",
      "tensor(0.1968)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.570366\n",
      "Epoch 5255\n",
      "-------------------------------\n",
      "tensor(64.7323)\n",
      "tensor(12.7046)\n",
      "tensor(31.5108)\n",
      "tensor(0.4417)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.674477\n",
      "Epoch 5256\n",
      "-------------------------------\n",
      "tensor(67.9302)\n",
      "tensor(31.7692)\n",
      "tensor(46.6409)\n",
      "tensor(1.0863)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.724609\n",
      "Epoch 5257\n",
      "-------------------------------\n",
      "tensor(44.6921)\n",
      "tensor(19.8399)\n",
      "tensor(19.6986)\n",
      "tensor(0.1828)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.589760\n",
      "Epoch 5258\n",
      "-------------------------------\n",
      "tensor(45.3990)\n",
      "tensor(12.5056)\n",
      "tensor(25.0909)\n",
      "tensor(0.8190)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.550583\n",
      "Epoch 5259\n",
      "-------------------------------\n",
      "tensor(45.4361)\n",
      "tensor(12.0033)\n",
      "tensor(24.8049)\n",
      "tensor(0.6971)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.516521\n",
      "Epoch 5260\n",
      "-------------------------------\n",
      "tensor(31.8383)\n",
      "tensor(25.5825)\n",
      "tensor(7.7774)\n",
      "tensor(0.1749)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.497211\n",
      "Epoch 5261\n",
      "-------------------------------\n",
      "tensor(38.1190)\n",
      "tensor(22.0727)\n",
      "tensor(5.7117)\n",
      "tensor(0.2071)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.502277\n",
      "Epoch 5262\n",
      "-------------------------------\n",
      "tensor(41.5441)\n",
      "tensor(20.3843)\n",
      "tensor(10.6659)\n",
      "tensor(0.4212)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.491497\n",
      "Epoch 5263\n",
      "-------------------------------\n",
      "tensor(39.9317)\n",
      "tensor(19.6396)\n",
      "tensor(10.9891)\n",
      "tensor(0.5035)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.454365\n",
      "Epoch 5264\n",
      "-------------------------------\n",
      "tensor(25.8109)\n",
      "tensor(14.0015)\n",
      "tensor(5.0738)\n",
      "tensor(0.4120)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.449436\n",
      "Epoch 5265\n",
      "-------------------------------\n",
      "tensor(44.2245)\n",
      "tensor(15.8266)\n",
      "tensor(7.9389)\n",
      "tensor(0.1356)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.495415\n",
      "Epoch 5266\n",
      "-------------------------------\n",
      "tensor(54.8547)\n",
      "tensor(19.6565)\n",
      "tensor(6.0485)\n",
      "tensor(0.0056)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.540874\n",
      "Epoch 5267\n",
      "-------------------------------\n",
      "tensor(49.6901)\n",
      "tensor(21.2620)\n",
      "tensor(12.6549)\n",
      "tensor(0.2040)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.532314\n",
      "Epoch 5268\n",
      "-------------------------------\n",
      "tensor(46.7544)\n",
      "tensor(17.8254)\n",
      "tensor(5.8406)\n",
      "tensor(0.1587)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.423218\n",
      "Epoch 5269\n",
      "-------------------------------\n",
      "tensor(43.0021)\n",
      "tensor(31.7232)\n",
      "tensor(17.6962)\n",
      "tensor(0.1063)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.568611\n",
      "Epoch 5270\n",
      "-------------------------------\n",
      "tensor(41.8081)\n",
      "tensor(29.3036)\n",
      "tensor(13.4296)\n",
      "tensor(0.1276)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.601009\n",
      "Epoch 5271\n",
      "-------------------------------\n",
      "tensor(41.8159)\n",
      "tensor(22.3474)\n",
      "tensor(8.2588)\n",
      "tensor(0.0980)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.445122\n",
      "Epoch 5272\n",
      "-------------------------------\n",
      "tensor(56.0985)\n",
      "tensor(13.3321)\n",
      "tensor(21.8782)\n",
      "tensor(0.3569)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.629059\n",
      "Epoch 5273\n",
      "-------------------------------\n",
      "tensor(52.0974)\n",
      "tensor(39.8410)\n",
      "tensor(45.1341)\n",
      "tensor(1.4451)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.749161\n",
      "Epoch 5274\n",
      "-------------------------------\n",
      "tensor(55.4718)\n",
      "tensor(14.5001)\n",
      "tensor(20.0519)\n",
      "tensor(0.3115)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.628395\n",
      "Epoch 5275\n",
      "-------------------------------\n",
      "tensor(32.7047)\n",
      "tensor(13.8431)\n",
      "tensor(2.8524)\n",
      "tensor(0.0456)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.468239\n",
      "Epoch 5276\n",
      "-------------------------------\n",
      "tensor(43.4490)\n",
      "tensor(15.2190)\n",
      "tensor(21.5055)\n",
      "tensor(0.3386)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.433609\n",
      "Epoch 5277\n",
      "-------------------------------\n",
      "tensor(49.2983)\n",
      "tensor(32.1343)\n",
      "tensor(8.8418)\n",
      "tensor(0.3888)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.414772\n",
      "Epoch 5278\n",
      "-------------------------------\n",
      "tensor(43.7914)\n",
      "tensor(39.2338)\n",
      "tensor(29.1006)\n",
      "tensor(0.8080)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.417141\n",
      "Epoch 5279\n",
      "-------------------------------\n",
      "tensor(44.7798)\n",
      "tensor(34.9575)\n",
      "tensor(25.3726)\n",
      "tensor(0.4972)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.359032\n",
      "Epoch 5280\n",
      "-------------------------------\n",
      "tensor(42.7031)\n",
      "tensor(12.5863)\n",
      "tensor(13.4093)\n",
      "tensor(0.0050)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.317593\n",
      "Epoch 5281\n",
      "-------------------------------\n",
      "tensor(36.8171)\n",
      "tensor(14.3964)\n",
      "tensor(6.6084)\n",
      "tensor(0.3455)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.297039\n",
      "Epoch 5282\n",
      "-------------------------------\n",
      "tensor(35.9245)\n",
      "tensor(17.1488)\n",
      "tensor(12.3269)\n",
      "tensor(0.5384)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.280464\n",
      "Epoch 5283\n",
      "-------------------------------\n",
      "tensor(34.7214)\n",
      "tensor(18.3428)\n",
      "tensor(18.0441)\n",
      "tensor(0.5842)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.285561\n",
      "Epoch 5284\n",
      "-------------------------------\n",
      "tensor(29.8019)\n",
      "tensor(16.6071)\n",
      "tensor(17.1229)\n",
      "tensor(0.3307)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.277069\n",
      "Epoch 5285\n",
      "-------------------------------\n",
      "tensor(43.3036)\n",
      "tensor(23.9219)\n",
      "tensor(8.4386)\n",
      "tensor(0.3619)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.270699\n",
      "Epoch 5286\n",
      "-------------------------------\n",
      "tensor(57.1688)\n",
      "tensor(13.2422)\n",
      "tensor(29.4018)\n",
      "tensor(0.9581)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.298653\n",
      "Epoch 5287\n",
      "-------------------------------\n",
      "tensor(40.9016)\n",
      "tensor(23.0853)\n",
      "tensor(6.6434)\n",
      "tensor(0.1534)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.355843\n",
      "Epoch 5288\n",
      "-------------------------------\n",
      "tensor(47.3554)\n",
      "tensor(25.7787)\n",
      "tensor(29.4916)\n",
      "tensor(1.7424)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.393368\n",
      "Epoch 5289\n",
      "-------------------------------\n",
      "tensor(58.0205)\n",
      "tensor(21.7925)\n",
      "tensor(17.1518)\n",
      "tensor(0.6700)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.400799\n",
      "Epoch 5290\n",
      "-------------------------------\n",
      "tensor(30.2705)\n",
      "tensor(17.1817)\n",
      "tensor(16.1431)\n",
      "tensor(0.1304)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.321529\n",
      "Epoch 5291\n",
      "-------------------------------\n",
      "tensor(30.2398)\n",
      "tensor(17.3260)\n",
      "tensor(16.8123)\n",
      "tensor(0.9773)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.273472\n",
      "Epoch 5292\n",
      "-------------------------------\n",
      "tensor(44.5145)\n",
      "tensor(36.3481)\n",
      "tensor(23.9338)\n",
      "tensor(0.5784)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.382118\n",
      "Epoch 5293\n",
      "-------------------------------\n",
      "tensor(55.1678)\n",
      "tensor(26.6852)\n",
      "tensor(22.1488)\n",
      "tensor(1.2373)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.305855\n",
      "Epoch 5294\n",
      "-------------------------------\n",
      "tensor(43.1407)\n",
      "tensor(19.0581)\n",
      "tensor(9.4878)\n",
      "tensor(0.6763)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.287720\n",
      "Epoch 5295\n",
      "-------------------------------\n",
      "tensor(40.0588)\n",
      "tensor(17.2830)\n",
      "tensor(14.9120)\n",
      "tensor(0.5013)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.331474\n",
      "Epoch 5296\n",
      "-------------------------------\n",
      "tensor(39.7158)\n",
      "tensor(17.4066)\n",
      "tensor(16.4476)\n",
      "tensor(0.8342)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.312256\n",
      "Epoch 5297\n",
      "-------------------------------\n",
      "tensor(26.7501)\n",
      "tensor(12.8841)\n",
      "tensor(5.4165)\n",
      "tensor(0.1481)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.252773\n",
      "Epoch 5298\n",
      "-------------------------------\n",
      "tensor(23.8076)\n",
      "tensor(14.9169)\n",
      "tensor(7.8681)\n",
      "tensor(0.5648)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.202629\n",
      "Epoch 5299\n",
      "-------------------------------\n",
      "tensor(38.6793)\n",
      "tensor(19.2897)\n",
      "tensor(12.0393)\n",
      "tensor(0.7372)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.162167\n",
      "Epoch 5300\n",
      "-------------------------------\n",
      "tensor(36.0116)\n",
      "tensor(20.4502)\n",
      "tensor(8.4282)\n",
      "tensor(0.5698)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.161980\n",
      "Epoch 5301\n",
      "-------------------------------\n",
      "tensor(32.9066)\n",
      "tensor(21.7223)\n",
      "tensor(6.5426)\n",
      "tensor(0.3373)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 41.142143\n",
      "Epoch 5302\n",
      "-------------------------------\n",
      "tensor(31.3889)\n",
      "tensor(9.8847)\n",
      "tensor(8.3163)\n",
      "tensor(0.1351)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.133686\n",
      "Epoch 5303\n",
      "-------------------------------\n",
      "tensor(29.8984)\n",
      "tensor(9.8018)\n",
      "tensor(7.9080)\n",
      "tensor(0.0516)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.123520\n",
      "Epoch 5304\n",
      "-------------------------------\n",
      "tensor(27.3791)\n",
      "tensor(13.0296)\n",
      "tensor(4.8101)\n",
      "tensor(0.1954)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.103642\n",
      "Epoch 5305\n",
      "-------------------------------\n",
      "tensor(59.1551)\n",
      "tensor(23.6611)\n",
      "tensor(14.6283)\n",
      "tensor(0.2423)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.120441\n",
      "Epoch 5306\n",
      "-------------------------------\n",
      "tensor(58.9373)\n",
      "tensor(18.4236)\n",
      "tensor(7.6443)\n",
      "tensor(0.3047)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.166634\n",
      "Epoch 5307\n",
      "-------------------------------\n",
      "tensor(56.1216)\n",
      "tensor(16.4376)\n",
      "tensor(16.3815)\n",
      "tensor(0.0721)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.161522\n",
      "Epoch 5308\n",
      "-------------------------------\n",
      "tensor(47.9743)\n",
      "tensor(23.3642)\n",
      "tensor(17.9909)\n",
      "tensor(1.2047)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.136028\n",
      "Epoch 5309\n",
      "-------------------------------\n",
      "tensor(36.2393)\n",
      "tensor(22.1333)\n",
      "tensor(8.6484)\n",
      "tensor(0.4906)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.117439\n",
      "Epoch 5310\n",
      "-------------------------------\n",
      "tensor(37.7336)\n",
      "tensor(13.2438)\n",
      "tensor(21.3647)\n",
      "tensor(0.9274)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.171051\n",
      "Epoch 5311\n",
      "-------------------------------\n",
      "tensor(62.8442)\n",
      "tensor(24.4843)\n",
      "tensor(36.3675)\n",
      "tensor(0.2603)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.285862\n",
      "Epoch 5312\n",
      "-------------------------------\n",
      "tensor(66.0669)\n",
      "tensor(12.7160)\n",
      "tensor(32.4777)\n",
      "tensor(0.4420)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.249790\n",
      "Epoch 5313\n",
      "-------------------------------\n",
      "tensor(33.5190)\n",
      "tensor(23.9472)\n",
      "tensor(33.1478)\n",
      "tensor(1.4424)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.147125\n",
      "Epoch 5314\n",
      "-------------------------------\n",
      "tensor(43.0790)\n",
      "tensor(28.6606)\n",
      "tensor(6.0152)\n",
      "tensor(0.3890)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.155102\n",
      "Epoch 5315\n",
      "-------------------------------\n",
      "tensor(44.1311)\n",
      "tensor(43.9931)\n",
      "tensor(44.9559)\n",
      "tensor(1.6278)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.173412\n",
      "Epoch 5316\n",
      "-------------------------------\n",
      "tensor(35.1675)\n",
      "tensor(19.9332)\n",
      "tensor(12.5816)\n",
      "tensor(0.5747)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.040573\n",
      "Epoch 5317\n",
      "-------------------------------\n",
      "tensor(49.3454)\n",
      "tensor(32.7280)\n",
      "tensor(36.0321)\n",
      "tensor(1.5026)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.069977\n",
      "Epoch 5318\n",
      "-------------------------------\n",
      "tensor(32.7030)\n",
      "tensor(18.7888)\n",
      "tensor(8.0789)\n",
      "tensor(0.6093)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.998161\n",
      "Epoch 5319\n",
      "-------------------------------\n",
      "tensor(37.7069)\n",
      "tensor(9.2624)\n",
      "tensor(13.9083)\n",
      "tensor(0.2061)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.971180\n",
      "Epoch 5320\n",
      "-------------------------------\n",
      "tensor(28.4378)\n",
      "tensor(26.2490)\n",
      "tensor(14.3387)\n",
      "tensor(0.4359)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.972549\n",
      "Epoch 5321\n",
      "-------------------------------\n",
      "tensor(34.0988)\n",
      "tensor(25.9486)\n",
      "tensor(10.5302)\n",
      "tensor(0.3821)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.970356\n",
      "Epoch 5322\n",
      "-------------------------------\n",
      "tensor(29.8040)\n",
      "tensor(23.7025)\n",
      "tensor(6.1396)\n",
      "tensor(0.2245)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.954105\n",
      "Epoch 5323\n",
      "-------------------------------\n",
      "tensor(41.3880)\n",
      "tensor(15.5241)\n",
      "tensor(3.1304)\n",
      "tensor(0.0300)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.936493\n",
      "Epoch 5324\n",
      "-------------------------------\n",
      "tensor(38.0557)\n",
      "tensor(18.1158)\n",
      "tensor(9.5171)\n",
      "tensor(0.4215)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.950981\n",
      "Epoch 5325\n",
      "-------------------------------\n",
      "tensor(35.9934)\n",
      "tensor(19.9800)\n",
      "tensor(14.7091)\n",
      "tensor(0.7132)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.951477\n",
      "Epoch 5326\n",
      "-------------------------------\n",
      "tensor(38.3117)\n",
      "tensor(16.3418)\n",
      "tensor(6.5851)\n",
      "tensor(0.4244)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.950607\n",
      "Epoch 5327\n",
      "-------------------------------\n",
      "tensor(38.0080)\n",
      "tensor(28.6206)\n",
      "tensor(11.8926)\n",
      "tensor(0.3021)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.992905\n",
      "Epoch 5328\n",
      "-------------------------------\n",
      "tensor(35.7209)\n",
      "tensor(31.0802)\n",
      "tensor(17.0880)\n",
      "tensor(0.5375)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.007774\n",
      "Epoch 5329\n",
      "-------------------------------\n",
      "tensor(29.7297)\n",
      "tensor(17.6310)\n",
      "tensor(8.9231)\n",
      "tensor(0.5273)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.012859\n",
      "Epoch 5330\n",
      "-------------------------------\n",
      "tensor(31.0245)\n",
      "tensor(20.8322)\n",
      "tensor(18.3086)\n",
      "tensor(0.6965)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.059589\n",
      "Epoch 5331\n",
      "-------------------------------\n",
      "tensor(49.8710)\n",
      "tensor(14.2976)\n",
      "tensor(21.5985)\n",
      "tensor(0.7779)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.046261\n",
      "Epoch 5332\n",
      "-------------------------------\n",
      "tensor(28.9486)\n",
      "tensor(14.5829)\n",
      "tensor(18.9881)\n",
      "tensor(0.3572)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.964104\n",
      "Epoch 5333\n",
      "-------------------------------\n",
      "tensor(43.3647)\n",
      "tensor(27.5008)\n",
      "tensor(7.0977)\n",
      "tensor(0.0289)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.035667\n",
      "Epoch 5334\n",
      "-------------------------------\n",
      "tensor(45.9307)\n",
      "tensor(32.4134)\n",
      "tensor(31.9111)\n",
      "tensor(0.3462)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 41.033974\n",
      "Epoch 5335\n",
      "-------------------------------\n",
      "tensor(30.5842)\n",
      "tensor(23.6442)\n",
      "tensor(25.2111)\n",
      "tensor(1.0403)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.896767\n",
      "Epoch 5336\n",
      "-------------------------------\n",
      "tensor(48.4572)\n",
      "tensor(22.4618)\n",
      "tensor(18.9435)\n",
      "tensor(0.3829)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.927132\n",
      "Epoch 5337\n",
      "-------------------------------\n",
      "tensor(50.1034)\n",
      "tensor(13.3957)\n",
      "tensor(18.4906)\n",
      "tensor(0.6394)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.879219\n",
      "Epoch 5338\n",
      "-------------------------------\n",
      "tensor(34.2647)\n",
      "tensor(25.9811)\n",
      "tensor(14.3652)\n",
      "tensor(0.4113)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.854507\n",
      "Epoch 5339\n",
      "-------------------------------\n",
      "tensor(38.7595)\n",
      "tensor(22.7150)\n",
      "tensor(4.0101)\n",
      "tensor(0.1937)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.847641\n",
      "Epoch 5340\n",
      "-------------------------------\n",
      "tensor(32.1797)\n",
      "tensor(17.6379)\n",
      "tensor(7.0432)\n",
      "tensor(0.5623)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.846664\n",
      "Epoch 5341\n",
      "-------------------------------\n",
      "tensor(26.8387)\n",
      "tensor(17.9691)\n",
      "tensor(9.7673)\n",
      "tensor(0.6842)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.842159\n",
      "Epoch 5342\n",
      "-------------------------------\n",
      "tensor(25.6382)\n",
      "tensor(16.1512)\n",
      "tensor(10.6457)\n",
      "tensor(0.6556)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.831188\n",
      "Epoch 5343\n",
      "-------------------------------\n",
      "tensor(19.9658)\n",
      "tensor(13.6194)\n",
      "tensor(9.5427)\n",
      "tensor(0.4647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.814064\n",
      "Epoch 5344\n",
      "-------------------------------\n",
      "tensor(46.1690)\n",
      "tensor(23.1550)\n",
      "tensor(6.1826)\n",
      "tensor(0.0259)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.840797\n",
      "Epoch 5345\n",
      "-------------------------------\n",
      "tensor(39.7088)\n",
      "tensor(29.4631)\n",
      "tensor(12.4760)\n",
      "tensor(0.6809)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.834915\n",
      "Epoch 5346\n",
      "-------------------------------\n",
      "tensor(45.1169)\n",
      "tensor(14.4849)\n",
      "tensor(26.3934)\n",
      "tensor(0.9656)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.833641\n",
      "Epoch 5347\n",
      "-------------------------------\n",
      "tensor(39.5684)\n",
      "tensor(16.4074)\n",
      "tensor(6.9626)\n",
      "tensor(0.2607)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.875790\n",
      "Epoch 5348\n",
      "-------------------------------\n",
      "tensor(47.5729)\n",
      "tensor(26.8389)\n",
      "tensor(31.8595)\n",
      "tensor(1.2520)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.901829\n",
      "Epoch 5349\n",
      "-------------------------------\n",
      "tensor(41.8500)\n",
      "tensor(31.4282)\n",
      "tensor(24.5069)\n",
      "tensor(0.4109)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.920418\n",
      "Epoch 5350\n",
      "-------------------------------\n",
      "tensor(38.0007)\n",
      "tensor(29.3615)\n",
      "tensor(19.8898)\n",
      "tensor(0.4624)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.805168\n",
      "Epoch 5351\n",
      "-------------------------------\n",
      "tensor(44.6746)\n",
      "tensor(30.0216)\n",
      "tensor(33.3377)\n",
      "tensor(1.2720)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.888729\n",
      "Epoch 5352\n",
      "-------------------------------\n",
      "tensor(47.8271)\n",
      "tensor(10.8972)\n",
      "tensor(19.5286)\n",
      "tensor(0.3182)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.932747\n",
      "Epoch 5353\n",
      "-------------------------------\n",
      "tensor(38.8136)\n",
      "tensor(18.0542)\n",
      "tensor(18.2033)\n",
      "tensor(0.1489)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.904106\n",
      "Epoch 5354\n",
      "-------------------------------\n",
      "tensor(27.8351)\n",
      "tensor(12.5673)\n",
      "tensor(3.8208)\n",
      "tensor(0.2051)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.813896\n",
      "Epoch 5355\n",
      "-------------------------------\n",
      "tensor(41.6368)\n",
      "tensor(30.1951)\n",
      "tensor(12.8764)\n",
      "tensor(0.2081)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.902245\n",
      "Epoch 5356\n",
      "-------------------------------\n",
      "tensor(47.1109)\n",
      "tensor(28.7272)\n",
      "tensor(12.8797)\n",
      "tensor(0.1552)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.888420\n",
      "Epoch 5357\n",
      "-------------------------------\n",
      "tensor(36.4976)\n",
      "tensor(13.2456)\n",
      "tensor(8.4169)\n",
      "tensor(0.3986)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.763355\n",
      "Epoch 5358\n",
      "-------------------------------\n",
      "tensor(28.2213)\n",
      "tensor(14.9980)\n",
      "tensor(9.1011)\n",
      "tensor(0.4041)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.775337\n",
      "Epoch 5359\n",
      "-------------------------------\n",
      "tensor(30.0825)\n",
      "tensor(14.7725)\n",
      "tensor(10.8504)\n",
      "tensor(0.1501)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.771805\n",
      "Epoch 5360\n",
      "-------------------------------\n",
      "tensor(25.7809)\n",
      "tensor(12.7513)\n",
      "tensor(8.4365)\n",
      "tensor(0.0996)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.749352\n",
      "Epoch 5361\n",
      "-------------------------------\n",
      "tensor(35.8754)\n",
      "tensor(23.0995)\n",
      "tensor(6.6705)\n",
      "tensor(0.2412)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.741058\n",
      "Epoch 5362\n",
      "-------------------------------\n",
      "tensor(30.9428)\n",
      "tensor(23.4013)\n",
      "tensor(6.4425)\n",
      "tensor(0.3034)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.732368\n",
      "Epoch 5363\n",
      "-------------------------------\n",
      "tensor(30.6547)\n",
      "tensor(9.5333)\n",
      "tensor(10.0439)\n",
      "tensor(0.2656)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.734009\n",
      "Epoch 5364\n",
      "-------------------------------\n",
      "tensor(33.9053)\n",
      "tensor(9.8598)\n",
      "tensor(9.9111)\n",
      "tensor(0.0049)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.725574\n",
      "Epoch 5365\n",
      "-------------------------------\n",
      "tensor(28.3487)\n",
      "tensor(17.2185)\n",
      "tensor(7.0102)\n",
      "tensor(0.5652)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.697639\n",
      "Epoch 5366\n",
      "-------------------------------\n",
      "tensor(66.0417)\n",
      "tensor(24.2101)\n",
      "tensor(22.3925)\n",
      "tensor(0.9829)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.784851\n",
      "Epoch 5367\n",
      "-------------------------------\n",
      "tensor(49.0431)\n",
      "tensor(26.6636)\n",
      "tensor(9.7602)\n",
      "tensor(0.1583)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.727852\n",
      "Epoch 5368\n",
      "-------------------------------\n",
      "tensor(67.8927)\n",
      "tensor(11.6275)\n",
      "tensor(35.8656)\n",
      "tensor(0.4903)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.750790\n",
      "Epoch 5369\n",
      "-------------------------------\n",
      "tensor(66.6209)\n",
      "tensor(28.0809)\n",
      "tensor(46.7446)\n",
      "tensor(1.0315)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.820637\n",
      "Epoch 5370\n",
      "-------------------------------\n",
      "tensor(35.5733)\n",
      "tensor(15.2242)\n",
      "tensor(16.6748)\n",
      "tensor(0.9205)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.774940\n",
      "Epoch 5371\n",
      "-------------------------------\n",
      "tensor(44.9446)\n",
      "tensor(14.6468)\n",
      "tensor(25.4847)\n",
      "tensor(0.7282)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.745834\n",
      "Epoch 5372\n",
      "-------------------------------\n",
      "tensor(96.5225)\n",
      "tensor(23.2670)\n",
      "tensor(55.2998)\n",
      "tensor(1.7914)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.942291\n",
      "Epoch 5373\n",
      "-------------------------------\n",
      "tensor(95.8363)\n",
      "tensor(52.1245)\n",
      "tensor(79.5736)\n",
      "tensor(1.8225)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.983932\n",
      "Epoch 5374\n",
      "-------------------------------\n",
      "tensor(62.4357)\n",
      "tensor(35.7434)\n",
      "tensor(45.8741)\n",
      "tensor(1.5059)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.744850\n",
      "Epoch 5375\n",
      "-------------------------------\n",
      "tensor(33.8960)\n",
      "tensor(26.3371)\n",
      "tensor(26.5071)\n",
      "tensor(0.8552)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.724155\n",
      "Epoch 5376\n",
      "-------------------------------\n",
      "tensor(69.8508)\n",
      "tensor(13.6623)\n",
      "tensor(39.6712)\n",
      "tensor(1.0447)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.774895\n",
      "Epoch 5377\n",
      "-------------------------------\n",
      "tensor(38.8892)\n",
      "tensor(13.4983)\n",
      "tensor(5.0905)\n",
      "tensor(0.1646)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.644554\n",
      "Epoch 5378\n",
      "-------------------------------\n",
      "tensor(63.4123)\n",
      "tensor(18.8857)\n",
      "tensor(28.0347)\n",
      "tensor(0.7700)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.731876\n",
      "Epoch 5379\n",
      "-------------------------------\n",
      "tensor(49.4563)\n",
      "tensor(20.4118)\n",
      "tensor(15.4359)\n",
      "tensor(0.5814)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.710560\n",
      "Epoch 5380\n",
      "-------------------------------\n",
      "tensor(33.7287)\n",
      "tensor(24.0884)\n",
      "tensor(8.0488)\n",
      "tensor(0.1405)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.666607\n",
      "Epoch 5381\n",
      "-------------------------------\n",
      "tensor(33.8748)\n",
      "tensor(26.8437)\n",
      "tensor(17.7643)\n",
      "tensor(0.1240)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.638271\n",
      "Epoch 5382\n",
      "-------------------------------\n",
      "tensor(46.7578)\n",
      "tensor(9.7577)\n",
      "tensor(21.1989)\n",
      "tensor(0.2052)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.627346\n",
      "Epoch 5383\n",
      "-------------------------------\n",
      "tensor(42.4269)\n",
      "tensor(10.7540)\n",
      "tensor(15.0824)\n",
      "tensor(0.1012)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.623867\n",
      "Epoch 5384\n",
      "-------------------------------\n",
      "tensor(28.9884)\n",
      "tensor(14.0998)\n",
      "tensor(5.5050)\n",
      "tensor(0.2365)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.606739\n",
      "Epoch 5385\n",
      "-------------------------------\n",
      "tensor(50.7564)\n",
      "tensor(22.5536)\n",
      "tensor(29.8897)\n",
      "tensor(0.5741)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.664345\n",
      "Epoch 5386\n",
      "-------------------------------\n",
      "tensor(48.3182)\n",
      "tensor(19.0889)\n",
      "tensor(22.8472)\n",
      "tensor(0.1489)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.629166\n",
      "Epoch 5387\n",
      "-------------------------------\n",
      "tensor(53.4892)\n",
      "tensor(15.1447)\n",
      "tensor(28.5644)\n",
      "tensor(0.9470)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.585060\n",
      "Epoch 5388\n",
      "-------------------------------\n",
      "tensor(42.5731)\n",
      "tensor(28.3361)\n",
      "tensor(15.7690)\n",
      "tensor(0.0444)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.632793\n",
      "Epoch 5389\n",
      "-------------------------------\n",
      "tensor(63.1577)\n",
      "tensor(25.1443)\n",
      "tensor(29.4597)\n",
      "tensor(1.5174)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.676605\n",
      "Epoch 5390\n",
      "-------------------------------\n",
      "tensor(61.4303)\n",
      "tensor(13.0325)\n",
      "tensor(29.8214)\n",
      "tensor(0.3019)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.672329\n",
      "Epoch 5391\n",
      "-------------------------------\n",
      "tensor(54.3902)\n",
      "tensor(21.7711)\n",
      "tensor(33.7342)\n",
      "tensor(0.1256)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.682354\n",
      "Epoch 5392\n",
      "-------------------------------\n",
      "tensor(49.1034)\n",
      "tensor(15.9708)\n",
      "tensor(20.8040)\n",
      "tensor(1.0170)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.590515\n",
      "Epoch 5393\n",
      "-------------------------------\n",
      "tensor(39.0962)\n",
      "tensor(22.6443)\n",
      "tensor(11.1322)\n",
      "tensor(0.6018)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.569977\n",
      "Epoch 5394\n",
      "-------------------------------\n",
      "tensor(32.1631)\n",
      "tensor(18.7169)\n",
      "tensor(17.0982)\n",
      "tensor(1.1801)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.553818\n",
      "Epoch 5395\n",
      "-------------------------------\n",
      "tensor(47.1360)\n",
      "tensor(22.4612)\n",
      "tensor(9.5834)\n",
      "tensor(0.0974)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.560581\n",
      "Epoch 5396\n",
      "-------------------------------\n",
      "tensor(50.0353)\n",
      "tensor(16.3179)\n",
      "tensor(27.0876)\n",
      "tensor(1.1988)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.607632\n",
      "Epoch 5397\n",
      "-------------------------------\n",
      "tensor(31.6352)\n",
      "tensor(12.9820)\n",
      "tensor(5.0694)\n",
      "tensor(0.1350)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.547470\n",
      "Epoch 5398\n",
      "-------------------------------\n",
      "tensor(23.0947)\n",
      "tensor(18.9173)\n",
      "tensor(20.2026)\n",
      "tensor(0.7682)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.531136\n",
      "Epoch 5399\n",
      "-------------------------------\n",
      "tensor(26.2708)\n",
      "tensor(18.0535)\n",
      "tensor(17.1311)\n",
      "tensor(0.7682)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.502110\n",
      "Epoch 5400\n",
      "-------------------------------\n",
      "tensor(36.2340)\n",
      "tensor(19.8994)\n",
      "tensor(7.4471)\n",
      "tensor(0.4354)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.491905\n",
      "Epoch 5401\n",
      "-------------------------------\n",
      "tensor(31.3257)\n",
      "tensor(22.4255)\n",
      "tensor(5.4151)\n",
      "tensor(0.1291)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.483345\n",
      "Epoch 5402\n",
      "-------------------------------\n",
      "tensor(33.6829)\n",
      "tensor(8.6627)\n",
      "tensor(11.6129)\n",
      "tensor(0.0929)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.468132\n",
      "Epoch 5403\n",
      "-------------------------------\n",
      "tensor(41.5874)\n",
      "tensor(11.4641)\n",
      "tensor(13.2019)\n",
      "tensor(0.2248)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.463833\n",
      "Epoch 5404\n",
      "-------------------------------\n",
      "tensor(42.3326)\n",
      "tensor(25.3751)\n",
      "tensor(6.3406)\n",
      "tensor(0.1842)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.447376\n",
      "Epoch 5405\n",
      "-------------------------------\n",
      "tensor(38.5431)\n",
      "tensor(14.7900)\n",
      "tensor(5.0324)\n",
      "tensor(0.0639)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.443085\n",
      "Epoch 5406\n",
      "-------------------------------\n",
      "tensor(32.1912)\n",
      "tensor(15.7555)\n",
      "tensor(12.6123)\n",
      "tensor(0.3397)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.452538\n",
      "Epoch 5407\n",
      "-------------------------------\n",
      "tensor(44.3795)\n",
      "tensor(22.8287)\n",
      "tensor(3.8462)\n",
      "tensor(0.1574)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.443981\n",
      "Epoch 5408\n",
      "-------------------------------\n",
      "tensor(57.9250)\n",
      "tensor(10.7339)\n",
      "tensor(27.5989)\n",
      "tensor(0.2556)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.480228\n",
      "Epoch 5409\n",
      "-------------------------------\n",
      "tensor(47.0884)\n",
      "tensor(23.7239)\n",
      "tensor(28.0063)\n",
      "tensor(0.8824)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.454483\n",
      "Epoch 5410\n",
      "-------------------------------\n",
      "tensor(41.6232)\n",
      "tensor(26.1526)\n",
      "tensor(5.8980)\n",
      "tensor(0.3509)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.439049\n",
      "Epoch 5411\n",
      "-------------------------------\n",
      "tensor(81.5143)\n",
      "tensor(20.4339)\n",
      "tensor(48.5556)\n",
      "tensor(1.0721)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.556198\n",
      "Epoch 5412\n",
      "-------------------------------\n",
      "tensor(122.2161)\n",
      "tensor(33.5472)\n",
      "tensor(80.8749)\n",
      "tensor(2.5927)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.638084\n",
      "Epoch 5413\n",
      "-------------------------------\n",
      "tensor(142.5415)\n",
      "tensor(32.2854)\n",
      "tensor(94.1607)\n",
      "tensor(1.9964)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.874001\n",
      "Epoch 5414\n",
      "-------------------------------\n",
      "tensor(94.1418)\n",
      "tensor(38.3221)\n",
      "tensor(73.8145)\n",
      "tensor(1.6280)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.684799\n",
      "Epoch 5415\n",
      "-------------------------------\n",
      "tensor(35.0188)\n",
      "tensor(13.8975)\n",
      "tensor(10.4811)\n",
      "tensor(0.3656)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.442261\n",
      "Epoch 5416\n",
      "-------------------------------\n",
      "tensor(84.7410)\n",
      "tensor(25.4348)\n",
      "tensor(59.0299)\n",
      "tensor(1.6081)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.550575\n",
      "Epoch 5417\n",
      "-------------------------------\n",
      "tensor(47.2374)\n",
      "tensor(21.1748)\n",
      "tensor(13.0618)\n",
      "tensor(0.5539)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.446812\n",
      "Epoch 5418\n",
      "-------------------------------\n",
      "tensor(71.6484)\n",
      "tensor(20.6984)\n",
      "tensor(36.4859)\n",
      "tensor(1.3502)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.479179\n",
      "Epoch 5419\n",
      "-------------------------------\n",
      "tensor(40.6705)\n",
      "tensor(19.4145)\n",
      "tensor(12.6612)\n",
      "tensor(0.6815)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.365753\n",
      "Epoch 5420\n",
      "-------------------------------\n",
      "tensor(36.8561)\n",
      "tensor(8.4359)\n",
      "tensor(13.5096)\n",
      "tensor(0.0083)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.345139\n",
      "Epoch 5421\n",
      "-------------------------------\n",
      "tensor(43.5892)\n",
      "tensor(8.4654)\n",
      "tensor(19.6531)\n",
      "tensor(0.2788)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.354511\n",
      "Epoch 5422\n",
      "-------------------------------\n",
      "tensor(41.2312)\n",
      "tensor(9.8787)\n",
      "tensor(16.4388)\n",
      "tensor(0.3272)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.342846\n",
      "Epoch 5423\n",
      "-------------------------------\n",
      "tensor(30.0994)\n",
      "tensor(11.1154)\n",
      "tensor(6.0846)\n",
      "tensor(0.1874)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.317905\n",
      "Epoch 5424\n",
      "-------------------------------\n",
      "tensor(42.4837)\n",
      "tensor(17.5456)\n",
      "tensor(17.7708)\n",
      "tensor(0.1451)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.310375\n",
      "Epoch 5425\n",
      "-------------------------------\n",
      "tensor(77.5560)\n",
      "tensor(20.8306)\n",
      "tensor(30.9196)\n",
      "tensor(0.3931)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.327141\n",
      "Epoch 5426\n",
      "-------------------------------\n",
      "tensor(47.2611)\n",
      "tensor(25.4176)\n",
      "tensor(5.6845)\n",
      "tensor(0.0956)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.261379\n",
      "Epoch 5427\n",
      "-------------------------------\n",
      "tensor(94.4739)\n",
      "tensor(16.0928)\n",
      "tensor(50.6128)\n",
      "tensor(0.3181)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.478485\n",
      "Epoch 5428\n",
      "-------------------------------\n",
      "tensor(42.7942)\n",
      "tensor(30.2327)\n",
      "tensor(33.6195)\n",
      "tensor(1.6970)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.357944\n",
      "Epoch 5429\n",
      "-------------------------------\n",
      "tensor(66.1286)\n",
      "tensor(22.3337)\n",
      "tensor(39.8117)\n",
      "tensor(0.5737)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.384907\n",
      "Epoch 5430\n",
      "-------------------------------\n",
      "tensor(70.2254)\n",
      "tensor(64.9857)\n",
      "tensor(81.6932)\n",
      "tensor(3.2171)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.573429\n",
      "Epoch 5431\n",
      "-------------------------------\n",
      "tensor(77.7936)\n",
      "tensor(19.7311)\n",
      "tensor(46.5354)\n",
      "tensor(1.6619)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.400627\n",
      "Epoch 5432\n",
      "-------------------------------\n",
      "tensor(57.0239)\n",
      "tensor(22.0527)\n",
      "tensor(21.3867)\n",
      "tensor(1.2005)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.437878\n",
      "Epoch 5433\n",
      "-------------------------------\n",
      "tensor(40.0689)\n",
      "tensor(18.0326)\n",
      "tensor(9.8556)\n",
      "tensor(0.0566)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.465019\n",
      "Epoch 5434\n",
      "-------------------------------\n",
      "tensor(55.6423)\n",
      "tensor(19.8744)\n",
      "tensor(26.1473)\n",
      "tensor(0.5817)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.448257\n",
      "Epoch 5435\n",
      "-------------------------------\n",
      "tensor(42.7835)\n",
      "tensor(11.9346)\n",
      "tensor(20.0206)\n",
      "tensor(0.6009)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.308151\n",
      "Epoch 5436\n",
      "-------------------------------\n",
      "tensor(36.6820)\n",
      "tensor(14.5708)\n",
      "tensor(13.8730)\n",
      "tensor(0.7983)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.267403\n",
      "Epoch 5437\n",
      "-------------------------------\n",
      "tensor(60.1472)\n",
      "tensor(22.2506)\n",
      "tensor(22.3819)\n",
      "tensor(1.0053)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.365112\n",
      "Epoch 5438\n",
      "-------------------------------\n",
      "tensor(39.2449)\n",
      "tensor(29.0761)\n",
      "tensor(9.8795)\n",
      "tensor(0.0744)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.316677\n",
      "Epoch 5439\n",
      "-------------------------------\n",
      "tensor(32.1726)\n",
      "tensor(33.9520)\n",
      "tensor(21.9212)\n",
      "tensor(0.7532)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.255657\n",
      "Epoch 5440\n",
      "-------------------------------\n",
      "tensor(32.3093)\n",
      "tensor(31.7109)\n",
      "tensor(20.8144)\n",
      "tensor(0.7866)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.193523\n",
      "Epoch 5441\n",
      "-------------------------------\n",
      "tensor(41.2666)\n",
      "tensor(12.4806)\n",
      "tensor(14.0774)\n",
      "tensor(0.5280)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.178555\n",
      "Epoch 5442\n",
      "-------------------------------\n",
      "tensor(41.3516)\n",
      "tensor(13.7493)\n",
      "tensor(4.9062)\n",
      "tensor(0.1602)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.187782\n",
      "Epoch 5443\n",
      "-------------------------------\n",
      "tensor(40.8206)\n",
      "tensor(17.8921)\n",
      "tensor(9.9555)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.186764\n",
      "Epoch 5444\n",
      "-------------------------------\n",
      "tensor(28.9602)\n",
      "tensor(21.8590)\n",
      "tensor(22.5100)\n",
      "tensor(0.8652)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.185562\n",
      "Epoch 5445\n",
      "-------------------------------\n",
      "tensor(33.4274)\n",
      "tensor(23.9539)\n",
      "tensor(21.7714)\n",
      "tensor(0.9893)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.194267\n",
      "Epoch 5446\n",
      "-------------------------------\n",
      "tensor(41.1412)\n",
      "tensor(14.9072)\n",
      "tensor(6.2583)\n",
      "tensor(0.2337)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.195755\n",
      "Epoch 5447\n",
      "-------------------------------\n",
      "tensor(40.4515)\n",
      "tensor(29.9787)\n",
      "tensor(20.4878)\n",
      "tensor(0.5242)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.217945\n",
      "Epoch 5448\n",
      "-------------------------------\n",
      "tensor(46.1318)\n",
      "tensor(24.8712)\n",
      "tensor(2.6987)\n",
      "tensor(0.0295)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.203362\n",
      "Epoch 5449\n",
      "-------------------------------\n",
      "tensor(22.8420)\n",
      "tensor(13.8912)\n",
      "tensor(9.2435)\n",
      "tensor(0.6598)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.192730\n",
      "Epoch 5450\n",
      "-------------------------------\n",
      "tensor(19.7911)\n",
      "tensor(12.7138)\n",
      "tensor(8.6882)\n",
      "tensor(0.2327)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.211704\n",
      "Epoch 5451\n",
      "-------------------------------\n",
      "tensor(35.8584)\n",
      "tensor(14.2670)\n",
      "tensor(13.0029)\n",
      "tensor(0.8226)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.214691\n",
      "Epoch 5452\n",
      "-------------------------------\n",
      "tensor(37.3377)\n",
      "tensor(23.0202)\n",
      "tensor(3.9555)\n",
      "tensor(0.0490)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.154182\n",
      "Epoch 5453\n",
      "-------------------------------\n",
      "tensor(54.7175)\n",
      "tensor(17.5792)\n",
      "tensor(19.4727)\n",
      "tensor(0.5163)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.201950\n",
      "Epoch 5454\n",
      "-------------------------------\n",
      "tensor(45.1130)\n",
      "tensor(25.1829)\n",
      "tensor(31.2904)\n",
      "tensor(1.1630)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.144962\n",
      "Epoch 5455\n",
      "-------------------------------\n",
      "tensor(29.8109)\n",
      "tensor(30.1341)\n",
      "tensor(16.5212)\n",
      "tensor(0.8850)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.139565\n",
      "Epoch 5456\n",
      "-------------------------------\n",
      "tensor(51.3796)\n",
      "tensor(14.9443)\n",
      "tensor(29.8253)\n",
      "tensor(1.0257)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.149696\n",
      "Epoch 5457\n",
      "-------------------------------\n",
      "tensor(48.6166)\n",
      "tensor(18.6125)\n",
      "tensor(18.6683)\n",
      "tensor(0.6970)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.113750\n",
      "Epoch 5458\n",
      "-------------------------------\n",
      "tensor(34.2228)\n",
      "tensor(23.3212)\n",
      "tensor(20.4624)\n",
      "tensor(1.0982)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.103569\n",
      "Epoch 5459\n",
      "-------------------------------\n",
      "tensor(28.0598)\n",
      "tensor(17.6112)\n",
      "tensor(8.4280)\n",
      "tensor(0.6845)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.069839\n",
      "Epoch 5460\n",
      "-------------------------------\n",
      "tensor(28.1951)\n",
      "tensor(10.4753)\n",
      "tensor(6.0437)\n",
      "tensor(0.2508)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.057671\n",
      "Epoch 5461\n",
      "-------------------------------\n",
      "tensor(26.5099)\n",
      "tensor(8.9845)\n",
      "tensor(6.0624)\n",
      "tensor(0.0015)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 40.042717\n",
      "Epoch 5462\n",
      "-------------------------------\n",
      "tensor(29.5431)\n",
      "tensor(10.5783)\n",
      "tensor(4.5287)\n",
      "tensor(0.1356)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.026752\n",
      "Epoch 5463\n",
      "-------------------------------\n",
      "tensor(37.5592)\n",
      "tensor(23.1768)\n",
      "tensor(5.0656)\n",
      "tensor(0.2066)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.029503\n",
      "Epoch 5464\n",
      "-------------------------------\n",
      "tensor(49.6415)\n",
      "tensor(24.6096)\n",
      "tensor(7.0433)\n",
      "tensor(0.2404)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.037643\n",
      "Epoch 5465\n",
      "-------------------------------\n",
      "tensor(50.5293)\n",
      "tensor(26.9809)\n",
      "tensor(5.2965)\n",
      "tensor(0.2465)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.028324\n",
      "Epoch 5466\n",
      "-------------------------------\n",
      "tensor(50.0212)\n",
      "tensor(11.4011)\n",
      "tensor(18.9259)\n",
      "tensor(0.0720)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.093597\n",
      "Epoch 5467\n",
      "-------------------------------\n",
      "tensor(43.5095)\n",
      "tensor(22.8968)\n",
      "tensor(12.9318)\n",
      "tensor(1.0047)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.134720\n",
      "Epoch 5468\n",
      "-------------------------------\n",
      "tensor(33.7293)\n",
      "tensor(21.5204)\n",
      "tensor(27.6637)\n",
      "tensor(0.9809)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.113129\n",
      "Epoch 5469\n",
      "-------------------------------\n",
      "tensor(31.9851)\n",
      "tensor(18.5845)\n",
      "tensor(22.9963)\n",
      "tensor(1.3114)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.082096\n",
      "Epoch 5470\n",
      "-------------------------------\n",
      "tensor(38.7901)\n",
      "tensor(32.3121)\n",
      "tensor(12.8927)\n",
      "tensor(0.9236)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.034115\n",
      "Epoch 5471\n",
      "-------------------------------\n",
      "tensor(44.2059)\n",
      "tensor(19.8476)\n",
      "tensor(15.1773)\n",
      "tensor(1.0062)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.072708\n",
      "Epoch 5472\n",
      "-------------------------------\n",
      "tensor(37.7335)\n",
      "tensor(23.3575)\n",
      "tensor(19.8254)\n",
      "tensor(1.2382)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.045929\n",
      "Epoch 5473\n",
      "-------------------------------\n",
      "tensor(34.5229)\n",
      "tensor(31.2109)\n",
      "tensor(17.4693)\n",
      "tensor(0.9766)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.086590\n",
      "Epoch 5474\n",
      "-------------------------------\n",
      "tensor(28.0279)\n",
      "tensor(28.0332)\n",
      "tensor(17.7872)\n",
      "tensor(0.8210)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.036797\n",
      "Epoch 5475\n",
      "-------------------------------\n",
      "tensor(21.7288)\n",
      "tensor(24.9780)\n",
      "tensor(17.3557)\n",
      "tensor(1.2142)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.089718\n",
      "Epoch 5476\n",
      "-------------------------------\n",
      "tensor(35.3141)\n",
      "tensor(29.6868)\n",
      "tensor(21.8647)\n",
      "tensor(1.3529)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.138344\n",
      "Epoch 5477\n",
      "-------------------------------\n",
      "tensor(41.7974)\n",
      "tensor(16.9797)\n",
      "tensor(4.2921)\n",
      "tensor(0.0991)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.090736\n",
      "Epoch 5478\n",
      "-------------------------------\n",
      "tensor(32.5976)\n",
      "tensor(11.6828)\n",
      "tensor(11.4937)\n",
      "tensor(0.5729)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.017216\n",
      "Epoch 5479\n",
      "-------------------------------\n",
      "tensor(20.3576)\n",
      "tensor(11.2695)\n",
      "tensor(8.4229)\n",
      "tensor(0.5691)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.991077\n",
      "Epoch 5480\n",
      "-------------------------------\n",
      "tensor(20.4356)\n",
      "tensor(10.5268)\n",
      "tensor(7.1750)\n",
      "tensor(0.3768)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.972332\n",
      "Epoch 5481\n",
      "-------------------------------\n",
      "tensor(43.1889)\n",
      "tensor(24.4135)\n",
      "tensor(7.3915)\n",
      "tensor(0.2091)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.983845\n",
      "Epoch 5482\n",
      "-------------------------------\n",
      "tensor(40.2790)\n",
      "tensor(24.1952)\n",
      "tensor(4.9947)\n",
      "tensor(0.1027)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.976883\n",
      "Epoch 5483\n",
      "-------------------------------\n",
      "tensor(34.6719)\n",
      "tensor(24.4134)\n",
      "tensor(5.1403)\n",
      "tensor(0.0183)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.947357\n",
      "Epoch 5484\n",
      "-------------------------------\n",
      "tensor(38.8134)\n",
      "tensor(11.3058)\n",
      "tensor(14.7085)\n",
      "tensor(0.1272)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.949070\n",
      "Epoch 5485\n",
      "-------------------------------\n",
      "tensor(50.0366)\n",
      "tensor(18.0515)\n",
      "tensor(14.3409)\n",
      "tensor(0.4643)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.964039\n",
      "Epoch 5486\n",
      "-------------------------------\n",
      "tensor(38.1893)\n",
      "tensor(22.2337)\n",
      "tensor(14.0636)\n",
      "tensor(0.9098)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.948734\n",
      "Epoch 5487\n",
      "-------------------------------\n",
      "tensor(54.8158)\n",
      "tensor(22.7794)\n",
      "tensor(21.5516)\n",
      "tensor(0.4867)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.055977\n",
      "Epoch 5488\n",
      "-------------------------------\n",
      "tensor(48.2486)\n",
      "tensor(15.3778)\n",
      "tensor(19.8723)\n",
      "tensor(0.9630)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.054203\n",
      "Epoch 5489\n",
      "-------------------------------\n",
      "tensor(42.8895)\n",
      "tensor(28.9334)\n",
      "tensor(10.3569)\n",
      "tensor(0.3042)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 40.106018\n",
      "Epoch 5490\n",
      "-------------------------------\n",
      "tensor(44.2593)\n",
      "tensor(21.5675)\n",
      "tensor(17.5245)\n",
      "tensor(1.1216)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.047165\n",
      "Epoch 5491\n",
      "-------------------------------\n",
      "tensor(40.0890)\n",
      "tensor(9.8427)\n",
      "tensor(13.2784)\n",
      "tensor(0.1904)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.950104\n",
      "Epoch 5492\n",
      "-------------------------------\n",
      "tensor(47.2361)\n",
      "tensor(19.1430)\n",
      "tensor(30.5126)\n",
      "tensor(0.0714)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.012836\n",
      "Epoch 5493\n",
      "-------------------------------\n",
      "tensor(43.7445)\n",
      "tensor(14.5316)\n",
      "tensor(28.0958)\n",
      "tensor(1.1831)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.971893\n",
      "Epoch 5494\n",
      "-------------------------------\n",
      "tensor(50.4954)\n",
      "tensor(21.0583)\n",
      "tensor(20.6987)\n",
      "tensor(1.0257)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.932777\n",
      "Epoch 5495\n",
      "-------------------------------\n",
      "tensor(48.3699)\n",
      "tensor(16.9499)\n",
      "tensor(17.5519)\n",
      "tensor(0.8938)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.928928\n",
      "Epoch 5496\n",
      "-------------------------------\n",
      "tensor(34.7833)\n",
      "tensor(13.6709)\n",
      "tensor(4.3005)\n",
      "tensor(0.2595)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.868347\n",
      "Epoch 5497\n",
      "-------------------------------\n",
      "tensor(56.6740)\n",
      "tensor(25.5887)\n",
      "tensor(12.6295)\n",
      "tensor(0.4278)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.894321\n",
      "Epoch 5498\n",
      "-------------------------------\n",
      "tensor(31.5833)\n",
      "tensor(13.9350)\n",
      "tensor(14.0991)\n",
      "tensor(0.8172)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.818329\n",
      "Epoch 5499\n",
      "-------------------------------\n",
      "tensor(30.4337)\n",
      "tensor(10.8341)\n",
      "tensor(13.1532)\n",
      "tensor(0.5273)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.798859\n",
      "Epoch 5500\n",
      "-------------------------------\n",
      "tensor(39.7466)\n",
      "tensor(24.6968)\n",
      "tensor(5.1006)\n",
      "tensor(0.0859)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.789101\n",
      "Epoch 5501\n",
      "-------------------------------\n",
      "tensor(35.5203)\n",
      "tensor(14.5165)\n",
      "tensor(3.7753)\n",
      "tensor(0.2259)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.791164\n",
      "Epoch 5502\n",
      "-------------------------------\n",
      "tensor(21.1622)\n",
      "tensor(12.7400)\n",
      "tensor(6.2509)\n",
      "tensor(0.4336)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.784046\n",
      "Epoch 5503\n",
      "-------------------------------\n",
      "tensor(24.7833)\n",
      "tensor(13.4789)\n",
      "tensor(9.3760)\n",
      "tensor(0.5654)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.779716\n",
      "Epoch 5504\n",
      "-------------------------------\n",
      "tensor(45.0027)\n",
      "tensor(20.2634)\n",
      "tensor(11.0815)\n",
      "tensor(0.5438)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.792549\n",
      "Epoch 5505\n",
      "-------------------------------\n",
      "tensor(37.3989)\n",
      "tensor(21.8746)\n",
      "tensor(2.3568)\n",
      "tensor(0.1456)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.763191\n",
      "Epoch 5506\n",
      "-------------------------------\n",
      "tensor(54.5991)\n",
      "tensor(13.3205)\n",
      "tensor(20.2355)\n",
      "tensor(0.3548)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.843555\n",
      "Epoch 5507\n",
      "-------------------------------\n",
      "tensor(40.4289)\n",
      "tensor(15.6106)\n",
      "tensor(3.7689)\n",
      "tensor(0.1252)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.842369\n",
      "Epoch 5508\n",
      "-------------------------------\n",
      "tensor(52.3747)\n",
      "tensor(25.0790)\n",
      "tensor(29.5792)\n",
      "tensor(0.8683)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.840645\n",
      "Epoch 5509\n",
      "-------------------------------\n",
      "tensor(39.1070)\n",
      "tensor(10.1112)\n",
      "tensor(18.8269)\n",
      "tensor(0.4055)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.802803\n",
      "Epoch 5510\n",
      "-------------------------------\n",
      "tensor(41.7858)\n",
      "tensor(27.3505)\n",
      "tensor(5.4879)\n",
      "tensor(0.1479)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.931965\n",
      "Epoch 5511\n",
      "-------------------------------\n",
      "tensor(46.3143)\n",
      "tensor(27.2509)\n",
      "tensor(5.4718)\n",
      "tensor(0.0378)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.893955\n",
      "Epoch 5512\n",
      "-------------------------------\n",
      "tensor(48.8097)\n",
      "tensor(11.7992)\n",
      "tensor(20.0878)\n",
      "tensor(0.0884)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.881622\n",
      "Epoch 5513\n",
      "-------------------------------\n",
      "tensor(60.8554)\n",
      "tensor(32.5042)\n",
      "tensor(48.2282)\n",
      "tensor(1.2527)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.978043\n",
      "Epoch 5514\n",
      "-------------------------------\n",
      "tensor(65.2045)\n",
      "tensor(16.6460)\n",
      "tensor(40.1430)\n",
      "tensor(1.3651)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.957165\n",
      "Epoch 5515\n",
      "-------------------------------\n",
      "tensor(28.3746)\n",
      "tensor(14.3011)\n",
      "tensor(10.8845)\n",
      "tensor(0.2279)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.813095\n",
      "Epoch 5516\n",
      "-------------------------------\n",
      "tensor(65.9763)\n",
      "tensor(19.6602)\n",
      "tensor(28.9460)\n",
      "tensor(0.9588)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.812244\n",
      "Epoch 5517\n",
      "-------------------------------\n",
      "tensor(48.2135)\n",
      "tensor(8.8651)\n",
      "tensor(24.3065)\n",
      "tensor(0.1695)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.776466\n",
      "Epoch 5518\n",
      "-------------------------------\n",
      "tensor(38.9956)\n",
      "tensor(29.9446)\n",
      "tensor(20.7056)\n",
      "tensor(0.3671)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.760132\n",
      "Epoch 5519\n",
      "-------------------------------\n",
      "tensor(33.7901)\n",
      "tensor(11.2505)\n",
      "tensor(4.2043)\n",
      "tensor(0.0532)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.726402\n",
      "Epoch 5520\n",
      "-------------------------------\n",
      "tensor(39.7819)\n",
      "tensor(14.8439)\n",
      "tensor(9.8112)\n",
      "tensor(0.1817)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.720184\n",
      "Epoch 5521\n",
      "-------------------------------\n",
      "tensor(26.8219)\n",
      "tensor(12.7587)\n",
      "tensor(13.4662)\n",
      "tensor(0.2346)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.708664\n",
      "Epoch 5522\n",
      "-------------------------------\n",
      "tensor(20.6181)\n",
      "tensor(11.4495)\n",
      "tensor(12.2575)\n",
      "tensor(0.1730)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.696793\n",
      "Epoch 5523\n",
      "-------------------------------\n",
      "tensor(40.7689)\n",
      "tensor(21.5695)\n",
      "tensor(7.0658)\n",
      "tensor(0.0088)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.697342\n",
      "Epoch 5524\n",
      "-------------------------------\n",
      "tensor(38.2411)\n",
      "tensor(11.5813)\n",
      "tensor(9.2442)\n",
      "tensor(0.2790)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.681732\n",
      "Epoch 5525\n",
      "-------------------------------\n",
      "tensor(39.9194)\n",
      "tensor(7.8930)\n",
      "tensor(19.3447)\n",
      "tensor(0.3715)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.680733\n",
      "Epoch 5526\n",
      "-------------------------------\n",
      "tensor(39.6827)\n",
      "tensor(22.4831)\n",
      "tensor(3.2265)\n",
      "tensor(0.2430)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.727486\n",
      "Epoch 5527\n",
      "-------------------------------\n",
      "tensor(53.3782)\n",
      "tensor(22.3659)\n",
      "tensor(15.9797)\n",
      "tensor(0.8665)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.780422\n",
      "Epoch 5528\n",
      "-------------------------------\n",
      "tensor(54.9685)\n",
      "tensor(19.5778)\n",
      "tensor(14.3285)\n",
      "tensor(0.4159)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.794891\n",
      "Epoch 5529\n",
      "-------------------------------\n",
      "tensor(48.3134)\n",
      "tensor(21.1539)\n",
      "tensor(6.9807)\n",
      "tensor(0.3749)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.721928\n",
      "Epoch 5530\n",
      "-------------------------------\n",
      "tensor(22.4782)\n",
      "tensor(10.9237)\n",
      "tensor(10.2351)\n",
      "tensor(0.1014)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.714050\n",
      "Epoch 5531\n",
      "-------------------------------\n",
      "tensor(25.1051)\n",
      "tensor(15.7315)\n",
      "tensor(16.9469)\n",
      "tensor(0.8320)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.741741\n",
      "Epoch 5532\n",
      "-------------------------------\n",
      "tensor(56.7681)\n",
      "tensor(23.4891)\n",
      "tensor(15.7031)\n",
      "tensor(0.3416)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.868988\n",
      "Epoch 5533\n",
      "-------------------------------\n",
      "tensor(46.4207)\n",
      "tensor(32.8386)\n",
      "tensor(32.2811)\n",
      "tensor(0.4106)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.761124\n",
      "Epoch 5534\n",
      "-------------------------------\n",
      "tensor(39.7427)\n",
      "tensor(25.7082)\n",
      "tensor(18.5350)\n",
      "tensor(0.9969)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.782272\n",
      "Epoch 5535\n",
      "-------------------------------\n",
      "tensor(34.1273)\n",
      "tensor(25.2096)\n",
      "tensor(20.6473)\n",
      "tensor(0.6031)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.799664\n",
      "Epoch 5536\n",
      "-------------------------------\n",
      "tensor(40.8790)\n",
      "tensor(11.8007)\n",
      "tensor(16.5977)\n",
      "tensor(0.5150)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.756863\n",
      "Epoch 5537\n",
      "-------------------------------\n",
      "tensor(36.0974)\n",
      "tensor(13.4118)\n",
      "tensor(6.7405)\n",
      "tensor(0.1739)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.704506\n",
      "Epoch 5538\n",
      "-------------------------------\n",
      "tensor(39.2487)\n",
      "tensor(17.0111)\n",
      "tensor(12.9940)\n",
      "tensor(0.4062)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.646072\n",
      "Epoch 5539\n",
      "-------------------------------\n",
      "tensor(43.3324)\n",
      "tensor(16.5131)\n",
      "tensor(11.7237)\n",
      "tensor(0.4537)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.602325\n",
      "Epoch 5540\n",
      "-------------------------------\n",
      "tensor(47.5427)\n",
      "tensor(24.3628)\n",
      "tensor(6.4655)\n",
      "tensor(0.2618)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.618324\n",
      "Epoch 5541\n",
      "-------------------------------\n",
      "tensor(37.7891)\n",
      "tensor(25.7913)\n",
      "tensor(9.0641)\n",
      "tensor(0.0571)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.631157\n",
      "Epoch 5542\n",
      "-------------------------------\n",
      "tensor(34.7899)\n",
      "tensor(27.2773)\n",
      "tensor(13.8657)\n",
      "tensor(0.1004)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.626499\n",
      "Epoch 5543\n",
      "-------------------------------\n",
      "tensor(35.1180)\n",
      "tensor(8.3802)\n",
      "tensor(16.6075)\n",
      "tensor(0.2073)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.600628\n",
      "Epoch 5544\n",
      "-------------------------------\n",
      "tensor(31.1196)\n",
      "tensor(9.2207)\n",
      "tensor(9.3106)\n",
      "tensor(0.1448)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.590767\n",
      "Epoch 5545\n",
      "-------------------------------\n",
      "tensor(33.4568)\n",
      "tensor(14.8520)\n",
      "tensor(11.9373)\n",
      "tensor(0.1261)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.584290\n",
      "Epoch 5546\n",
      "-------------------------------\n",
      "tensor(42.9038)\n",
      "tensor(18.0879)\n",
      "tensor(23.9761)\n",
      "tensor(0.2023)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.592323\n",
      "Epoch 5547\n",
      "-------------------------------\n",
      "tensor(47.6817)\n",
      "tensor(14.2927)\n",
      "tensor(9.3651)\n",
      "tensor(0.3973)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.599262\n",
      "Epoch 5548\n",
      "-------------------------------\n",
      "tensor(47.8461)\n",
      "tensor(28.1355)\n",
      "tensor(23.0117)\n",
      "tensor(0.1524)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.626511\n",
      "Epoch 5549\n",
      "-------------------------------\n",
      "tensor(42.7379)\n",
      "tensor(23.2709)\n",
      "tensor(20.5013)\n",
      "tensor(1.4905)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.600990\n",
      "Epoch 5550\n",
      "-------------------------------\n",
      "tensor(23.9131)\n",
      "tensor(16.6916)\n",
      "tensor(8.5234)\n",
      "tensor(0.7333)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.600586\n",
      "Epoch 5551\n",
      "-------------------------------\n",
      "tensor(50.4574)\n",
      "tensor(16.6240)\n",
      "tensor(17.1784)\n",
      "tensor(0.9331)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.641220\n",
      "Epoch 5552\n",
      "-------------------------------\n",
      "tensor(26.9383)\n",
      "tensor(12.7571)\n",
      "tensor(13.0554)\n",
      "tensor(0.7383)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.620644\n",
      "Epoch 5553\n",
      "-------------------------------\n",
      "tensor(32.8300)\n",
      "tensor(15.5341)\n",
      "tensor(22.0750)\n",
      "tensor(1.0935)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.642418\n",
      "Epoch 5554\n",
      "-------------------------------\n",
      "tensor(41.7943)\n",
      "tensor(27.8018)\n",
      "tensor(12.5084)\n",
      "tensor(0.0798)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.662888\n",
      "Epoch 5555\n",
      "-------------------------------\n",
      "tensor(34.3865)\n",
      "tensor(33.3663)\n",
      "tensor(23.0913)\n",
      "tensor(0.8422)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.598709\n",
      "Epoch 5556\n",
      "-------------------------------\n",
      "tensor(28.8938)\n",
      "tensor(15.5825)\n",
      "tensor(7.9299)\n",
      "tensor(0.1686)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.587425\n",
      "Epoch 5557\n",
      "-------------------------------\n",
      "tensor(41.2251)\n",
      "tensor(24.3261)\n",
      "tensor(20.9457)\n",
      "tensor(0.7507)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.595348\n",
      "Epoch 5558\n",
      "-------------------------------\n",
      "tensor(31.5326)\n",
      "tensor(17.9181)\n",
      "tensor(6.8282)\n",
      "tensor(0.4520)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.570641\n",
      "Epoch 5559\n",
      "-------------------------------\n",
      "tensor(43.1873)\n",
      "tensor(15.4613)\n",
      "tensor(6.2067)\n",
      "tensor(0.1424)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.553001\n",
      "Epoch 5560\n",
      "-------------------------------\n",
      "tensor(36.6578)\n",
      "tensor(12.6642)\n",
      "tensor(5.9906)\n",
      "tensor(0.0565)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.524174\n",
      "Epoch 5561\n",
      "-------------------------------\n",
      "tensor(34.7272)\n",
      "tensor(12.8079)\n",
      "tensor(2.9992)\n",
      "tensor(0.0702)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.505173\n",
      "Epoch 5562\n",
      "-------------------------------\n",
      "tensor(19.1938)\n",
      "tensor(9.9761)\n",
      "tensor(3.0967)\n",
      "tensor(0.1057)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.488884\n",
      "Epoch 5563\n",
      "-------------------------------\n",
      "tensor(18.0761)\n",
      "tensor(9.9707)\n",
      "tensor(7.3436)\n",
      "tensor(0.1306)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.472160\n",
      "Epoch 5564\n",
      "-------------------------------\n",
      "tensor(46.9969)\n",
      "tensor(21.6154)\n",
      "tensor(10.9870)\n",
      "tensor(0.1081)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.501976\n",
      "Epoch 5565\n",
      "-------------------------------\n",
      "tensor(39.7311)\n",
      "tensor(25.6446)\n",
      "tensor(3.8577)\n",
      "tensor(0.1503)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.519882\n",
      "Epoch 5566\n",
      "-------------------------------\n",
      "tensor(48.0935)\n",
      "tensor(33.3508)\n",
      "tensor(23.3734)\n",
      "tensor(0.5041)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.527493\n",
      "Epoch 5567\n",
      "-------------------------------\n",
      "tensor(55.6982)\n",
      "tensor(15.7904)\n",
      "tensor(22.2622)\n",
      "tensor(0.0755)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.506390\n",
      "Epoch 5568\n",
      "-------------------------------\n",
      "tensor(52.2802)\n",
      "tensor(29.7953)\n",
      "tensor(37.0114)\n",
      "tensor(1.4721)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.530994\n",
      "Epoch 5569\n",
      "-------------------------------\n",
      "tensor(27.4762)\n",
      "tensor(10.6883)\n",
      "tensor(5.6349)\n",
      "tensor(0.2631)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.500504\n",
      "Epoch 5570\n",
      "-------------------------------\n",
      "tensor(48.3438)\n",
      "tensor(19.3887)\n",
      "tensor(32.3398)\n",
      "tensor(1.4244)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.525501\n",
      "Epoch 5571\n",
      "-------------------------------\n",
      "tensor(78.4924)\n",
      "tensor(18.6025)\n",
      "tensor(43.7650)\n",
      "tensor(1.2992)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.634598\n",
      "Epoch 5572\n",
      "-------------------------------\n",
      "tensor(100.2488)\n",
      "tensor(18.3327)\n",
      "tensor(58.9681)\n",
      "tensor(0.6161)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.671402\n",
      "Epoch 5573\n",
      "-------------------------------\n",
      "tensor(108.2934)\n",
      "tensor(25.5658)\n",
      "tensor(68.5715)\n",
      "tensor(1.8196)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.710094\n",
      "Epoch 5574\n",
      "-------------------------------\n",
      "tensor(87.2188)\n",
      "tensor(23.8322)\n",
      "tensor(60.1070)\n",
      "tensor(1.8666)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.637783\n",
      "Epoch 5575\n",
      "-------------------------------\n",
      "tensor(23.2087)\n",
      "tensor(16.7772)\n",
      "tensor(16.7686)\n",
      "tensor(0.3839)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.461498\n",
      "Epoch 5576\n",
      "-------------------------------\n",
      "tensor(58.6372)\n",
      "tensor(30.0910)\n",
      "tensor(43.0111)\n",
      "tensor(1.3729)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.515121\n",
      "Epoch 5577\n",
      "-------------------------------\n",
      "tensor(40.5145)\n",
      "tensor(9.2024)\n",
      "tensor(15.9935)\n",
      "tensor(0.1185)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.397797\n",
      "Epoch 5578\n",
      "-------------------------------\n",
      "tensor(42.4681)\n",
      "tensor(33.2997)\n",
      "tensor(28.8395)\n",
      "tensor(0.6547)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.423347\n",
      "Epoch 5579\n",
      "-------------------------------\n",
      "tensor(49.3202)\n",
      "tensor(30.2080)\n",
      "tensor(14.3281)\n",
      "tensor(0.3076)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.412868\n",
      "Epoch 5580\n",
      "-------------------------------\n",
      "tensor(44.2413)\n",
      "tensor(24.5021)\n",
      "tensor(3.7796)\n",
      "tensor(0.0987)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.386028\n",
      "Epoch 5581\n",
      "-------------------------------\n",
      "tensor(30.9255)\n",
      "tensor(12.2086)\n",
      "tensor(5.9061)\n",
      "tensor(0.3228)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.355389\n",
      "Epoch 5582\n",
      "-------------------------------\n",
      "tensor(18.8572)\n",
      "tensor(12.5925)\n",
      "tensor(8.5228)\n",
      "tensor(0.4295)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.355587\n",
      "Epoch 5583\n",
      "-------------------------------\n",
      "tensor(18.5885)\n",
      "tensor(13.1574)\n",
      "tensor(9.4039)\n",
      "tensor(0.4197)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.360531\n",
      "Epoch 5584\n",
      "-------------------------------\n",
      "tensor(24.1281)\n",
      "tensor(11.7528)\n",
      "tensor(7.0246)\n",
      "tensor(0.2140)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.360687\n",
      "Epoch 5585\n",
      "-------------------------------\n",
      "tensor(38.9693)\n",
      "tensor(13.1275)\n",
      "tensor(5.2004)\n",
      "tensor(0.2327)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.369141\n",
      "Epoch 5586\n",
      "-------------------------------\n",
      "tensor(32.5456)\n",
      "tensor(11.1434)\n",
      "tensor(11.0614)\n",
      "tensor(0.5551)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.348221\n",
      "Epoch 5587\n",
      "-------------------------------\n",
      "tensor(44.4065)\n",
      "tensor(23.8694)\n",
      "tensor(3.2767)\n",
      "tensor(0.1021)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.379478\n",
      "Epoch 5588\n",
      "-------------------------------\n",
      "tensor(34.7645)\n",
      "tensor(21.5402)\n",
      "tensor(10.2318)\n",
      "tensor(0.5638)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.373657\n",
      "Epoch 5589\n",
      "-------------------------------\n",
      "tensor(49.5094)\n",
      "tensor(24.0995)\n",
      "tensor(12.9380)\n",
      "tensor(0.9511)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.422062\n",
      "Epoch 5590\n",
      "-------------------------------\n",
      "tensor(32.2725)\n",
      "tensor(19.2749)\n",
      "tensor(22.2005)\n",
      "tensor(0.5653)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.420025\n",
      "Epoch 5591\n",
      "-------------------------------\n",
      "tensor(34.4063)\n",
      "tensor(18.7570)\n",
      "tensor(25.2435)\n",
      "tensor(1.4284)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.418777\n",
      "Epoch 5592\n",
      "-------------------------------\n",
      "tensor(22.3374)\n",
      "tensor(8.8731)\n",
      "tensor(9.9238)\n",
      "tensor(0.0788)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.359890\n",
      "Epoch 5593\n",
      "-------------------------------\n",
      "tensor(48.1315)\n",
      "tensor(23.1988)\n",
      "tensor(13.8610)\n",
      "tensor(0.7200)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.416416\n",
      "Epoch 5594\n",
      "-------------------------------\n",
      "tensor(60.8243)\n",
      "tensor(14.4663)\n",
      "tensor(31.5283)\n",
      "tensor(0.3498)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.355019\n",
      "Epoch 5595\n",
      "-------------------------------\n",
      "tensor(76.1198)\n",
      "tensor(19.7749)\n",
      "tensor(32.5048)\n",
      "tensor(0.5621)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.406773\n",
      "Epoch 5596\n",
      "-------------------------------\n",
      "tensor(45.9424)\n",
      "tensor(13.7435)\n",
      "tensor(14.0414)\n",
      "tensor(0.3973)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.349487\n",
      "Epoch 5597\n",
      "-------------------------------\n",
      "tensor(48.6629)\n",
      "tensor(11.6556)\n",
      "tensor(16.7041)\n",
      "tensor(0.1392)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.333630\n",
      "Epoch 5598\n",
      "-------------------------------\n",
      "tensor(30.4908)\n",
      "tensor(18.3448)\n",
      "tensor(11.1442)\n",
      "tensor(0.6466)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.308949\n",
      "Epoch 5599\n",
      "-------------------------------\n",
      "tensor(21.4749)\n",
      "tensor(18.2311)\n",
      "tensor(20.6941)\n",
      "tensor(0.8092)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.300354\n",
      "Epoch 5600\n",
      "-------------------------------\n",
      "tensor(25.9232)\n",
      "tensor(14.9602)\n",
      "tensor(15.6188)\n",
      "tensor(0.5213)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.282722\n",
      "Epoch 5601\n",
      "-------------------------------\n",
      "tensor(37.6359)\n",
      "tensor(19.9145)\n",
      "tensor(7.3513)\n",
      "tensor(0.1776)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.266491\n",
      "Epoch 5602\n",
      "-------------------------------\n",
      "tensor(31.2270)\n",
      "tensor(22.9361)\n",
      "tensor(3.8635)\n",
      "tensor(0.1264)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.262024\n",
      "Epoch 5603\n",
      "-------------------------------\n",
      "tensor(30.5485)\n",
      "tensor(8.2973)\n",
      "tensor(14.6181)\n",
      "tensor(0.4080)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.266376\n",
      "Epoch 5604\n",
      "-------------------------------\n",
      "tensor(41.5427)\n",
      "tensor(12.6865)\n",
      "tensor(19.3817)\n",
      "tensor(0.5252)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.270046\n",
      "Epoch 5605\n",
      "-------------------------------\n",
      "tensor(43.4208)\n",
      "tensor(14.8443)\n",
      "tensor(5.7592)\n",
      "tensor(0.1521)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.258804\n",
      "Epoch 5606\n",
      "-------------------------------\n",
      "tensor(70.8100)\n",
      "tensor(23.5823)\n",
      "tensor(20.2460)\n",
      "tensor(0.6061)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.306915\n",
      "Epoch 5607\n",
      "-------------------------------\n",
      "tensor(38.6952)\n",
      "tensor(13.2713)\n",
      "tensor(8.4921)\n",
      "tensor(0.4164)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.280037\n",
      "Epoch 5608\n",
      "-------------------------------\n",
      "tensor(40.9798)\n",
      "tensor(8.6617)\n",
      "tensor(17.7241)\n",
      "tensor(0.0589)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.300011\n",
      "Epoch 5609\n",
      "-------------------------------\n",
      "tensor(64.8294)\n",
      "tensor(19.1831)\n",
      "tensor(25.3828)\n",
      "tensor(0.2763)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.362446\n",
      "Epoch 5610\n",
      "-------------------------------\n",
      "tensor(52.5509)\n",
      "tensor(14.2155)\n",
      "tensor(33.7593)\n",
      "tensor(1.0234)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.354000\n",
      "Epoch 5611\n",
      "-------------------------------\n",
      "tensor(38.7793)\n",
      "tensor(25.4542)\n",
      "tensor(34.4162)\n",
      "tensor(1.1892)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.338951\n",
      "Epoch 5612\n",
      "-------------------------------\n",
      "tensor(32.6752)\n",
      "tensor(7.7106)\n",
      "tensor(11.5296)\n",
      "tensor(0.0606)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.265919\n",
      "Epoch 5613\n",
      "-------------------------------\n",
      "tensor(46.4083)\n",
      "tensor(24.5109)\n",
      "tensor(4.5002)\n",
      "tensor(0.1992)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.272728\n",
      "Epoch 5614\n",
      "-------------------------------\n",
      "tensor(46.7782)\n",
      "tensor(27.6732)\n",
      "tensor(12.7593)\n",
      "tensor(0.0221)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.266922\n",
      "Epoch 5615\n",
      "-------------------------------\n",
      "tensor(46.8064)\n",
      "tensor(20.2458)\n",
      "tensor(13.1787)\n",
      "tensor(0.8809)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.275631\n",
      "Epoch 5616\n",
      "-------------------------------\n",
      "tensor(26.7706)\n",
      "tensor(20.2211)\n",
      "tensor(21.3748)\n",
      "tensor(0.9634)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.275303\n",
      "Epoch 5617\n",
      "-------------------------------\n",
      "tensor(29.3177)\n",
      "tensor(12.2076)\n",
      "tensor(8.4259)\n",
      "tensor(0.2531)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.275955\n",
      "Epoch 5618\n",
      "-------------------------------\n",
      "tensor(26.8679)\n",
      "tensor(14.7692)\n",
      "tensor(15.9394)\n",
      "tensor(0.9649)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.282600\n",
      "Epoch 5619\n",
      "-------------------------------\n",
      "tensor(25.4131)\n",
      "tensor(13.5073)\n",
      "tensor(13.4223)\n",
      "tensor(0.8066)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.266361\n",
      "Epoch 5620\n",
      "-------------------------------\n",
      "tensor(23.7085)\n",
      "tensor(11.8133)\n",
      "tensor(5.8020)\n",
      "tensor(0.3898)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.241879\n",
      "Epoch 5621\n",
      "-------------------------------\n",
      "tensor(24.2254)\n",
      "tensor(9.5795)\n",
      "tensor(4.2821)\n",
      "tensor(0.0624)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.225681\n",
      "Epoch 5622\n",
      "-------------------------------\n",
      "tensor(29.1486)\n",
      "tensor(10.7720)\n",
      "tensor(7.0517)\n",
      "tensor(0.1620)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.217293\n",
      "Epoch 5623\n",
      "-------------------------------\n",
      "tensor(25.0741)\n",
      "tensor(9.9802)\n",
      "tensor(8.7231)\n",
      "tensor(0.3255)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.202824\n",
      "Epoch 5624\n",
      "-------------------------------\n",
      "tensor(43.6003)\n",
      "tensor(22.9258)\n",
      "tensor(7.6760)\n",
      "tensor(0.3834)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.219753\n",
      "Epoch 5625\n",
      "-------------------------------\n",
      "tensor(38.3115)\n",
      "tensor(24.6767)\n",
      "tensor(10.4824)\n",
      "tensor(0.1755)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.214958\n",
      "Epoch 5626\n",
      "-------------------------------\n",
      "tensor(41.8004)\n",
      "tensor(26.0884)\n",
      "tensor(20.2103)\n",
      "tensor(0.0465)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.181595\n",
      "Epoch 5627\n",
      "-------------------------------\n",
      "tensor(50.9579)\n",
      "tensor(20.6975)\n",
      "tensor(6.1261)\n",
      "tensor(0.4579)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.288876\n",
      "Epoch 5628\n",
      "-------------------------------\n",
      "tensor(54.8704)\n",
      "tensor(31.8574)\n",
      "tensor(32.5976)\n",
      "tensor(1.0697)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.367451\n",
      "Epoch 5629\n",
      "-------------------------------\n",
      "tensor(50.8468)\n",
      "tensor(13.9042)\n",
      "tensor(14.0638)\n",
      "tensor(0.3299)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.248165\n",
      "Epoch 5630\n",
      "-------------------------------\n",
      "tensor(27.3263)\n",
      "tensor(9.7953)\n",
      "tensor(10.5595)\n",
      "tensor(0.2088)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.193905\n",
      "Epoch 5631\n",
      "-------------------------------\n",
      "tensor(78.3157)\n",
      "tensor(18.9652)\n",
      "tensor(35.0833)\n",
      "tensor(0.8509)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.413422\n",
      "Epoch 5632\n",
      "-------------------------------\n",
      "tensor(77.0656)\n",
      "tensor(54.3747)\n",
      "tensor(70.6398)\n",
      "tensor(1.8722)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.458675\n",
      "Epoch 5633\n",
      "-------------------------------\n",
      "tensor(71.9422)\n",
      "tensor(33.8236)\n",
      "tensor(55.9254)\n",
      "tensor(1.8137)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.380074\n",
      "Epoch 5634\n",
      "-------------------------------\n",
      "tensor(42.4171)\n",
      "tensor(11.5719)\n",
      "tensor(13.0748)\n",
      "tensor(0.1552)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.321945\n",
      "Epoch 5635\n",
      "-------------------------------\n",
      "tensor(46.9198)\n",
      "tensor(11.1138)\n",
      "tensor(21.3693)\n",
      "tensor(0.6478)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.325752\n",
      "Epoch 5636\n",
      "-------------------------------\n",
      "tensor(53.5368)\n",
      "tensor(24.8061)\n",
      "tensor(32.0037)\n",
      "tensor(0.7128)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.322754\n",
      "Epoch 5637\n",
      "-------------------------------\n",
      "tensor(21.4865)\n",
      "tensor(14.7010)\n",
      "tensor(11.1597)\n",
      "tensor(0.4165)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.219959\n",
      "Epoch 5638\n",
      "-------------------------------\n",
      "tensor(35.8706)\n",
      "tensor(9.1000)\n",
      "tensor(14.8582)\n",
      "tensor(0.2235)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.201950\n",
      "Epoch 5639\n",
      "-------------------------------\n",
      "tensor(32.8038)\n",
      "tensor(10.0117)\n",
      "tensor(14.2578)\n",
      "tensor(0.3194)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.170506\n",
      "Epoch 5640\n",
      "-------------------------------\n",
      "tensor(36.0342)\n",
      "tensor(25.8283)\n",
      "tensor(5.4106)\n",
      "tensor(0.1629)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.171909\n",
      "Epoch 5641\n",
      "-------------------------------\n",
      "tensor(39.1493)\n",
      "tensor(24.1840)\n",
      "tensor(4.0732)\n",
      "tensor(0.0439)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.170258\n",
      "Epoch 5642\n",
      "-------------------------------\n",
      "tensor(43.4026)\n",
      "tensor(23.8754)\n",
      "tensor(4.3869)\n",
      "tensor(0.0118)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.151161\n",
      "Epoch 5643\n",
      "-------------------------------\n",
      "tensor(26.5865)\n",
      "tensor(9.4280)\n",
      "tensor(2.7183)\n",
      "tensor(0.0303)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.115585\n",
      "Epoch 5644\n",
      "-------------------------------\n",
      "tensor(21.5266)\n",
      "tensor(10.1272)\n",
      "tensor(2.9652)\n",
      "tensor(0.0545)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.107655\n",
      "Epoch 5645\n",
      "-------------------------------\n",
      "tensor(34.0431)\n",
      "tensor(14.0520)\n",
      "tensor(2.7112)\n",
      "tensor(0.1082)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.119553\n",
      "Epoch 5646\n",
      "-------------------------------\n",
      "tensor(37.8559)\n",
      "tensor(16.5145)\n",
      "tensor(6.9554)\n",
      "tensor(0.2573)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.134853\n",
      "Epoch 5647\n",
      "-------------------------------\n",
      "tensor(48.3467)\n",
      "tensor(19.2594)\n",
      "tensor(8.5912)\n",
      "tensor(0.2695)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.220703\n",
      "Epoch 5648\n",
      "-------------------------------\n",
      "tensor(49.5204)\n",
      "tensor(24.5358)\n",
      "tensor(4.8545)\n",
      "tensor(0.1235)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.236584\n",
      "Epoch 5649\n",
      "-------------------------------\n",
      "tensor(53.2780)\n",
      "tensor(13.0843)\n",
      "tensor(20.7716)\n",
      "tensor(0.3044)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.178284\n",
      "Epoch 5650\n",
      "-------------------------------\n",
      "tensor(80.0106)\n",
      "tensor(21.2257)\n",
      "tensor(37.7551)\n",
      "tensor(1.1689)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.198700\n",
      "Epoch 5651\n",
      "-------------------------------\n",
      "tensor(82.2355)\n",
      "tensor(29.3390)\n",
      "tensor(61.9206)\n",
      "tensor(1.8477)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.274029\n",
      "Epoch 5652\n",
      "-------------------------------\n",
      "tensor(96.8195)\n",
      "tensor(31.5513)\n",
      "tensor(68.3599)\n",
      "tensor(1.4158)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.378601\n",
      "Epoch 5653\n",
      "-------------------------------\n",
      "tensor(82.3015)\n",
      "tensor(22.3295)\n",
      "tensor(55.6560)\n",
      "tensor(1.3337)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.309349\n",
      "Epoch 5654\n",
      "-------------------------------\n",
      "tensor(43.0628)\n",
      "tensor(19.9607)\n",
      "tensor(27.9607)\n",
      "tensor(0.9165)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.214993\n",
      "Epoch 5655\n",
      "-------------------------------\n",
      "tensor(60.0121)\n",
      "tensor(21.3375)\n",
      "tensor(19.1606)\n",
      "tensor(0.4995)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.199432\n",
      "Epoch 5656\n",
      "-------------------------------\n",
      "tensor(76.6607)\n",
      "tensor(18.5561)\n",
      "tensor(48.6120)\n",
      "tensor(1.0374)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.238998\n",
      "Epoch 5657\n",
      "-------------------------------\n",
      "tensor(37.2600)\n",
      "tensor(20.1495)\n",
      "tensor(4.3449)\n",
      "tensor(0.2413)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.133163\n",
      "Epoch 5658\n",
      "-------------------------------\n",
      "tensor(44.3204)\n",
      "tensor(23.6804)\n",
      "tensor(28.5647)\n",
      "tensor(0.9582)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.172577\n",
      "Epoch 5659\n",
      "-------------------------------\n",
      "tensor(31.2589)\n",
      "tensor(18.5064)\n",
      "tensor(17.8815)\n",
      "tensor(0.6612)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.139122\n",
      "Epoch 5660\n",
      "-------------------------------\n",
      "tensor(25.5686)\n",
      "tensor(11.6267)\n",
      "tensor(3.1554)\n",
      "tensor(0.1869)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.110996\n",
      "Epoch 5661\n",
      "-------------------------------\n",
      "tensor(26.1814)\n",
      "tensor(8.0025)\n",
      "tensor(7.2556)\n",
      "tensor(0.1082)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.102863\n",
      "Epoch 5662\n",
      "-------------------------------\n",
      "tensor(27.6456)\n",
      "tensor(7.6766)\n",
      "tensor(10.0587)\n",
      "tensor(0.2507)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.094269\n",
      "Epoch 5663\n",
      "-------------------------------\n",
      "tensor(23.6891)\n",
      "tensor(7.8441)\n",
      "tensor(8.0292)\n",
      "tensor(0.2734)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.074863\n",
      "Epoch 5664\n",
      "-------------------------------\n",
      "tensor(47.5161)\n",
      "tensor(23.7712)\n",
      "tensor(4.4177)\n",
      "tensor(0.1034)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.093472\n",
      "Epoch 5665\n",
      "-------------------------------\n",
      "tensor(46.4615)\n",
      "tensor(21.8827)\n",
      "tensor(7.6312)\n",
      "tensor(0.1569)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.089905\n",
      "Epoch 5666\n",
      "-------------------------------\n",
      "tensor(37.8407)\n",
      "tensor(23.0423)\n",
      "tensor(6.7680)\n",
      "tensor(0.2089)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.041096\n",
      "Epoch 5667\n",
      "-------------------------------\n",
      "tensor(54.6587)\n",
      "tensor(15.7027)\n",
      "tensor(20.2674)\n",
      "tensor(0.3230)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.098351\n",
      "Epoch 5668\n",
      "-------------------------------\n",
      "tensor(35.5372)\n",
      "tensor(22.2439)\n",
      "tensor(21.0286)\n",
      "tensor(0.9419)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.075348\n",
      "Epoch 5669\n",
      "-------------------------------\n",
      "tensor(44.1249)\n",
      "tensor(15.4066)\n",
      "tensor(14.6210)\n",
      "tensor(0.1988)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.132442\n",
      "Epoch 5670\n",
      "-------------------------------\n",
      "tensor(53.2139)\n",
      "tensor(20.3251)\n",
      "tensor(36.0197)\n",
      "tensor(1.3558)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.214195\n",
      "Epoch 5671\n",
      "-------------------------------\n",
      "tensor(63.5076)\n",
      "tensor(25.5128)\n",
      "tensor(41.5811)\n",
      "tensor(1.4269)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.209324\n",
      "Epoch 5672\n",
      "-------------------------------\n",
      "tensor(58.9560)\n",
      "tensor(18.4476)\n",
      "tensor(35.6949)\n",
      "tensor(0.5068)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.054832\n",
      "Epoch 5673\n",
      "-------------------------------\n",
      "tensor(66.6506)\n",
      "tensor(25.7541)\n",
      "tensor(16.3211)\n",
      "tensor(0.0172)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.197327\n",
      "Epoch 5674\n",
      "-------------------------------\n",
      "tensor(44.0451)\n",
      "tensor(32.4335)\n",
      "tensor(22.0854)\n",
      "tensor(0.7494)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.091675\n",
      "Epoch 5675\n",
      "-------------------------------\n",
      "tensor(41.8929)\n",
      "tensor(14.1871)\n",
      "tensor(12.8625)\n",
      "tensor(0.5745)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.089291\n",
      "Epoch 5676\n",
      "-------------------------------\n",
      "tensor(36.7316)\n",
      "tensor(36.4918)\n",
      "tensor(40.1623)\n",
      "tensor(1.7041)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.179363\n",
      "Epoch 5677\n",
      "-------------------------------\n",
      "tensor(28.8435)\n",
      "tensor(18.5266)\n",
      "tensor(12.9994)\n",
      "tensor(0.3435)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.137520\n",
      "Epoch 5678\n",
      "-------------------------------\n",
      "tensor(40.5234)\n",
      "tensor(10.6747)\n",
      "tensor(19.7241)\n",
      "tensor(0.7461)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.114304\n",
      "Epoch 5679\n",
      "-------------------------------\n",
      "tensor(33.4095)\n",
      "tensor(10.5519)\n",
      "tensor(16.3585)\n",
      "tensor(0.7071)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.063271\n",
      "Epoch 5680\n",
      "-------------------------------\n",
      "tensor(19.4555)\n",
      "tensor(8.4590)\n",
      "tensor(5.8750)\n",
      "tensor(0.2925)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.027355\n",
      "Epoch 5681\n",
      "-------------------------------\n",
      "tensor(42.9559)\n",
      "tensor(21.1825)\n",
      "tensor(7.6875)\n",
      "tensor(0.0535)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.039768\n",
      "Epoch 5682\n",
      "-------------------------------\n",
      "tensor(44.1892)\n",
      "tensor(20.4781)\n",
      "tensor(9.4789)\n",
      "tensor(0.2684)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.041130\n",
      "Epoch 5683\n",
      "-------------------------------\n",
      "tensor(40.6102)\n",
      "tensor(20.7519)\n",
      "tensor(7.1391)\n",
      "tensor(0.3901)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.015308\n",
      "Epoch 5684\n",
      "-------------------------------\n",
      "tensor(29.9402)\n",
      "tensor(8.3583)\n",
      "tensor(9.8257)\n",
      "tensor(0.4026)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.003578\n",
      "Epoch 5685\n",
      "-------------------------------\n",
      "tensor(46.1450)\n",
      "tensor(12.9806)\n",
      "tensor(15.3349)\n",
      "tensor(0.3484)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.039055\n",
      "Epoch 5686\n",
      "-------------------------------\n",
      "tensor(35.4557)\n",
      "tensor(15.2245)\n",
      "tensor(5.1058)\n",
      "tensor(0.3753)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.017128\n",
      "Epoch 5687\n",
      "-------------------------------\n",
      "tensor(50.3199)\n",
      "tensor(16.7206)\n",
      "tensor(19.1016)\n",
      "tensor(0.1082)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.017426\n",
      "Epoch 5688\n",
      "-------------------------------\n",
      "tensor(35.0252)\n",
      "tensor(15.8906)\n",
      "tensor(14.7355)\n",
      "tensor(0.9323)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.998379\n",
      "Epoch 5689\n",
      "-------------------------------\n",
      "tensor(43.3262)\n",
      "tensor(14.6206)\n",
      "tensor(16.1142)\n",
      "tensor(0.4749)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.979614\n",
      "Epoch 5690\n",
      "-------------------------------\n",
      "tensor(63.7914)\n",
      "tensor(21.6160)\n",
      "tensor(27.9163)\n",
      "tensor(1.2810)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.126904\n",
      "Epoch 5691\n",
      "-------------------------------\n",
      "tensor(69.4597)\n",
      "tensor(15.6825)\n",
      "tensor(38.8988)\n",
      "tensor(0.5721)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.054638\n",
      "Epoch 5692\n",
      "-------------------------------\n",
      "tensor(89.3277)\n",
      "tensor(19.6457)\n",
      "tensor(47.5261)\n",
      "tensor(0.7750)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.091988\n",
      "Epoch 5693\n",
      "-------------------------------\n",
      "tensor(96.2717)\n",
      "tensor(18.2523)\n",
      "tensor(58.8746)\n",
      "tensor(1.3863)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.180321\n",
      "Epoch 5694\n",
      "-------------------------------\n",
      "tensor(57.9406)\n",
      "tensor(41.1571)\n",
      "tensor(58.4351)\n",
      "tensor(2.1150)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 39.130894\n",
      "Epoch 5695\n",
      "-------------------------------\n",
      "tensor(23.9361)\n",
      "tensor(11.6295)\n",
      "tensor(3.1659)\n",
      "tensor(0.2700)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.992817\n",
      "Epoch 5696\n",
      "-------------------------------\n",
      "tensor(48.4650)\n",
      "tensor(18.9761)\n",
      "tensor(34.8863)\n",
      "tensor(1.2264)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.996857\n",
      "Epoch 5697\n",
      "-------------------------------\n",
      "tensor(48.7215)\n",
      "tensor(23.9874)\n",
      "tensor(11.3122)\n",
      "tensor(0.2244)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.974781\n",
      "Epoch 5698\n",
      "-------------------------------\n",
      "tensor(57.7417)\n",
      "tensor(21.6252)\n",
      "tensor(15.6959)\n",
      "tensor(0.2830)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.951027\n",
      "Epoch 5699\n",
      "-------------------------------\n",
      "tensor(31.9684)\n",
      "tensor(9.9494)\n",
      "tensor(6.2391)\n",
      "tensor(0.1818)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.894135\n",
      "Epoch 5700\n",
      "-------------------------------\n",
      "tensor(32.6314)\n",
      "tensor(6.4677)\n",
      "tensor(12.6624)\n",
      "tensor(0.1099)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.883671\n",
      "Epoch 5701\n",
      "-------------------------------\n",
      "tensor(30.9199)\n",
      "tensor(9.6015)\n",
      "tensor(11.0817)\n",
      "tensor(0.1140)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.873199\n",
      "Epoch 5702\n",
      "-------------------------------\n",
      "tensor(23.8792)\n",
      "tensor(7.0823)\n",
      "tensor(5.7073)\n",
      "tensor(0.1526)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.856125\n",
      "Epoch 5703\n",
      "-------------------------------\n",
      "tensor(16.9742)\n",
      "tensor(8.8941)\n",
      "tensor(5.2363)\n",
      "tensor(0.2006)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.837482\n",
      "Epoch 5704\n",
      "-------------------------------\n",
      "tensor(40.9326)\n",
      "tensor(14.7933)\n",
      "tensor(16.2011)\n",
      "tensor(0.1730)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.831997\n",
      "Epoch 5705\n",
      "-------------------------------\n",
      "tensor(67.4770)\n",
      "tensor(23.7828)\n",
      "tensor(16.6681)\n",
      "tensor(0.1356)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.876335\n",
      "Epoch 5706\n",
      "-------------------------------\n",
      "tensor(52.4577)\n",
      "tensor(14.4666)\n",
      "tensor(20.4358)\n",
      "tensor(0.6960)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.873108\n",
      "Epoch 5707\n",
      "-------------------------------\n",
      "tensor(51.6540)\n",
      "tensor(28.4094)\n",
      "tensor(22.7722)\n",
      "tensor(0.0169)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.835682\n",
      "Epoch 5708\n",
      "-------------------------------\n",
      "tensor(30.0311)\n",
      "tensor(27.2133)\n",
      "tensor(24.6859)\n",
      "tensor(1.7387)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.883148\n",
      "Epoch 5709\n",
      "-------------------------------\n",
      "tensor(44.9522)\n",
      "tensor(22.5924)\n",
      "tensor(27.6229)\n",
      "tensor(0.9786)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.929680\n",
      "Epoch 5710\n",
      "-------------------------------\n",
      "tensor(38.2884)\n",
      "tensor(46.8040)\n",
      "tensor(48.0843)\n",
      "tensor(2.2385)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.980301\n",
      "Epoch 5711\n",
      "-------------------------------\n",
      "tensor(21.8519)\n",
      "tensor(11.5039)\n",
      "tensor(13.9554)\n",
      "tensor(0.3573)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.922760\n",
      "Epoch 5712\n",
      "-------------------------------\n",
      "tensor(25.0037)\n",
      "tensor(24.5442)\n",
      "tensor(28.5280)\n",
      "tensor(1.5384)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.959137\n",
      "Epoch 5713\n",
      "-------------------------------\n",
      "tensor(59.7287)\n",
      "tensor(16.0126)\n",
      "tensor(35.8119)\n",
      "tensor(0.9062)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.972271\n",
      "Epoch 5714\n",
      "-------------------------------\n",
      "tensor(54.5383)\n",
      "tensor(15.5754)\n",
      "tensor(34.8760)\n",
      "tensor(0.1435)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.918594\n",
      "Epoch 5715\n",
      "-------------------------------\n",
      "tensor(34.3427)\n",
      "tensor(28.6972)\n",
      "tensor(11.6377)\n",
      "tensor(0.6141)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.866970\n",
      "Epoch 5716\n",
      "-------------------------------\n",
      "tensor(65.6884)\n",
      "tensor(12.5279)\n",
      "tensor(34.2623)\n",
      "tensor(0.2506)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.850079\n",
      "Epoch 5717\n",
      "-------------------------------\n",
      "tensor(49.5060)\n",
      "tensor(20.7568)\n",
      "tensor(23.7975)\n",
      "tensor(1.2933)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.779129\n",
      "Epoch 5718\n",
      "-------------------------------\n",
      "tensor(34.8609)\n",
      "tensor(24.3672)\n",
      "tensor(24.1755)\n",
      "tensor(1.1250)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.797039\n",
      "Epoch 5719\n",
      "-------------------------------\n",
      "tensor(36.9229)\n",
      "tensor(15.1583)\n",
      "tensor(5.2475)\n",
      "tensor(0.2732)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.781654\n",
      "Epoch 5720\n",
      "-------------------------------\n",
      "tensor(47.3258)\n",
      "tensor(13.7128)\n",
      "tensor(9.2012)\n",
      "tensor(0.2818)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.768410\n",
      "Epoch 5721\n",
      "-------------------------------\n",
      "tensor(45.1066)\n",
      "tensor(12.6748)\n",
      "tensor(12.7416)\n",
      "tensor(0.4614)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.742813\n",
      "Epoch 5722\n",
      "-------------------------------\n",
      "tensor(25.2060)\n",
      "tensor(7.9642)\n",
      "tensor(11.3973)\n",
      "tensor(0.4414)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.738480\n",
      "Epoch 5723\n",
      "-------------------------------\n",
      "tensor(20.7713)\n",
      "tensor(7.9268)\n",
      "tensor(5.3157)\n",
      "tensor(0.2484)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.728497\n",
      "Epoch 5724\n",
      "-------------------------------\n",
      "tensor(41.5554)\n",
      "tensor(20.1528)\n",
      "tensor(9.7803)\n",
      "tensor(0.1630)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.749603\n",
      "Epoch 5725\n",
      "-------------------------------\n",
      "tensor(49.3738)\n",
      "tensor(21.1926)\n",
      "tensor(13.8018)\n",
      "tensor(0.5133)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.769539\n",
      "Epoch 5726\n",
      "-------------------------------\n",
      "tensor(45.0076)\n",
      "tensor(13.9200)\n",
      "tensor(11.7145)\n",
      "tensor(0.3118)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.772820\n",
      "Epoch 5727\n",
      "-------------------------------\n",
      "tensor(48.2312)\n",
      "tensor(12.7211)\n",
      "tensor(17.0672)\n",
      "tensor(0.1103)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.770718\n",
      "Epoch 5728\n",
      "-------------------------------\n",
      "tensor(37.0020)\n",
      "tensor(14.6846)\n",
      "tensor(17.5830)\n",
      "tensor(0.3328)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.778069\n",
      "Epoch 5729\n",
      "-------------------------------\n",
      "tensor(52.4950)\n",
      "tensor(26.8126)\n",
      "tensor(10.4553)\n",
      "tensor(0.5974)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.762016\n",
      "Epoch 5730\n",
      "-------------------------------\n",
      "tensor(77.9131)\n",
      "tensor(15.8469)\n",
      "tensor(47.9348)\n",
      "tensor(0.8538)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.916599\n",
      "Epoch 5731\n",
      "-------------------------------\n",
      "tensor(91.0337)\n",
      "tensor(47.8716)\n",
      "tensor(83.7878)\n",
      "tensor(2.8112)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.060524\n",
      "Epoch 5732\n",
      "-------------------------------\n",
      "tensor(106.9623)\n",
      "tensor(32.4586)\n",
      "tensor(77.0571)\n",
      "tensor(2.2534)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.050739\n",
      "Epoch 5733\n",
      "-------------------------------\n",
      "tensor(97.0895)\n",
      "tensor(18.3326)\n",
      "tensor(54.1248)\n",
      "tensor(0.9186)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.888336\n",
      "Epoch 5734\n",
      "-------------------------------\n",
      "tensor(59.5484)\n",
      "tensor(9.9227)\n",
      "tensor(30.6448)\n",
      "tensor(0.2285)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.807297\n",
      "Epoch 5735\n",
      "-------------------------------\n",
      "tensor(41.4361)\n",
      "tensor(16.5346)\n",
      "tensor(7.4870)\n",
      "tensor(0.5731)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.751514\n",
      "Epoch 5736\n",
      "-------------------------------\n",
      "tensor(44.1713)\n",
      "tensor(17.5790)\n",
      "tensor(26.8936)\n",
      "tensor(0.6726)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.736317\n",
      "Epoch 5737\n",
      "-------------------------------\n",
      "tensor(43.4179)\n",
      "tensor(28.3692)\n",
      "tensor(8.6578)\n",
      "tensor(0.5492)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.733940\n",
      "Epoch 5738\n",
      "-------------------------------\n",
      "tensor(39.7287)\n",
      "tensor(34.4193)\n",
      "tensor(29.9720)\n",
      "tensor(0.9472)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.724472\n",
      "Epoch 5739\n",
      "-------------------------------\n",
      "tensor(44.1820)\n",
      "tensor(9.6847)\n",
      "tensor(20.4064)\n",
      "tensor(0.2987)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.702763\n",
      "Epoch 5740\n",
      "-------------------------------\n",
      "tensor(21.6097)\n",
      "tensor(9.4976)\n",
      "tensor(4.8905)\n",
      "tensor(0.3888)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.683113\n",
      "Epoch 5741\n",
      "-------------------------------\n",
      "tensor(17.3081)\n",
      "tensor(14.6515)\n",
      "tensor(14.6346)\n",
      "tensor(0.7418)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.683846\n",
      "Epoch 5742\n",
      "-------------------------------\n",
      "tensor(21.4247)\n",
      "tensor(16.6878)\n",
      "tensor(20.5510)\n",
      "tensor(0.8341)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.684631\n",
      "Epoch 5743\n",
      "-------------------------------\n",
      "tensor(22.9454)\n",
      "tensor(15.3999)\n",
      "tensor(20.0349)\n",
      "tensor(0.6726)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.673561\n",
      "Epoch 5744\n",
      "-------------------------------\n",
      "tensor(31.1754)\n",
      "tensor(12.1289)\n",
      "tensor(9.0294)\n",
      "tensor(0.0964)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.646309\n",
      "Epoch 5745\n",
      "-------------------------------\n",
      "tensor(48.4403)\n",
      "tensor(15.6266)\n",
      "tensor(17.5499)\n",
      "tensor(0.8307)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.668602\n",
      "Epoch 5746\n",
      "-------------------------------\n",
      "tensor(41.6845)\n",
      "tensor(34.8731)\n",
      "tensor(26.7403)\n",
      "tensor(1.0477)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.718620\n",
      "Epoch 5747\n",
      "-------------------------------\n",
      "tensor(42.7192)\n",
      "tensor(24.9561)\n",
      "tensor(6.2393)\n",
      "tensor(0.2910)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.697716\n",
      "Epoch 5748\n",
      "-------------------------------\n",
      "tensor(35.0502)\n",
      "tensor(24.1439)\n",
      "tensor(23.3033)\n",
      "tensor(1.4979)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.665726\n",
      "Epoch 5749\n",
      "-------------------------------\n",
      "tensor(38.0373)\n",
      "tensor(14.3358)\n",
      "tensor(4.1280)\n",
      "tensor(0.3362)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.673847\n",
      "Epoch 5750\n",
      "-------------------------------\n",
      "tensor(27.6547)\n",
      "tensor(14.0620)\n",
      "tensor(16.6355)\n",
      "tensor(1.0119)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.676052\n",
      "Epoch 5751\n",
      "-------------------------------\n",
      "tensor(29.3284)\n",
      "tensor(11.7534)\n",
      "tensor(16.2232)\n",
      "tensor(0.1375)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.655216\n",
      "Epoch 5752\n",
      "-------------------------------\n",
      "tensor(45.3207)\n",
      "tensor(27.4727)\n",
      "tensor(17.6618)\n",
      "tensor(0.2011)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.739449\n",
      "Epoch 5753\n",
      "-------------------------------\n",
      "tensor(37.0486)\n",
      "tensor(12.1253)\n",
      "tensor(15.5457)\n",
      "tensor(0.8781)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.652546\n",
      "Epoch 5754\n",
      "-------------------------------\n",
      "tensor(54.0879)\n",
      "tensor(19.2455)\n",
      "tensor(35.4679)\n",
      "tensor(0.5158)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.705307\n",
      "Epoch 5755\n",
      "-------------------------------\n",
      "tensor(55.7341)\n",
      "tensor(23.0793)\n",
      "tensor(37.4425)\n",
      "tensor(1.7773)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.724457\n",
      "Epoch 5756\n",
      "-------------------------------\n",
      "tensor(48.0949)\n",
      "tensor(26.6601)\n",
      "tensor(8.3523)\n",
      "tensor(0.2119)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.651203\n",
      "Epoch 5757\n",
      "-------------------------------\n",
      "tensor(42.8438)\n",
      "tensor(21.1953)\n",
      "tensor(19.9363)\n",
      "tensor(1.3648)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.634342\n",
      "Epoch 5758\n",
      "-------------------------------\n",
      "tensor(31.3409)\n",
      "tensor(20.3104)\n",
      "tensor(18.0847)\n",
      "tensor(1.3647)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.660812\n",
      "Epoch 5759\n",
      "-------------------------------\n",
      "tensor(26.2038)\n",
      "tensor(14.1380)\n",
      "tensor(9.2201)\n",
      "tensor(0.7515)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.646637\n",
      "Epoch 5760\n",
      "-------------------------------\n",
      "tensor(19.1856)\n",
      "tensor(9.7709)\n",
      "tensor(2.8570)\n",
      "tensor(0.2066)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.631992\n",
      "Epoch 5761\n",
      "-------------------------------\n",
      "tensor(19.8224)\n",
      "tensor(10.9980)\n",
      "tensor(5.7204)\n",
      "tensor(0.1359)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.626431\n",
      "Epoch 5762\n",
      "-------------------------------\n",
      "tensor(19.9893)\n",
      "tensor(9.8543)\n",
      "tensor(8.7496)\n",
      "tensor(0.3570)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.621174\n",
      "Epoch 5763\n",
      "-------------------------------\n",
      "tensor(26.8580)\n",
      "tensor(11.7557)\n",
      "tensor(10.7554)\n",
      "tensor(0.5245)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.611759\n",
      "Epoch 5764\n",
      "-------------------------------\n",
      "tensor(37.8005)\n",
      "tensor(13.4702)\n",
      "tensor(10.5143)\n",
      "tensor(0.6008)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.606941\n",
      "Epoch 5765\n",
      "-------------------------------\n",
      "tensor(45.0439)\n",
      "tensor(26.9127)\n",
      "tensor(6.4152)\n",
      "tensor(0.4117)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.573769\n",
      "Epoch 5766\n",
      "-------------------------------\n",
      "tensor(42.0141)\n",
      "tensor(9.9958)\n",
      "tensor(16.1325)\n",
      "tensor(0.1714)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.570488\n",
      "Epoch 5767\n",
      "-------------------------------\n",
      "tensor(54.2020)\n",
      "tensor(26.2007)\n",
      "tensor(16.9572)\n",
      "tensor(1.1040)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.675507\n",
      "Epoch 5768\n",
      "-------------------------------\n",
      "tensor(39.8282)\n",
      "tensor(16.7587)\n",
      "tensor(11.5170)\n",
      "tensor(0.8019)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.650940\n",
      "Epoch 5769\n",
      "-------------------------------\n",
      "tensor(31.6460)\n",
      "tensor(11.8150)\n",
      "tensor(9.6546)\n",
      "tensor(0.5396)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.594402\n",
      "Epoch 5770\n",
      "-------------------------------\n",
      "tensor(24.3803)\n",
      "tensor(15.3936)\n",
      "tensor(16.4205)\n",
      "tensor(1.0641)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.588135\n",
      "Epoch 5771\n",
      "-------------------------------\n",
      "tensor(39.1142)\n",
      "tensor(20.4158)\n",
      "tensor(6.4435)\n",
      "tensor(0.4345)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.596928\n",
      "Epoch 5772\n",
      "-------------------------------\n",
      "tensor(60.6106)\n",
      "tensor(18.7624)\n",
      "tensor(24.6553)\n",
      "tensor(0.9904)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.686134\n",
      "Epoch 5773\n",
      "-------------------------------\n",
      "tensor(64.0167)\n",
      "tensor(27.9114)\n",
      "tensor(43.4972)\n",
      "tensor(1.1636)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.673599\n",
      "Epoch 5774\n",
      "-------------------------------\n",
      "tensor(58.0506)\n",
      "tensor(28.7116)\n",
      "tensor(48.3680)\n",
      "tensor(2.1528)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.670223\n",
      "Epoch 5775\n",
      "-------------------------------\n",
      "tensor(45.9758)\n",
      "tensor(21.7319)\n",
      "tensor(10.8086)\n",
      "tensor(0.0387)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.627708\n",
      "Epoch 5776\n",
      "-------------------------------\n",
      "tensor(26.3679)\n",
      "tensor(16.2255)\n",
      "tensor(18.5780)\n",
      "tensor(1.1421)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.593987\n",
      "Epoch 5777\n",
      "-------------------------------\n",
      "tensor(39.0940)\n",
      "tensor(14.0857)\n",
      "tensor(11.3524)\n",
      "tensor(0.6564)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.589428\n",
      "Epoch 5778\n",
      "-------------------------------\n",
      "tensor(26.5742)\n",
      "tensor(10.0634)\n",
      "tensor(5.9024)\n",
      "tensor(0.0149)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.545570\n",
      "Epoch 5779\n",
      "-------------------------------\n",
      "tensor(47.4846)\n",
      "tensor(25.0035)\n",
      "tensor(5.6290)\n",
      "tensor(0.2844)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.551987\n",
      "Epoch 5780\n",
      "-------------------------------\n",
      "tensor(26.9379)\n",
      "tensor(11.3629)\n",
      "tensor(6.7733)\n",
      "tensor(0.3737)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.534241\n",
      "Epoch 5781\n",
      "-------------------------------\n",
      "tensor(29.2584)\n",
      "tensor(10.2959)\n",
      "tensor(6.1990)\n",
      "tensor(0.3334)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.527355\n",
      "Epoch 5782\n",
      "-------------------------------\n",
      "tensor(26.8230)\n",
      "tensor(9.5471)\n",
      "tensor(4.8276)\n",
      "tensor(0.2355)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.520752\n",
      "Epoch 5783\n",
      "-------------------------------\n",
      "tensor(40.6439)\n",
      "tensor(22.5533)\n",
      "tensor(2.6743)\n",
      "tensor(0.0640)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.516338\n",
      "Epoch 5784\n",
      "-------------------------------\n",
      "tensor(21.6838)\n",
      "tensor(7.5699)\n",
      "tensor(4.0468)\n",
      "tensor(0.2162)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.515224\n",
      "Epoch 5785\n",
      "-------------------------------\n",
      "tensor(24.5712)\n",
      "tensor(10.5022)\n",
      "tensor(7.8395)\n",
      "tensor(0.5911)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.509872\n",
      "Epoch 5786\n",
      "-------------------------------\n",
      "tensor(31.4970)\n",
      "tensor(16.1861)\n",
      "tensor(11.5984)\n",
      "tensor(0.7600)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.490955\n",
      "Epoch 5787\n",
      "-------------------------------\n",
      "tensor(49.8606)\n",
      "tensor(22.0283)\n",
      "tensor(7.4985)\n",
      "tensor(0.1584)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.533787\n",
      "Epoch 5788\n",
      "-------------------------------\n",
      "tensor(57.3680)\n",
      "tensor(16.4420)\n",
      "tensor(27.4232)\n",
      "tensor(0.9106)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.597431\n",
      "Epoch 5789\n",
      "-------------------------------\n",
      "tensor(39.7303)\n",
      "tensor(16.0995)\n",
      "tensor(13.4186)\n",
      "tensor(0.4972)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.501633\n",
      "Epoch 5790\n",
      "-------------------------------\n",
      "tensor(47.6858)\n",
      "tensor(20.0680)\n",
      "tensor(13.9786)\n",
      "tensor(0.7074)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.516518\n",
      "Epoch 5791\n",
      "-------------------------------\n",
      "tensor(77.6731)\n",
      "tensor(19.0499)\n",
      "tensor(46.8903)\n",
      "tensor(0.8032)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.628628\n",
      "Epoch 5792\n",
      "-------------------------------\n",
      "tensor(95.0704)\n",
      "tensor(34.2938)\n",
      "tensor(73.2231)\n",
      "tensor(1.6615)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.769379\n",
      "Epoch 5793\n",
      "-------------------------------\n",
      "tensor(97.7703)\n",
      "tensor(34.8746)\n",
      "tensor(74.4920)\n",
      "tensor(2.3611)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.811775\n",
      "Epoch 5794\n",
      "-------------------------------\n",
      "tensor(53.4124)\n",
      "tensor(23.0441)\n",
      "tensor(44.1793)\n",
      "tensor(1.2899)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.619949\n",
      "Epoch 5795\n",
      "-------------------------------\n",
      "tensor(46.0553)\n",
      "tensor(17.9981)\n",
      "tensor(16.7554)\n",
      "tensor(0.7247)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.546566\n",
      "Epoch 5796\n",
      "-------------------------------\n",
      "tensor(84.0950)\n",
      "tensor(19.7550)\n",
      "tensor(53.7707)\n",
      "tensor(1.0730)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.645870\n",
      "Epoch 5797\n",
      "-------------------------------\n",
      "tensor(33.3560)\n",
      "tensor(14.4729)\n",
      "tensor(7.9924)\n",
      "tensor(0.1995)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.494572\n",
      "Epoch 5798\n",
      "-------------------------------\n",
      "tensor(82.2130)\n",
      "tensor(20.9193)\n",
      "tensor(39.6106)\n",
      "tensor(0.8568)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.540150\n",
      "Epoch 5799\n",
      "-------------------------------\n",
      "tensor(32.1566)\n",
      "tensor(14.5765)\n",
      "tensor(13.7480)\n",
      "tensor(0.3641)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.455505\n",
      "Epoch 5800\n",
      "-------------------------------\n",
      "tensor(30.8331)\n",
      "tensor(7.7575)\n",
      "tensor(9.9064)\n",
      "tensor(0.1195)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.456509\n",
      "Epoch 5801\n",
      "-------------------------------\n",
      "tensor(42.3422)\n",
      "tensor(9.2945)\n",
      "tensor(18.0526)\n",
      "tensor(0.2919)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.466648\n",
      "Epoch 5802\n",
      "-------------------------------\n",
      "tensor(40.8028)\n",
      "tensor(9.1814)\n",
      "tensor(16.8119)\n",
      "tensor(0.2556)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.456966\n",
      "Epoch 5803\n",
      "-------------------------------\n",
      "tensor(29.0068)\n",
      "tensor(8.0756)\n",
      "tensor(7.2073)\n",
      "tensor(0.0334)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.429108\n",
      "Epoch 5804\n",
      "-------------------------------\n",
      "tensor(42.2687)\n",
      "tensor(18.3467)\n",
      "tensor(12.7272)\n",
      "tensor(0.3879)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.431328\n",
      "Epoch 5805\n",
      "-------------------------------\n",
      "tensor(60.4696)\n",
      "tensor(19.1987)\n",
      "tensor(22.8644)\n",
      "tensor(0.6187)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.455784\n",
      "Epoch 5806\n",
      "-------------------------------\n",
      "tensor(39.1864)\n",
      "tensor(12.2421)\n",
      "tensor(5.3937)\n",
      "tensor(0.0449)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.439312\n",
      "Epoch 5807\n",
      "-------------------------------\n",
      "tensor(61.4903)\n",
      "tensor(15.4839)\n",
      "tensor(27.0521)\n",
      "tensor(0.4410)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.513000\n",
      "Epoch 5808\n",
      "-------------------------------\n",
      "tensor(32.0912)\n",
      "tensor(16.3344)\n",
      "tensor(15.3739)\n",
      "tensor(0.6173)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.486065\n",
      "Epoch 5809\n",
      "-------------------------------\n",
      "tensor(41.8796)\n",
      "tensor(15.0328)\n",
      "tensor(20.8629)\n",
      "tensor(0.3936)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.481590\n",
      "Epoch 5810\n",
      "-------------------------------\n",
      "tensor(41.7057)\n",
      "tensor(41.7683)\n",
      "tensor(42.7033)\n",
      "tensor(1.4390)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.499218\n",
      "Epoch 5811\n",
      "-------------------------------\n",
      "tensor(37.5152)\n",
      "tensor(19.1340)\n",
      "tensor(22.9950)\n",
      "tensor(0.9826)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.510925\n",
      "Epoch 5812\n",
      "-------------------------------\n",
      "tensor(27.3050)\n",
      "tensor(15.3589)\n",
      "tensor(10.4416)\n",
      "tensor(0.6480)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.476318\n",
      "Epoch 5813\n",
      "-------------------------------\n",
      "tensor(42.4147)\n",
      "tensor(12.1874)\n",
      "tensor(24.5928)\n",
      "tensor(0.8535)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.473366\n",
      "Epoch 5814\n",
      "-------------------------------\n",
      "tensor(66.5273)\n",
      "tensor(16.8195)\n",
      "tensor(31.7782)\n",
      "tensor(0.4749)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.466297\n",
      "Epoch 5815\n",
      "-------------------------------\n",
      "tensor(60.8137)\n",
      "tensor(11.3306)\n",
      "tensor(30.1693)\n",
      "tensor(0.2798)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.465828\n",
      "Epoch 5816\n",
      "-------------------------------\n",
      "tensor(31.6752)\n",
      "tensor(15.8461)\n",
      "tensor(9.2030)\n",
      "tensor(0.7228)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.414333\n",
      "Epoch 5817\n",
      "-------------------------------\n",
      "tensor(50.8374)\n",
      "tensor(20.7129)\n",
      "tensor(26.6150)\n",
      "tensor(0.8117)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.414162\n",
      "Epoch 5818\n",
      "-------------------------------\n",
      "tensor(45.3793)\n",
      "tensor(23.7454)\n",
      "tensor(4.3253)\n",
      "tensor(0.1500)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.391975\n",
      "Epoch 5819\n",
      "-------------------------------\n",
      "tensor(33.6685)\n",
      "tensor(11.8083)\n",
      "tensor(21.7507)\n",
      "tensor(0.7169)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.373501\n",
      "Epoch 5820\n",
      "-------------------------------\n",
      "tensor(43.2735)\n",
      "tensor(13.4870)\n",
      "tensor(20.7384)\n",
      "tensor(0.6345)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.385521\n",
      "Epoch 5821\n",
      "-------------------------------\n",
      "tensor(37.2966)\n",
      "tensor(11.3549)\n",
      "tensor(11.2532)\n",
      "tensor(0.3276)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.368580\n",
      "Epoch 5822\n",
      "-------------------------------\n",
      "tensor(37.3124)\n",
      "tensor(22.5136)\n",
      "tensor(2.0972)\n",
      "tensor(0.0134)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.356960\n",
      "Epoch 5823\n",
      "-------------------------------\n",
      "tensor(17.5697)\n",
      "tensor(9.0861)\n",
      "tensor(10.2510)\n",
      "tensor(0.3836)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.354042\n",
      "Epoch 5824\n",
      "-------------------------------\n",
      "tensor(22.1403)\n",
      "tensor(13.1946)\n",
      "tensor(18.6221)\n",
      "tensor(0.6906)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.357464\n",
      "Epoch 5825\n",
      "-------------------------------\n",
      "tensor(32.7420)\n",
      "tensor(14.6777)\n",
      "tensor(13.7736)\n",
      "tensor(0.5461)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.359009\n",
      "Epoch 5826\n",
      "-------------------------------\n",
      "tensor(37.3971)\n",
      "tensor(12.2124)\n",
      "tensor(10.1660)\n",
      "tensor(0.2169)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.327568\n",
      "Epoch 5827\n",
      "-------------------------------\n",
      "tensor(43.3576)\n",
      "tensor(29.6825)\n",
      "tensor(17.8064)\n",
      "tensor(0.5655)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.417084\n",
      "Epoch 5828\n",
      "-------------------------------\n",
      "tensor(51.7052)\n",
      "tensor(23.4128)\n",
      "tensor(4.0297)\n",
      "tensor(0.3435)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.406784\n",
      "Epoch 5829\n",
      "-------------------------------\n",
      "tensor(36.3719)\n",
      "tensor(19.4286)\n",
      "tensor(12.6897)\n",
      "tensor(1.0360)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.437904\n",
      "Epoch 5830\n",
      "-------------------------------\n",
      "tensor(18.5024)\n",
      "tensor(15.9276)\n",
      "tensor(14.5677)\n",
      "tensor(0.5480)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.438705\n",
      "Epoch 5831\n",
      "-------------------------------\n",
      "tensor(30.8346)\n",
      "tensor(16.1549)\n",
      "tensor(18.4743)\n",
      "tensor(1.1404)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.480911\n",
      "Epoch 5832\n",
      "-------------------------------\n",
      "tensor(24.5438)\n",
      "tensor(9.9282)\n",
      "tensor(4.4226)\n",
      "tensor(0.2161)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.430061\n",
      "Epoch 5833\n",
      "-------------------------------\n",
      "tensor(34.9603)\n",
      "tensor(12.9078)\n",
      "tensor(13.8708)\n",
      "tensor(0.7901)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.440693\n",
      "Epoch 5834\n",
      "-------------------------------\n",
      "tensor(25.5306)\n",
      "tensor(9.7519)\n",
      "tensor(8.9638)\n",
      "tensor(0.0411)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.368397\n",
      "Epoch 5835\n",
      "-------------------------------\n",
      "tensor(52.9475)\n",
      "tensor(28.8707)\n",
      "tensor(9.0820)\n",
      "tensor(0.5827)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.449871\n",
      "Epoch 5836\n",
      "-------------------------------\n",
      "tensor(45.3236)\n",
      "tensor(30.3848)\n",
      "tensor(18.8792)\n",
      "tensor(0.6981)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.395721\n",
      "Epoch 5837\n",
      "-------------------------------\n",
      "tensor(49.7982)\n",
      "tensor(16.2795)\n",
      "tensor(13.1024)\n",
      "tensor(0.4465)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.366966\n",
      "Epoch 5838\n",
      "-------------------------------\n",
      "tensor(17.8515)\n",
      "tensor(23.8732)\n",
      "tensor(19.3064)\n",
      "tensor(1.3489)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.351936\n",
      "Epoch 5839\n",
      "-------------------------------\n",
      "tensor(17.6178)\n",
      "tensor(23.7885)\n",
      "tensor(23.6125)\n",
      "tensor(1.2219)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.352955\n",
      "Epoch 5840\n",
      "-------------------------------\n",
      "tensor(18.0041)\n",
      "tensor(17.2214)\n",
      "tensor(15.8899)\n",
      "tensor(0.6677)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.333069\n",
      "Epoch 5841\n",
      "-------------------------------\n",
      "tensor(18.4365)\n",
      "tensor(11.5436)\n",
      "tensor(7.9365)\n",
      "tensor(0.1682)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.317230\n",
      "Epoch 5842\n",
      "-------------------------------\n",
      "tensor(20.3069)\n",
      "tensor(8.5771)\n",
      "tensor(6.5425)\n",
      "tensor(0.2220)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.305389\n",
      "Epoch 5843\n",
      "-------------------------------\n",
      "tensor(31.3754)\n",
      "tensor(10.4513)\n",
      "tensor(11.4169)\n",
      "tensor(0.5620)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.294487\n",
      "Epoch 5844\n",
      "-------------------------------\n",
      "tensor(33.0926)\n",
      "tensor(12.3192)\n",
      "tensor(14.2052)\n",
      "tensor(0.7407)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.275459\n",
      "Epoch 5845\n",
      "-------------------------------\n",
      "tensor(33.4557)\n",
      "tensor(26.1674)\n",
      "tensor(6.5505)\n",
      "tensor(0.3965)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.303970\n",
      "Epoch 5846\n",
      "-------------------------------\n",
      "tensor(55.0827)\n",
      "tensor(26.1866)\n",
      "tensor(7.3287)\n",
      "tensor(0.3267)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.385326\n",
      "Epoch 5847\n",
      "-------------------------------\n",
      "tensor(57.6937)\n",
      "tensor(29.8806)\n",
      "tensor(17.8425)\n",
      "tensor(0.7115)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.368755\n",
      "Epoch 5848\n",
      "-------------------------------\n",
      "tensor(54.5085)\n",
      "tensor(15.5110)\n",
      "tensor(23.7978)\n",
      "tensor(0.6468)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.372494\n",
      "Epoch 5849\n",
      "-------------------------------\n",
      "tensor(53.4622)\n",
      "tensor(20.6801)\n",
      "tensor(34.5839)\n",
      "tensor(0.5242)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.412739\n",
      "Epoch 5850\n",
      "-------------------------------\n",
      "tensor(39.2293)\n",
      "tensor(22.4113)\n",
      "tensor(31.8961)\n",
      "tensor(1.8186)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.431278\n",
      "Epoch 5851\n",
      "-------------------------------\n",
      "tensor(21.8006)\n",
      "tensor(10.0140)\n",
      "tensor(2.3832)\n",
      "tensor(0.1497)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.398682\n",
      "Epoch 5852\n",
      "-------------------------------\n",
      "tensor(35.2911)\n",
      "tensor(29.6414)\n",
      "tensor(38.5399)\n",
      "tensor(1.9979)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.430111\n",
      "Epoch 5853\n",
      "-------------------------------\n",
      "tensor(56.6584)\n",
      "tensor(18.9804)\n",
      "tensor(38.0415)\n",
      "tensor(1.0848)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.404037\n",
      "Epoch 5854\n",
      "-------------------------------\n",
      "tensor(67.8976)\n",
      "tensor(15.9795)\n",
      "tensor(29.8831)\n",
      "tensor(0.3637)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.410538\n",
      "Epoch 5855\n",
      "-------------------------------\n",
      "tensor(46.3635)\n",
      "tensor(31.4261)\n",
      "tensor(13.0454)\n",
      "tensor(0.5437)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.362705\n",
      "Epoch 5856\n",
      "-------------------------------\n",
      "tensor(58.6451)\n",
      "tensor(30.7317)\n",
      "tensor(35.1742)\n",
      "tensor(0.1705)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.344204\n",
      "Epoch 5857\n",
      "-------------------------------\n",
      "tensor(29.7189)\n",
      "tensor(22.8019)\n",
      "tensor(20.0931)\n",
      "tensor(1.4754)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.303246\n",
      "Epoch 5858\n",
      "-------------------------------\n",
      "tensor(39.8334)\n",
      "tensor(26.4189)\n",
      "tensor(32.6140)\n",
      "tensor(1.2846)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.322388\n",
      "Epoch 5859\n",
      "-------------------------------\n",
      "tensor(32.0969)\n",
      "tensor(16.4446)\n",
      "tensor(17.5245)\n",
      "tensor(0.2227)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.303440\n",
      "Epoch 5860\n",
      "-------------------------------\n",
      "tensor(30.8709)\n",
      "tensor(12.1573)\n",
      "tensor(11.5503)\n",
      "tensor(0.5467)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.289261\n",
      "Epoch 5861\n",
      "-------------------------------\n",
      "tensor(27.4991)\n",
      "tensor(10.8997)\n",
      "tensor(16.6925)\n",
      "tensor(0.8381)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.286625\n",
      "Epoch 5862\n",
      "-------------------------------\n",
      "tensor(28.1452)\n",
      "tensor(10.7334)\n",
      "tensor(17.3507)\n",
      "tensor(0.8283)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.277534\n",
      "Epoch 5863\n",
      "-------------------------------\n",
      "tensor(23.5075)\n",
      "tensor(8.4446)\n",
      "tensor(11.7269)\n",
      "tensor(0.5368)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.255230\n",
      "Epoch 5864\n",
      "-------------------------------\n",
      "tensor(22.7950)\n",
      "tensor(9.2589)\n",
      "tensor(4.6032)\n",
      "tensor(0.1493)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.230167\n",
      "Epoch 5865\n",
      "-------------------------------\n",
      "tensor(52.0072)\n",
      "tensor(19.2337)\n",
      "tensor(22.4173)\n",
      "tensor(1.0437)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.275318\n",
      "Epoch 5866\n",
      "-------------------------------\n",
      "tensor(50.5824)\n",
      "tensor(23.8042)\n",
      "tensor(16.5828)\n",
      "tensor(1.0611)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.261143\n",
      "Epoch 5867\n",
      "-------------------------------\n",
      "tensor(68.8660)\n",
      "tensor(18.3255)\n",
      "tensor(33.8890)\n",
      "tensor(0.1291)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.321960\n",
      "Epoch 5868\n",
      "-------------------------------\n",
      "tensor(43.3981)\n",
      "tensor(14.6295)\n",
      "tensor(4.7502)\n",
      "tensor(0.1196)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.296074\n",
      "Epoch 5869\n",
      "-------------------------------\n",
      "tensor(66.5654)\n",
      "tensor(16.8684)\n",
      "tensor(28.9154)\n",
      "tensor(0.1034)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.291031\n",
      "Epoch 5870\n",
      "-------------------------------\n",
      "tensor(70.0986)\n",
      "tensor(23.0025)\n",
      "tensor(50.1084)\n",
      "tensor(1.3091)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.345413\n",
      "Epoch 5871\n",
      "-------------------------------\n",
      "tensor(88.3901)\n",
      "tensor(23.8710)\n",
      "tensor(56.4765)\n",
      "tensor(2.0325)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.408772\n",
      "Epoch 5872\n",
      "-------------------------------\n",
      "tensor(94.3275)\n",
      "tensor(19.5137)\n",
      "tensor(58.2837)\n",
      "tensor(1.0010)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.484062\n",
      "Epoch 5873\n",
      "-------------------------------\n",
      "tensor(89.0348)\n",
      "tensor(33.5277)\n",
      "tensor(63.1253)\n",
      "tensor(1.1805)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.526028\n",
      "Epoch 5874\n",
      "-------------------------------\n",
      "tensor(57.2914)\n",
      "tensor(16.1060)\n",
      "tensor(38.5813)\n",
      "tensor(1.2596)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.374371\n",
      "Epoch 5875\n",
      "-------------------------------\n",
      "tensor(26.0059)\n",
      "tensor(9.3665)\n",
      "tensor(3.7959)\n",
      "tensor(0.2739)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.275429\n",
      "Epoch 5876\n",
      "-------------------------------\n",
      "tensor(74.2561)\n",
      "tensor(21.1722)\n",
      "tensor(36.9492)\n",
      "tensor(1.3409)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.335251\n",
      "Epoch 5877\n",
      "-------------------------------\n",
      "tensor(38.6316)\n",
      "tensor(27.3768)\n",
      "tensor(19.5142)\n",
      "tensor(0.1471)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.266441\n",
      "Epoch 5878\n",
      "-------------------------------\n",
      "tensor(57.9505)\n",
      "tensor(13.4309)\n",
      "tensor(34.0183)\n",
      "tensor(0.6338)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.288330\n",
      "Epoch 5879\n",
      "-------------------------------\n",
      "tensor(30.3048)\n",
      "tensor(9.0699)\n",
      "tensor(6.6646)\n",
      "tensor(0.1150)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.221523\n",
      "Epoch 5880\n",
      "-------------------------------\n",
      "tensor(26.1863)\n",
      "tensor(12.4789)\n",
      "tensor(15.3120)\n",
      "tensor(0.3126)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.216991\n",
      "Epoch 5881\n",
      "-------------------------------\n",
      "tensor(37.7731)\n",
      "tensor(15.6253)\n",
      "tensor(22.2467)\n",
      "tensor(0.4285)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.218624\n",
      "Epoch 5882\n",
      "-------------------------------\n",
      "tensor(32.6928)\n",
      "tensor(14.0969)\n",
      "tensor(19.9658)\n",
      "tensor(0.3481)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.206814\n",
      "Epoch 5883\n",
      "-------------------------------\n",
      "tensor(18.0508)\n",
      "tensor(9.6072)\n",
      "tensor(9.8039)\n",
      "tensor(0.0875)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.184227\n",
      "Epoch 5884\n",
      "-------------------------------\n",
      "tensor(24.0431)\n",
      "tensor(7.5091)\n",
      "tensor(10.7190)\n",
      "tensor(0.3559)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.167362\n",
      "Epoch 5885\n",
      "-------------------------------\n",
      "tensor(47.9231)\n",
      "tensor(14.6023)\n",
      "tensor(24.5938)\n",
      "tensor(0.6241)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.172596\n",
      "Epoch 5886\n",
      "-------------------------------\n",
      "tensor(47.2031)\n",
      "tensor(26.7096)\n",
      "tensor(6.9738)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.180965\n",
      "Epoch 5887\n",
      "-------------------------------\n",
      "tensor(58.0676)\n",
      "tensor(23.2235)\n",
      "tensor(18.0126)\n",
      "tensor(0.9778)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.241638\n",
      "Epoch 5888\n",
      "-------------------------------\n",
      "tensor(48.2569)\n",
      "tensor(14.9467)\n",
      "tensor(14.9973)\n",
      "tensor(0.4623)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.230553\n",
      "Epoch 5889\n",
      "-------------------------------\n",
      "tensor(31.5956)\n",
      "tensor(11.0435)\n",
      "tensor(2.2802)\n",
      "tensor(0.0480)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.198872\n",
      "Epoch 5890\n",
      "-------------------------------\n",
      "tensor(35.5066)\n",
      "tensor(11.9077)\n",
      "tensor(18.3832)\n",
      "tensor(0.2530)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.181206\n",
      "Epoch 5891\n",
      "-------------------------------\n",
      "tensor(40.3099)\n",
      "tensor(13.9106)\n",
      "tensor(26.0167)\n",
      "tensor(0.8256)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.180443\n",
      "Epoch 5892\n",
      "-------------------------------\n",
      "tensor(66.0581)\n",
      "tensor(19.6901)\n",
      "tensor(33.2071)\n",
      "tensor(1.3310)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.197395\n",
      "Epoch 5893\n",
      "-------------------------------\n",
      "tensor(79.4182)\n",
      "tensor(16.0173)\n",
      "tensor(45.6864)\n",
      "tensor(0.5077)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.305588\n",
      "Epoch 5894\n",
      "-------------------------------\n",
      "tensor(73.7080)\n",
      "tensor(27.0206)\n",
      "tensor(48.3466)\n",
      "tensor(1.0071)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.302616\n",
      "Epoch 5895\n",
      "-------------------------------\n",
      "tensor(35.2947)\n",
      "tensor(14.6361)\n",
      "tensor(20.5741)\n",
      "tensor(0.9440)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.185513\n",
      "Epoch 5896\n",
      "-------------------------------\n",
      "tensor(38.8192)\n",
      "tensor(11.1123)\n",
      "tensor(23.2113)\n",
      "tensor(0.6428)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.153915\n",
      "Epoch 5897\n",
      "-------------------------------\n",
      "tensor(54.7799)\n",
      "tensor(18.0712)\n",
      "tensor(22.8035)\n",
      "tensor(0.8290)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.139072\n",
      "Epoch 5898\n",
      "-------------------------------\n",
      "tensor(31.5602)\n",
      "tensor(15.3404)\n",
      "tensor(15.5486)\n",
      "tensor(0.8026)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.122852\n",
      "Epoch 5899\n",
      "-------------------------------\n",
      "tensor(40.0068)\n",
      "tensor(12.9857)\n",
      "tensor(5.6991)\n",
      "tensor(0.2366)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.112049\n",
      "Epoch 5900\n",
      "-------------------------------\n",
      "tensor(35.3631)\n",
      "tensor(9.3301)\n",
      "tensor(10.8550)\n",
      "tensor(0.1109)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.101315\n",
      "Epoch 5901\n",
      "-------------------------------\n",
      "tensor(28.9765)\n",
      "tensor(8.0159)\n",
      "tensor(9.7669)\n",
      "tensor(0.2187)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.093742\n",
      "Epoch 5902\n",
      "-------------------------------\n",
      "tensor(18.6280)\n",
      "tensor(6.5832)\n",
      "tensor(5.5243)\n",
      "tensor(0.2074)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.085911\n",
      "Epoch 5903\n",
      "-------------------------------\n",
      "tensor(36.2193)\n",
      "tensor(21.4516)\n",
      "tensor(5.1215)\n",
      "tensor(0.1102)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.086525\n",
      "Epoch 5904\n",
      "-------------------------------\n",
      "tensor(41.1676)\n",
      "tensor(19.6618)\n",
      "tensor(10.2298)\n",
      "tensor(0.0288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.076687\n",
      "Epoch 5905\n",
      "-------------------------------\n",
      "tensor(26.2822)\n",
      "tensor(10.3749)\n",
      "tensor(4.3159)\n",
      "tensor(0.1027)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.066116\n",
      "Epoch 5906\n",
      "-------------------------------\n",
      "tensor(47.4522)\n",
      "tensor(15.1301)\n",
      "tensor(7.4316)\n",
      "tensor(0.1456)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.088257\n",
      "Epoch 5907\n",
      "-------------------------------\n",
      "tensor(43.4776)\n",
      "tensor(16.1020)\n",
      "tensor(6.9832)\n",
      "tensor(0.4604)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.140247\n",
      "Epoch 5908\n",
      "-------------------------------\n",
      "tensor(37.8610)\n",
      "tensor(18.9232)\n",
      "tensor(16.4097)\n",
      "tensor(0.7033)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.138752\n",
      "Epoch 5909\n",
      "-------------------------------\n",
      "tensor(31.9354)\n",
      "tensor(13.1184)\n",
      "tensor(7.1141)\n",
      "tensor(0.4421)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.070110\n",
      "Epoch 5910\n",
      "-------------------------------\n",
      "tensor(37.2233)\n",
      "tensor(33.0910)\n",
      "tensor(15.7581)\n",
      "tensor(0.7765)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.149345\n",
      "Epoch 5911\n",
      "-------------------------------\n",
      "tensor(37.7777)\n",
      "tensor(12.9645)\n",
      "tensor(8.7258)\n",
      "tensor(0.4631)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.139549\n",
      "Epoch 5912\n",
      "-------------------------------\n",
      "tensor(25.0485)\n",
      "tensor(15.0505)\n",
      "tensor(17.2410)\n",
      "tensor(0.8931)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.111656\n",
      "Epoch 5913\n",
      "-------------------------------\n",
      "tensor(33.0291)\n",
      "tensor(11.9417)\n",
      "tensor(18.4960)\n",
      "tensor(0.8090)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.130856\n",
      "Epoch 5914\n",
      "-------------------------------\n",
      "tensor(52.8138)\n",
      "tensor(19.9070)\n",
      "tensor(15.6782)\n",
      "tensor(0.0841)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.140106\n",
      "Epoch 5915\n",
      "-------------------------------\n",
      "tensor(52.7703)\n",
      "tensor(13.1793)\n",
      "tensor(18.9235)\n",
      "tensor(0.2602)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.163910\n",
      "Epoch 5916\n",
      "-------------------------------\n",
      "tensor(18.7193)\n",
      "tensor(19.3532)\n",
      "tensor(16.5508)\n",
      "tensor(1.1763)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.130661\n",
      "Epoch 5917\n",
      "-------------------------------\n",
      "tensor(23.6280)\n",
      "tensor(14.8529)\n",
      "tensor(20.2841)\n",
      "tensor(0.6672)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.106445\n",
      "Epoch 5918\n",
      "-------------------------------\n",
      "tensor(25.8382)\n",
      "tensor(11.4022)\n",
      "tensor(7.8578)\n",
      "tensor(0.4525)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.076393\n",
      "Epoch 5919\n",
      "-------------------------------\n",
      "tensor(29.5484)\n",
      "tensor(30.1615)\n",
      "tensor(15.9875)\n",
      "tensor(0.9067)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.074856\n",
      "Epoch 5920\n",
      "-------------------------------\n",
      "tensor(28.9659)\n",
      "tensor(29.3940)\n",
      "tensor(17.0924)\n",
      "tensor(0.7931)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.054108\n",
      "Epoch 5921\n",
      "-------------------------------\n",
      "tensor(26.0894)\n",
      "tensor(8.9208)\n",
      "tensor(13.2790)\n",
      "tensor(0.4647)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.040791\n",
      "Epoch 5922\n",
      "-------------------------------\n",
      "tensor(28.4193)\n",
      "tensor(8.3985)\n",
      "tensor(6.0367)\n",
      "tensor(0.0759)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.031654\n",
      "Epoch 5923\n",
      "-------------------------------\n",
      "tensor(33.1957)\n",
      "tensor(12.9098)\n",
      "tensor(6.4363)\n",
      "tensor(0.4128)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.027401\n",
      "Epoch 5924\n",
      "-------------------------------\n",
      "tensor(37.9126)\n",
      "tensor(18.9706)\n",
      "tensor(18.7146)\n",
      "tensor(0.9351)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.034962\n",
      "Epoch 5925\n",
      "-------------------------------\n",
      "tensor(38.1710)\n",
      "tensor(19.6613)\n",
      "tensor(20.0573)\n",
      "tensor(1.0276)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.023350\n",
      "Epoch 5926\n",
      "-------------------------------\n",
      "tensor(33.7990)\n",
      "tensor(10.1948)\n",
      "tensor(5.9399)\n",
      "tensor(0.1413)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.998821\n",
      "Epoch 5927\n",
      "-------------------------------\n",
      "tensor(43.8309)\n",
      "tensor(32.2380)\n",
      "tensor(21.8276)\n",
      "tensor(0.8680)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.067894\n",
      "Epoch 5928\n",
      "-------------------------------\n",
      "tensor(37.5938)\n",
      "tensor(13.1434)\n",
      "tensor(6.1744)\n",
      "tensor(0.2900)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.030262\n",
      "Epoch 5929\n",
      "-------------------------------\n",
      "tensor(38.6270)\n",
      "tensor(20.4906)\n",
      "tensor(25.9420)\n",
      "tensor(1.0071)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.024910\n",
      "Epoch 5930\n",
      "-------------------------------\n",
      "tensor(42.2915)\n",
      "tensor(7.7891)\n",
      "tensor(22.3555)\n",
      "tensor(0.2610)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.038288\n",
      "Epoch 5931\n",
      "-------------------------------\n",
      "tensor(63.7690)\n",
      "tensor(20.6190)\n",
      "tensor(20.1887)\n",
      "tensor(0.1659)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.127018\n",
      "Epoch 5932\n",
      "-------------------------------\n",
      "tensor(60.9603)\n",
      "tensor(13.4274)\n",
      "tensor(36.0399)\n",
      "tensor(0.7405)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.099964\n",
      "Epoch 5933\n",
      "-------------------------------\n",
      "tensor(54.1489)\n",
      "tensor(30.4413)\n",
      "tensor(51.1320)\n",
      "tensor(1.7153)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.121113\n",
      "Epoch 5934\n",
      "-------------------------------\n",
      "tensor(54.3330)\n",
      "tensor(14.4330)\n",
      "tensor(32.0738)\n",
      "tensor(0.8942)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.094402\n",
      "Epoch 5935\n",
      "-------------------------------\n",
      "tensor(37.2160)\n",
      "tensor(11.7463)\n",
      "tensor(4.9909)\n",
      "tensor(0.2768)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.013760\n",
      "Epoch 5936\n",
      "-------------------------------\n",
      "tensor(55.6109)\n",
      "tensor(17.5300)\n",
      "tensor(22.7730)\n",
      "tensor(0.6288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.057415\n",
      "Epoch 5937\n",
      "-------------------------------\n",
      "tensor(33.7009)\n",
      "tensor(24.9843)\n",
      "tensor(15.9007)\n",
      "tensor(0.0308)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.983906\n",
      "Epoch 5938\n",
      "-------------------------------\n",
      "tensor(49.3075)\n",
      "tensor(8.4950)\n",
      "tensor(23.5501)\n",
      "tensor(0.0934)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.997593\n",
      "Epoch 5939\n",
      "-------------------------------\n",
      "tensor(43.1275)\n",
      "tensor(15.4113)\n",
      "tensor(3.7479)\n",
      "tensor(0.3020)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.977890\n",
      "Epoch 5940\n",
      "-------------------------------\n",
      "tensor(40.7688)\n",
      "tensor(17.5181)\n",
      "tensor(13.4944)\n",
      "tensor(0.5142)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.980240\n",
      "Epoch 5941\n",
      "-------------------------------\n",
      "tensor(35.7472)\n",
      "tensor(16.2294)\n",
      "tensor(16.6720)\n",
      "tensor(0.4970)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.968407\n",
      "Epoch 5942\n",
      "-------------------------------\n",
      "tensor(19.3474)\n",
      "tensor(11.7692)\n",
      "tensor(13.6664)\n",
      "tensor(0.3446)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.958946\n",
      "Epoch 5943\n",
      "-------------------------------\n",
      "tensor(30.0847)\n",
      "tensor(11.3044)\n",
      "tensor(6.0721)\n",
      "tensor(0.0518)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.950920\n",
      "Epoch 5944\n",
      "-------------------------------\n",
      "tensor(27.2075)\n",
      "tensor(9.0524)\n",
      "tensor(9.7076)\n",
      "tensor(0.3778)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.953720\n",
      "Epoch 5945\n",
      "-------------------------------\n",
      "tensor(42.6714)\n",
      "tensor(14.0659)\n",
      "tensor(18.2506)\n",
      "tensor(0.6326)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.950119\n",
      "Epoch 5946\n",
      "-------------------------------\n",
      "tensor(39.1939)\n",
      "tensor(25.4722)\n",
      "tensor(5.6015)\n",
      "tensor(0.1388)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.971252\n",
      "Epoch 5947\n",
      "-------------------------------\n",
      "tensor(47.8563)\n",
      "tensor(22.1042)\n",
      "tensor(12.0164)\n",
      "tensor(0.6251)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.985046\n",
      "Epoch 5948\n",
      "-------------------------------\n",
      "tensor(38.7206)\n",
      "tensor(12.4037)\n",
      "tensor(13.5090)\n",
      "tensor(0.3821)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.973667\n",
      "Epoch 5949\n",
      "-------------------------------\n",
      "tensor(33.3813)\n",
      "tensor(14.9203)\n",
      "tensor(9.3350)\n",
      "tensor(0.2039)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.950603\n",
      "Epoch 5950\n",
      "-------------------------------\n",
      "tensor(42.1007)\n",
      "tensor(13.3740)\n",
      "tensor(11.3293)\n",
      "tensor(0.4239)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.019634\n",
      "Epoch 5951\n",
      "-------------------------------\n",
      "tensor(49.2709)\n",
      "tensor(13.8932)\n",
      "tensor(13.5053)\n",
      "tensor(0.1736)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.997688\n",
      "Epoch 5952\n",
      "-------------------------------\n",
      "tensor(26.9567)\n",
      "tensor(15.9138)\n",
      "tensor(23.8087)\n",
      "tensor(1.2721)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.005123\n",
      "Epoch 5953\n",
      "-------------------------------\n",
      "tensor(36.8327)\n",
      "tensor(29.8221)\n",
      "tensor(10.9880)\n",
      "tensor(0.4788)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.060661\n",
      "Epoch 5954\n",
      "-------------------------------\n",
      "tensor(30.6236)\n",
      "tensor(17.3971)\n",
      "tensor(25.3442)\n",
      "tensor(1.1108)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.015724\n",
      "Epoch 5955\n",
      "-------------------------------\n",
      "tensor(51.0136)\n",
      "tensor(19.8974)\n",
      "tensor(34.0346)\n",
      "tensor(0.7873)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.032578\n",
      "Epoch 5956\n",
      "-------------------------------\n",
      "tensor(29.8010)\n",
      "tensor(9.9099)\n",
      "tensor(5.6503)\n",
      "tensor(0.2615)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.972023\n",
      "Epoch 5957\n",
      "-------------------------------\n",
      "tensor(46.7642)\n",
      "tensor(10.6793)\n",
      "tensor(17.0198)\n",
      "tensor(0.0691)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.991699\n",
      "Epoch 5958\n",
      "-------------------------------\n",
      "tensor(41.0597)\n",
      "tensor(20.4188)\n",
      "tensor(4.4215)\n",
      "tensor(0.2657)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.957699\n",
      "Epoch 5959\n",
      "-------------------------------\n",
      "tensor(39.9642)\n",
      "tensor(18.5929)\n",
      "tensor(9.3442)\n",
      "tensor(0.3676)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.927395\n",
      "Epoch 5960\n",
      "-------------------------------\n",
      "tensor(16.3806)\n",
      "tensor(9.2452)\n",
      "tensor(4.4942)\n",
      "tensor(0.2795)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.918579\n",
      "Epoch 5961\n",
      "-------------------------------\n",
      "tensor(18.3787)\n",
      "tensor(8.1820)\n",
      "tensor(2.2683)\n",
      "tensor(0.1817)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.915546\n",
      "Epoch 5962\n",
      "-------------------------------\n",
      "tensor(19.0689)\n",
      "tensor(7.5156)\n",
      "tensor(2.3460)\n",
      "tensor(0.1047)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.909752\n",
      "Epoch 5963\n",
      "-------------------------------\n",
      "tensor(34.9091)\n",
      "tensor(11.5741)\n",
      "tensor(2.4313)\n",
      "tensor(0.0364)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.909420\n",
      "Epoch 5964\n",
      "-------------------------------\n",
      "tensor(37.0075)\n",
      "tensor(12.0973)\n",
      "tensor(2.8404)\n",
      "tensor(0.0095)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.892006\n",
      "Epoch 5965\n",
      "-------------------------------\n",
      "tensor(25.5750)\n",
      "tensor(9.0514)\n",
      "tensor(3.8904)\n",
      "tensor(0.0377)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.894066\n",
      "Epoch 5966\n",
      "-------------------------------\n",
      "tensor(45.4786)\n",
      "tensor(23.8388)\n",
      "tensor(3.6136)\n",
      "tensor(0.0618)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.919235\n",
      "Epoch 5967\n",
      "-------------------------------\n",
      "tensor(45.7438)\n",
      "tensor(13.6855)\n",
      "tensor(16.3981)\n",
      "tensor(0.2056)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.979282\n",
      "Epoch 5968\n",
      "-------------------------------\n",
      "tensor(33.9005)\n",
      "tensor(13.2674)\n",
      "tensor(7.5092)\n",
      "tensor(0.4258)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.993359\n",
      "Epoch 5969\n",
      "-------------------------------\n",
      "tensor(62.9496)\n",
      "tensor(22.2238)\n",
      "tensor(17.3611)\n",
      "tensor(0.4210)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.994789\n",
      "Epoch 5970\n",
      "-------------------------------\n",
      "tensor(70.4345)\n",
      "tensor(18.9662)\n",
      "tensor(45.5747)\n",
      "tensor(1.0335)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.996643\n",
      "Epoch 5971\n",
      "-------------------------------\n",
      "tensor(80.6955)\n",
      "tensor(34.9401)\n",
      "tensor(60.7372)\n",
      "tensor(1.6483)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.130260\n",
      "Epoch 5972\n",
      "-------------------------------\n",
      "tensor(84.5098)\n",
      "tensor(20.6007)\n",
      "tensor(54.6656)\n",
      "tensor(1.2968)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.104515\n",
      "Epoch 5973\n",
      "-------------------------------\n",
      "tensor(55.8309)\n",
      "tensor(23.6723)\n",
      "tensor(44.0221)\n",
      "tensor(1.2358)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.043545\n",
      "Epoch 5974\n",
      "-------------------------------\n",
      "tensor(31.7728)\n",
      "tensor(11.3027)\n",
      "tensor(13.3003)\n",
      "tensor(0.4229)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.961609\n",
      "Epoch 5975\n",
      "-------------------------------\n",
      "tensor(34.6141)\n",
      "tensor(32.3081)\n",
      "tensor(21.5407)\n",
      "tensor(0.7112)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.954422\n",
      "Epoch 5976\n",
      "-------------------------------\n",
      "tensor(19.0721)\n",
      "tensor(8.3193)\n",
      "tensor(10.8343)\n",
      "tensor(0.4904)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.922577\n",
      "Epoch 5977\n",
      "-------------------------------\n",
      "tensor(30.4724)\n",
      "tensor(14.2247)\n",
      "tensor(16.2568)\n",
      "tensor(0.6229)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.934864\n",
      "Epoch 5978\n",
      "-------------------------------\n",
      "tensor(28.4423)\n",
      "tensor(9.5100)\n",
      "tensor(2.1329)\n",
      "tensor(0.0235)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.910496\n",
      "Epoch 5979\n",
      "-------------------------------\n",
      "tensor(30.6521)\n",
      "tensor(8.6869)\n",
      "tensor(9.3463)\n",
      "tensor(0.2848)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.880070\n",
      "Epoch 5980\n",
      "-------------------------------\n",
      "tensor(37.9048)\n",
      "tensor(11.1341)\n",
      "tensor(7.5298)\n",
      "tensor(0.2604)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.863907\n",
      "Epoch 5981\n",
      "-------------------------------\n",
      "tensor(44.2032)\n",
      "tensor(23.2882)\n",
      "tensor(3.8753)\n",
      "tensor(0.1265)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.858047\n",
      "Epoch 5982\n",
      "-------------------------------\n",
      "tensor(41.3841)\n",
      "tensor(21.9173)\n",
      "tensor(1.9677)\n",
      "tensor(0.0275)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.844692\n",
      "Epoch 5983\n",
      "-------------------------------\n",
      "tensor(18.0816)\n",
      "tensor(7.3410)\n",
      "tensor(2.3543)\n",
      "tensor(0.2170)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.847805\n",
      "Epoch 5984\n",
      "-------------------------------\n",
      "tensor(18.4676)\n",
      "tensor(9.3986)\n",
      "tensor(5.2263)\n",
      "tensor(0.4502)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.848434\n",
      "Epoch 5985\n",
      "-------------------------------\n",
      "tensor(29.5250)\n",
      "tensor(13.9578)\n",
      "tensor(8.2875)\n",
      "tensor(0.5858)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.871403\n",
      "Epoch 5986\n",
      "-------------------------------\n",
      "tensor(27.9261)\n",
      "tensor(11.4747)\n",
      "tensor(6.7729)\n",
      "tensor(0.3474)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.864708\n",
      "Epoch 5987\n",
      "-------------------------------\n",
      "tensor(40.4215)\n",
      "tensor(12.7811)\n",
      "tensor(5.7691)\n",
      "tensor(0.3461)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.875237\n",
      "Epoch 5988\n",
      "-------------------------------\n",
      "tensor(45.6333)\n",
      "tensor(29.5591)\n",
      "tensor(11.2972)\n",
      "tensor(0.5987)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.926594\n",
      "Epoch 5989\n",
      "-------------------------------\n",
      "tensor(43.6000)\n",
      "tensor(14.5296)\n",
      "tensor(13.6275)\n",
      "tensor(0.2430)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.904732\n",
      "Epoch 5990\n",
      "-------------------------------\n",
      "tensor(33.5222)\n",
      "tensor(20.4948)\n",
      "tensor(28.0483)\n",
      "tensor(1.2725)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.896397\n",
      "Epoch 5991\n",
      "-------------------------------\n",
      "tensor(45.1068)\n",
      "tensor(18.1217)\n",
      "tensor(23.8973)\n",
      "tensor(1.0576)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.928986\n",
      "Epoch 5992\n",
      "-------------------------------\n",
      "tensor(22.2600)\n",
      "tensor(8.9717)\n",
      "tensor(10.5731)\n",
      "tensor(0.2741)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.905441\n",
      "Epoch 5993\n",
      "-------------------------------\n",
      "tensor(37.4024)\n",
      "tensor(18.7111)\n",
      "tensor(8.3769)\n",
      "tensor(0.4705)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.909065\n",
      "Epoch 5994\n",
      "-------------------------------\n",
      "tensor(63.9305)\n",
      "tensor(12.6608)\n",
      "tensor(29.2604)\n",
      "tensor(0.1457)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.986893\n",
      "Epoch 5995\n",
      "-------------------------------\n",
      "tensor(41.3533)\n",
      "tensor(23.9115)\n",
      "tensor(38.0220)\n",
      "tensor(1.1628)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.952110\n",
      "Epoch 5996\n",
      "-------------------------------\n",
      "tensor(21.1702)\n",
      "tensor(9.7657)\n",
      "tensor(10.6681)\n",
      "tensor(0.4765)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.887474\n",
      "Epoch 5997\n",
      "-------------------------------\n",
      "tensor(39.5617)\n",
      "tensor(15.8005)\n",
      "tensor(27.4223)\n",
      "tensor(1.1437)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.889030\n",
      "Epoch 5998\n",
      "-------------------------------\n",
      "tensor(24.1304)\n",
      "tensor(7.8207)\n",
      "tensor(5.7544)\n",
      "tensor(0.2208)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.827106\n",
      "Epoch 5999\n",
      "-------------------------------\n",
      "tensor(47.7228)\n",
      "tensor(18.8079)\n",
      "tensor(14.8280)\n",
      "tensor(0.5888)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.832386\n",
      "Epoch 6000\n",
      "-------------------------------\n",
      "tensor(46.4758)\n",
      "tensor(19.0918)\n",
      "tensor(14.6564)\n",
      "tensor(0.7355)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.817818\n",
      "Epoch 6001\n",
      "-------------------------------\n",
      "tensor(31.1238)\n",
      "tensor(12.6787)\n",
      "tensor(8.5106)\n",
      "tensor(0.5896)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.791306\n",
      "Epoch 6002\n",
      "-------------------------------\n",
      "tensor(33.2461)\n",
      "tensor(11.1153)\n",
      "tensor(6.4334)\n",
      "tensor(0.3852)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.784500\n",
      "Epoch 6003\n",
      "-------------------------------\n",
      "tensor(30.5411)\n",
      "tensor(7.5179)\n",
      "tensor(8.9615)\n",
      "tensor(0.1126)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.772400\n",
      "Epoch 6004\n",
      "-------------------------------\n",
      "tensor(34.4532)\n",
      "tensor(9.7356)\n",
      "tensor(9.5175)\n",
      "tensor(0.1982)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.759407\n",
      "Epoch 6005\n",
      "-------------------------------\n",
      "tensor(45.4896)\n",
      "tensor(13.4883)\n",
      "tensor(6.9057)\n",
      "tensor(0.3769)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.806686\n",
      "Epoch 6006\n",
      "-------------------------------\n",
      "tensor(58.3318)\n",
      "tensor(22.9264)\n",
      "tensor(12.2890)\n",
      "tensor(0.2426)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.815479\n",
      "Epoch 6007\n",
      "-------------------------------\n",
      "tensor(42.4168)\n",
      "tensor(12.0416)\n",
      "tensor(10.5899)\n",
      "tensor(0.0408)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.783871\n",
      "Epoch 6008\n",
      "-------------------------------\n",
      "tensor(38.7614)\n",
      "tensor(15.6074)\n",
      "tensor(13.3894)\n",
      "tensor(0.7217)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.812733\n",
      "Epoch 6009\n",
      "-------------------------------\n",
      "tensor(40.0363)\n",
      "tensor(19.2449)\n",
      "tensor(26.4414)\n",
      "tensor(0.9673)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.850498\n",
      "Epoch 6010\n",
      "-------------------------------\n",
      "tensor(27.0150)\n",
      "tensor(20.7111)\n",
      "tensor(26.7341)\n",
      "tensor(1.3752)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.858463\n",
      "Epoch 6011\n",
      "-------------------------------\n",
      "tensor(51.0770)\n",
      "tensor(24.4583)\n",
      "tensor(10.8659)\n",
      "tensor(0.3755)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.876392\n",
      "Epoch 6012\n",
      "-------------------------------\n",
      "tensor(53.1421)\n",
      "tensor(13.0674)\n",
      "tensor(22.0263)\n",
      "tensor(0.5996)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.924156\n",
      "Epoch 6013\n",
      "-------------------------------\n",
      "tensor(45.1765)\n",
      "tensor(30.0620)\n",
      "tensor(38.8878)\n",
      "tensor(1.6509)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.907932\n",
      "Epoch 6014\n",
      "-------------------------------\n",
      "tensor(48.7746)\n",
      "tensor(17.4782)\n",
      "tensor(31.7860)\n",
      "tensor(1.3301)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.868790\n",
      "Epoch 6015\n",
      "-------------------------------\n",
      "tensor(20.5355)\n",
      "tensor(10.4604)\n",
      "tensor(11.1856)\n",
      "tensor(0.5163)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.812977\n",
      "Epoch 6016\n",
      "-------------------------------\n",
      "tensor(51.2825)\n",
      "tensor(18.1417)\n",
      "tensor(20.7774)\n",
      "tensor(0.6300)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.808022\n",
      "Epoch 6017\n",
      "-------------------------------\n",
      "tensor(44.3906)\n",
      "tensor(8.7210)\n",
      "tensor(18.7328)\n",
      "tensor(0.3027)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.791008\n",
      "Epoch 6018\n",
      "-------------------------------\n",
      "tensor(40.3037)\n",
      "tensor(9.7513)\n",
      "tensor(14.1476)\n",
      "tensor(0.2669)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.769096\n",
      "Epoch 6019\n",
      "-------------------------------\n",
      "tensor(27.7229)\n",
      "tensor(10.7607)\n",
      "tensor(6.3120)\n",
      "tensor(0.3359)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.731270\n",
      "Epoch 6020\n",
      "-------------------------------\n",
      "tensor(31.9014)\n",
      "tensor(11.5966)\n",
      "tensor(13.4832)\n",
      "tensor(0.2410)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.717178\n",
      "Epoch 6021\n",
      "-------------------------------\n",
      "tensor(36.2007)\n",
      "tensor(11.7825)\n",
      "tensor(12.8817)\n",
      "tensor(0.0621)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.711109\n",
      "Epoch 6022\n",
      "-------------------------------\n",
      "tensor(55.9158)\n",
      "tensor(23.9738)\n",
      "tensor(8.6003)\n",
      "tensor(0.1359)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.715599\n",
      "Epoch 6023\n",
      "-------------------------------\n",
      "tensor(29.9343)\n",
      "tensor(10.1124)\n",
      "tensor(6.9496)\n",
      "tensor(0.3559)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.700806\n",
      "Epoch 6024\n",
      "-------------------------------\n",
      "tensor(31.4633)\n",
      "tensor(8.7264)\n",
      "tensor(18.0480)\n",
      "tensor(0.4870)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.709553\n",
      "Epoch 6025\n",
      "-------------------------------\n",
      "tensor(32.5992)\n",
      "tensor(5.8894)\n",
      "tensor(15.4617)\n",
      "tensor(0.1570)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.700558\n",
      "Epoch 6026\n",
      "-------------------------------\n",
      "tensor(60.5329)\n",
      "tensor(24.1367)\n",
      "tensor(15.0872)\n",
      "tensor(0.7401)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.752289\n",
      "Epoch 6027\n",
      "-------------------------------\n",
      "tensor(35.8600)\n",
      "tensor(18.0051)\n",
      "tensor(13.0241)\n",
      "tensor(0.8686)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.761162\n",
      "Epoch 6028\n",
      "-------------------------------\n",
      "tensor(39.7896)\n",
      "tensor(9.9035)\n",
      "tensor(13.8853)\n",
      "tensor(0.1068)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.734039\n",
      "Epoch 6029\n",
      "-------------------------------\n",
      "tensor(32.8899)\n",
      "tensor(10.6681)\n",
      "tensor(9.0464)\n",
      "tensor(0.2607)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.726585\n",
      "Epoch 6030\n",
      "-------------------------------\n",
      "tensor(28.3118)\n",
      "tensor(9.6395)\n",
      "tensor(6.1570)\n",
      "tensor(0.2453)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.755184\n",
      "Epoch 6031\n",
      "-------------------------------\n",
      "tensor(35.5986)\n",
      "tensor(26.0980)\n",
      "tensor(15.2473)\n",
      "tensor(0.0237)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.797153\n",
      "Epoch 6032\n",
      "-------------------------------\n",
      "tensor(35.7664)\n",
      "tensor(16.2648)\n",
      "tensor(10.6576)\n",
      "tensor(0.7958)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.794823\n",
      "Epoch 6033\n",
      "-------------------------------\n",
      "tensor(36.2459)\n",
      "tensor(15.5869)\n",
      "tensor(13.3797)\n",
      "tensor(0.2802)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.773582\n",
      "Epoch 6034\n",
      "-------------------------------\n",
      "tensor(41.1761)\n",
      "tensor(14.8934)\n",
      "tensor(22.1495)\n",
      "tensor(0.9314)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.803467\n",
      "Epoch 6035\n",
      "-------------------------------\n",
      "tensor(20.0402)\n",
      "tensor(9.9160)\n",
      "tensor(13.4224)\n",
      "tensor(0.3699)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.768742\n",
      "Epoch 6036\n",
      "-------------------------------\n",
      "tensor(28.0016)\n",
      "tensor(11.1884)\n",
      "tensor(10.9454)\n",
      "tensor(0.5351)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.785160\n",
      "Epoch 6037\n",
      "-------------------------------\n",
      "tensor(37.8960)\n",
      "tensor(25.4087)\n",
      "tensor(10.1468)\n",
      "tensor(0.0493)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.800285\n",
      "Epoch 6038\n",
      "-------------------------------\n",
      "tensor(35.7528)\n",
      "tensor(26.6198)\n",
      "tensor(15.5452)\n",
      "tensor(0.2603)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.749855\n",
      "Epoch 6039\n",
      "-------------------------------\n",
      "tensor(25.7154)\n",
      "tensor(6.1110)\n",
      "tensor(8.1774)\n",
      "tensor(0.0338)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.726948\n",
      "Epoch 6040\n",
      "-------------------------------\n",
      "tensor(30.7201)\n",
      "tensor(11.2969)\n",
      "tensor(3.6253)\n",
      "tensor(0.2086)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.730091\n",
      "Epoch 6041\n",
      "-------------------------------\n",
      "tensor(30.0470)\n",
      "tensor(12.5461)\n",
      "tensor(9.2862)\n",
      "tensor(0.3181)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.725025\n",
      "Epoch 6042\n",
      "-------------------------------\n",
      "tensor(30.8600)\n",
      "tensor(12.9629)\n",
      "tensor(11.0104)\n",
      "tensor(0.3246)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.712765\n",
      "Epoch 6043\n",
      "-------------------------------\n",
      "tensor(15.8839)\n",
      "tensor(9.3769)\n",
      "tensor(8.7904)\n",
      "tensor(0.2244)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.705029\n",
      "Epoch 6044\n",
      "-------------------------------\n",
      "tensor(26.3261)\n",
      "tensor(9.3910)\n",
      "tensor(3.2353)\n",
      "tensor(0.0290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.708168\n",
      "Epoch 6045\n",
      "-------------------------------\n",
      "tensor(37.5127)\n",
      "tensor(11.2389)\n",
      "tensor(9.4813)\n",
      "tensor(0.3312)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.725639\n",
      "Epoch 6046\n",
      "-------------------------------\n",
      "tensor(38.6436)\n",
      "tensor(11.7987)\n",
      "tensor(8.0752)\n",
      "tensor(0.2423)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.707050\n",
      "Epoch 6047\n",
      "-------------------------------\n",
      "tensor(54.1656)\n",
      "tensor(22.9628)\n",
      "tensor(9.3391)\n",
      "tensor(0.3899)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.738007\n",
      "Epoch 6048\n",
      "-------------------------------\n",
      "tensor(44.6032)\n",
      "tensor(25.8109)\n",
      "tensor(13.7557)\n",
      "tensor(0.2444)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.749264\n",
      "Epoch 6049\n",
      "-------------------------------\n",
      "tensor(48.0057)\n",
      "tensor(15.7526)\n",
      "tensor(14.6905)\n",
      "tensor(0.2542)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.732513\n",
      "Epoch 6050\n",
      "-------------------------------\n",
      "tensor(41.7627)\n",
      "tensor(20.7772)\n",
      "tensor(33.2319)\n",
      "tensor(0.7678)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.807339\n",
      "Epoch 6051\n",
      "-------------------------------\n",
      "tensor(46.7699)\n",
      "tensor(18.2720)\n",
      "tensor(34.0362)\n",
      "tensor(1.3362)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.843216\n",
      "Epoch 6052\n",
      "-------------------------------\n",
      "tensor(43.7233)\n",
      "tensor(18.7805)\n",
      "tensor(28.1513)\n",
      "tensor(0.8049)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.843300\n",
      "Epoch 6053\n",
      "-------------------------------\n",
      "tensor(42.6155)\n",
      "tensor(10.8096)\n",
      "tensor(16.8478)\n",
      "tensor(0.1630)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.803135\n",
      "Epoch 6054\n",
      "-------------------------------\n",
      "tensor(24.0317)\n",
      "tensor(7.4649)\n",
      "tensor(4.6259)\n",
      "tensor(0.0601)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.770874\n",
      "Epoch 6055\n",
      "-------------------------------\n",
      "tensor(34.6326)\n",
      "tensor(11.3519)\n",
      "tensor(11.4674)\n",
      "tensor(0.3679)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.765087\n",
      "Epoch 6056\n",
      "-------------------------------\n",
      "tensor(39.1379)\n",
      "tensor(31.1537)\n",
      "tensor(15.2889)\n",
      "tensor(0.7513)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.768566\n",
      "Epoch 6057\n",
      "-------------------------------\n",
      "tensor(38.9995)\n",
      "tensor(8.0795)\n",
      "tensor(17.1255)\n",
      "tensor(0.0721)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.711678\n",
      "Epoch 6058\n",
      "-------------------------------\n",
      "tensor(22.7601)\n",
      "tensor(15.0293)\n",
      "tensor(12.5309)\n",
      "tensor(0.8422)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.684357\n",
      "Epoch 6059\n",
      "-------------------------------\n",
      "tensor(30.4775)\n",
      "tensor(19.1963)\n",
      "tensor(20.6705)\n",
      "tensor(0.9494)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.684757\n",
      "Epoch 6060\n",
      "-------------------------------\n",
      "tensor(34.5007)\n",
      "tensor(16.5270)\n",
      "tensor(14.8222)\n",
      "tensor(0.5947)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.666252\n",
      "Epoch 6061\n",
      "-------------------------------\n",
      "tensor(38.0110)\n",
      "tensor(18.9941)\n",
      "tensor(6.4267)\n",
      "tensor(0.2161)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.650028\n",
      "Epoch 6062\n",
      "-------------------------------\n",
      "tensor(26.2463)\n",
      "tensor(8.0561)\n",
      "tensor(5.1192)\n",
      "tensor(0.0994)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.643055\n",
      "Epoch 6063\n",
      "-------------------------------\n",
      "tensor(26.7819)\n",
      "tensor(6.4340)\n",
      "tensor(12.4876)\n",
      "tensor(0.3674)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.640251\n",
      "Epoch 6064\n",
      "-------------------------------\n",
      "tensor(26.7234)\n",
      "tensor(7.2377)\n",
      "tensor(13.8909)\n",
      "tensor(0.4645)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.626656\n",
      "Epoch 6065\n",
      "-------------------------------\n",
      "tensor(37.1373)\n",
      "tensor(21.6302)\n",
      "tensor(3.9575)\n",
      "tensor(0.1106)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.612240\n",
      "Epoch 6066\n",
      "-------------------------------\n",
      "tensor(38.6344)\n",
      "tensor(14.5680)\n",
      "tensor(10.8776)\n",
      "tensor(0.4144)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.652901\n",
      "Epoch 6067\n",
      "-------------------------------\n",
      "tensor(45.3525)\n",
      "tensor(16.4062)\n",
      "tensor(6.2474)\n",
      "tensor(0.4710)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.739845\n",
      "Epoch 6068\n",
      "-------------------------------\n",
      "tensor(48.2817)\n",
      "tensor(14.7774)\n",
      "tensor(13.4340)\n",
      "tensor(0.0680)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.731903\n",
      "Epoch 6069\n",
      "-------------------------------\n",
      "tensor(53.5879)\n",
      "tensor(24.3820)\n",
      "tensor(6.9337)\n",
      "tensor(0.0581)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.666077\n",
      "Epoch 6070\n",
      "-------------------------------\n",
      "tensor(37.1174)\n",
      "tensor(14.1033)\n",
      "tensor(21.2292)\n",
      "tensor(0.5324)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.677704\n",
      "Epoch 6071\n",
      "-------------------------------\n",
      "tensor(41.1494)\n",
      "tensor(17.7029)\n",
      "tensor(27.8148)\n",
      "tensor(0.6966)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.748726\n",
      "Epoch 6072\n",
      "-------------------------------\n",
      "tensor(45.5599)\n",
      "tensor(12.8594)\n",
      "tensor(23.9076)\n",
      "tensor(0.5943)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.769489\n",
      "Epoch 6073\n",
      "-------------------------------\n",
      "tensor(27.7386)\n",
      "tensor(16.0765)\n",
      "tensor(21.8364)\n",
      "tensor(0.6879)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.738323\n",
      "Epoch 6074\n",
      "-------------------------------\n",
      "tensor(32.9607)\n",
      "tensor(10.2398)\n",
      "tensor(4.6655)\n",
      "tensor(0.0614)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.747173\n",
      "Epoch 6075\n",
      "-------------------------------\n",
      "tensor(29.4674)\n",
      "tensor(9.0976)\n",
      "tensor(9.2788)\n",
      "tensor(0.2290)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.698204\n",
      "Epoch 6076\n",
      "-------------------------------\n",
      "tensor(45.1448)\n",
      "tensor(19.3560)\n",
      "tensor(11.6599)\n",
      "tensor(0.2499)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.668518\n",
      "Epoch 6077\n",
      "-------------------------------\n",
      "tensor(22.2576)\n",
      "tensor(5.3856)\n",
      "tensor(7.0898)\n",
      "tensor(0.0154)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.630829\n",
      "Epoch 6078\n",
      "-------------------------------\n",
      "tensor(37.3389)\n",
      "tensor(11.0683)\n",
      "tensor(6.7139)\n",
      "tensor(0.0471)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.640377\n",
      "Epoch 6079\n",
      "-------------------------------\n",
      "tensor(35.7449)\n",
      "tensor(11.6159)\n",
      "tensor(1.6413)\n",
      "tensor(0.0952)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.621056\n",
      "Epoch 6080\n",
      "-------------------------------\n",
      "tensor(31.5048)\n",
      "tensor(10.9796)\n",
      "tensor(5.1497)\n",
      "tensor(0.1749)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.597145\n",
      "Epoch 6081\n",
      "-------------------------------\n",
      "tensor(35.4402)\n",
      "tensor(19.0256)\n",
      "tensor(5.7105)\n",
      "tensor(0.1764)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.582870\n",
      "Epoch 6082\n",
      "-------------------------------\n",
      "tensor(15.2039)\n",
      "tensor(6.4809)\n",
      "tensor(2.9360)\n",
      "tensor(0.1300)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.578064\n",
      "Epoch 6083\n",
      "-------------------------------\n",
      "tensor(16.7643)\n",
      "tensor(5.9180)\n",
      "tensor(1.7118)\n",
      "tensor(0.0580)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.572617\n",
      "Epoch 6084\n",
      "-------------------------------\n",
      "tensor(22.9991)\n",
      "tensor(7.0510)\n",
      "tensor(3.7423)\n",
      "tensor(0.0241)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.564941\n",
      "Epoch 6085\n",
      "-------------------------------\n",
      "tensor(36.7397)\n",
      "tensor(11.4591)\n",
      "tensor(2.7995)\n",
      "tensor(0.0437)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.587585\n",
      "Epoch 6086\n",
      "-------------------------------\n",
      "tensor(47.9403)\n",
      "tensor(22.1249)\n",
      "tensor(4.3483)\n",
      "tensor(0.1003)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.612339\n",
      "Epoch 6087\n",
      "-------------------------------\n",
      "tensor(45.5810)\n",
      "tensor(13.7885)\n",
      "tensor(8.5695)\n",
      "tensor(0.1194)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.616917\n",
      "Epoch 6088\n",
      "-------------------------------\n",
      "tensor(37.7982)\n",
      "tensor(15.5589)\n",
      "tensor(6.2229)\n",
      "tensor(0.4719)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.645802\n",
      "Epoch 6089\n",
      "-------------------------------\n",
      "tensor(49.0846)\n",
      "tensor(17.4591)\n",
      "tensor(13.4865)\n",
      "tensor(0.4057)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.644043\n",
      "Epoch 6090\n",
      "-------------------------------\n",
      "tensor(31.5065)\n",
      "tensor(32.5508)\n",
      "tensor(22.5926)\n",
      "tensor(0.8134)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.638077\n",
      "Epoch 6091\n",
      "-------------------------------\n",
      "tensor(19.0073)\n",
      "tensor(5.9255)\n",
      "tensor(4.0765)\n",
      "tensor(0.2419)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.649845\n",
      "Epoch 6092\n",
      "-------------------------------\n",
      "tensor(42.6290)\n",
      "tensor(19.2878)\n",
      "tensor(26.3662)\n",
      "tensor(0.8360)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.721287\n",
      "Epoch 6093\n",
      "-------------------------------\n",
      "tensor(60.0881)\n",
      "tensor(15.7829)\n",
      "tensor(39.6883)\n",
      "tensor(1.0663)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.765888\n",
      "Epoch 6094\n",
      "-------------------------------\n",
      "tensor(56.2828)\n",
      "tensor(22.9209)\n",
      "tensor(44.4458)\n",
      "tensor(1.0313)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.751339\n",
      "Epoch 6095\n",
      "-------------------------------\n",
      "tensor(29.2701)\n",
      "tensor(7.5183)\n",
      "tensor(14.7067)\n",
      "tensor(0.3554)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.646801\n",
      "Epoch 6096\n",
      "-------------------------------\n",
      "tensor(34.9095)\n",
      "tensor(9.6437)\n",
      "tensor(18.2479)\n",
      "tensor(0.4014)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.617821\n",
      "Epoch 6097\n",
      "-------------------------------\n",
      "tensor(58.1684)\n",
      "tensor(20.6905)\n",
      "tensor(18.0197)\n",
      "tensor(0.4727)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.622063\n",
      "Epoch 6098\n",
      "-------------------------------\n",
      "tensor(36.4763)\n",
      "tensor(14.3221)\n",
      "tensor(7.6252)\n",
      "tensor(0.3541)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.573231\n",
      "Epoch 6099\n",
      "-------------------------------\n",
      "tensor(35.6057)\n",
      "tensor(10.3806)\n",
      "tensor(7.5394)\n",
      "tensor(0.0439)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.538048\n",
      "Epoch 6100\n",
      "-------------------------------\n",
      "tensor(41.7480)\n",
      "tensor(12.1773)\n",
      "tensor(10.3045)\n",
      "tensor(0.0731)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.525040\n",
      "Epoch 6101\n",
      "-------------------------------\n",
      "tensor(27.4025)\n",
      "tensor(7.3043)\n",
      "tensor(7.8026)\n",
      "tensor(0.0540)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.517265\n",
      "Epoch 6102\n",
      "-------------------------------\n",
      "tensor(35.6767)\n",
      "tensor(11.0189)\n",
      "tensor(2.9875)\n",
      "tensor(0.0126)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.512108\n",
      "Epoch 6103\n",
      "-------------------------------\n",
      "tensor(35.8915)\n",
      "tensor(20.1256)\n",
      "tensor(4.4700)\n",
      "tensor(0.1179)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.516834\n",
      "Epoch 6104\n",
      "-------------------------------\n",
      "tensor(41.8792)\n",
      "tensor(19.7623)\n",
      "tensor(7.7274)\n",
      "tensor(0.1719)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.517555\n",
      "Epoch 6105\n",
      "-------------------------------\n",
      "tensor(25.4725)\n",
      "tensor(8.7728)\n",
      "tensor(1.6559)\n",
      "tensor(0.0950)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.525078\n",
      "Epoch 6106\n",
      "-------------------------------\n",
      "tensor(31.4147)\n",
      "tensor(8.8371)\n",
      "tensor(7.2720)\n",
      "tensor(0.0245)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.526493\n",
      "Epoch 6107\n",
      "-------------------------------\n",
      "tensor(40.9751)\n",
      "tensor(16.9392)\n",
      "tensor(4.9696)\n",
      "tensor(0.2802)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.557106\n",
      "Epoch 6108\n",
      "-------------------------------\n",
      "tensor(43.5660)\n",
      "tensor(16.9223)\n",
      "tensor(10.3632)\n",
      "tensor(0.3901)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.607063\n",
      "Epoch 6109\n",
      "-------------------------------\n",
      "tensor(34.0616)\n",
      "tensor(9.6633)\n",
      "tensor(10.3560)\n",
      "tensor(0.1537)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.577164\n",
      "Epoch 6110\n",
      "-------------------------------\n",
      "tensor(20.9903)\n",
      "tensor(6.4740)\n",
      "tensor(6.8494)\n",
      "tensor(0.0517)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.539783\n",
      "Epoch 6111\n",
      "-------------------------------\n",
      "tensor(25.6237)\n",
      "tensor(11.9053)\n",
      "tensor(5.6406)\n",
      "tensor(0.4003)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.551262\n",
      "Epoch 6112\n",
      "-------------------------------\n",
      "tensor(42.6659)\n",
      "tensor(30.1991)\n",
      "tensor(11.8865)\n",
      "tensor(0.4442)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.588947\n",
      "Epoch 6113\n",
      "-------------------------------\n",
      "tensor(42.1128)\n",
      "tensor(16.2827)\n",
      "tensor(11.2356)\n",
      "tensor(0.5364)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.577248\n",
      "Epoch 6114\n",
      "-------------------------------\n",
      "tensor(39.4663)\n",
      "tensor(25.2430)\n",
      "tensor(29.4251)\n",
      "tensor(1.2062)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.575474\n",
      "Epoch 6115\n",
      "-------------------------------\n",
      "tensor(35.6897)\n",
      "tensor(8.7908)\n",
      "tensor(20.1743)\n",
      "tensor(0.6847)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.558533\n",
      "Epoch 6116\n",
      "-------------------------------\n",
      "tensor(26.4778)\n",
      "tensor(9.4306)\n",
      "tensor(8.9634)\n",
      "tensor(0.4231)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.545425\n",
      "Epoch 6117\n",
      "-------------------------------\n",
      "tensor(55.5580)\n",
      "tensor(19.6086)\n",
      "tensor(15.9406)\n",
      "tensor(0.3676)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.553799\n",
      "Epoch 6118\n",
      "-------------------------------\n",
      "tensor(27.9053)\n",
      "tensor(8.3413)\n",
      "tensor(7.3246)\n",
      "tensor(0.3677)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.509716\n",
      "Epoch 6119\n",
      "-------------------------------\n",
      "tensor(32.0042)\n",
      "tensor(6.6050)\n",
      "tensor(12.1980)\n",
      "tensor(0.2968)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.511368\n",
      "Epoch 6120\n",
      "-------------------------------\n",
      "tensor(31.0392)\n",
      "tensor(9.1184)\n",
      "tensor(7.3319)\n",
      "tensor(0.2757)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.496437\n",
      "Epoch 6121\n",
      "-------------------------------\n",
      "tensor(30.6128)\n",
      "tensor(10.0801)\n",
      "tensor(3.7334)\n",
      "tensor(0.2457)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.489372\n",
      "Epoch 6122\n",
      "-------------------------------\n",
      "tensor(40.4171)\n",
      "tensor(20.3300)\n",
      "tensor(5.5121)\n",
      "tensor(0.1884)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.486168\n",
      "Epoch 6123\n",
      "-------------------------------\n",
      "tensor(27.0138)\n",
      "tensor(8.8174)\n",
      "tensor(5.6568)\n",
      "tensor(0.0531)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.477776\n",
      "Epoch 6124\n",
      "-------------------------------\n",
      "tensor(29.8057)\n",
      "tensor(9.6711)\n",
      "tensor(4.7534)\n",
      "tensor(0.1916)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.481003\n",
      "Epoch 6125\n",
      "-------------------------------\n",
      "tensor(34.5802)\n",
      "tensor(11.6503)\n",
      "tensor(7.8252)\n",
      "tensor(0.4483)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.476196\n",
      "Epoch 6126\n",
      "-------------------------------\n",
      "tensor(32.4520)\n",
      "tensor(10.3333)\n",
      "tensor(8.3592)\n",
      "tensor(0.3404)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.451706\n",
      "Epoch 6127\n",
      "-------------------------------\n",
      "tensor(38.8624)\n",
      "tensor(13.5964)\n",
      "tensor(5.0294)\n",
      "tensor(0.3401)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.489502\n",
      "Epoch 6128\n",
      "-------------------------------\n",
      "tensor(48.3949)\n",
      "tensor(21.3713)\n",
      "tensor(10.9241)\n",
      "tensor(0.7042)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.556282\n",
      "Epoch 6129\n",
      "-------------------------------\n",
      "tensor(48.4999)\n",
      "tensor(11.4033)\n",
      "tensor(23.4170)\n",
      "tensor(0.0880)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.522926\n",
      "Epoch 6130\n",
      "-------------------------------\n",
      "tensor(44.3128)\n",
      "tensor(17.3114)\n",
      "tensor(32.1420)\n",
      "tensor(0.5156)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.523392\n",
      "Epoch 6131\n",
      "-------------------------------\n",
      "tensor(35.0889)\n",
      "tensor(16.4561)\n",
      "tensor(26.6818)\n",
      "tensor(1.1432)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.491711\n",
      "Epoch 6132\n",
      "-------------------------------\n",
      "tensor(43.9159)\n",
      "tensor(16.8521)\n",
      "tensor(14.9570)\n",
      "tensor(0.6096)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.482269\n",
      "Epoch 6133\n",
      "-------------------------------\n",
      "tensor(48.3372)\n",
      "tensor(24.2718)\n",
      "tensor(11.2099)\n",
      "tensor(0.5943)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.467541\n",
      "Epoch 6134\n",
      "-------------------------------\n",
      "tensor(50.3679)\n",
      "tensor(10.4093)\n",
      "tensor(26.0550)\n",
      "tensor(0.1549)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.504791\n",
      "Epoch 6135\n",
      "-------------------------------\n",
      "tensor(55.6705)\n",
      "tensor(17.4291)\n",
      "tensor(38.5399)\n",
      "tensor(0.5944)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.545124\n",
      "Epoch 6136\n",
      "-------------------------------\n",
      "tensor(20.0318)\n",
      "tensor(11.7999)\n",
      "tensor(13.8228)\n",
      "tensor(0.8076)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.486591\n",
      "Epoch 6137\n",
      "-------------------------------\n",
      "tensor(44.8254)\n",
      "tensor(13.5799)\n",
      "tensor(25.3424)\n",
      "tensor(0.7842)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.511242\n",
      "Epoch 6138\n",
      "-------------------------------\n",
      "tensor(37.1103)\n",
      "tensor(13.4480)\n",
      "tensor(4.6193)\n",
      "tensor(0.3635)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.468521\n",
      "Epoch 6139\n",
      "-------------------------------\n",
      "tensor(41.4575)\n",
      "tensor(18.9140)\n",
      "tensor(19.1626)\n",
      "tensor(0.9263)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.469040\n",
      "Epoch 6140\n",
      "-------------------------------\n",
      "tensor(33.4530)\n",
      "tensor(15.4602)\n",
      "tensor(16.7654)\n",
      "tensor(0.8337)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.457512\n",
      "Epoch 6141\n",
      "-------------------------------\n",
      "tensor(17.3453)\n",
      "tensor(7.9985)\n",
      "tensor(9.3277)\n",
      "tensor(0.5282)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.445820\n",
      "Epoch 6142\n",
      "-------------------------------\n",
      "tensor(17.5922)\n",
      "tensor(5.0683)\n",
      "tensor(3.3757)\n",
      "tensor(0.1803)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.441368\n",
      "Epoch 6143\n",
      "-------------------------------\n",
      "tensor(30.1118)\n",
      "tensor(24.1500)\n",
      "tensor(8.0028)\n",
      "tensor(0.2336)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.439438\n",
      "Epoch 6144\n",
      "-------------------------------\n",
      "tensor(24.6178)\n",
      "tensor(11.1103)\n",
      "tensor(16.5908)\n",
      "tensor(0.6563)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.435310\n",
      "Epoch 6145\n",
      "-------------------------------\n",
      "tensor(33.3664)\n",
      "tensor(27.0859)\n",
      "tensor(12.8538)\n",
      "tensor(0.6989)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.420303\n",
      "Epoch 6146\n",
      "-------------------------------\n",
      "tensor(31.2113)\n",
      "tensor(10.5726)\n",
      "tensor(6.0634)\n",
      "tensor(0.0436)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.410660\n",
      "Epoch 6147\n",
      "-------------------------------\n",
      "tensor(56.0418)\n",
      "tensor(24.1222)\n",
      "tensor(14.8492)\n",
      "tensor(0.7920)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.476021\n",
      "Epoch 6148\n",
      "-------------------------------\n",
      "tensor(49.2255)\n",
      "tensor(17.2235)\n",
      "tensor(14.3181)\n",
      "tensor(0.6971)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.479656\n",
      "Epoch 6149\n",
      "-------------------------------\n",
      "tensor(21.9918)\n",
      "tensor(11.2954)\n",
      "tensor(5.2838)\n",
      "tensor(0.3780)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.417252\n",
      "Epoch 6150\n",
      "-------------------------------\n",
      "tensor(53.2695)\n",
      "tensor(25.4730)\n",
      "tensor(16.3493)\n",
      "tensor(0.5899)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.448536\n",
      "Epoch 6151\n",
      "-------------------------------\n",
      "tensor(69.6238)\n",
      "tensor(23.9638)\n",
      "tensor(47.0993)\n",
      "tensor(1.3197)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.546844\n",
      "Epoch 6152\n",
      "-------------------------------\n",
      "tensor(73.8268)\n",
      "tensor(38.7140)\n",
      "tensor(67.5047)\n",
      "tensor(2.5935)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.622147\n",
      "Epoch 6153\n",
      "-------------------------------\n",
      "tensor(82.6348)\n",
      "tensor(23.1868)\n",
      "tensor(56.8678)\n",
      "tensor(1.3562)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.624466\n",
      "Epoch 6154\n",
      "-------------------------------\n",
      "tensor(61.2683)\n",
      "tensor(16.7363)\n",
      "tensor(39.0627)\n",
      "tensor(0.1803)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.544857\n",
      "Epoch 6155\n",
      "-------------------------------\n",
      "tensor(23.7835)\n",
      "tensor(8.9336)\n",
      "tensor(6.4269)\n",
      "tensor(0.3318)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.442814\n",
      "Epoch 6156\n",
      "-------------------------------\n",
      "tensor(50.0198)\n",
      "tensor(8.8662)\n",
      "tensor(27.6948)\n",
      "tensor(0.2605)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.444633\n",
      "Epoch 6157\n",
      "-------------------------------\n",
      "tensor(44.5056)\n",
      "tensor(19.9263)\n",
      "tensor(16.1327)\n",
      "tensor(0.8814)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.385212\n",
      "Epoch 6158\n",
      "-------------------------------\n",
      "tensor(45.5958)\n",
      "tensor(20.4018)\n",
      "tensor(16.6063)\n",
      "tensor(0.7910)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.399822\n",
      "Epoch 6159\n",
      "-------------------------------\n",
      "tensor(33.8554)\n",
      "tensor(11.3243)\n",
      "tensor(2.2184)\n",
      "tensor(0.1997)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.387569\n",
      "Epoch 6160\n",
      "-------------------------------\n",
      "tensor(41.2723)\n",
      "tensor(12.1312)\n",
      "tensor(8.0569)\n",
      "tensor(0.1776)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.371132\n",
      "Epoch 6161\n",
      "-------------------------------\n",
      "tensor(33.7354)\n",
      "tensor(9.9977)\n",
      "tensor(9.7704)\n",
      "tensor(0.2909)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.345360\n",
      "Epoch 6162\n",
      "-------------------------------\n",
      "tensor(36.9493)\n",
      "tensor(11.1966)\n",
      "tensor(7.8416)\n",
      "tensor(0.2730)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.346329\n",
      "Epoch 6163\n",
      "-------------------------------\n",
      "tensor(35.9859)\n",
      "tensor(22.5531)\n",
      "tensor(3.0553)\n",
      "tensor(0.1304)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.350311\n",
      "Epoch 6164\n",
      "-------------------------------\n",
      "tensor(34.7607)\n",
      "tensor(19.8394)\n",
      "tensor(3.7962)\n",
      "tensor(0.1193)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.346905\n",
      "Epoch 6165\n",
      "-------------------------------\n",
      "tensor(22.6078)\n",
      "tensor(9.3126)\n",
      "tensor(5.0258)\n",
      "tensor(0.3864)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.366886\n",
      "Epoch 6166\n",
      "-------------------------------\n",
      "tensor(38.7529)\n",
      "tensor(14.9314)\n",
      "tensor(5.4350)\n",
      "tensor(0.4724)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.400845\n",
      "Epoch 6167\n",
      "-------------------------------\n",
      "tensor(31.0322)\n",
      "tensor(11.2628)\n",
      "tensor(3.1050)\n",
      "tensor(0.2813)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.402534\n",
      "Epoch 6168\n",
      "-------------------------------\n",
      "tensor(24.7578)\n",
      "tensor(11.1514)\n",
      "tensor(6.0429)\n",
      "tensor(0.0702)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.411530\n",
      "Epoch 6169\n",
      "-------------------------------\n",
      "tensor(32.6523)\n",
      "tensor(12.7335)\n",
      "tensor(6.7064)\n",
      "tensor(0.4052)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.434998\n",
      "Epoch 6170\n",
      "-------------------------------\n",
      "tensor(32.0060)\n",
      "tensor(10.0041)\n",
      "tensor(6.2580)\n",
      "tensor(0.0611)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.389652\n",
      "Epoch 6171\n",
      "-------------------------------\n",
      "tensor(27.4101)\n",
      "tensor(8.5555)\n",
      "tensor(11.1122)\n",
      "tensor(0.5379)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.396751\n",
      "Epoch 6172\n",
      "-------------------------------\n",
      "tensor(40.4018)\n",
      "tensor(14.0667)\n",
      "tensor(7.8968)\n",
      "tensor(0.4058)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.429672\n",
      "Epoch 6173\n",
      "-------------------------------\n",
      "tensor(45.7239)\n",
      "tensor(26.4223)\n",
      "tensor(5.7640)\n",
      "tensor(0.3261)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.450825\n",
      "Epoch 6174\n",
      "-------------------------------\n",
      "tensor(40.5051)\n",
      "tensor(10.8422)\n",
      "tensor(13.0061)\n",
      "tensor(0.3391)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.444889\n",
      "Epoch 6175\n",
      "-------------------------------\n",
      "tensor(22.7799)\n",
      "tensor(21.1634)\n",
      "tensor(26.5441)\n",
      "tensor(1.1527)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.452393\n",
      "Epoch 6176\n",
      "-------------------------------\n",
      "tensor(26.0997)\n",
      "tensor(9.1846)\n",
      "tensor(6.6409)\n",
      "tensor(0.1707)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.446415\n",
      "Epoch 6177\n",
      "-------------------------------\n",
      "tensor(26.7222)\n",
      "tensor(10.0378)\n",
      "tensor(16.7294)\n",
      "tensor(0.7425)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.430889\n",
      "Epoch 6178\n",
      "-------------------------------\n",
      "tensor(38.8010)\n",
      "tensor(21.8568)\n",
      "tensor(5.1597)\n",
      "tensor(0.2129)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.433586\n",
      "Epoch 6179\n",
      "-------------------------------\n",
      "tensor(41.5325)\n",
      "tensor(20.1239)\n",
      "tensor(4.4228)\n",
      "tensor(0.2623)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.402187\n",
      "Epoch 6180\n",
      "-------------------------------\n",
      "tensor(23.7433)\n",
      "tensor(9.7689)\n",
      "tensor(5.6695)\n",
      "tensor(0.4748)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.392487\n",
      "Epoch 6181\n",
      "-------------------------------\n",
      "tensor(25.1304)\n",
      "tensor(10.1844)\n",
      "tensor(6.6221)\n",
      "tensor(0.5262)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.387451\n",
      "Epoch 6182\n",
      "-------------------------------\n",
      "tensor(20.4572)\n",
      "tensor(8.7535)\n",
      "tensor(6.1502)\n",
      "tensor(0.5003)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.381134\n",
      "Epoch 6183\n",
      "-------------------------------\n",
      "tensor(17.2835)\n",
      "tensor(7.9863)\n",
      "tensor(4.7370)\n",
      "tensor(0.3949)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.369160\n",
      "Epoch 6184\n",
      "-------------------------------\n",
      "tensor(22.4780)\n",
      "tensor(8.5580)\n",
      "tensor(5.4217)\n",
      "tensor(0.1427)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.355587\n",
      "Epoch 6185\n",
      "-------------------------------\n",
      "tensor(31.5847)\n",
      "tensor(10.3302)\n",
      "tensor(8.9193)\n",
      "tensor(0.3069)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.344131\n",
      "Epoch 6186\n",
      "-------------------------------\n",
      "tensor(46.6219)\n",
      "tensor(16.4799)\n",
      "tensor(11.3067)\n",
      "tensor(0.7606)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.359062\n",
      "Epoch 6187\n",
      "-------------------------------\n",
      "tensor(45.4313)\n",
      "tensor(16.8934)\n",
      "tensor(14.1246)\n",
      "tensor(0.5086)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.371120\n",
      "Epoch 6188\n",
      "-------------------------------\n",
      "tensor(55.6824)\n",
      "tensor(25.8256)\n",
      "tensor(14.4737)\n",
      "tensor(0.8041)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.412449\n",
      "Epoch 6189\n",
      "-------------------------------\n",
      "tensor(39.3713)\n",
      "tensor(9.7693)\n",
      "tensor(17.9735)\n",
      "tensor(0.8293)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.354454\n",
      "Epoch 6190\n",
      "-------------------------------\n",
      "tensor(41.7506)\n",
      "tensor(12.3686)\n",
      "tensor(18.1693)\n",
      "tensor(0.1629)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.365795\n",
      "Epoch 6191\n",
      "-------------------------------\n",
      "tensor(40.1590)\n",
      "tensor(20.6413)\n",
      "tensor(29.7352)\n",
      "tensor(1.6144)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.411118\n",
      "Epoch 6192\n",
      "-------------------------------\n",
      "tensor(37.6120)\n",
      "tensor(23.4675)\n",
      "tensor(20.8516)\n",
      "tensor(1.0779)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.418232\n",
      "Epoch 6193\n",
      "-------------------------------\n",
      "tensor(39.7625)\n",
      "tensor(16.5789)\n",
      "tensor(17.5132)\n",
      "tensor(1.1434)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.417538\n",
      "Epoch 6194\n",
      "-------------------------------\n",
      "tensor(39.3158)\n",
      "tensor(20.9778)\n",
      "tensor(6.7173)\n",
      "tensor(0.0919)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.403934\n",
      "Epoch 6195\n",
      "-------------------------------\n",
      "tensor(24.2459)\n",
      "tensor(17.4073)\n",
      "tensor(22.6103)\n",
      "tensor(1.2259)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.387970\n",
      "Epoch 6196\n",
      "-------------------------------\n",
      "tensor(28.6095)\n",
      "tensor(9.6610)\n",
      "tensor(8.9806)\n",
      "tensor(0.0203)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.361183\n",
      "Epoch 6197\n",
      "-------------------------------\n",
      "tensor(31.2960)\n",
      "tensor(14.9530)\n",
      "tensor(13.1760)\n",
      "tensor(0.7156)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.342300\n",
      "Epoch 6198\n",
      "-------------------------------\n",
      "tensor(27.6966)\n",
      "tensor(10.1034)\n",
      "tensor(7.7656)\n",
      "tensor(0.5612)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.319763\n",
      "Epoch 6199\n",
      "-------------------------------\n",
      "tensor(32.4443)\n",
      "tensor(9.6596)\n",
      "tensor(6.4365)\n",
      "tensor(0.2547)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.294975\n",
      "Epoch 6200\n",
      "-------------------------------\n",
      "tensor(40.1340)\n",
      "tensor(12.4682)\n",
      "tensor(3.3804)\n",
      "tensor(0.0558)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.301460\n",
      "Epoch 6201\n",
      "-------------------------------\n",
      "tensor(32.7422)\n",
      "tensor(10.2784)\n",
      "tensor(1.7924)\n",
      "tensor(0.0373)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.294575\n",
      "Epoch 6202\n",
      "-------------------------------\n",
      "tensor(44.7225)\n",
      "tensor(22.7865)\n",
      "tensor(2.8166)\n",
      "tensor(0.0843)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.285740\n",
      "Epoch 6203\n",
      "-------------------------------\n",
      "tensor(17.3847)\n",
      "tensor(6.7123)\n",
      "tensor(2.8509)\n",
      "tensor(0.1245)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.276897\n",
      "Epoch 6204\n",
      "-------------------------------\n",
      "tensor(16.1391)\n",
      "tensor(8.5849)\n",
      "tensor(2.8913)\n",
      "tensor(0.1355)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.275471\n",
      "Epoch 6205\n",
      "-------------------------------\n",
      "tensor(23.6685)\n",
      "tensor(9.8634)\n",
      "tensor(2.1025)\n",
      "tensor(0.0589)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.266376\n",
      "Epoch 6206\n",
      "-------------------------------\n",
      "tensor(47.5824)\n",
      "tensor(22.5096)\n",
      "tensor(2.0427)\n",
      "tensor(0.1177)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.283394\n",
      "Epoch 6207\n",
      "-------------------------------\n",
      "tensor(44.8584)\n",
      "tensor(13.1075)\n",
      "tensor(12.5764)\n",
      "tensor(0.2593)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.327202\n",
      "Epoch 6208\n",
      "-------------------------------\n",
      "tensor(33.5823)\n",
      "tensor(15.9573)\n",
      "tensor(9.6528)\n",
      "tensor(0.7169)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.353775\n",
      "Epoch 6209\n",
      "-------------------------------\n",
      "tensor(36.1111)\n",
      "tensor(12.4436)\n",
      "tensor(13.2569)\n",
      "tensor(0.2304)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.319283\n",
      "Epoch 6210\n",
      "-------------------------------\n",
      "tensor(38.2848)\n",
      "tensor(17.8536)\n",
      "tensor(25.2793)\n",
      "tensor(1.0826)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.326046\n",
      "Epoch 6211\n",
      "-------------------------------\n",
      "tensor(31.9618)\n",
      "tensor(9.1738)\n",
      "tensor(21.2350)\n",
      "tensor(0.5662)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.329304\n",
      "Epoch 6212\n",
      "-------------------------------\n",
      "tensor(41.8189)\n",
      "tensor(28.0340)\n",
      "tensor(14.5422)\n",
      "tensor(0.1133)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.382660\n",
      "Epoch 6213\n",
      "-------------------------------\n",
      "tensor(43.6185)\n",
      "tensor(10.6068)\n",
      "tensor(15.9536)\n",
      "tensor(0.1066)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.374294\n",
      "Epoch 6214\n",
      "-------------------------------\n",
      "tensor(49.0918)\n",
      "tensor(23.3346)\n",
      "tensor(41.3833)\n",
      "tensor(1.0583)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.383812\n",
      "Epoch 6215\n",
      "-------------------------------\n",
      "tensor(46.2277)\n",
      "tensor(13.9296)\n",
      "tensor(30.7808)\n",
      "tensor(1.0827)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.355167\n",
      "Epoch 6216\n",
      "-------------------------------\n",
      "tensor(42.4914)\n",
      "tensor(12.5337)\n",
      "tensor(6.7433)\n",
      "tensor(0.2205)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.332863\n",
      "Epoch 6217\n",
      "-------------------------------\n",
      "tensor(41.8828)\n",
      "tensor(18.8513)\n",
      "tensor(23.4362)\n",
      "tensor(0.9175)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.318584\n",
      "Epoch 6218\n",
      "-------------------------------\n",
      "tensor(19.2610)\n",
      "tensor(8.5811)\n",
      "tensor(11.4547)\n",
      "tensor(0.6503)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.273708\n",
      "Epoch 6219\n",
      "-------------------------------\n",
      "tensor(27.9241)\n",
      "tensor(7.5402)\n",
      "tensor(8.1185)\n",
      "tensor(0.0403)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.268970\n",
      "Epoch 6220\n",
      "-------------------------------\n",
      "tensor(23.2560)\n",
      "tensor(10.6068)\n",
      "tensor(11.1354)\n",
      "tensor(0.2804)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.262581\n",
      "Epoch 6221\n",
      "-------------------------------\n",
      "tensor(31.7347)\n",
      "tensor(26.5926)\n",
      "tensor(8.6950)\n",
      "tensor(0.3511)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.265274\n",
      "Epoch 6222\n",
      "-------------------------------\n",
      "tensor(35.3888)\n",
      "tensor(25.8650)\n",
      "tensor(6.1707)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.255314\n",
      "Epoch 6223\n",
      "-------------------------------\n",
      "tensor(32.2407)\n",
      "tensor(10.4239)\n",
      "tensor(3.7280)\n",
      "tensor(0.2192)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.247540\n",
      "Epoch 6224\n",
      "-------------------------------\n",
      "tensor(32.6495)\n",
      "tensor(10.4222)\n",
      "tensor(3.8446)\n",
      "tensor(0.0290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.235043\n",
      "Epoch 6225\n",
      "-------------------------------\n",
      "tensor(24.8475)\n",
      "tensor(10.8034)\n",
      "tensor(7.3756)\n",
      "tensor(0.3365)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.226322\n",
      "Epoch 6226\n",
      "-------------------------------\n",
      "tensor(46.6166)\n",
      "tensor(17.8715)\n",
      "tensor(5.4019)\n",
      "tensor(0.4406)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.255688\n",
      "Epoch 6227\n",
      "-------------------------------\n",
      "tensor(48.8859)\n",
      "tensor(16.2082)\n",
      "tensor(7.9282)\n",
      "tensor(0.2849)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.288776\n",
      "Epoch 6228\n",
      "-------------------------------\n",
      "tensor(23.1099)\n",
      "tensor(9.4588)\n",
      "tensor(3.8225)\n",
      "tensor(0.3391)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.239292\n",
      "Epoch 6229\n",
      "-------------------------------\n",
      "tensor(29.6112)\n",
      "tensor(8.6745)\n",
      "tensor(12.3414)\n",
      "tensor(0.0284)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.263107\n",
      "Epoch 6230\n",
      "-------------------------------\n",
      "tensor(29.6706)\n",
      "tensor(32.6495)\n",
      "tensor(19.6152)\n",
      "tensor(0.8735)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.307617\n",
      "Epoch 6231\n",
      "-------------------------------\n",
      "tensor(32.7394)\n",
      "tensor(9.5753)\n",
      "tensor(7.9098)\n",
      "tensor(0.3092)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.283928\n",
      "Epoch 6232\n",
      "-------------------------------\n",
      "tensor(50.8937)\n",
      "tensor(23.7318)\n",
      "tensor(30.0356)\n",
      "tensor(1.1664)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.280632\n",
      "Epoch 6233\n",
      "-------------------------------\n",
      "tensor(64.1538)\n",
      "tensor(18.2785)\n",
      "tensor(43.5589)\n",
      "tensor(1.2133)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.335888\n",
      "Epoch 6234\n",
      "-------------------------------\n",
      "tensor(65.9214)\n",
      "tensor(23.0916)\n",
      "tensor(46.3060)\n",
      "tensor(0.8837)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.347977\n",
      "Epoch 6235\n",
      "-------------------------------\n",
      "tensor(38.0235)\n",
      "tensor(12.2583)\n",
      "tensor(18.4562)\n",
      "tensor(0.4125)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.268864\n",
      "Epoch 6236\n",
      "-------------------------------\n",
      "tensor(42.6743)\n",
      "tensor(11.3072)\n",
      "tensor(19.1974)\n",
      "tensor(0.1915)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.268524\n",
      "Epoch 6237\n",
      "-------------------------------\n",
      "tensor(56.9997)\n",
      "tensor(18.9243)\n",
      "tensor(20.7682)\n",
      "tensor(0.7304)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.313831\n",
      "Epoch 6238\n",
      "-------------------------------\n",
      "tensor(51.8768)\n",
      "tensor(21.5397)\n",
      "tensor(9.0685)\n",
      "tensor(0.3611)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.260429\n",
      "Epoch 6239\n",
      "-------------------------------\n",
      "tensor(38.4791)\n",
      "tensor(10.2044)\n",
      "tensor(13.6915)\n",
      "tensor(0.1500)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.233307\n",
      "Epoch 6240\n",
      "-------------------------------\n",
      "tensor(39.6305)\n",
      "tensor(9.5688)\n",
      "tensor(16.7796)\n",
      "tensor(0.2383)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.235367\n",
      "Epoch 6241\n",
      "-------------------------------\n",
      "tensor(30.3339)\n",
      "tensor(7.2532)\n",
      "tensor(10.6507)\n",
      "tensor(0.1169)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.230911\n",
      "Epoch 6242\n",
      "-------------------------------\n",
      "tensor(17.4030)\n",
      "tensor(5.8857)\n",
      "tensor(2.0930)\n",
      "tensor(0.0671)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.223064\n",
      "Epoch 6243\n",
      "-------------------------------\n",
      "tensor(28.9620)\n",
      "tensor(11.4466)\n",
      "tensor(10.4555)\n",
      "tensor(0.2875)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.224827\n",
      "Epoch 6244\n",
      "-------------------------------\n",
      "tensor(31.2154)\n",
      "tensor(12.8772)\n",
      "tensor(20.3295)\n",
      "tensor(0.4426)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.226818\n",
      "Epoch 6245\n",
      "-------------------------------\n",
      "tensor(21.7835)\n",
      "tensor(9.3463)\n",
      "tensor(14.5764)\n",
      "tensor(0.2017)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.206158\n",
      "Epoch 6246\n",
      "-------------------------------\n",
      "tensor(27.9093)\n",
      "tensor(9.7155)\n",
      "tensor(13.2398)\n",
      "tensor(0.4886)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.180298\n",
      "Epoch 6247\n",
      "-------------------------------\n",
      "tensor(50.9785)\n",
      "tensor(16.9605)\n",
      "tensor(19.0148)\n",
      "tensor(0.4969)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.234402\n",
      "Epoch 6248\n",
      "-------------------------------\n",
      "tensor(66.6137)\n",
      "tensor(25.0085)\n",
      "tensor(18.7188)\n",
      "tensor(0.8409)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.289349\n",
      "Epoch 6249\n",
      "-------------------------------\n",
      "tensor(48.4836)\n",
      "tensor(12.4284)\n",
      "tensor(20.2611)\n",
      "tensor(0.3381)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.220955\n",
      "Epoch 6250\n",
      "-------------------------------\n",
      "tensor(31.2557)\n",
      "tensor(9.6358)\n",
      "tensor(8.7543)\n",
      "tensor(0.1135)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.219944\n",
      "Epoch 6251\n",
      "-------------------------------\n",
      "tensor(27.7519)\n",
      "tensor(12.3966)\n",
      "tensor(12.0616)\n",
      "tensor(0.7087)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.268501\n",
      "Epoch 6252\n",
      "-------------------------------\n",
      "tensor(31.9167)\n",
      "tensor(8.4369)\n",
      "tensor(15.0357)\n",
      "tensor(0.3527)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.306381\n",
      "Epoch 6253\n",
      "-------------------------------\n",
      "tensor(55.0560)\n",
      "tensor(19.2999)\n",
      "tensor(31.8202)\n",
      "tensor(1.4430)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.329086\n",
      "Epoch 6254\n",
      "-------------------------------\n",
      "tensor(71.8834)\n",
      "tensor(12.1335)\n",
      "tensor(39.3705)\n",
      "tensor(0.4293)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.393120\n",
      "Epoch 6255\n",
      "-------------------------------\n",
      "tensor(44.4297)\n",
      "tensor(19.3900)\n",
      "tensor(34.6410)\n",
      "tensor(0.6506)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.329689\n",
      "Epoch 6256\n",
      "-------------------------------\n",
      "tensor(23.8947)\n",
      "tensor(9.8366)\n",
      "tensor(13.3476)\n",
      "tensor(0.2413)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.261940\n",
      "Epoch 6257\n",
      "-------------------------------\n",
      "tensor(42.5281)\n",
      "tensor(13.1412)\n",
      "tensor(25.6009)\n",
      "tensor(0.7901)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.254555\n",
      "Epoch 6258\n",
      "-------------------------------\n",
      "tensor(30.0675)\n",
      "tensor(10.4166)\n",
      "tensor(8.8116)\n",
      "tensor(0.0068)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.197048\n",
      "Epoch 6259\n",
      "-------------------------------\n",
      "tensor(37.4792)\n",
      "tensor(14.0454)\n",
      "tensor(13.0611)\n",
      "tensor(0.6179)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.214314\n",
      "Epoch 6260\n",
      "-------------------------------\n",
      "tensor(44.9867)\n",
      "tensor(16.1573)\n",
      "tensor(16.7903)\n",
      "tensor(0.6873)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.220348\n",
      "Epoch 6261\n",
      "-------------------------------\n",
      "tensor(43.2182)\n",
      "tensor(14.4775)\n",
      "tensor(12.0109)\n",
      "tensor(0.4955)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.194298\n",
      "Epoch 6262\n",
      "-------------------------------\n",
      "tensor(32.2618)\n",
      "tensor(12.0202)\n",
      "tensor(4.9104)\n",
      "tensor(0.2150)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.160118\n",
      "Epoch 6263\n",
      "-------------------------------\n",
      "tensor(20.1531)\n",
      "tensor(7.1341)\n",
      "tensor(7.8379)\n",
      "tensor(0.1774)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.158260\n",
      "Epoch 6264\n",
      "-------------------------------\n",
      "tensor(29.2730)\n",
      "tensor(28.9237)\n",
      "tensor(16.9502)\n",
      "tensor(0.6116)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.167999\n",
      "Epoch 6265\n",
      "-------------------------------\n",
      "tensor(26.5980)\n",
      "tensor(12.5861)\n",
      "tensor(18.0335)\n",
      "tensor(0.7182)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.154140\n",
      "Epoch 6266\n",
      "-------------------------------\n",
      "tensor(41.4446)\n",
      "tensor(13.7946)\n",
      "tensor(8.7281)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.201813\n",
      "Epoch 6267\n",
      "-------------------------------\n",
      "tensor(42.7364)\n",
      "tensor(18.5902)\n",
      "tensor(20.4847)\n",
      "tensor(0.7185)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.274120\n",
      "Epoch 6268\n",
      "-------------------------------\n",
      "tensor(50.9859)\n",
      "tensor(14.9358)\n",
      "tensor(14.6403)\n",
      "tensor(0.2548)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.273674\n",
      "Epoch 6269\n",
      "-------------------------------\n",
      "tensor(35.0396)\n",
      "tensor(20.8437)\n",
      "tensor(6.7891)\n",
      "tensor(0.3644)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.219162\n",
      "Epoch 6270\n",
      "-------------------------------\n",
      "tensor(44.1355)\n",
      "tensor(19.2154)\n",
      "tensor(8.2821)\n",
      "tensor(0.2027)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.220345\n",
      "Epoch 6271\n",
      "-------------------------------\n",
      "tensor(48.2585)\n",
      "tensor(8.8098)\n",
      "tensor(23.8631)\n",
      "tensor(0.3957)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.266663\n",
      "Epoch 6272\n",
      "-------------------------------\n",
      "tensor(56.0449)\n",
      "tensor(28.3261)\n",
      "tensor(45.3488)\n",
      "tensor(1.3392)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.339039\n",
      "Epoch 6273\n",
      "-------------------------------\n",
      "tensor(68.4445)\n",
      "tensor(18.6565)\n",
      "tensor(46.2695)\n",
      "tensor(1.1830)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.350826\n",
      "Epoch 6274\n",
      "-------------------------------\n",
      "tensor(61.8697)\n",
      "tensor(20.9513)\n",
      "tensor(38.7569)\n",
      "tensor(0.8447)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.362942\n",
      "Epoch 6275\n",
      "-------------------------------\n",
      "tensor(38.3130)\n",
      "tensor(13.3323)\n",
      "tensor(10.4257)\n",
      "tensor(0.3023)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.297726\n",
      "Epoch 6276\n",
      "-------------------------------\n",
      "tensor(41.2539)\n",
      "tensor(15.0273)\n",
      "tensor(26.1004)\n",
      "tensor(0.5753)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.252857\n",
      "Epoch 6277\n",
      "-------------------------------\n",
      "tensor(27.7831)\n",
      "tensor(7.3111)\n",
      "tensor(14.7190)\n",
      "tensor(0.3221)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.203716\n",
      "Epoch 6278\n",
      "-------------------------------\n",
      "tensor(42.0706)\n",
      "tensor(13.1383)\n",
      "tensor(19.4817)\n",
      "tensor(0.2900)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.199448\n",
      "Epoch 6279\n",
      "-------------------------------\n",
      "tensor(40.0461)\n",
      "tensor(12.4499)\n",
      "tensor(3.7157)\n",
      "tensor(0.1999)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.176159\n",
      "Epoch 6280\n",
      "-------------------------------\n",
      "tensor(37.8242)\n",
      "tensor(12.5354)\n",
      "tensor(11.7819)\n",
      "tensor(0.4353)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.153374\n",
      "Epoch 6281\n",
      "-------------------------------\n",
      "tensor(36.1376)\n",
      "tensor(11.4747)\n",
      "tensor(14.3039)\n",
      "tensor(0.4168)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.142845\n",
      "Epoch 6282\n",
      "-------------------------------\n",
      "tensor(28.9968)\n",
      "tensor(8.5277)\n",
      "tensor(11.2480)\n",
      "tensor(0.2621)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.137718\n",
      "Epoch 6283\n",
      "-------------------------------\n",
      "tensor(24.3444)\n",
      "tensor(7.3913)\n",
      "tensor(2.9613)\n",
      "tensor(0.0355)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.125076\n",
      "Epoch 6284\n",
      "-------------------------------\n",
      "tensor(18.5315)\n",
      "tensor(11.2792)\n",
      "tensor(12.2398)\n",
      "tensor(0.4647)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.118591\n",
      "Epoch 6285\n",
      "-------------------------------\n",
      "tensor(43.1861)\n",
      "tensor(17.8941)\n",
      "tensor(21.2131)\n",
      "tensor(0.6768)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.132683\n",
      "Epoch 6286\n",
      "-------------------------------\n",
      "tensor(38.2873)\n",
      "tensor(12.1264)\n",
      "tensor(2.0667)\n",
      "tensor(0.0997)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.116180\n",
      "Epoch 6287\n",
      "-------------------------------\n",
      "tensor(41.9922)\n",
      "tensor(30.2687)\n",
      "tensor(25.8485)\n",
      "tensor(0.6016)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.246387\n",
      "Epoch 6288\n",
      "-------------------------------\n",
      "tensor(39.0857)\n",
      "tensor(21.9906)\n",
      "tensor(4.2597)\n",
      "tensor(0.2571)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.255074\n",
      "Epoch 6289\n",
      "-------------------------------\n",
      "tensor(51.4453)\n",
      "tensor(21.2726)\n",
      "tensor(21.3559)\n",
      "tensor(1.0612)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.198090\n",
      "Epoch 6290\n",
      "-------------------------------\n",
      "tensor(69.6355)\n",
      "tensor(11.3807)\n",
      "tensor(31.5519)\n",
      "tensor(0.1437)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.242821\n",
      "Epoch 6291\n",
      "-------------------------------\n",
      "tensor(63.3640)\n",
      "tensor(30.9174)\n",
      "tensor(49.6298)\n",
      "tensor(1.1981)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.332184\n",
      "Epoch 6292\n",
      "-------------------------------\n",
      "tensor(66.8603)\n",
      "tensor(21.8456)\n",
      "tensor(45.8503)\n",
      "tensor(1.4705)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.321220\n",
      "Epoch 6293\n",
      "-------------------------------\n",
      "tensor(58.4915)\n",
      "tensor(18.9778)\n",
      "tensor(34.7217)\n",
      "tensor(1.0466)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.346375\n",
      "Epoch 6294\n",
      "-------------------------------\n",
      "tensor(42.2291)\n",
      "tensor(15.5829)\n",
      "tensor(19.6462)\n",
      "tensor(0.2067)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.335644\n",
      "Epoch 6295\n",
      "-------------------------------\n",
      "tensor(27.5976)\n",
      "tensor(16.8926)\n",
      "tensor(14.8448)\n",
      "tensor(0.6136)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.312145\n",
      "Epoch 6296\n",
      "-------------------------------\n",
      "tensor(42.9843)\n",
      "tensor(11.5604)\n",
      "tensor(17.9193)\n",
      "tensor(0.1493)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.317333\n",
      "Epoch 6297\n",
      "-------------------------------\n",
      "tensor(24.7589)\n",
      "tensor(11.7569)\n",
      "tensor(5.0700)\n",
      "tensor(0.3613)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.265015\n",
      "Epoch 6298\n",
      "-------------------------------\n",
      "tensor(31.2565)\n",
      "tensor(10.8585)\n",
      "tensor(12.2169)\n",
      "tensor(0.2880)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.232273\n",
      "Epoch 6299\n",
      "-------------------------------\n",
      "tensor(31.5783)\n",
      "tensor(11.7112)\n",
      "tensor(5.5968)\n",
      "tensor(0.1059)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.194939\n",
      "Epoch 6300\n",
      "-------------------------------\n",
      "tensor(22.8276)\n",
      "tensor(12.8088)\n",
      "tensor(5.1805)\n",
      "tensor(0.3931)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.178646\n",
      "Epoch 6301\n",
      "-------------------------------\n",
      "tensor(21.0547)\n",
      "tensor(12.1122)\n",
      "tensor(9.4679)\n",
      "tensor(0.4922)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.166210\n",
      "Epoch 6302\n",
      "-------------------------------\n",
      "tensor(15.3421)\n",
      "tensor(11.1922)\n",
      "tensor(10.7835)\n",
      "tensor(0.4659)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.153763\n",
      "Epoch 6303\n",
      "-------------------------------\n",
      "tensor(15.2965)\n",
      "tensor(9.6937)\n",
      "tensor(9.1842)\n",
      "tensor(0.3084)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.138748\n",
      "Epoch 6304\n",
      "-------------------------------\n",
      "tensor(23.8887)\n",
      "tensor(8.5693)\n",
      "tensor(4.7368)\n",
      "tensor(0.0437)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.127037\n",
      "Epoch 6305\n",
      "-------------------------------\n",
      "tensor(25.3433)\n",
      "tensor(9.2045)\n",
      "tensor(9.6516)\n",
      "tensor(0.4841)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.100025\n",
      "Epoch 6306\n",
      "-------------------------------\n",
      "tensor(54.9621)\n",
      "tensor(28.2797)\n",
      "tensor(9.9691)\n",
      "tensor(0.5125)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.067978\n",
      "Epoch 6307\n",
      "-------------------------------\n",
      "tensor(53.5341)\n",
      "tensor(27.9909)\n",
      "tensor(12.7131)\n",
      "tensor(0.1816)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.159843\n",
      "Epoch 6308\n",
      "-------------------------------\n",
      "tensor(44.8787)\n",
      "tensor(19.1558)\n",
      "tensor(18.9863)\n",
      "tensor(1.1933)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.127563\n",
      "Epoch 6309\n",
      "-------------------------------\n",
      "tensor(33.4073)\n",
      "tensor(15.4832)\n",
      "tensor(23.2764)\n",
      "tensor(0.8964)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.107529\n",
      "Epoch 6310\n",
      "-------------------------------\n",
      "tensor(35.5159)\n",
      "tensor(24.0665)\n",
      "tensor(30.5611)\n",
      "tensor(1.7603)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.152397\n",
      "Epoch 6311\n",
      "-------------------------------\n",
      "tensor(33.9844)\n",
      "tensor(10.6266)\n",
      "tensor(9.7337)\n",
      "tensor(0.2991)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.139317\n",
      "Epoch 6312\n",
      "-------------------------------\n",
      "tensor(27.5872)\n",
      "tensor(15.7618)\n",
      "tensor(18.3656)\n",
      "tensor(1.2791)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.156410\n",
      "Epoch 6313\n",
      "-------------------------------\n",
      "tensor(33.4520)\n",
      "tensor(12.9736)\n",
      "tensor(6.2891)\n",
      "tensor(0.3472)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.171871\n",
      "Epoch 6314\n",
      "-------------------------------\n",
      "tensor(54.0875)\n",
      "tensor(27.5382)\n",
      "tensor(12.2747)\n",
      "tensor(0.7554)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.158463\n",
      "Epoch 6315\n",
      "-------------------------------\n",
      "tensor(37.8268)\n",
      "tensor(11.5400)\n",
      "tensor(24.3989)\n",
      "tensor(0.7052)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.148663\n",
      "Epoch 6316\n",
      "-------------------------------\n",
      "tensor(27.0198)\n",
      "tensor(18.7084)\n",
      "tensor(22.5448)\n",
      "tensor(1.0884)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.164211\n",
      "Epoch 6317\n",
      "-------------------------------\n",
      "tensor(25.1867)\n",
      "tensor(17.0337)\n",
      "tensor(18.9699)\n",
      "tensor(0.9432)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.146786\n",
      "Epoch 6318\n",
      "-------------------------------\n",
      "tensor(34.3641)\n",
      "tensor(9.8535)\n",
      "tensor(6.7527)\n",
      "tensor(0.0553)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.122448\n",
      "Epoch 6319\n",
      "-------------------------------\n",
      "tensor(36.0754)\n",
      "tensor(11.0651)\n",
      "tensor(14.1448)\n",
      "tensor(0.4853)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.106045\n",
      "Epoch 6320\n",
      "-------------------------------\n",
      "tensor(18.6577)\n",
      "tensor(8.0060)\n",
      "tensor(9.3256)\n",
      "tensor(0.4244)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.078705\n",
      "Epoch 6321\n",
      "-------------------------------\n",
      "tensor(29.5658)\n",
      "tensor(9.8903)\n",
      "tensor(4.5747)\n",
      "tensor(0.2498)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.085152\n",
      "Epoch 6322\n",
      "-------------------------------\n",
      "tensor(30.2302)\n",
      "tensor(9.7919)\n",
      "tensor(5.8126)\n",
      "tensor(0.0653)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.083820\n",
      "Epoch 6323\n",
      "-------------------------------\n",
      "tensor(31.3147)\n",
      "tensor(10.0619)\n",
      "tensor(8.9713)\n",
      "tensor(0.1304)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.071709\n",
      "Epoch 6324\n",
      "-------------------------------\n",
      "tensor(23.7976)\n",
      "tensor(8.0693)\n",
      "tensor(8.5831)\n",
      "tensor(0.2786)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.055779\n",
      "Epoch 6325\n",
      "-------------------------------\n",
      "tensor(24.5313)\n",
      "tensor(7.1129)\n",
      "tensor(4.1155)\n",
      "tensor(0.2224)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.041977\n",
      "Epoch 6326\n",
      "-------------------------------\n",
      "tensor(40.5886)\n",
      "tensor(25.0959)\n",
      "tensor(10.5041)\n",
      "tensor(0.0087)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.065273\n",
      "Epoch 6327\n",
      "-------------------------------\n",
      "tensor(44.7739)\n",
      "tensor(25.5407)\n",
      "tensor(11.9811)\n",
      "tensor(0.0297)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.038139\n",
      "Epoch 6328\n",
      "-------------------------------\n",
      "tensor(44.8639)\n",
      "tensor(18.8211)\n",
      "tensor(5.2415)\n",
      "tensor(0.4910)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.123245\n",
      "Epoch 6329\n",
      "-------------------------------\n",
      "tensor(31.9356)\n",
      "tensor(19.3019)\n",
      "tensor(15.8797)\n",
      "tensor(0.7013)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.114185\n",
      "Epoch 6330\n",
      "-------------------------------\n",
      "tensor(36.0286)\n",
      "tensor(9.8320)\n",
      "tensor(14.0538)\n",
      "tensor(0.4455)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.101295\n",
      "Epoch 6331\n",
      "-------------------------------\n",
      "tensor(23.0889)\n",
      "tensor(8.2585)\n",
      "tensor(11.0753)\n",
      "tensor(0.1389)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.086327\n",
      "Epoch 6332\n",
      "-------------------------------\n",
      "tensor(30.9604)\n",
      "tensor(10.1575)\n",
      "tensor(6.4393)\n",
      "tensor(0.0784)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.131203\n",
      "Epoch 6333\n",
      "-------------------------------\n",
      "tensor(31.1712)\n",
      "tensor(10.2460)\n",
      "tensor(5.4551)\n",
      "tensor(0.0163)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.102531\n",
      "Epoch 6334\n",
      "-------------------------------\n",
      "tensor(37.1191)\n",
      "tensor(11.7220)\n",
      "tensor(4.3964)\n",
      "tensor(0.1022)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.128029\n",
      "Epoch 6335\n",
      "-------------------------------\n",
      "tensor(35.7453)\n",
      "tensor(12.0246)\n",
      "tensor(7.7396)\n",
      "tensor(0.3039)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.075153\n",
      "Epoch 6336\n",
      "-------------------------------\n",
      "tensor(16.7477)\n",
      "tensor(4.9150)\n",
      "tensor(1.9589)\n",
      "tensor(0.0971)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.057899\n",
      "Epoch 6337\n",
      "-------------------------------\n",
      "tensor(27.0057)\n",
      "tensor(12.1865)\n",
      "tensor(8.2166)\n",
      "tensor(0.3197)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.085453\n",
      "Epoch 6338\n",
      "-------------------------------\n",
      "tensor(32.0025)\n",
      "tensor(12.5077)\n",
      "tensor(4.0264)\n",
      "tensor(0.1464)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.083687\n",
      "Epoch 6339\n",
      "-------------------------------\n",
      "tensor(40.8213)\n",
      "tensor(22.0069)\n",
      "tensor(2.5738)\n",
      "tensor(0.0097)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.075592\n",
      "Epoch 6340\n",
      "-------------------------------\n",
      "tensor(33.5376)\n",
      "tensor(21.8567)\n",
      "tensor(6.9773)\n",
      "tensor(0.0583)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.055786\n",
      "Epoch 6341\n",
      "-------------------------------\n",
      "tensor(29.8592)\n",
      "tensor(7.3491)\n",
      "tensor(8.3430)\n",
      "tensor(0.0018)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.047333\n",
      "Epoch 6342\n",
      "-------------------------------\n",
      "tensor(31.3873)\n",
      "tensor(9.0625)\n",
      "tensor(5.8030)\n",
      "tensor(0.1089)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.048656\n",
      "Epoch 6343\n",
      "-------------------------------\n",
      "tensor(22.2257)\n",
      "tensor(8.7408)\n",
      "tensor(2.8747)\n",
      "tensor(0.2774)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.043564\n",
      "Epoch 6344\n",
      "-------------------------------\n",
      "tensor(21.5747)\n",
      "tensor(11.4190)\n",
      "tensor(11.5059)\n",
      "tensor(0.4640)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.034359\n",
      "Epoch 6345\n",
      "-------------------------------\n",
      "tensor(21.0178)\n",
      "tensor(10.5341)\n",
      "tensor(15.9953)\n",
      "tensor(0.4074)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.022919\n",
      "Epoch 6346\n",
      "-------------------------------\n",
      "tensor(14.6962)\n",
      "tensor(6.2720)\n",
      "tensor(4.8023)\n",
      "tensor(0.2022)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.986897\n",
      "Epoch 6347\n",
      "-------------------------------\n",
      "tensor(50.2192)\n",
      "tensor(18.0618)\n",
      "tensor(19.1033)\n",
      "tensor(0.7617)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.071301\n",
      "Epoch 6348\n",
      "-------------------------------\n",
      "tensor(42.9347)\n",
      "tensor(13.6784)\n",
      "tensor(5.0882)\n",
      "tensor(0.1820)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.091496\n",
      "Epoch 6349\n",
      "-------------------------------\n",
      "tensor(60.7824)\n",
      "tensor(22.6749)\n",
      "tensor(18.8665)\n",
      "tensor(0.8996)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.086151\n",
      "Epoch 6350\n",
      "-------------------------------\n",
      "tensor(66.5071)\n",
      "tensor(19.3813)\n",
      "tensor(41.6077)\n",
      "tensor(0.7012)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.090229\n",
      "Epoch 6351\n",
      "-------------------------------\n",
      "tensor(75.2413)\n",
      "tensor(23.5305)\n",
      "tensor(53.0267)\n",
      "tensor(0.9210)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.177116\n",
      "Epoch 6352\n",
      "-------------------------------\n",
      "tensor(76.5720)\n",
      "tensor(24.6527)\n",
      "tensor(55.2330)\n",
      "tensor(1.7316)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.209145\n",
      "Epoch 6353\n",
      "-------------------------------\n",
      "tensor(56.2414)\n",
      "tensor(32.0730)\n",
      "tensor(51.2681)\n",
      "tensor(1.8020)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.164703\n",
      "Epoch 6354\n",
      "-------------------------------\n",
      "tensor(51.2092)\n",
      "tensor(12.0354)\n",
      "tensor(20.8182)\n",
      "tensor(0.0206)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.140362\n",
      "Epoch 6355\n",
      "-------------------------------\n",
      "tensor(21.7983)\n",
      "tensor(7.9932)\n",
      "tensor(10.2649)\n",
      "tensor(0.3326)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.061859\n",
      "Epoch 6356\n",
      "-------------------------------\n",
      "tensor(45.7348)\n",
      "tensor(11.0667)\n",
      "tensor(25.7323)\n",
      "tensor(0.1590)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.076389\n",
      "Epoch 6357\n",
      "-------------------------------\n",
      "tensor(16.1103)\n",
      "tensor(10.1151)\n",
      "tensor(6.0191)\n",
      "tensor(0.4361)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.023441\n",
      "Epoch 6358\n",
      "-------------------------------\n",
      "tensor(40.6084)\n",
      "tensor(14.8719)\n",
      "tensor(18.7140)\n",
      "tensor(0.5534)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.016090\n",
      "Epoch 6359\n",
      "-------------------------------\n",
      "tensor(35.4548)\n",
      "tensor(25.6693)\n",
      "tensor(10.9049)\n",
      "tensor(0.1129)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.997459\n",
      "Epoch 6360\n",
      "-------------------------------\n",
      "tensor(25.6259)\n",
      "tensor(10.2862)\n",
      "tensor(5.1392)\n",
      "tensor(0.2659)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.977356\n",
      "Epoch 6361\n",
      "-------------------------------\n",
      "tensor(16.4654)\n",
      "tensor(6.7771)\n",
      "tensor(7.0076)\n",
      "tensor(0.4565)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.967514\n",
      "Epoch 6362\n",
      "-------------------------------\n",
      "tensor(15.8029)\n",
      "tensor(8.0928)\n",
      "tensor(9.6376)\n",
      "tensor(0.5035)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.961266\n",
      "Epoch 6363\n",
      "-------------------------------\n",
      "tensor(15.8494)\n",
      "tensor(8.1396)\n",
      "tensor(10.2227)\n",
      "tensor(0.4115)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.950787\n",
      "Epoch 6364\n",
      "-------------------------------\n",
      "tensor(37.2907)\n",
      "tensor(11.8987)\n",
      "tensor(6.8416)\n",
      "tensor(0.0863)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.941200\n",
      "Epoch 6365\n",
      "-------------------------------\n",
      "tensor(41.3518)\n",
      "tensor(12.2824)\n",
      "tensor(9.1445)\n",
      "tensor(0.4446)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.949169\n",
      "Epoch 6366\n",
      "-------------------------------\n",
      "tensor(48.4202)\n",
      "tensor(27.4261)\n",
      "tensor(15.9406)\n",
      "tensor(0.6120)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.993191\n",
      "Epoch 6367\n",
      "-------------------------------\n",
      "tensor(47.5394)\n",
      "tensor(15.2628)\n",
      "tensor(9.5378)\n",
      "tensor(0.3231)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.964012\n",
      "Epoch 6368\n",
      "-------------------------------\n",
      "tensor(41.5848)\n",
      "tensor(26.5963)\n",
      "tensor(25.8032)\n",
      "tensor(1.5061)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.005775\n",
      "Epoch 6369\n",
      "-------------------------------\n",
      "tensor(19.8484)\n",
      "tensor(4.7209)\n",
      "tensor(5.2704)\n",
      "tensor(0.0555)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.977131\n",
      "Epoch 6370\n",
      "-------------------------------\n",
      "tensor(20.5911)\n",
      "tensor(20.1599)\n",
      "tensor(18.8689)\n",
      "tensor(1.2904)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.048004\n",
      "Epoch 6371\n",
      "-------------------------------\n",
      "tensor(35.8475)\n",
      "tensor(9.6726)\n",
      "tensor(13.8720)\n",
      "tensor(0.1230)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.065166\n",
      "Epoch 6372\n",
      "-------------------------------\n",
      "tensor(45.8332)\n",
      "tensor(8.3691)\n",
      "tensor(22.1430)\n",
      "tensor(0.3351)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.065590\n",
      "Epoch 6373\n",
      "-------------------------------\n",
      "tensor(69.4676)\n",
      "tensor(22.3961)\n",
      "tensor(31.0455)\n",
      "tensor(1.1413)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.090832\n",
      "Epoch 6374\n",
      "-------------------------------\n",
      "tensor(71.6191)\n",
      "tensor(18.0340)\n",
      "tensor(47.2303)\n",
      "tensor(1.1928)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.102238\n",
      "Epoch 6375\n",
      "-------------------------------\n",
      "tensor(48.8862)\n",
      "tensor(22.1676)\n",
      "tensor(39.4908)\n",
      "tensor(0.8295)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.052399\n",
      "Epoch 6376\n",
      "-------------------------------\n",
      "tensor(18.1773)\n",
      "tensor(11.6588)\n",
      "tensor(12.6175)\n",
      "tensor(0.3155)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.973530\n",
      "Epoch 6377\n",
      "-------------------------------\n",
      "tensor(48.0528)\n",
      "tensor(9.9160)\n",
      "tensor(25.8475)\n",
      "tensor(0.4687)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.972961\n",
      "Epoch 6378\n",
      "-------------------------------\n",
      "tensor(45.4424)\n",
      "tensor(13.6016)\n",
      "tensor(8.5620)\n",
      "tensor(0.0057)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.941170\n",
      "Epoch 6379\n",
      "-------------------------------\n",
      "tensor(37.5610)\n",
      "tensor(13.1615)\n",
      "tensor(11.3642)\n",
      "tensor(0.4631)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.934547\n",
      "Epoch 6380\n",
      "-------------------------------\n",
      "tensor(44.0964)\n",
      "tensor(14.4584)\n",
      "tensor(15.0839)\n",
      "tensor(0.5020)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.925747\n",
      "Epoch 6381\n",
      "-------------------------------\n",
      "tensor(33.8039)\n",
      "tensor(10.4437)\n",
      "tensor(10.2648)\n",
      "tensor(0.3264)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.919498\n",
      "Epoch 6382\n",
      "-------------------------------\n",
      "tensor(31.5785)\n",
      "tensor(9.7121)\n",
      "tensor(3.6043)\n",
      "tensor(0.0849)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.908260\n",
      "Epoch 6383\n",
      "-------------------------------\n",
      "tensor(32.6881)\n",
      "tensor(12.9033)\n",
      "tensor(7.6197)\n",
      "tensor(0.2294)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.913422\n",
      "Epoch 6384\n",
      "-------------------------------\n",
      "tensor(29.7172)\n",
      "tensor(14.2711)\n",
      "tensor(16.6802)\n",
      "tensor(0.5459)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.927036\n",
      "Epoch 6385\n",
      "-------------------------------\n",
      "tensor(37.1635)\n",
      "tensor(28.1977)\n",
      "tensor(13.0025)\n",
      "tensor(0.5205)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.925274\n",
      "Epoch 6386\n",
      "-------------------------------\n",
      "tensor(22.9712)\n",
      "tensor(7.2434)\n",
      "tensor(2.6595)\n",
      "tensor(0.0852)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.919537\n",
      "Epoch 6387\n",
      "-------------------------------\n",
      "tensor(37.9156)\n",
      "tensor(16.6170)\n",
      "tensor(16.9019)\n",
      "tensor(0.6961)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.955742\n",
      "Epoch 6388\n",
      "-------------------------------\n",
      "tensor(29.0576)\n",
      "tensor(8.1029)\n",
      "tensor(7.1948)\n",
      "tensor(0.1871)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.956863\n",
      "Epoch 6389\n",
      "-------------------------------\n",
      "tensor(35.4315)\n",
      "tensor(10.9895)\n",
      "tensor(4.7023)\n",
      "tensor(0.0404)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.968918\n",
      "Epoch 6390\n",
      "-------------------------------\n",
      "tensor(51.5337)\n",
      "tensor(18.7975)\n",
      "tensor(14.4360)\n",
      "tensor(0.2723)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.961460\n",
      "Epoch 6391\n",
      "-------------------------------\n",
      "tensor(68.8649)\n",
      "tensor(16.0838)\n",
      "tensor(38.2611)\n",
      "tensor(0.3842)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.033871\n",
      "Epoch 6392\n",
      "-------------------------------\n",
      "tensor(83.2332)\n",
      "tensor(33.0123)\n",
      "tensor(63.2369)\n",
      "tensor(1.9089)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.098969\n",
      "Epoch 6393\n",
      "-------------------------------\n",
      "tensor(91.4289)\n",
      "tensor(37.4384)\n",
      "tensor(73.4926)\n",
      "tensor(2.2548)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.173531\n",
      "Epoch 6394\n",
      "-------------------------------\n",
      "tensor(88.2653)\n",
      "tensor(22.7443)\n",
      "tensor(56.8076)\n",
      "tensor(1.0287)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.170784\n",
      "Epoch 6395\n",
      "-------------------------------\n",
      "tensor(37.7248)\n",
      "tensor(11.5177)\n",
      "tensor(15.7140)\n",
      "tensor(0.2222)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.021690\n",
      "Epoch 6396\n",
      "-------------------------------\n",
      "tensor(52.6422)\n",
      "tensor(12.5102)\n",
      "tensor(31.7080)\n",
      "tensor(0.4044)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.994911\n",
      "Epoch 6397\n",
      "-------------------------------\n",
      "tensor(41.0587)\n",
      "tensor(14.6544)\n",
      "tensor(24.1521)\n",
      "tensor(0.6994)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.968128\n",
      "Epoch 6398\n",
      "-------------------------------\n",
      "tensor(55.3662)\n",
      "tensor(17.7166)\n",
      "tensor(25.3878)\n",
      "tensor(0.4696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.976818\n",
      "Epoch 6399\n",
      "-------------------------------\n",
      "tensor(32.1298)\n",
      "tensor(10.1288)\n",
      "tensor(4.3402)\n",
      "tensor(0.2350)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.921513\n",
      "Epoch 6400\n",
      "-------------------------------\n",
      "tensor(27.7990)\n",
      "tensor(9.6852)\n",
      "tensor(17.1728)\n",
      "tensor(0.5426)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.915375\n",
      "Epoch 6401\n",
      "-------------------------------\n",
      "tensor(29.6144)\n",
      "tensor(9.4008)\n",
      "tensor(17.8745)\n",
      "tensor(0.5022)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.913712\n",
      "Epoch 6402\n",
      "-------------------------------\n",
      "tensor(23.9669)\n",
      "tensor(9.5074)\n",
      "tensor(11.4386)\n",
      "tensor(0.2895)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.901382\n",
      "Epoch 6403\n",
      "-------------------------------\n",
      "tensor(32.6120)\n",
      "tensor(19.6771)\n",
      "tensor(1.8563)\n",
      "tensor(0.0908)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.894943\n",
      "Epoch 6404\n",
      "-------------------------------\n",
      "tensor(42.4018)\n",
      "tensor(16.3659)\n",
      "tensor(15.7533)\n",
      "tensor(0.5774)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.884174\n",
      "Epoch 6405\n",
      "-------------------------------\n",
      "tensor(31.9361)\n",
      "tensor(16.4416)\n",
      "tensor(16.3502)\n",
      "tensor(0.7927)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.880535\n",
      "Epoch 6406\n",
      "-------------------------------\n",
      "tensor(56.9046)\n",
      "tensor(19.2511)\n",
      "tensor(6.5857)\n",
      "tensor(0.3069)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.909790\n",
      "Epoch 6407\n",
      "-------------------------------\n",
      "tensor(53.0810)\n",
      "tensor(12.3885)\n",
      "tensor(20.5296)\n",
      "tensor(0.1647)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.945374\n",
      "Epoch 6408\n",
      "-------------------------------\n",
      "tensor(40.8438)\n",
      "tensor(16.7168)\n",
      "tensor(18.3496)\n",
      "tensor(0.4927)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.937275\n",
      "Epoch 6409\n",
      "-------------------------------\n",
      "tensor(25.9437)\n",
      "tensor(8.5680)\n",
      "tensor(7.0988)\n",
      "tensor(0.0774)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.916790\n",
      "Epoch 6410\n",
      "-------------------------------\n",
      "tensor(40.7866)\n",
      "tensor(19.5126)\n",
      "tensor(29.3914)\n",
      "tensor(0.9302)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.971439\n",
      "Epoch 6411\n",
      "-------------------------------\n",
      "tensor(60.5617)\n",
      "tensor(16.3338)\n",
      "tensor(36.4709)\n",
      "tensor(0.9583)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.041794\n",
      "Epoch 6412\n",
      "-------------------------------\n",
      "tensor(58.1013)\n",
      "tensor(20.5343)\n",
      "tensor(41.5933)\n",
      "tensor(0.9977)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.998283\n",
      "Epoch 6413\n",
      "-------------------------------\n",
      "tensor(63.7218)\n",
      "tensor(21.2391)\n",
      "tensor(40.8551)\n",
      "tensor(0.8773)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.026539\n",
      "Epoch 6414\n",
      "-------------------------------\n",
      "tensor(57.4584)\n",
      "tensor(15.3773)\n",
      "tensor(28.6692)\n",
      "tensor(0.6878)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.956818\n",
      "Epoch 6415\n",
      "-------------------------------\n",
      "tensor(35.0258)\n",
      "tensor(19.3009)\n",
      "tensor(5.3110)\n",
      "tensor(0.4608)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.942604\n",
      "Epoch 6416\n",
      "-------------------------------\n",
      "tensor(15.5734)\n",
      "tensor(16.1023)\n",
      "tensor(17.2395)\n",
      "tensor(0.9687)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.923321\n",
      "Epoch 6417\n",
      "-------------------------------\n",
      "tensor(24.0452)\n",
      "tensor(9.6163)\n",
      "tensor(4.0779)\n",
      "tensor(0.2194)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.920227\n",
      "Epoch 6418\n",
      "-------------------------------\n",
      "tensor(26.6243)\n",
      "tensor(8.3052)\n",
      "tensor(8.7031)\n",
      "tensor(0.3568)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.908794\n",
      "Epoch 6419\n",
      "-------------------------------\n",
      "tensor(16.9070)\n",
      "tensor(7.0836)\n",
      "tensor(7.5717)\n",
      "tensor(0.3757)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.881508\n",
      "Epoch 6420\n",
      "-------------------------------\n",
      "tensor(14.9916)\n",
      "tensor(6.2028)\n",
      "tensor(4.8108)\n",
      "tensor(0.1979)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.867805\n",
      "Epoch 6421\n",
      "-------------------------------\n",
      "tensor(43.1640)\n",
      "tensor(20.9030)\n",
      "tensor(4.7682)\n",
      "tensor(0.0374)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.866158\n",
      "Epoch 6422\n",
      "-------------------------------\n",
      "tensor(34.2691)\n",
      "tensor(10.7831)\n",
      "tensor(3.0147)\n",
      "tensor(0.0777)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.857601\n",
      "Epoch 6423\n",
      "-------------------------------\n",
      "tensor(21.2767)\n",
      "tensor(6.8167)\n",
      "tensor(2.2750)\n",
      "tensor(0.1933)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.850037\n",
      "Epoch 6424\n",
      "-------------------------------\n",
      "tensor(24.2223)\n",
      "tensor(10.2042)\n",
      "tensor(5.1688)\n",
      "tensor(0.3091)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.845364\n",
      "Epoch 6425\n",
      "-------------------------------\n",
      "tensor(30.7785)\n",
      "tensor(12.4482)\n",
      "tensor(6.0436)\n",
      "tensor(0.3858)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.827702\n",
      "Epoch 6426\n",
      "-------------------------------\n",
      "tensor(32.9783)\n",
      "tensor(10.6613)\n",
      "tensor(4.3521)\n",
      "tensor(0.2631)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.856899\n",
      "Epoch 6427\n",
      "-------------------------------\n",
      "tensor(50.6925)\n",
      "tensor(15.0191)\n",
      "tensor(3.9419)\n",
      "tensor(0.1910)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.905563\n",
      "Epoch 6428\n",
      "-------------------------------\n",
      "tensor(48.1349)\n",
      "tensor(29.5850)\n",
      "tensor(12.5565)\n",
      "tensor(0.5099)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.876316\n",
      "Epoch 6429\n",
      "-------------------------------\n",
      "tensor(44.0332)\n",
      "tensor(11.1441)\n",
      "tensor(17.4247)\n",
      "tensor(0.2360)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.875782\n",
      "Epoch 6430\n",
      "-------------------------------\n",
      "tensor(43.5364)\n",
      "tensor(21.9109)\n",
      "tensor(36.5461)\n",
      "tensor(1.3313)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.931942\n",
      "Epoch 6431\n",
      "-------------------------------\n",
      "tensor(48.5037)\n",
      "tensor(20.0404)\n",
      "tensor(38.0211)\n",
      "tensor(1.4657)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.980396\n",
      "Epoch 6432\n",
      "-------------------------------\n",
      "tensor(56.2732)\n",
      "tensor(19.8750)\n",
      "tensor(34.3378)\n",
      "tensor(0.4639)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.014606\n",
      "Epoch 6433\n",
      "-------------------------------\n",
      "tensor(53.1792)\n",
      "tensor(9.8210)\n",
      "tensor(25.4654)\n",
      "tensor(0.0321)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.990559\n",
      "Epoch 6434\n",
      "-------------------------------\n",
      "tensor(27.9057)\n",
      "tensor(14.5715)\n",
      "tensor(21.0929)\n",
      "tensor(0.9786)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.950676\n",
      "Epoch 6435\n",
      "-------------------------------\n",
      "tensor(34.9409)\n",
      "tensor(11.2770)\n",
      "tensor(6.4908)\n",
      "tensor(0.3120)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.933441\n",
      "Epoch 6436\n",
      "-------------------------------\n",
      "tensor(30.3508)\n",
      "tensor(18.7316)\n",
      "tensor(23.4429)\n",
      "tensor(1.0923)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.917992\n",
      "Epoch 6437\n",
      "-------------------------------\n",
      "tensor(43.4553)\n",
      "tensor(23.1575)\n",
      "tensor(2.7988)\n",
      "tensor(0.1179)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.885201\n",
      "Epoch 6438\n",
      "-------------------------------\n",
      "tensor(18.2045)\n",
      "tensor(10.6974)\n",
      "tensor(8.8662)\n",
      "tensor(0.5700)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.859215\n",
      "Epoch 6439\n",
      "-------------------------------\n",
      "tensor(28.3083)\n",
      "tensor(12.3059)\n",
      "tensor(9.0048)\n",
      "tensor(0.6566)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.850983\n",
      "Epoch 6440\n",
      "-------------------------------\n",
      "tensor(19.8210)\n",
      "tensor(8.5918)\n",
      "tensor(6.0221)\n",
      "tensor(0.4887)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.840145\n",
      "Epoch 6441\n",
      "-------------------------------\n",
      "tensor(35.5631)\n",
      "tensor(12.3098)\n",
      "tensor(3.2302)\n",
      "tensor(0.2971)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.833984\n",
      "Epoch 6442\n",
      "-------------------------------\n",
      "tensor(16.9205)\n",
      "tensor(8.8912)\n",
      "tensor(1.5278)\n",
      "tensor(0.1245)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.823280\n",
      "Epoch 6443\n",
      "-------------------------------\n",
      "tensor(26.3746)\n",
      "tensor(8.3787)\n",
      "tensor(3.0105)\n",
      "tensor(0.0702)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.819149\n",
      "Epoch 6444\n",
      "-------------------------------\n",
      "tensor(24.0284)\n",
      "tensor(8.3053)\n",
      "tensor(5.9762)\n",
      "tensor(0.2773)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.807049\n",
      "Epoch 6445\n",
      "-------------------------------\n",
      "tensor(34.0753)\n",
      "tensor(10.8646)\n",
      "tensor(7.6879)\n",
      "tensor(0.3953)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.798203\n",
      "Epoch 6446\n",
      "-------------------------------\n",
      "tensor(50.0825)\n",
      "tensor(24.4912)\n",
      "tensor(4.3370)\n",
      "tensor(0.2462)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.827122\n",
      "Epoch 6447\n",
      "-------------------------------\n",
      "tensor(48.1877)\n",
      "tensor(13.6436)\n",
      "tensor(16.4835)\n",
      "tensor(0.2453)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.826241\n",
      "Epoch 6448\n",
      "-------------------------------\n",
      "tensor(39.3541)\n",
      "tensor(20.4022)\n",
      "tensor(16.3134)\n",
      "tensor(1.1094)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.894211\n",
      "Epoch 6449\n",
      "-------------------------------\n",
      "tensor(31.5894)\n",
      "tensor(9.5455)\n",
      "tensor(12.7951)\n",
      "tensor(0.2759)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.862076\n",
      "Epoch 6450\n",
      "-------------------------------\n",
      "tensor(36.6933)\n",
      "tensor(24.6510)\n",
      "tensor(31.0325)\n",
      "tensor(1.6371)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.891689\n",
      "Epoch 6451\n",
      "-------------------------------\n",
      "tensor(42.0910)\n",
      "tensor(12.3958)\n",
      "tensor(20.5299)\n",
      "tensor(0.4350)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.878551\n",
      "Epoch 6452\n",
      "-------------------------------\n",
      "tensor(42.6249)\n",
      "tensor(8.0283)\n",
      "tensor(19.4520)\n",
      "tensor(0.5423)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.883583\n",
      "Epoch 6453\n",
      "-------------------------------\n",
      "tensor(53.1412)\n",
      "tensor(18.5578)\n",
      "tensor(19.6490)\n",
      "tensor(0.5752)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.899082\n",
      "Epoch 6454\n",
      "-------------------------------\n",
      "tensor(54.0681)\n",
      "tensor(15.9336)\n",
      "tensor(32.7799)\n",
      "tensor(1.0753)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.902538\n",
      "Epoch 6455\n",
      "-------------------------------\n",
      "tensor(44.1401)\n",
      "tensor(19.6115)\n",
      "tensor(24.0903)\n",
      "tensor(0.6772)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.892288\n",
      "Epoch 6456\n",
      "-------------------------------\n",
      "tensor(23.2879)\n",
      "tensor(13.8549)\n",
      "tensor(10.8417)\n",
      "tensor(0.6836)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.862610\n",
      "Epoch 6457\n",
      "-------------------------------\n",
      "tensor(31.7056)\n",
      "tensor(5.1632)\n",
      "tensor(13.8385)\n",
      "tensor(0.0125)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.845741\n",
      "Epoch 6458\n",
      "-------------------------------\n",
      "tensor(32.5406)\n",
      "tensor(10.1175)\n",
      "tensor(4.8524)\n",
      "tensor(0.1262)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.866356\n",
      "Epoch 6459\n",
      "-------------------------------\n",
      "tensor(33.9198)\n",
      "tensor(10.4190)\n",
      "tensor(6.5488)\n",
      "tensor(0.0454)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.864014\n",
      "Epoch 6460\n",
      "-------------------------------\n",
      "tensor(35.1332)\n",
      "tensor(10.5117)\n",
      "tensor(8.1923)\n",
      "tensor(0.0397)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.843178\n",
      "Epoch 6461\n",
      "-------------------------------\n",
      "tensor(33.5923)\n",
      "tensor(10.3904)\n",
      "tensor(5.5306)\n",
      "tensor(0.0806)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.814095\n",
      "Epoch 6462\n",
      "-------------------------------\n",
      "tensor(22.7974)\n",
      "tensor(8.1121)\n",
      "tensor(2.4999)\n",
      "tensor(0.1354)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.809246\n",
      "Epoch 6463\n",
      "-------------------------------\n",
      "tensor(28.9494)\n",
      "tensor(9.8204)\n",
      "tensor(5.4543)\n",
      "tensor(0.1827)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.808372\n",
      "Epoch 6464\n",
      "-------------------------------\n",
      "tensor(27.3285)\n",
      "tensor(8.5400)\n",
      "tensor(9.3503)\n",
      "tensor(0.1554)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.801037\n",
      "Epoch 6465\n",
      "-------------------------------\n",
      "tensor(31.2883)\n",
      "tensor(11.4899)\n",
      "tensor(6.0282)\n",
      "tensor(0.0630)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.816025\n",
      "Epoch 6466\n",
      "-------------------------------\n",
      "tensor(48.7316)\n",
      "tensor(21.3536)\n",
      "tensor(9.3999)\n",
      "tensor(0.4439)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.827812\n",
      "Epoch 6467\n",
      "-------------------------------\n",
      "tensor(35.2987)\n",
      "tensor(13.9209)\n",
      "tensor(4.1503)\n",
      "tensor(0.3000)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.790783\n",
      "Epoch 6468\n",
      "-------------------------------\n",
      "tensor(36.2419)\n",
      "tensor(10.4371)\n",
      "tensor(7.1097)\n",
      "tensor(0.0430)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.867420\n",
      "Epoch 6469\n",
      "-------------------------------\n",
      "tensor(36.6936)\n",
      "tensor(14.1160)\n",
      "tensor(15.2611)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.881275\n",
      "Epoch 6470\n",
      "-------------------------------\n",
      "tensor(34.4852)\n",
      "tensor(9.9972)\n",
      "tensor(9.5674)\n",
      "tensor(0.2460)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.820316\n",
      "Epoch 6471\n",
      "-------------------------------\n",
      "tensor(35.8520)\n",
      "tensor(20.1562)\n",
      "tensor(5.0759)\n",
      "tensor(0.2829)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.839554\n",
      "Epoch 6472\n",
      "-------------------------------\n",
      "tensor(34.8668)\n",
      "tensor(9.4173)\n",
      "tensor(10.9342)\n",
      "tensor(0.0618)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.838448\n",
      "Epoch 6473\n",
      "-------------------------------\n",
      "tensor(49.0111)\n",
      "tensor(15.6205)\n",
      "tensor(18.5527)\n",
      "tensor(0.3736)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.904434\n",
      "Epoch 6474\n",
      "-------------------------------\n",
      "tensor(47.3649)\n",
      "tensor(14.4036)\n",
      "tensor(23.7890)\n",
      "tensor(0.6611)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.938507\n",
      "Epoch 6475\n",
      "-------------------------------\n",
      "tensor(36.0823)\n",
      "tensor(13.4249)\n",
      "tensor(13.6259)\n",
      "tensor(0.5411)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.874634\n",
      "Epoch 6476\n",
      "-------------------------------\n",
      "tensor(25.1614)\n",
      "tensor(8.7623)\n",
      "tensor(8.8988)\n",
      "tensor(0.4116)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.872421\n",
      "Epoch 6477\n",
      "-------------------------------\n",
      "tensor(22.0216)\n",
      "tensor(7.9648)\n",
      "tensor(10.1145)\n",
      "tensor(0.2834)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.862373\n",
      "Epoch 6478\n",
      "-------------------------------\n",
      "tensor(17.1308)\n",
      "tensor(8.4658)\n",
      "tensor(6.9547)\n",
      "tensor(0.3704)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.852306\n",
      "Epoch 6479\n",
      "-------------------------------\n",
      "tensor(16.9671)\n",
      "tensor(6.8149)\n",
      "tensor(5.0660)\n",
      "tensor(0.1776)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.839272\n",
      "Epoch 6480\n",
      "-------------------------------\n",
      "tensor(18.1335)\n",
      "tensor(6.3303)\n",
      "tensor(7.2022)\n",
      "tensor(0.0463)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.827229\n",
      "Epoch 6481\n",
      "-------------------------------\n",
      "tensor(16.9009)\n",
      "tensor(5.9874)\n",
      "tensor(6.2113)\n",
      "tensor(0.0009)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.815590\n",
      "Epoch 6482\n",
      "-------------------------------\n",
      "tensor(23.5736)\n",
      "tensor(7.6426)\n",
      "tensor(3.7725)\n",
      "tensor(0.0040)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.807999\n",
      "Epoch 6483\n",
      "-------------------------------\n",
      "tensor(23.4509)\n",
      "tensor(7.4299)\n",
      "tensor(1.3394)\n",
      "tensor(0.0040)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.793652\n",
      "Epoch 6484\n",
      "-------------------------------\n",
      "tensor(21.3577)\n",
      "tensor(5.5954)\n",
      "tensor(4.9191)\n",
      "tensor(0.0039)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.775345\n",
      "Epoch 6485\n",
      "-------------------------------\n",
      "tensor(37.3678)\n",
      "tensor(11.4036)\n",
      "tensor(4.5059)\n",
      "tensor(0.1007)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.774097\n",
      "Epoch 6486\n",
      "-------------------------------\n",
      "tensor(52.1748)\n",
      "tensor(22.3466)\n",
      "tensor(4.4172)\n",
      "tensor(0.3110)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.776981\n",
      "Epoch 6487\n",
      "-------------------------------\n",
      "tensor(41.5584)\n",
      "tensor(15.4348)\n",
      "tensor(6.9232)\n",
      "tensor(0.3076)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.764950\n",
      "Epoch 6488\n",
      "-------------------------------\n",
      "tensor(37.4532)\n",
      "tensor(14.8259)\n",
      "tensor(3.3644)\n",
      "tensor(0.3161)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.853817\n",
      "Epoch 6489\n",
      "-------------------------------\n",
      "tensor(52.7528)\n",
      "tensor(21.4907)\n",
      "tensor(9.8243)\n",
      "tensor(0.1738)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.886196\n",
      "Epoch 6490\n",
      "-------------------------------\n",
      "tensor(56.6551)\n",
      "tensor(14.7298)\n",
      "tensor(31.0889)\n",
      "tensor(0.5447)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.880352\n",
      "Epoch 6491\n",
      "-------------------------------\n",
      "tensor(57.2527)\n",
      "tensor(23.3679)\n",
      "tensor(45.0162)\n",
      "tensor(1.3952)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.839012\n",
      "Epoch 6492\n",
      "-------------------------------\n",
      "tensor(66.2143)\n",
      "tensor(24.5722)\n",
      "tensor(47.5393)\n",
      "tensor(1.4274)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.916233\n",
      "Epoch 6493\n",
      "-------------------------------\n",
      "tensor(54.9072)\n",
      "tensor(16.6218)\n",
      "tensor(39.6355)\n",
      "tensor(0.8405)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.887989\n",
      "Epoch 6494\n",
      "-------------------------------\n",
      "tensor(40.2894)\n",
      "tensor(12.5729)\n",
      "tensor(20.6256)\n",
      "tensor(0.4960)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.881691\n",
      "Epoch 6495\n",
      "-------------------------------\n",
      "tensor(30.9153)\n",
      "tensor(9.0436)\n",
      "tensor(6.0598)\n",
      "tensor(0.0120)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.834545\n",
      "Epoch 6496\n",
      "-------------------------------\n",
      "tensor(30.4752)\n",
      "tensor(12.4164)\n",
      "tensor(22.0186)\n",
      "tensor(0.6437)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.822411\n",
      "Epoch 6497\n",
      "-------------------------------\n",
      "tensor(36.7276)\n",
      "tensor(11.2983)\n",
      "tensor(2.3063)\n",
      "tensor(0.0856)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.816517\n",
      "Epoch 6498\n",
      "-------------------------------\n",
      "tensor(40.1151)\n",
      "tensor(12.3861)\n",
      "tensor(15.5160)\n",
      "tensor(0.4453)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.789886\n",
      "Epoch 6499\n",
      "-------------------------------\n",
      "tensor(21.6679)\n",
      "tensor(6.2410)\n",
      "tensor(9.2178)\n",
      "tensor(0.2063)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.776699\n",
      "Epoch 6500\n",
      "-------------------------------\n",
      "tensor(15.4567)\n",
      "tensor(4.9676)\n",
      "tensor(2.0874)\n",
      "tensor(0.0972)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.766930\n",
      "Epoch 6501\n",
      "-------------------------------\n",
      "tensor(16.7666)\n",
      "tensor(6.0943)\n",
      "tensor(8.0418)\n",
      "tensor(0.2529)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.761959\n",
      "Epoch 6502\n",
      "-------------------------------\n",
      "tensor(18.1410)\n",
      "tensor(6.6045)\n",
      "tensor(10.1483)\n",
      "tensor(0.2888)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.756039\n",
      "Epoch 6503\n",
      "-------------------------------\n",
      "tensor(17.1117)\n",
      "tensor(6.0248)\n",
      "tensor(8.5151)\n",
      "tensor(0.2114)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.743866\n",
      "Epoch 6504\n",
      "-------------------------------\n",
      "tensor(25.8478)\n",
      "tensor(8.1121)\n",
      "tensor(2.1333)\n",
      "tensor(0.0382)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.741077\n",
      "Epoch 6505\n",
      "-------------------------------\n",
      "tensor(36.1033)\n",
      "tensor(24.7185)\n",
      "tensor(10.3606)\n",
      "tensor(0.3489)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.736851\n",
      "Epoch 6506\n",
      "-------------------------------\n",
      "tensor(46.0739)\n",
      "tensor(12.6037)\n",
      "tensor(16.5479)\n",
      "tensor(0.2817)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.734863\n",
      "Epoch 6507\n",
      "-------------------------------\n",
      "tensor(53.2170)\n",
      "tensor(20.5052)\n",
      "tensor(9.9975)\n",
      "tensor(0.6374)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.792671\n",
      "Epoch 6508\n",
      "-------------------------------\n",
      "tensor(52.5021)\n",
      "tensor(23.0933)\n",
      "tensor(15.5690)\n",
      "tensor(0.8785)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.799580\n",
      "Epoch 6509\n",
      "-------------------------------\n",
      "tensor(41.8025)\n",
      "tensor(29.7959)\n",
      "tensor(24.0687)\n",
      "tensor(0.4026)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.793896\n",
      "Epoch 6510\n",
      "-------------------------------\n",
      "tensor(17.1064)\n",
      "tensor(8.4961)\n",
      "tensor(2.5535)\n",
      "tensor(0.0120)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.798130\n",
      "Epoch 6511\n",
      "-------------------------------\n",
      "tensor(34.4726)\n",
      "tensor(10.4779)\n",
      "tensor(22.4373)\n",
      "tensor(0.2927)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.863831\n",
      "Epoch 6512\n",
      "-------------------------------\n",
      "tensor(58.3121)\n",
      "tensor(17.4104)\n",
      "tensor(39.8694)\n",
      "tensor(1.0445)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.933674\n",
      "Epoch 6513\n",
      "-------------------------------\n",
      "tensor(69.7594)\n",
      "tensor(29.9434)\n",
      "tensor(56.4123)\n",
      "tensor(1.6585)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.969223\n",
      "Epoch 6514\n",
      "-------------------------------\n",
      "tensor(70.1458)\n",
      "tensor(18.8426)\n",
      "tensor(46.7574)\n",
      "tensor(1.0742)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.933289\n",
      "Epoch 6515\n",
      "-------------------------------\n",
      "tensor(29.2812)\n",
      "tensor(10.4878)\n",
      "tensor(18.5217)\n",
      "tensor(0.3293)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.802937\n",
      "Epoch 6516\n",
      "-------------------------------\n",
      "tensor(43.7275)\n",
      "tensor(13.6212)\n",
      "tensor(23.2705)\n",
      "tensor(0.4156)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.806923\n",
      "Epoch 6517\n",
      "-------------------------------\n",
      "tensor(40.3049)\n",
      "tensor(12.9381)\n",
      "tensor(20.6166)\n",
      "tensor(0.5327)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.763435\n",
      "Epoch 6518\n",
      "-------------------------------\n",
      "tensor(41.4010)\n",
      "tensor(12.3635)\n",
      "tensor(19.0340)\n",
      "tensor(0.3646)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.747337\n",
      "Epoch 6519\n",
      "-------------------------------\n",
      "tensor(31.9781)\n",
      "tensor(10.0363)\n",
      "tensor(3.4326)\n",
      "tensor(0.1967)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.721012\n",
      "Epoch 6520\n",
      "-------------------------------\n",
      "tensor(47.4766)\n",
      "tensor(15.3697)\n",
      "tensor(13.1811)\n",
      "tensor(0.4435)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.720047\n",
      "Epoch 6521\n",
      "-------------------------------\n",
      "tensor(47.6736)\n",
      "tensor(15.2362)\n",
      "tensor(13.2332)\n",
      "tensor(0.4245)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.697083\n",
      "Epoch 6522\n",
      "-------------------------------\n",
      "tensor(30.5730)\n",
      "tensor(9.6016)\n",
      "tensor(7.8580)\n",
      "tensor(0.2705)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.682949\n",
      "Epoch 6523\n",
      "-------------------------------\n",
      "tensor(16.3008)\n",
      "tensor(4.9945)\n",
      "tensor(2.5855)\n",
      "tensor(0.0064)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.673042\n",
      "Epoch 6524\n",
      "-------------------------------\n",
      "tensor(28.6485)\n",
      "tensor(10.0197)\n",
      "tensor(13.2494)\n",
      "tensor(0.3792)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.688290\n",
      "Epoch 6525\n",
      "-------------------------------\n",
      "tensor(40.9822)\n",
      "tensor(27.4780)\n",
      "tensor(14.8677)\n",
      "tensor(0.5088)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.722065\n",
      "Epoch 6526\n",
      "-------------------------------\n",
      "tensor(35.2155)\n",
      "tensor(10.6736)\n",
      "tensor(3.0986)\n",
      "tensor(0.0055)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.692089\n",
      "Epoch 6527\n",
      "-------------------------------\n",
      "tensor(33.2099)\n",
      "tensor(18.0874)\n",
      "tensor(17.3977)\n",
      "tensor(0.7862)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.726650\n",
      "Epoch 6528\n",
      "-------------------------------\n",
      "tensor(26.8484)\n",
      "tensor(12.6753)\n",
      "tensor(3.9781)\n",
      "tensor(0.3788)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.761642\n",
      "Epoch 6529\n",
      "-------------------------------\n",
      "tensor(28.0577)\n",
      "tensor(7.9317)\n",
      "tensor(10.7638)\n",
      "tensor(0.3142)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.752453\n",
      "Epoch 6530\n",
      "-------------------------------\n",
      "tensor(49.8578)\n",
      "tensor(17.0988)\n",
      "tensor(18.1051)\n",
      "tensor(0.1781)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.765594\n",
      "Epoch 6531\n",
      "-------------------------------\n",
      "tensor(63.3427)\n",
      "tensor(14.4053)\n",
      "tensor(36.4557)\n",
      "tensor(0.5266)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.833454\n",
      "Epoch 6532\n",
      "-------------------------------\n",
      "tensor(73.0770)\n",
      "tensor(32.7636)\n",
      "tensor(56.7463)\n",
      "tensor(1.8693)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.839447\n",
      "Epoch 6533\n",
      "-------------------------------\n",
      "tensor(85.7710)\n",
      "tensor(28.8775)\n",
      "tensor(62.2038)\n",
      "tensor(1.7174)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.912598\n",
      "Epoch 6534\n",
      "-------------------------------\n",
      "tensor(75.2711)\n",
      "tensor(21.7550)\n",
      "tensor(47.8212)\n",
      "tensor(0.9619)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.844543\n",
      "Epoch 6535\n",
      "-------------------------------\n",
      "tensor(34.4891)\n",
      "tensor(13.1050)\n",
      "tensor(12.6104)\n",
      "tensor(0.2536)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.760109\n",
      "Epoch 6536\n",
      "-------------------------------\n",
      "tensor(49.2570)\n",
      "tensor(16.7391)\n",
      "tensor(28.2125)\n",
      "tensor(0.4985)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.777306\n",
      "Epoch 6537\n",
      "-------------------------------\n",
      "tensor(32.4018)\n",
      "tensor(10.0371)\n",
      "tensor(17.6626)\n",
      "tensor(0.5330)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.736073\n",
      "Epoch 6538\n",
      "-------------------------------\n",
      "tensor(43.4836)\n",
      "tensor(12.5962)\n",
      "tensor(21.6178)\n",
      "tensor(0.4429)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.753334\n",
      "Epoch 6539\n",
      "-------------------------------\n",
      "tensor(29.6302)\n",
      "tensor(9.4960)\n",
      "tensor(3.0740)\n",
      "tensor(0.1442)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.715225\n",
      "Epoch 6540\n",
      "-------------------------------\n",
      "tensor(32.4637)\n",
      "tensor(11.2830)\n",
      "tensor(12.4664)\n",
      "tensor(0.4479)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.691742\n",
      "Epoch 6541\n",
      "-------------------------------\n",
      "tensor(25.1583)\n",
      "tensor(9.0977)\n",
      "tensor(14.7917)\n",
      "tensor(0.4591)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.686531\n",
      "Epoch 6542\n",
      "-------------------------------\n",
      "tensor(21.9996)\n",
      "tensor(7.2147)\n",
      "tensor(10.8775)\n",
      "tensor(0.3127)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.678619\n",
      "Epoch 6543\n",
      "-------------------------------\n",
      "tensor(15.5185)\n",
      "tensor(4.9071)\n",
      "tensor(1.6202)\n",
      "tensor(0.0133)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.666515\n",
      "Epoch 6544\n",
      "-------------------------------\n",
      "tensor(28.0179)\n",
      "tensor(10.5659)\n",
      "tensor(13.7502)\n",
      "tensor(0.4240)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.664989\n",
      "Epoch 6545\n",
      "-------------------------------\n",
      "tensor(38.4605)\n",
      "tensor(14.9425)\n",
      "tensor(20.7739)\n",
      "tensor(0.6549)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.662155\n",
      "Epoch 6546\n",
      "-------------------------------\n",
      "tensor(51.0501)\n",
      "tensor(23.2666)\n",
      "tensor(1.8397)\n",
      "tensor(0.1192)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.661530\n",
      "Epoch 6547\n",
      "-------------------------------\n",
      "tensor(71.0179)\n",
      "tensor(18.6566)\n",
      "tensor(37.5863)\n",
      "tensor(0.4958)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.808823\n",
      "Epoch 6548\n",
      "-------------------------------\n",
      "tensor(34.7436)\n",
      "tensor(19.4858)\n",
      "tensor(20.6609)\n",
      "tensor(0.9695)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.740585\n",
      "Epoch 6549\n",
      "-------------------------------\n",
      "tensor(64.0320)\n",
      "tensor(17.4492)\n",
      "tensor(27.0793)\n",
      "tensor(0.5932)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.740429\n",
      "Epoch 6550\n",
      "-------------------------------\n",
      "tensor(100.3988)\n",
      "tensor(31.8571)\n",
      "tensor(73.0768)\n",
      "tensor(1.9082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.900593\n",
      "Epoch 6551\n",
      "-------------------------------\n",
      "tensor(125.8011)\n",
      "tensor(47.2966)\n",
      "tensor(98.3219)\n",
      "tensor(2.6470)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.064770\n",
      "Epoch 6552\n",
      "-------------------------------\n",
      "tensor(141.1579)\n",
      "tensor(42.9891)\n",
      "tensor(102.7980)\n",
      "tensor(2.5127)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.134159\n",
      "Epoch 6553\n",
      "-------------------------------\n",
      "tensor(136.6950)\n",
      "tensor(45.8000)\n",
      "tensor(99.0998)\n",
      "tensor(2.4141)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.147587\n",
      "Epoch 6554\n",
      "-------------------------------\n",
      "tensor(95.4097)\n",
      "tensor(31.5037)\n",
      "tensor(68.2476)\n",
      "tensor(1.6958)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.909439\n",
      "Epoch 6555\n",
      "-------------------------------\n",
      "tensor(31.4051)\n",
      "tensor(9.4691)\n",
      "tensor(5.1577)\n",
      "tensor(0.2029)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.739960\n",
      "Epoch 6556\n",
      "-------------------------------\n",
      "tensor(74.3754)\n",
      "tensor(22.9460)\n",
      "tensor(52.5756)\n",
      "tensor(1.3746)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.853909\n",
      "Epoch 6557\n",
      "-------------------------------\n",
      "tensor(30.7812)\n",
      "tensor(13.5944)\n",
      "tensor(17.0312)\n",
      "tensor(0.6288)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.740387\n",
      "Epoch 6558\n",
      "-------------------------------\n",
      "tensor(50.7277)\n",
      "tensor(21.1663)\n",
      "tensor(39.1122)\n",
      "tensor(1.2343)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.764244\n",
      "Epoch 6559\n",
      "-------------------------------\n",
      "tensor(21.2070)\n",
      "tensor(9.0005)\n",
      "tensor(11.8789)\n",
      "tensor(0.4591)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.700272\n",
      "Epoch 6560\n",
      "-------------------------------\n",
      "tensor(21.5096)\n",
      "tensor(7.6156)\n",
      "tensor(13.1246)\n",
      "tensor(0.2519)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.689510\n",
      "Epoch 6561\n",
      "-------------------------------\n",
      "tensor(30.3106)\n",
      "tensor(13.1251)\n",
      "tensor(21.9792)\n",
      "tensor(0.5456)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.692375\n",
      "Epoch 6562\n",
      "-------------------------------\n",
      "tensor(28.1686)\n",
      "tensor(13.2170)\n",
      "tensor(20.9083)\n",
      "tensor(0.5666)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.680904\n",
      "Epoch 6563\n",
      "-------------------------------\n",
      "tensor(20.9174)\n",
      "tensor(10.9878)\n",
      "tensor(10.9984)\n",
      "tensor(0.3566)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.656094\n",
      "Epoch 6564\n",
      "-------------------------------\n",
      "tensor(28.6870)\n",
      "tensor(7.4098)\n",
      "tensor(9.3068)\n",
      "tensor(0.1227)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.639778\n",
      "Epoch 6565\n",
      "-------------------------------\n",
      "tensor(54.3296)\n",
      "tensor(15.0425)\n",
      "tensor(26.2319)\n",
      "tensor(0.5539)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.676403\n",
      "Epoch 6566\n",
      "-------------------------------\n",
      "tensor(44.2501)\n",
      "tensor(24.1456)\n",
      "tensor(8.6315)\n",
      "tensor(0.0720)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.642673\n",
      "Epoch 6567\n",
      "-------------------------------\n",
      "tensor(68.1757)\n",
      "tensor(24.2080)\n",
      "tensor(20.2956)\n",
      "tensor(0.9031)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.677296\n",
      "Epoch 6568\n",
      "-------------------------------\n",
      "tensor(53.3912)\n",
      "tensor(16.2062)\n",
      "tensor(17.3248)\n",
      "tensor(0.4806)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.703388\n",
      "Epoch 6569\n",
      "-------------------------------\n",
      "tensor(27.8554)\n",
      "tensor(9.2627)\n",
      "tensor(3.2400)\n",
      "tensor(0.2545)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.647217\n",
      "Epoch 6570\n",
      "-------------------------------\n",
      "tensor(45.0024)\n",
      "tensor(12.6499)\n",
      "tensor(24.1677)\n",
      "tensor(0.1431)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.722763\n",
      "Epoch 6571\n",
      "-------------------------------\n",
      "tensor(49.3184)\n",
      "tensor(22.2392)\n",
      "tensor(39.3868)\n",
      "tensor(1.3686)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.788494\n",
      "Epoch 6572\n",
      "-------------------------------\n",
      "tensor(58.5566)\n",
      "tensor(25.4377)\n",
      "tensor(49.8769)\n",
      "tensor(1.7518)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.831818\n",
      "Epoch 6573\n",
      "-------------------------------\n",
      "tensor(67.7552)\n",
      "tensor(19.7030)\n",
      "tensor(43.1328)\n",
      "tensor(0.8832)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.801163\n",
      "Epoch 6574\n",
      "-------------------------------\n",
      "tensor(48.5452)\n",
      "tensor(14.6482)\n",
      "tensor(26.8708)\n",
      "tensor(0.3276)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.757618\n",
      "Epoch 6575\n",
      "-------------------------------\n",
      "tensor(22.8399)\n",
      "tensor(10.1477)\n",
      "tensor(4.5164)\n",
      "tensor(0.2417)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.696556\n",
      "Epoch 6576\n",
      "-------------------------------\n",
      "tensor(39.7334)\n",
      "tensor(10.4996)\n",
      "tensor(20.1193)\n",
      "tensor(0.3704)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.682552\n",
      "Epoch 6577\n",
      "-------------------------------\n",
      "tensor(34.0572)\n",
      "tensor(12.8682)\n",
      "tensor(9.5756)\n",
      "tensor(0.5076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.639961\n",
      "Epoch 6578\n",
      "-------------------------------\n",
      "tensor(33.3687)\n",
      "tensor(12.9312)\n",
      "tensor(16.0519)\n",
      "tensor(0.6132)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.631763\n",
      "Epoch 6579\n",
      "-------------------------------\n",
      "tensor(23.3685)\n",
      "tensor(7.5352)\n",
      "tensor(4.5674)\n",
      "tensor(0.1796)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.608719\n",
      "Epoch 6580\n",
      "-------------------------------\n",
      "tensor(16.6646)\n",
      "tensor(5.7148)\n",
      "tensor(5.0998)\n",
      "tensor(0.1645)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.601933\n",
      "Epoch 6581\n",
      "-------------------------------\n",
      "tensor(33.0388)\n",
      "tensor(10.7187)\n",
      "tensor(8.1518)\n",
      "tensor(0.3040)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.607735\n",
      "Epoch 6582\n",
      "-------------------------------\n",
      "tensor(32.7938)\n",
      "tensor(10.7247)\n",
      "tensor(7.7116)\n",
      "tensor(0.3104)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.602257\n",
      "Epoch 6583\n",
      "-------------------------------\n",
      "tensor(23.4209)\n",
      "tensor(7.9438)\n",
      "tensor(4.3662)\n",
      "tensor(0.2030)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.588600\n",
      "Epoch 6584\n",
      "-------------------------------\n",
      "tensor(40.7534)\n",
      "tensor(20.5463)\n",
      "tensor(4.1681)\n",
      "tensor(0.0399)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.585976\n",
      "Epoch 6585\n",
      "-------------------------------\n",
      "tensor(34.9716)\n",
      "tensor(11.9164)\n",
      "tensor(5.6658)\n",
      "tensor(0.3179)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.573967\n",
      "Epoch 6586\n",
      "-------------------------------\n",
      "tensor(37.5995)\n",
      "tensor(13.0786)\n",
      "tensor(5.2709)\n",
      "tensor(0.3934)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.591934\n",
      "Epoch 6587\n",
      "-------------------------------\n",
      "tensor(41.0169)\n",
      "tensor(15.2092)\n",
      "tensor(5.8200)\n",
      "tensor(0.2777)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.591400\n",
      "Epoch 6588\n",
      "-------------------------------\n",
      "tensor(39.5574)\n",
      "tensor(14.0481)\n",
      "tensor(4.3924)\n",
      "tensor(0.1248)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.622097\n",
      "Epoch 6589\n",
      "-------------------------------\n",
      "tensor(31.6121)\n",
      "tensor(10.5889)\n",
      "tensor(3.8036)\n",
      "tensor(0.2425)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.625648\n",
      "Epoch 6590\n",
      "-------------------------------\n",
      "tensor(31.9708)\n",
      "tensor(24.1781)\n",
      "tensor(7.0439)\n",
      "tensor(0.2350)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.643551\n",
      "Epoch 6591\n",
      "-------------------------------\n",
      "tensor(25.5225)\n",
      "tensor(5.6104)\n",
      "tensor(8.2690)\n",
      "tensor(0.3512)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.663353\n",
      "Epoch 6592\n",
      "-------------------------------\n",
      "tensor(32.3571)\n",
      "tensor(16.7765)\n",
      "tensor(26.3253)\n",
      "tensor(0.7018)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.680504\n",
      "Epoch 6593\n",
      "-------------------------------\n",
      "tensor(55.5689)\n",
      "tensor(16.1783)\n",
      "tensor(33.3170)\n",
      "tensor(1.1491)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.690773\n",
      "Epoch 6594\n",
      "-------------------------------\n",
      "tensor(40.2015)\n",
      "tensor(19.9276)\n",
      "tensor(29.1858)\n",
      "tensor(0.9246)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.674820\n",
      "Epoch 6595\n",
      "-------------------------------\n",
      "tensor(38.5441)\n",
      "tensor(12.6646)\n",
      "tensor(6.8318)\n",
      "tensor(0.3625)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.642853\n",
      "Epoch 6596\n",
      "-------------------------------\n",
      "tensor(42.5373)\n",
      "tensor(13.1672)\n",
      "tensor(17.6031)\n",
      "tensor(0.2480)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.635445\n",
      "Epoch 6597\n",
      "-------------------------------\n",
      "tensor(37.6163)\n",
      "tensor(11.1405)\n",
      "tensor(6.9260)\n",
      "tensor(0.0128)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.596539\n",
      "Epoch 6598\n",
      "-------------------------------\n",
      "tensor(25.4324)\n",
      "tensor(9.4616)\n",
      "tensor(9.5717)\n",
      "tensor(0.0464)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.602528\n",
      "Epoch 6599\n",
      "-------------------------------\n",
      "tensor(18.3804)\n",
      "tensor(10.5296)\n",
      "tensor(3.8301)\n",
      "tensor(0.2680)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.590981\n",
      "Epoch 6600\n",
      "-------------------------------\n",
      "tensor(17.5472)\n",
      "tensor(11.1859)\n",
      "tensor(6.8861)\n",
      "tensor(0.3491)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.580341\n",
      "Epoch 6601\n",
      "-------------------------------\n",
      "tensor(36.5565)\n",
      "tensor(12.7389)\n",
      "tensor(7.6024)\n",
      "tensor(0.3100)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.578999\n",
      "Epoch 6602\n",
      "-------------------------------\n",
      "tensor(36.5671)\n",
      "tensor(12.0154)\n",
      "tensor(6.2976)\n",
      "tensor(0.1995)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.572720\n",
      "Epoch 6603\n",
      "-------------------------------\n",
      "tensor(16.8271)\n",
      "tensor(4.9819)\n",
      "tensor(3.3565)\n",
      "tensor(0.0048)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.562462\n",
      "Epoch 6604\n",
      "-------------------------------\n",
      "tensor(27.8260)\n",
      "tensor(9.1739)\n",
      "tensor(4.9581)\n",
      "tensor(0.2847)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.555782\n",
      "Epoch 6605\n",
      "-------------------------------\n",
      "tensor(28.3707)\n",
      "tensor(11.5659)\n",
      "tensor(10.2373)\n",
      "tensor(0.5066)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.558262\n",
      "Epoch 6606\n",
      "-------------------------------\n",
      "tensor(34.0460)\n",
      "tensor(12.3282)\n",
      "tensor(4.7940)\n",
      "tensor(0.3150)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.546402\n",
      "Epoch 6607\n",
      "-------------------------------\n",
      "tensor(43.4645)\n",
      "tensor(22.9755)\n",
      "tensor(9.5107)\n",
      "tensor(0.1717)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.552258\n",
      "Epoch 6608\n",
      "-------------------------------\n",
      "tensor(44.6075)\n",
      "tensor(11.8538)\n",
      "tensor(12.3179)\n",
      "tensor(0.0502)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.572708\n",
      "Epoch 6609\n",
      "-------------------------------\n",
      "tensor(44.5874)\n",
      "tensor(22.8764)\n",
      "tensor(27.5067)\n",
      "tensor(1.0861)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.628815\n",
      "Epoch 6610\n",
      "-------------------------------\n",
      "tensor(36.2946)\n",
      "tensor(12.2597)\n",
      "tensor(18.5980)\n",
      "tensor(0.4293)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.621117\n",
      "Epoch 6611\n",
      "-------------------------------\n",
      "tensor(44.5623)\n",
      "tensor(20.9441)\n",
      "tensor(10.1540)\n",
      "tensor(0.0795)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.620773\n",
      "Epoch 6612\n",
      "-------------------------------\n",
      "tensor(35.5978)\n",
      "tensor(10.0029)\n",
      "tensor(19.7451)\n",
      "tensor(0.3376)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.602901\n",
      "Epoch 6613\n",
      "-------------------------------\n",
      "tensor(35.1987)\n",
      "tensor(16.9091)\n",
      "tensor(30.6816)\n",
      "tensor(0.9708)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.593754\n",
      "Epoch 6614\n",
      "-------------------------------\n",
      "tensor(54.4229)\n",
      "tensor(15.2865)\n",
      "tensor(24.4714)\n",
      "tensor(0.7669)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.636711\n",
      "Epoch 6615\n",
      "-------------------------------\n",
      "tensor(39.4710)\n",
      "tensor(13.4092)\n",
      "tensor(3.9821)\n",
      "tensor(0.1691)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.541611\n",
      "Epoch 6616\n",
      "-------------------------------\n",
      "tensor(32.5651)\n",
      "tensor(16.2043)\n",
      "tensor(16.0556)\n",
      "tensor(0.7176)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.586956\n",
      "Epoch 6617\n",
      "-------------------------------\n",
      "tensor(32.5170)\n",
      "tensor(9.7378)\n",
      "tensor(5.3153)\n",
      "tensor(0.0963)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.579018\n",
      "Epoch 6618\n",
      "-------------------------------\n",
      "tensor(27.3389)\n",
      "tensor(9.6962)\n",
      "tensor(10.3522)\n",
      "tensor(0.2867)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.558704\n",
      "Epoch 6619\n",
      "-------------------------------\n",
      "tensor(33.8108)\n",
      "tensor(11.6380)\n",
      "tensor(4.5368)\n",
      "tensor(0.2681)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.560547\n",
      "Epoch 6620\n",
      "-------------------------------\n",
      "tensor(23.5975)\n",
      "tensor(8.2365)\n",
      "tensor(3.5168)\n",
      "tensor(0.1579)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.555271\n",
      "Epoch 6621\n",
      "-------------------------------\n",
      "tensor(18.4246)\n",
      "tensor(8.8019)\n",
      "tensor(4.7183)\n",
      "tensor(0.0827)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.549976\n",
      "Epoch 6622\n",
      "-------------------------------\n",
      "tensor(31.1832)\n",
      "tensor(9.5629)\n",
      "tensor(4.4084)\n",
      "tensor(0.0475)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.552349\n",
      "Epoch 6623\n",
      "-------------------------------\n",
      "tensor(31.7140)\n",
      "tensor(9.8322)\n",
      "tensor(2.3112)\n",
      "tensor(0.0296)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.546043\n",
      "Epoch 6624\n",
      "-------------------------------\n",
      "tensor(24.2619)\n",
      "tensor(7.3767)\n",
      "tensor(3.6188)\n",
      "tensor(0.0175)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.537632\n",
      "Epoch 6625\n",
      "-------------------------------\n",
      "tensor(21.1430)\n",
      "tensor(8.4786)\n",
      "tensor(6.4987)\n",
      "tensor(0.0536)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.526878\n",
      "Epoch 6626\n",
      "-------------------------------\n",
      "tensor(30.9451)\n",
      "tensor(10.3973)\n",
      "tensor(2.9183)\n",
      "tensor(0.2498)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.540211\n",
      "Epoch 6627\n",
      "-------------------------------\n",
      "tensor(50.3346)\n",
      "tensor(16.6247)\n",
      "tensor(9.5717)\n",
      "tensor(0.3086)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.539551\n",
      "Epoch 6628\n",
      "-------------------------------\n",
      "tensor(46.6474)\n",
      "tensor(24.5684)\n",
      "tensor(9.3316)\n",
      "tensor(0.1175)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.539135\n",
      "Epoch 6629\n",
      "-------------------------------\n",
      "tensor(42.0038)\n",
      "tensor(7.7093)\n",
      "tensor(18.3025)\n",
      "tensor(0.1348)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.549744\n",
      "Epoch 6630\n",
      "-------------------------------\n",
      "tensor(76.4877)\n",
      "tensor(20.0340)\n",
      "tensor(44.0385)\n",
      "tensor(1.2612)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.657806\n",
      "Epoch 6631\n",
      "-------------------------------\n",
      "tensor(97.1213)\n",
      "tensor(27.9145)\n",
      "tensor(68.1370)\n",
      "tensor(1.7292)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.765430\n",
      "Epoch 6632\n",
      "-------------------------------\n",
      "tensor(120.3716)\n",
      "tensor(45.0799)\n",
      "tensor(91.3496)\n",
      "tensor(2.2878)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.899124\n",
      "Epoch 6633\n",
      "-------------------------------\n",
      "tensor(134.8105)\n",
      "tensor(39.4559)\n",
      "tensor(96.4397)\n",
      "tensor(2.2426)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.956654\n",
      "Epoch 6634\n",
      "-------------------------------\n",
      "tensor(107.0805)\n",
      "tensor(36.4318)\n",
      "tensor(81.7555)\n",
      "tensor(2.1316)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.811485\n",
      "Epoch 6635\n",
      "-------------------------------\n",
      "tensor(47.3125)\n",
      "tensor(18.2034)\n",
      "tensor(21.7930)\n",
      "tensor(0.7979)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.605610\n",
      "Epoch 6636\n",
      "-------------------------------\n",
      "tensor(72.3966)\n",
      "tensor(27.4188)\n",
      "tensor(52.3899)\n",
      "tensor(1.3961)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.639496\n",
      "Epoch 6637\n",
      "-------------------------------\n",
      "tensor(53.1126)\n",
      "tensor(16.5454)\n",
      "tensor(28.6565)\n",
      "tensor(0.8466)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.578125\n",
      "Epoch 6638\n",
      "-------------------------------\n",
      "tensor(55.9845)\n",
      "tensor(18.4648)\n",
      "tensor(37.8079)\n",
      "tensor(1.1058)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.598110\n",
      "Epoch 6639\n",
      "-------------------------------\n",
      "tensor(17.2358)\n",
      "tensor(5.1020)\n",
      "tensor(4.2038)\n",
      "tensor(0.1470)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.537880\n",
      "Epoch 6640\n",
      "-------------------------------\n",
      "tensor(34.3530)\n",
      "tensor(13.5949)\n",
      "tensor(18.5207)\n",
      "tensor(0.5153)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.556335\n",
      "Epoch 6641\n",
      "-------------------------------\n",
      "tensor(38.2179)\n",
      "tensor(15.0763)\n",
      "tensor(23.0699)\n",
      "tensor(0.6869)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.560635\n",
      "Epoch 6642\n",
      "-------------------------------\n",
      "tensor(32.6545)\n",
      "tensor(11.7621)\n",
      "tensor(17.8578)\n",
      "tensor(0.5777)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.543732\n",
      "Epoch 6643\n",
      "-------------------------------\n",
      "tensor(23.8374)\n",
      "tensor(8.2627)\n",
      "tensor(4.6760)\n",
      "tensor(0.2252)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.518665\n",
      "Epoch 6644\n",
      "-------------------------------\n",
      "tensor(28.2602)\n",
      "tensor(10.2395)\n",
      "tensor(18.4786)\n",
      "tensor(0.3737)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.521843\n",
      "Epoch 6645\n",
      "-------------------------------\n",
      "tensor(39.4330)\n",
      "tensor(15.9678)\n",
      "tensor(30.4377)\n",
      "tensor(0.7796)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.523083\n",
      "Epoch 6646\n",
      "-------------------------------\n",
      "tensor(39.9492)\n",
      "tensor(13.0302)\n",
      "tensor(2.1271)\n",
      "tensor(0.1632)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.529175\n",
      "Epoch 6647\n",
      "-------------------------------\n",
      "tensor(71.0394)\n",
      "tensor(18.9724)\n",
      "tensor(35.8644)\n",
      "tensor(0.6070)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.609570\n",
      "Epoch 6648\n",
      "-------------------------------\n",
      "tensor(57.8040)\n",
      "tensor(24.2641)\n",
      "tensor(13.6431)\n",
      "tensor(0.7699)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.574131\n",
      "Epoch 6649\n",
      "-------------------------------\n",
      "tensor(38.9865)\n",
      "tensor(17.5420)\n",
      "tensor(13.5549)\n",
      "tensor(0.8001)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.546829\n",
      "Epoch 6650\n",
      "-------------------------------\n",
      "tensor(53.2576)\n",
      "tensor(14.3866)\n",
      "tensor(35.1466)\n",
      "tensor(0.8599)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.607555\n",
      "Epoch 6651\n",
      "-------------------------------\n",
      "tensor(80.1133)\n",
      "tensor(26.6916)\n",
      "tensor(56.6996)\n",
      "tensor(0.9696)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.734486\n",
      "Epoch 6652\n",
      "-------------------------------\n",
      "tensor(97.7477)\n",
      "tensor(25.6618)\n",
      "tensor(66.4239)\n",
      "tensor(1.5747)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.788246\n",
      "Epoch 6653\n",
      "-------------------------------\n",
      "tensor(89.1682)\n",
      "tensor(40.9154)\n",
      "tensor(76.4432)\n",
      "tensor(2.4979)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.783100\n",
      "Epoch 6654\n",
      "-------------------------------\n",
      "tensor(73.2340)\n",
      "tensor(25.1787)\n",
      "tensor(52.7584)\n",
      "tensor(1.3119)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.679150\n",
      "Epoch 6655\n",
      "-------------------------------\n",
      "tensor(35.7779)\n",
      "tensor(11.8518)\n",
      "tensor(9.8427)\n",
      "tensor(0.3512)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.563663\n",
      "Epoch 6656\n",
      "-------------------------------\n",
      "tensor(63.9006)\n",
      "tensor(16.2498)\n",
      "tensor(28.2611)\n",
      "tensor(0.3971)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.634171\n",
      "Epoch 6657\n",
      "-------------------------------\n",
      "tensor(45.6877)\n",
      "tensor(14.4756)\n",
      "tensor(20.9359)\n",
      "tensor(0.3076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.589077\n",
      "Epoch 6658\n",
      "-------------------------------\n",
      "tensor(47.0665)\n",
      "tensor(12.3805)\n",
      "tensor(24.0012)\n",
      "tensor(0.1686)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.537670\n",
      "Epoch 6659\n",
      "-------------------------------\n",
      "tensor(25.2091)\n",
      "tensor(7.1711)\n",
      "tensor(5.5786)\n",
      "tensor(0.3045)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.506947\n",
      "Epoch 6660\n",
      "-------------------------------\n",
      "tensor(22.2646)\n",
      "tensor(7.8236)\n",
      "tensor(14.6312)\n",
      "tensor(0.4765)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.500065\n",
      "Epoch 6661\n",
      "-------------------------------\n",
      "tensor(26.2004)\n",
      "tensor(10.5541)\n",
      "tensor(16.8445)\n",
      "tensor(0.3830)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.498028\n",
      "Epoch 6662\n",
      "-------------------------------\n",
      "tensor(29.9792)\n",
      "tensor(10.8938)\n",
      "tensor(12.9360)\n",
      "tensor(0.1631)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.492222\n",
      "Epoch 6663\n",
      "-------------------------------\n",
      "tensor(24.1134)\n",
      "tensor(8.1771)\n",
      "tensor(5.1221)\n",
      "tensor(0.1903)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.479683\n",
      "Epoch 6664\n",
      "-------------------------------\n",
      "tensor(21.8017)\n",
      "tensor(10.1205)\n",
      "tensor(14.6199)\n",
      "tensor(0.6307)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.470074\n",
      "Epoch 6665\n",
      "-------------------------------\n",
      "tensor(49.4701)\n",
      "tensor(15.3460)\n",
      "tensor(21.5289)\n",
      "tensor(0.7345)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.501148\n",
      "Epoch 6666\n",
      "-------------------------------\n",
      "tensor(32.4158)\n",
      "tensor(10.4148)\n",
      "tensor(1.9914)\n",
      "tensor(0.1595)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.496807\n",
      "Epoch 6667\n",
      "-------------------------------\n",
      "tensor(49.1873)\n",
      "tensor(23.4181)\n",
      "tensor(27.9625)\n",
      "tensor(1.2212)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.560650\n",
      "Epoch 6668\n",
      "-------------------------------\n",
      "tensor(50.1114)\n",
      "tensor(25.3528)\n",
      "tensor(10.0289)\n",
      "tensor(0.2549)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.517506\n",
      "Epoch 6669\n",
      "-------------------------------\n",
      "tensor(60.5707)\n",
      "tensor(18.1168)\n",
      "tensor(37.3514)\n",
      "tensor(0.6973)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.547054\n",
      "Epoch 6670\n",
      "-------------------------------\n",
      "tensor(108.3628)\n",
      "tensor(21.8448)\n",
      "tensor(62.9346)\n",
      "tensor(1.2966)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.703140\n",
      "Epoch 6671\n",
      "-------------------------------\n",
      "tensor(125.0079)\n",
      "tensor(35.6893)\n",
      "tensor(88.6540)\n",
      "tensor(2.0972)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.856331\n",
      "Epoch 6672\n",
      "-------------------------------\n",
      "tensor(134.5871)\n",
      "tensor(56.3340)\n",
      "tensor(112.6430)\n",
      "tensor(3.1957)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.002129\n",
      "Epoch 6673\n",
      "-------------------------------\n",
      "tensor(146.5319)\n",
      "tensor(44.1275)\n",
      "tensor(107.0392)\n",
      "tensor(2.7598)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.004700\n",
      "Epoch 6674\n",
      "-------------------------------\n",
      "tensor(111.9647)\n",
      "tensor(37.8479)\n",
      "tensor(80.5796)\n",
      "tensor(1.8453)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.795006\n",
      "Epoch 6675\n",
      "-------------------------------\n",
      "tensor(40.4652)\n",
      "tensor(12.4216)\n",
      "tensor(13.6506)\n",
      "tensor(0.2755)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.538834\n",
      "Epoch 6676\n",
      "-------------------------------\n",
      "tensor(77.1172)\n",
      "tensor(24.4430)\n",
      "tensor(51.9281)\n",
      "tensor(1.0593)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.600822\n",
      "Epoch 6677\n",
      "-------------------------------\n",
      "tensor(43.5158)\n",
      "tensor(14.4016)\n",
      "tensor(25.6571)\n",
      "tensor(0.7719)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.513145\n",
      "Epoch 6678\n",
      "-------------------------------\n",
      "tensor(62.9218)\n",
      "tensor(17.9593)\n",
      "tensor(37.6804)\n",
      "tensor(0.8496)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.542866\n",
      "Epoch 6679\n",
      "-------------------------------\n",
      "tensor(31.6258)\n",
      "tensor(10.1875)\n",
      "tensor(4.8843)\n",
      "tensor(0.1258)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.478249\n",
      "Epoch 6680\n",
      "-------------------------------\n",
      "tensor(27.5731)\n",
      "tensor(14.1207)\n",
      "tensor(20.1343)\n",
      "tensor(0.7264)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.481533\n",
      "Epoch 6681\n",
      "-------------------------------\n",
      "tensor(40.3209)\n",
      "tensor(16.5662)\n",
      "tensor(25.1120)\n",
      "tensor(0.8258)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.485867\n",
      "Epoch 6682\n",
      "-------------------------------\n",
      "tensor(35.6671)\n",
      "tensor(13.7498)\n",
      "tensor(19.9098)\n",
      "tensor(0.6392)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.472347\n",
      "Epoch 6683\n",
      "-------------------------------\n",
      "tensor(30.1848)\n",
      "tensor(9.7775)\n",
      "tensor(5.5893)\n",
      "tensor(0.1857)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.452808\n",
      "Epoch 6684\n",
      "-------------------------------\n",
      "tensor(32.1033)\n",
      "tensor(11.9860)\n",
      "tensor(17.8865)\n",
      "tensor(0.5455)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.454868\n",
      "Epoch 6685\n",
      "-------------------------------\n",
      "tensor(41.7244)\n",
      "tensor(20.1236)\n",
      "tensor(31.9997)\n",
      "tensor(1.0460)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.473457\n",
      "Epoch 6686\n",
      "-------------------------------\n",
      "tensor(32.0683)\n",
      "tensor(12.2448)\n",
      "tensor(5.0399)\n",
      "tensor(0.3630)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.454716\n",
      "Epoch 6687\n",
      "-------------------------------\n",
      "tensor(64.9428)\n",
      "tensor(16.5614)\n",
      "tensor(34.2291)\n",
      "tensor(0.7414)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.500668\n",
      "Epoch 6688\n",
      "-------------------------------\n",
      "tensor(50.0632)\n",
      "tensor(16.6057)\n",
      "tensor(9.9432)\n",
      "tensor(0.3279)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.479477\n",
      "Epoch 6689\n",
      "-------------------------------\n",
      "tensor(52.1427)\n",
      "tensor(17.0043)\n",
      "tensor(24.4697)\n",
      "tensor(0.9285)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.557320\n",
      "Epoch 6690\n",
      "-------------------------------\n",
      "tensor(68.3462)\n",
      "tensor(40.1410)\n",
      "tensor(54.2706)\n",
      "tensor(1.1389)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.656296\n",
      "Epoch 6691\n",
      "-------------------------------\n",
      "tensor(69.0296)\n",
      "tensor(29.9122)\n",
      "tensor(55.5689)\n",
      "tensor(1.6101)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.618649\n",
      "Epoch 6692\n",
      "-------------------------------\n",
      "tensor(65.6857)\n",
      "tensor(15.9167)\n",
      "tensor(42.8846)\n",
      "tensor(1.1141)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.600151\n",
      "Epoch 6693\n",
      "-------------------------------\n",
      "tensor(59.1247)\n",
      "tensor(23.7246)\n",
      "tensor(35.9131)\n",
      "tensor(0.8865)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.584240\n",
      "Epoch 6694\n",
      "-------------------------------\n",
      "tensor(36.3970)\n",
      "tensor(6.3564)\n",
      "tensor(17.9708)\n",
      "tensor(0.1192)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.522701\n",
      "Epoch 6695\n",
      "-------------------------------\n",
      "tensor(17.6969)\n",
      "tensor(5.1172)\n",
      "tensor(2.8782)\n",
      "tensor(0.1155)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.490852\n",
      "Epoch 6696\n",
      "-------------------------------\n",
      "tensor(42.1479)\n",
      "tensor(10.9814)\n",
      "tensor(14.9280)\n",
      "tensor(0.0775)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.519043\n",
      "Epoch 6697\n",
      "-------------------------------\n",
      "tensor(38.7341)\n",
      "tensor(14.9898)\n",
      "tensor(10.5591)\n",
      "tensor(0.5305)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.501144\n",
      "Epoch 6698\n",
      "-------------------------------\n",
      "tensor(38.6083)\n",
      "tensor(14.2188)\n",
      "tensor(15.7570)\n",
      "tensor(0.4363)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.477654\n",
      "Epoch 6699\n",
      "-------------------------------\n",
      "tensor(39.2573)\n",
      "tensor(12.5043)\n",
      "tensor(5.9352)\n",
      "tensor(0.0198)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.445946\n",
      "Epoch 6700\n",
      "-------------------------------\n",
      "tensor(30.7304)\n",
      "tensor(11.8848)\n",
      "tensor(6.0937)\n",
      "tensor(0.2998)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.447903\n",
      "Epoch 6701\n",
      "-------------------------------\n",
      "tensor(31.2011)\n",
      "tensor(12.2071)\n",
      "tensor(8.2078)\n",
      "tensor(0.3683)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.448345\n",
      "Epoch 6702\n",
      "-------------------------------\n",
      "tensor(18.5361)\n",
      "tensor(8.9174)\n",
      "tensor(7.5643)\n",
      "tensor(0.3169)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.437927\n",
      "Epoch 6703\n",
      "-------------------------------\n",
      "tensor(24.0416)\n",
      "tensor(7.4707)\n",
      "tensor(4.1616)\n",
      "tensor(0.1417)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.434494\n",
      "Epoch 6704\n",
      "-------------------------------\n",
      "tensor(15.3853)\n",
      "tensor(6.0847)\n",
      "tensor(3.5185)\n",
      "tensor(0.1836)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.425560\n",
      "Epoch 6705\n",
      "-------------------------------\n",
      "tensor(25.3231)\n",
      "tensor(9.9342)\n",
      "tensor(9.9008)\n",
      "tensor(0.5088)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.416122\n",
      "Epoch 6706\n",
      "-------------------------------\n",
      "tensor(34.2644)\n",
      "tensor(10.7039)\n",
      "tensor(7.6021)\n",
      "tensor(0.3918)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.428837\n",
      "Epoch 6707\n",
      "-------------------------------\n",
      "tensor(53.7296)\n",
      "tensor(17.9656)\n",
      "tensor(8.6189)\n",
      "tensor(0.2968)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.441826\n",
      "Epoch 6708\n",
      "-------------------------------\n",
      "tensor(38.3119)\n",
      "tensor(14.0454)\n",
      "tensor(10.7179)\n",
      "tensor(0.6616)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.448307\n",
      "Epoch 6709\n",
      "-------------------------------\n",
      "tensor(39.2715)\n",
      "tensor(19.9839)\n",
      "tensor(7.6616)\n",
      "tensor(0.5354)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.467834\n",
      "Epoch 6710\n",
      "-------------------------------\n",
      "tensor(33.3672)\n",
      "tensor(25.3268)\n",
      "tensor(12.0091)\n",
      "tensor(0.5367)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.461151\n",
      "Epoch 6711\n",
      "-------------------------------\n",
      "tensor(23.7796)\n",
      "tensor(7.8070)\n",
      "tensor(4.5839)\n",
      "tensor(0.0514)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.463711\n",
      "Epoch 6712\n",
      "-------------------------------\n",
      "tensor(41.5688)\n",
      "tensor(23.2994)\n",
      "tensor(22.1182)\n",
      "tensor(0.9144)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.484806\n",
      "Epoch 6713\n",
      "-------------------------------\n",
      "tensor(53.0360)\n",
      "tensor(12.2169)\n",
      "tensor(27.4060)\n",
      "tensor(0.3542)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.516644\n",
      "Epoch 6714\n",
      "-------------------------------\n",
      "tensor(47.2837)\n",
      "tensor(15.5636)\n",
      "tensor(34.7764)\n",
      "tensor(0.7227)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.515995\n",
      "Epoch 6715\n",
      "-------------------------------\n",
      "tensor(29.6329)\n",
      "tensor(16.2923)\n",
      "tensor(19.5213)\n",
      "tensor(0.8684)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.479507\n",
      "Epoch 6716\n",
      "-------------------------------\n",
      "tensor(36.0400)\n",
      "tensor(14.6737)\n",
      "tensor(15.9586)\n",
      "tensor(0.5364)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.513256\n",
      "Epoch 6717\n",
      "-------------------------------\n",
      "tensor(41.7201)\n",
      "tensor(12.8466)\n",
      "tensor(16.1384)\n",
      "tensor(0.5549)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.491367\n",
      "Epoch 6718\n",
      "-------------------------------\n",
      "tensor(35.8237)\n",
      "tensor(11.5905)\n",
      "tensor(12.7169)\n",
      "tensor(0.5276)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.423714\n",
      "Epoch 6719\n",
      "-------------------------------\n",
      "tensor(35.2184)\n",
      "tensor(10.9325)\n",
      "tensor(5.2975)\n",
      "tensor(0.0427)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.418396\n",
      "Epoch 6720\n",
      "-------------------------------\n",
      "tensor(42.0902)\n",
      "tensor(13.3692)\n",
      "tensor(10.9073)\n",
      "tensor(0.2257)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.418209\n",
      "Epoch 6721\n",
      "-------------------------------\n",
      "tensor(29.9470)\n",
      "tensor(9.8173)\n",
      "tensor(10.6288)\n",
      "tensor(0.2716)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.414413\n",
      "Epoch 6722\n",
      "-------------------------------\n",
      "tensor(29.0948)\n",
      "tensor(9.5242)\n",
      "tensor(6.6171)\n",
      "tensor(0.2073)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.408897\n",
      "Epoch 6723\n",
      "-------------------------------\n",
      "tensor(23.6651)\n",
      "tensor(7.8026)\n",
      "tensor(1.9844)\n",
      "tensor(0.0545)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.406128\n",
      "Epoch 6724\n",
      "-------------------------------\n",
      "tensor(19.4558)\n",
      "tensor(6.7989)\n",
      "tensor(10.7929)\n",
      "tensor(0.1686)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.406338\n",
      "Epoch 6725\n",
      "-------------------------------\n",
      "tensor(32.3084)\n",
      "tensor(10.8161)\n",
      "tensor(14.0364)\n",
      "tensor(0.2623)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.401009\n",
      "Epoch 6726\n",
      "-------------------------------\n",
      "tensor(25.6509)\n",
      "tensor(7.7206)\n",
      "tensor(3.4701)\n",
      "tensor(0.0461)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.394867\n",
      "Epoch 6727\n",
      "-------------------------------\n",
      "tensor(48.0013)\n",
      "tensor(13.2359)\n",
      "tensor(14.8601)\n",
      "tensor(0.1873)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.408722\n",
      "Epoch 6728\n",
      "-------------------------------\n",
      "tensor(18.7773)\n",
      "tensor(12.3941)\n",
      "tensor(10.8594)\n",
      "tensor(0.5999)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.372044\n",
      "Epoch 6729\n",
      "-------------------------------\n",
      "tensor(51.3828)\n",
      "tensor(21.0161)\n",
      "tensor(11.9777)\n",
      "tensor(0.3520)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.494698\n",
      "Epoch 6730\n",
      "-------------------------------\n",
      "tensor(68.6536)\n",
      "tensor(20.6875)\n",
      "tensor(42.1522)\n",
      "tensor(0.9695)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.496155\n",
      "Epoch 6731\n",
      "-------------------------------\n",
      "tensor(88.5808)\n",
      "tensor(22.3301)\n",
      "tensor(57.5544)\n",
      "tensor(1.7248)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.511753\n",
      "Epoch 6732\n",
      "-------------------------------\n",
      "tensor(113.1202)\n",
      "tensor(29.5672)\n",
      "tensor(75.9382)\n",
      "tensor(1.6322)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.691292\n",
      "Epoch 6733\n",
      "-------------------------------\n",
      "tensor(114.8037)\n",
      "tensor(44.7529)\n",
      "tensor(89.4083)\n",
      "tensor(2.3577)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.746796\n",
      "Epoch 6734\n",
      "-------------------------------\n",
      "tensor(98.7844)\n",
      "tensor(30.8747)\n",
      "tensor(71.2171)\n",
      "tensor(1.8775)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.642269\n",
      "Epoch 6735\n",
      "-------------------------------\n",
      "tensor(27.5416)\n",
      "tensor(9.7895)\n",
      "tensor(18.9927)\n",
      "tensor(0.4648)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.462864\n",
      "Epoch 6736\n",
      "-------------------------------\n",
      "tensor(59.4257)\n",
      "tensor(19.8171)\n",
      "tensor(41.3744)\n",
      "tensor(1.0344)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.534229\n",
      "Epoch 6737\n",
      "-------------------------------\n",
      "tensor(42.0154)\n",
      "tensor(15.7989)\n",
      "tensor(26.9439)\n",
      "tensor(0.7394)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.474121\n",
      "Epoch 6738\n",
      "-------------------------------\n",
      "tensor(51.6171)\n",
      "tensor(18.9119)\n",
      "tensor(32.2891)\n",
      "tensor(0.9000)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.485947\n",
      "Epoch 6739\n",
      "-------------------------------\n",
      "tensor(38.2631)\n",
      "tensor(12.1308)\n",
      "tensor(2.9396)\n",
      "tensor(0.1086)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.429333\n",
      "Epoch 6740\n",
      "-------------------------------\n",
      "tensor(35.6298)\n",
      "tensor(11.1445)\n",
      "tensor(15.7912)\n",
      "tensor(0.4041)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.421223\n",
      "Epoch 6741\n",
      "-------------------------------\n",
      "tensor(27.8540)\n",
      "tensor(9.3664)\n",
      "tensor(19.2426)\n",
      "tensor(0.5160)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.417572\n",
      "Epoch 6742\n",
      "-------------------------------\n",
      "tensor(23.9320)\n",
      "tensor(10.5291)\n",
      "tensor(14.9625)\n",
      "tensor(0.4054)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.408764\n",
      "Epoch 6743\n",
      "-------------------------------\n",
      "tensor(16.7343)\n",
      "tensor(8.6529)\n",
      "tensor(4.0575)\n",
      "tensor(0.1036)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.393707\n",
      "Epoch 6744\n",
      "-------------------------------\n",
      "tensor(38.0182)\n",
      "tensor(11.8108)\n",
      "tensor(13.6170)\n",
      "tensor(0.3868)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.418015\n",
      "Epoch 6745\n",
      "-------------------------------\n",
      "tensor(45.7114)\n",
      "tensor(13.9674)\n",
      "tensor(23.4756)\n",
      "tensor(0.6496)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.433380\n",
      "Epoch 6746\n",
      "-------------------------------\n",
      "tensor(38.8760)\n",
      "tensor(12.0510)\n",
      "tensor(1.5463)\n",
      "tensor(0.0264)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.366215\n",
      "Epoch 6747\n",
      "-------------------------------\n",
      "tensor(48.4890)\n",
      "tensor(19.4284)\n",
      "tensor(26.8064)\n",
      "tensor(0.8969)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.401566\n",
      "Epoch 6748\n",
      "-------------------------------\n",
      "tensor(36.4979)\n",
      "tensor(9.7919)\n",
      "tensor(12.0733)\n",
      "tensor(0.0003)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.433056\n",
      "Epoch 6749\n",
      "-------------------------------\n",
      "tensor(42.8037)\n",
      "tensor(27.0353)\n",
      "tensor(13.9324)\n",
      "tensor(0.3589)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.468842\n",
      "Epoch 6750\n",
      "-------------------------------\n",
      "tensor(38.6358)\n",
      "tensor(13.8419)\n",
      "tensor(12.5842)\n",
      "tensor(0.5171)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.439194\n",
      "Epoch 6751\n",
      "-------------------------------\n",
      "tensor(28.7399)\n",
      "tensor(10.2908)\n",
      "tensor(7.6302)\n",
      "tensor(0.0102)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.428684\n",
      "Epoch 6752\n",
      "-------------------------------\n",
      "tensor(23.1989)\n",
      "tensor(7.6877)\n",
      "tensor(8.2067)\n",
      "tensor(0.0593)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.439404\n",
      "Epoch 6753\n",
      "-------------------------------\n",
      "tensor(16.3405)\n",
      "tensor(6.4428)\n",
      "tensor(5.4868)\n",
      "tensor(0.2590)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.442516\n",
      "Epoch 6754\n",
      "-------------------------------\n",
      "tensor(22.5990)\n",
      "tensor(7.2727)\n",
      "tensor(3.3287)\n",
      "tensor(0.1433)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.441631\n",
      "Epoch 6755\n",
      "-------------------------------\n",
      "tensor(28.9132)\n",
      "tensor(9.2730)\n",
      "tensor(3.3851)\n",
      "tensor(0.2198)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.412094\n",
      "Epoch 6756\n",
      "-------------------------------\n",
      "tensor(19.2998)\n",
      "tensor(5.6318)\n",
      "tensor(3.1732)\n",
      "tensor(0.0081)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.375668\n",
      "Epoch 6757\n",
      "-------------------------------\n",
      "tensor(34.5176)\n",
      "tensor(12.3106)\n",
      "tensor(2.2011)\n",
      "tensor(0.0712)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.384762\n",
      "Epoch 6758\n",
      "-------------------------------\n",
      "tensor(45.6304)\n",
      "tensor(15.1703)\n",
      "tensor(1.9209)\n",
      "tensor(0.0690)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.414562\n",
      "Epoch 6759\n",
      "-------------------------------\n",
      "tensor(47.8456)\n",
      "tensor(14.9040)\n",
      "tensor(2.6591)\n",
      "tensor(0.0063)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.381111\n",
      "Epoch 6760\n",
      "-------------------------------\n",
      "tensor(34.9869)\n",
      "tensor(10.7784)\n",
      "tensor(4.8879)\n",
      "tensor(0.0643)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.357399\n",
      "Epoch 6761\n",
      "-------------------------------\n",
      "tensor(19.4229)\n",
      "tensor(4.5784)\n",
      "tensor(5.0274)\n",
      "tensor(0.1113)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.357208\n",
      "Epoch 6762\n",
      "-------------------------------\n",
      "tensor(22.9191)\n",
      "tensor(6.3985)\n",
      "tensor(3.7018)\n",
      "tensor(0.1386)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.359909\n",
      "Epoch 6763\n",
      "-------------------------------\n",
      "tensor(21.6771)\n",
      "tensor(6.3625)\n",
      "tensor(2.9633)\n",
      "tensor(0.1464)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.361160\n",
      "Epoch 6764\n",
      "-------------------------------\n",
      "tensor(24.7810)\n",
      "tensor(7.4053)\n",
      "tensor(5.5868)\n",
      "tensor(0.0906)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.362949\n",
      "Epoch 6765\n",
      "-------------------------------\n",
      "tensor(16.6702)\n",
      "tensor(5.9876)\n",
      "tensor(6.2220)\n",
      "tensor(0.1105)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.355747\n",
      "Epoch 6766\n",
      "-------------------------------\n",
      "tensor(18.1471)\n",
      "tensor(8.1299)\n",
      "tensor(6.8879)\n",
      "tensor(0.3926)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.346039\n",
      "Epoch 6767\n",
      "-------------------------------\n",
      "tensor(30.7978)\n",
      "tensor(11.2019)\n",
      "tensor(7.1727)\n",
      "tensor(0.2403)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.358398\n",
      "Epoch 6768\n",
      "-------------------------------\n",
      "tensor(42.6150)\n",
      "tensor(16.6838)\n",
      "tensor(8.4984)\n",
      "tensor(0.5714)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.390690\n",
      "Epoch 6769\n",
      "-------------------------------\n",
      "tensor(46.4190)\n",
      "tensor(17.5377)\n",
      "tensor(8.1178)\n",
      "tensor(0.6231)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.389706\n",
      "Epoch 6770\n",
      "-------------------------------\n",
      "tensor(38.5093)\n",
      "tensor(24.0800)\n",
      "tensor(8.9464)\n",
      "tensor(0.0761)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.384491\n",
      "Epoch 6771\n",
      "-------------------------------\n",
      "tensor(24.4890)\n",
      "tensor(10.8951)\n",
      "tensor(5.4670)\n",
      "tensor(0.2715)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.348980\n",
      "Epoch 6772\n",
      "-------------------------------\n",
      "tensor(38.4923)\n",
      "tensor(12.2107)\n",
      "tensor(14.8307)\n",
      "tensor(0.2901)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.380169\n",
      "Epoch 6773\n",
      "-------------------------------\n",
      "tensor(42.0230)\n",
      "tensor(8.3903)\n",
      "tensor(23.5355)\n",
      "tensor(0.3107)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.417313\n",
      "Epoch 6774\n",
      "-------------------------------\n",
      "tensor(54.6951)\n",
      "tensor(20.7796)\n",
      "tensor(31.8305)\n",
      "tensor(0.9277)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.498539\n",
      "Epoch 6775\n",
      "-------------------------------\n",
      "tensor(43.0906)\n",
      "tensor(13.1605)\n",
      "tensor(19.5458)\n",
      "tensor(0.4891)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.420788\n",
      "Epoch 6776\n",
      "-------------------------------\n",
      "tensor(22.4625)\n",
      "tensor(7.3190)\n",
      "tensor(9.9287)\n",
      "tensor(0.2342)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.421368\n",
      "Epoch 6777\n",
      "-------------------------------\n",
      "tensor(29.3239)\n",
      "tensor(9.0087)\n",
      "tensor(19.2991)\n",
      "tensor(0.4525)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.425701\n",
      "Epoch 6778\n",
      "-------------------------------\n",
      "tensor(29.8658)\n",
      "tensor(9.0884)\n",
      "tensor(10.4451)\n",
      "tensor(0.1746)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.403641\n",
      "Epoch 6779\n",
      "-------------------------------\n",
      "tensor(33.2087)\n",
      "tensor(10.8604)\n",
      "tensor(6.7987)\n",
      "tensor(0.2441)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.402565\n",
      "Epoch 6780\n",
      "-------------------------------\n",
      "tensor(36.2073)\n",
      "tensor(13.4071)\n",
      "tensor(12.3382)\n",
      "tensor(0.3413)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.396385\n",
      "Epoch 6781\n",
      "-------------------------------\n",
      "tensor(22.8859)\n",
      "tensor(9.7903)\n",
      "tensor(10.2530)\n",
      "tensor(0.2425)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.379475\n",
      "Epoch 6782\n",
      "-------------------------------\n",
      "tensor(18.0347)\n",
      "tensor(5.2947)\n",
      "tensor(4.6720)\n",
      "tensor(0.0718)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.370422\n",
      "Epoch 6783\n",
      "-------------------------------\n",
      "tensor(15.6128)\n",
      "tensor(5.2665)\n",
      "tensor(4.3936)\n",
      "tensor(0.1664)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.360638\n",
      "Epoch 6784\n",
      "-------------------------------\n",
      "tensor(28.6796)\n",
      "tensor(10.1202)\n",
      "tensor(14.3032)\n",
      "tensor(0.4126)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.351223\n",
      "Epoch 6785\n",
      "-------------------------------\n",
      "tensor(32.1867)\n",
      "tensor(11.0045)\n",
      "tensor(13.3541)\n",
      "tensor(0.3596)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.353466\n",
      "Epoch 6786\n",
      "-------------------------------\n",
      "tensor(30.2962)\n",
      "tensor(8.9753)\n",
      "tensor(7.7158)\n",
      "tensor(0.1869)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.331314\n",
      "Epoch 6787\n",
      "-------------------------------\n",
      "tensor(47.9956)\n",
      "tensor(14.2254)\n",
      "tensor(16.7829)\n",
      "tensor(0.3901)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.356834\n",
      "Epoch 6788\n",
      "-------------------------------\n",
      "tensor(58.9168)\n",
      "tensor(23.0525)\n",
      "tensor(12.1842)\n",
      "tensor(0.5310)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.475037\n",
      "Epoch 6789\n",
      "-------------------------------\n",
      "tensor(47.3958)\n",
      "tensor(14.7920)\n",
      "tensor(13.6899)\n",
      "tensor(0.4238)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.517620\n",
      "Epoch 6790\n",
      "-------------------------------\n",
      "tensor(37.6210)\n",
      "tensor(12.7427)\n",
      "tensor(7.6855)\n",
      "tensor(0.3671)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.403419\n",
      "Epoch 6791\n",
      "-------------------------------\n",
      "tensor(37.6425)\n",
      "tensor(13.3377)\n",
      "tensor(8.5099)\n",
      "tensor(0.5631)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.361320\n",
      "Epoch 6792\n",
      "-------------------------------\n",
      "tensor(20.7459)\n",
      "tensor(8.7463)\n",
      "tensor(9.9816)\n",
      "tensor(0.3210)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.409260\n",
      "Epoch 6793\n",
      "-------------------------------\n",
      "tensor(34.7090)\n",
      "tensor(12.8917)\n",
      "tensor(22.4733)\n",
      "tensor(0.8279)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.467945\n",
      "Epoch 6794\n",
      "-------------------------------\n",
      "tensor(37.1484)\n",
      "tensor(13.3667)\n",
      "tensor(22.6725)\n",
      "tensor(0.5321)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.483143\n",
      "Epoch 6795\n",
      "-------------------------------\n",
      "tensor(28.6290)\n",
      "tensor(10.6609)\n",
      "tensor(15.2627)\n",
      "tensor(0.1010)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.467575\n",
      "Epoch 6796\n",
      "-------------------------------\n",
      "tensor(20.2529)\n",
      "tensor(7.2863)\n",
      "tensor(7.8760)\n",
      "tensor(0.0126)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.436363\n",
      "Epoch 6797\n",
      "-------------------------------\n",
      "tensor(43.2188)\n",
      "tensor(12.4260)\n",
      "tensor(10.4146)\n",
      "tensor(0.1728)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.409569\n",
      "Epoch 6798\n",
      "-------------------------------\n",
      "tensor(35.3153)\n",
      "tensor(10.2091)\n",
      "tensor(7.2537)\n",
      "tensor(0.1312)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.399940\n",
      "Epoch 6799\n",
      "-------------------------------\n",
      "tensor(16.6786)\n",
      "tensor(6.5286)\n",
      "tensor(5.8613)\n",
      "tensor(0.4074)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.385555\n",
      "Epoch 6800\n",
      "-------------------------------\n",
      "tensor(16.9760)\n",
      "tensor(7.0816)\n",
      "tensor(9.5243)\n",
      "tensor(0.4219)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.378811\n",
      "Epoch 6801\n",
      "-------------------------------\n",
      "tensor(17.1683)\n",
      "tensor(6.2315)\n",
      "tensor(8.8196)\n",
      "tensor(0.2961)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.370842\n",
      "Epoch 6802\n",
      "-------------------------------\n",
      "tensor(33.3727)\n",
      "tensor(10.2582)\n",
      "tensor(5.8585)\n",
      "tensor(0.1136)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.362839\n",
      "Epoch 6803\n",
      "-------------------------------\n",
      "tensor(29.9520)\n",
      "tensor(9.4664)\n",
      "tensor(2.7664)\n",
      "tensor(0.1409)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.361279\n",
      "Epoch 6804\n",
      "-------------------------------\n",
      "tensor(25.2769)\n",
      "tensor(10.0143)\n",
      "tensor(9.6292)\n",
      "tensor(0.4308)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.354271\n",
      "Epoch 6805\n",
      "-------------------------------\n",
      "tensor(26.2026)\n",
      "tensor(10.7227)\n",
      "tensor(11.6332)\n",
      "tensor(0.4866)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.328041\n",
      "Epoch 6806\n",
      "-------------------------------\n",
      "tensor(39.2957)\n",
      "tensor(11.8915)\n",
      "tensor(2.7370)\n",
      "tensor(0.0021)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.325272\n",
      "Epoch 6807\n",
      "-------------------------------\n",
      "tensor(56.2835)\n",
      "tensor(21.7553)\n",
      "tensor(11.5725)\n",
      "tensor(0.5547)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.334686\n",
      "Epoch 6808\n",
      "-------------------------------\n",
      "tensor(54.0762)\n",
      "tensor(15.0554)\n",
      "tensor(24.3496)\n",
      "tensor(0.2060)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.410690\n",
      "Epoch 6809\n",
      "-------------------------------\n",
      "tensor(39.4262)\n",
      "tensor(17.2219)\n",
      "tensor(17.7064)\n",
      "tensor(0.6661)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.367134\n",
      "Epoch 6810\n",
      "-------------------------------\n",
      "tensor(27.5307)\n",
      "tensor(11.0605)\n",
      "tensor(9.9322)\n",
      "tensor(0.5443)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.365467\n",
      "Epoch 6811\n",
      "-------------------------------\n",
      "tensor(44.4929)\n",
      "tensor(15.1555)\n",
      "tensor(20.5287)\n",
      "tensor(0.7178)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.435837\n",
      "Epoch 6812\n",
      "-------------------------------\n",
      "tensor(43.9767)\n",
      "tensor(21.9251)\n",
      "tensor(34.2741)\n",
      "tensor(1.5151)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.441048\n",
      "Epoch 6813\n",
      "-------------------------------\n",
      "tensor(51.9369)\n",
      "tensor(15.6223)\n",
      "tensor(32.6489)\n",
      "tensor(0.5948)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.410839\n",
      "Epoch 6814\n",
      "-------------------------------\n",
      "tensor(47.0124)\n",
      "tensor(10.4345)\n",
      "tensor(27.4869)\n",
      "tensor(0.0553)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.401230\n",
      "Epoch 6815\n",
      "-------------------------------\n",
      "tensor(26.0362)\n",
      "tensor(12.7075)\n",
      "tensor(13.3276)\n",
      "tensor(0.6952)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.372528\n",
      "Epoch 6816\n",
      "-------------------------------\n",
      "tensor(47.0832)\n",
      "tensor(13.7723)\n",
      "tensor(13.6847)\n",
      "tensor(0.0476)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.390495\n",
      "Epoch 6817\n",
      "-------------------------------\n",
      "tensor(27.7131)\n",
      "tensor(15.0523)\n",
      "tensor(15.4766)\n",
      "tensor(0.9647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.353794\n",
      "Epoch 6818\n",
      "-------------------------------\n",
      "tensor(26.6979)\n",
      "tensor(15.0835)\n",
      "tensor(15.6913)\n",
      "tensor(0.7969)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.334377\n",
      "Epoch 6819\n",
      "-------------------------------\n",
      "tensor(24.4958)\n",
      "tensor(10.2900)\n",
      "tensor(2.6998)\n",
      "tensor(0.1380)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.316116\n",
      "Epoch 6820\n",
      "-------------------------------\n",
      "tensor(16.4455)\n",
      "tensor(6.9178)\n",
      "tensor(6.5323)\n",
      "tensor(0.2939)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.302567\n",
      "Epoch 6821\n",
      "-------------------------------\n",
      "tensor(16.8231)\n",
      "tensor(8.5599)\n",
      "tensor(9.0373)\n",
      "tensor(0.4466)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.298172\n",
      "Epoch 6822\n",
      "-------------------------------\n",
      "tensor(15.7955)\n",
      "tensor(8.4410)\n",
      "tensor(8.1118)\n",
      "tensor(0.4399)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.290977\n",
      "Epoch 6823\n",
      "-------------------------------\n",
      "tensor(33.1605)\n",
      "tensor(10.7042)\n",
      "tensor(5.1279)\n",
      "tensor(0.2999)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.285179\n",
      "Epoch 6824\n",
      "-------------------------------\n",
      "tensor(45.5499)\n",
      "tensor(13.4916)\n",
      "tensor(5.3068)\n",
      "tensor(0.0113)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.287373\n",
      "Epoch 6825\n",
      "-------------------------------\n",
      "tensor(43.4782)\n",
      "tensor(14.6340)\n",
      "tensor(7.0208)\n",
      "tensor(0.3901)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.302608\n",
      "Epoch 6826\n",
      "-------------------------------\n",
      "tensor(40.0438)\n",
      "tensor(13.4770)\n",
      "tensor(9.9552)\n",
      "tensor(0.5392)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.283024\n",
      "Epoch 6827\n",
      "-------------------------------\n",
      "tensor(38.1201)\n",
      "tensor(23.1686)\n",
      "tensor(10.8018)\n",
      "tensor(0.3228)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.306091\n",
      "Epoch 6828\n",
      "-------------------------------\n",
      "tensor(26.5367)\n",
      "tensor(8.2882)\n",
      "tensor(2.7515)\n",
      "tensor(0.0827)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.305943\n",
      "Epoch 6829\n",
      "-------------------------------\n",
      "tensor(27.4304)\n",
      "tensor(9.0321)\n",
      "tensor(14.2821)\n",
      "tensor(0.2332)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.326035\n",
      "Epoch 6830\n",
      "-------------------------------\n",
      "tensor(37.7811)\n",
      "tensor(10.2611)\n",
      "tensor(13.6914)\n",
      "tensor(0.4061)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.363804\n",
      "Epoch 6831\n",
      "-------------------------------\n",
      "tensor(38.6481)\n",
      "tensor(22.0331)\n",
      "tensor(17.7873)\n",
      "tensor(1.0671)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.384785\n",
      "Epoch 6832\n",
      "-------------------------------\n",
      "tensor(28.9990)\n",
      "tensor(7.0664)\n",
      "tensor(8.3003)\n",
      "tensor(0.1834)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.336548\n",
      "Epoch 6833\n",
      "-------------------------------\n",
      "tensor(22.4987)\n",
      "tensor(10.7676)\n",
      "tensor(10.2379)\n",
      "tensor(0.4749)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.305061\n",
      "Epoch 6834\n",
      "-------------------------------\n",
      "tensor(32.3315)\n",
      "tensor(15.1020)\n",
      "tensor(11.0300)\n",
      "tensor(0.7127)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.314953\n",
      "Epoch 6835\n",
      "-------------------------------\n",
      "tensor(26.8166)\n",
      "tensor(7.3466)\n",
      "tensor(6.2688)\n",
      "tensor(0.3006)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.267738\n",
      "Epoch 6836\n",
      "-------------------------------\n",
      "tensor(43.7951)\n",
      "tensor(17.0609)\n",
      "tensor(12.0301)\n",
      "tensor(0.7264)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.286587\n",
      "Epoch 6837\n",
      "-------------------------------\n",
      "tensor(47.0418)\n",
      "tensor(15.1754)\n",
      "tensor(3.8193)\n",
      "tensor(0.1783)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.254997\n",
      "Epoch 6838\n",
      "-------------------------------\n",
      "tensor(20.6471)\n",
      "tensor(6.4326)\n",
      "tensor(9.2436)\n",
      "tensor(0.2136)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.257202\n",
      "Epoch 6839\n",
      "-------------------------------\n",
      "tensor(32.3659)\n",
      "tensor(10.3000)\n",
      "tensor(4.0939)\n",
      "tensor(0.2093)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.270660\n",
      "Epoch 6840\n",
      "-------------------------------\n",
      "tensor(26.9970)\n",
      "tensor(8.4844)\n",
      "tensor(3.9029)\n",
      "tensor(0.0984)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.269943\n",
      "Epoch 6841\n",
      "-------------------------------\n",
      "tensor(17.1080)\n",
      "tensor(8.4350)\n",
      "tensor(4.9783)\n",
      "tensor(0.0221)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.268600\n",
      "Epoch 6842\n",
      "-------------------------------\n",
      "tensor(16.8011)\n",
      "tensor(8.3748)\n",
      "tensor(4.6251)\n",
      "tensor(0.0187)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.268078\n",
      "Epoch 6843\n",
      "-------------------------------\n",
      "tensor(14.8757)\n",
      "tensor(4.8394)\n",
      "tensor(2.7854)\n",
      "tensor(0.0393)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.263245\n",
      "Epoch 6844\n",
      "-------------------------------\n",
      "tensor(15.5210)\n",
      "tensor(4.5522)\n",
      "tensor(1.3859)\n",
      "tensor(0.0408)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.254368\n",
      "Epoch 6845\n",
      "-------------------------------\n",
      "tensor(17.3782)\n",
      "tensor(4.3878)\n",
      "tensor(3.6831)\n",
      "tensor(0.0594)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.236481\n",
      "Epoch 6846\n",
      "-------------------------------\n",
      "tensor(39.2722)\n",
      "tensor(12.4175)\n",
      "tensor(1.5314)\n",
      "tensor(0.1426)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.243134\n",
      "Epoch 6847\n",
      "-------------------------------\n",
      "tensor(52.0762)\n",
      "tensor(16.4619)\n",
      "tensor(3.1813)\n",
      "tensor(0.1801)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.255260\n",
      "Epoch 6848\n",
      "-------------------------------\n",
      "tensor(51.7211)\n",
      "tensor(25.6878)\n",
      "tensor(9.1295)\n",
      "tensor(0.0585)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.291382\n",
      "Epoch 6849\n",
      "-------------------------------\n",
      "tensor(38.2272)\n",
      "tensor(11.4290)\n",
      "tensor(15.8704)\n",
      "tensor(0.2727)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.261795\n",
      "Epoch 6850\n",
      "-------------------------------\n",
      "tensor(50.1750)\n",
      "tensor(16.2085)\n",
      "tensor(33.9574)\n",
      "tensor(0.7693)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.338005\n",
      "Epoch 6851\n",
      "-------------------------------\n",
      "tensor(56.7395)\n",
      "tensor(25.3747)\n",
      "tensor(44.7136)\n",
      "tensor(1.6642)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.381626\n",
      "Epoch 6852\n",
      "-------------------------------\n",
      "tensor(53.6204)\n",
      "tensor(21.5329)\n",
      "tensor(43.8631)\n",
      "tensor(1.2443)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.396492\n",
      "Epoch 6853\n",
      "-------------------------------\n",
      "tensor(57.4885)\n",
      "tensor(11.0186)\n",
      "tensor(33.2143)\n",
      "tensor(0.3453)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.382187\n",
      "Epoch 6854\n",
      "-------------------------------\n",
      "tensor(39.9633)\n",
      "tensor(15.1180)\n",
      "tensor(25.0336)\n",
      "tensor(0.5484)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.333038\n",
      "Epoch 6855\n",
      "-------------------------------\n",
      "tensor(39.6032)\n",
      "tensor(13.0008)\n",
      "tensor(7.6924)\n",
      "tensor(0.4683)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.287704\n",
      "Epoch 6856\n",
      "-------------------------------\n",
      "tensor(33.7555)\n",
      "tensor(11.0131)\n",
      "tensor(21.2596)\n",
      "tensor(0.5354)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.261478\n",
      "Epoch 6857\n",
      "-------------------------------\n",
      "tensor(34.2648)\n",
      "tensor(12.1396)\n",
      "tensor(11.5496)\n",
      "tensor(0.5212)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.261681\n",
      "Epoch 6858\n",
      "-------------------------------\n",
      "tensor(51.5717)\n",
      "tensor(17.9673)\n",
      "tensor(16.9271)\n",
      "tensor(0.6615)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.265884\n",
      "Epoch 6859\n",
      "-------------------------------\n",
      "tensor(16.7374)\n",
      "tensor(4.3161)\n",
      "tensor(3.4047)\n",
      "tensor(0.2214)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.211609\n",
      "Epoch 6860\n",
      "-------------------------------\n",
      "tensor(20.4073)\n",
      "tensor(6.0216)\n",
      "tensor(8.0963)\n",
      "tensor(0.1251)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.212090\n",
      "Epoch 6861\n",
      "-------------------------------\n",
      "tensor(19.9411)\n",
      "tensor(7.1885)\n",
      "tensor(9.2936)\n",
      "tensor(0.2534)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.208134\n",
      "Epoch 6862\n",
      "-------------------------------\n",
      "tensor(16.7097)\n",
      "tensor(6.9630)\n",
      "tensor(6.7396)\n",
      "tensor(0.2572)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.200577\n",
      "Epoch 6863\n",
      "-------------------------------\n",
      "tensor(32.5235)\n",
      "tensor(10.1646)\n",
      "tensor(3.1894)\n",
      "tensor(0.1663)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.196899\n",
      "Epoch 6864\n",
      "-------------------------------\n",
      "tensor(42.2438)\n",
      "tensor(13.6459)\n",
      "tensor(8.6915)\n",
      "tensor(0.0205)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.196514\n",
      "Epoch 6865\n",
      "-------------------------------\n",
      "tensor(48.4064)\n",
      "tensor(15.9529)\n",
      "tensor(8.6386)\n",
      "tensor(0.1667)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.211620\n",
      "Epoch 6866\n",
      "-------------------------------\n",
      "tensor(36.7398)\n",
      "tensor(11.0095)\n",
      "tensor(7.7807)\n",
      "tensor(0.1255)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.190506\n",
      "Epoch 6867\n",
      "-------------------------------\n",
      "tensor(33.8523)\n",
      "tensor(7.8703)\n",
      "tensor(13.4462)\n",
      "tensor(0.1976)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.241596\n",
      "Epoch 6868\n",
      "-------------------------------\n",
      "tensor(53.1453)\n",
      "tensor(17.8657)\n",
      "tensor(17.8048)\n",
      "tensor(0.5480)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.300724\n",
      "Epoch 6869\n",
      "-------------------------------\n",
      "tensor(48.5656)\n",
      "tensor(14.8778)\n",
      "tensor(19.1028)\n",
      "tensor(0.5872)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.287033\n",
      "Epoch 6870\n",
      "-------------------------------\n",
      "tensor(14.8105)\n",
      "tensor(6.7895)\n",
      "tensor(4.4416)\n",
      "tensor(0.2876)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.248924\n",
      "Epoch 6871\n",
      "-------------------------------\n",
      "tensor(30.3065)\n",
      "tensor(14.0877)\n",
      "tensor(19.5306)\n",
      "tensor(0.6721)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.294811\n",
      "Epoch 6872\n",
      "-------------------------------\n",
      "tensor(46.7762)\n",
      "tensor(16.2766)\n",
      "tensor(31.8045)\n",
      "tensor(0.8628)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.305477\n",
      "Epoch 6873\n",
      "-------------------------------\n",
      "tensor(63.4997)\n",
      "tensor(19.4492)\n",
      "tensor(44.0944)\n",
      "tensor(0.9607)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.316319\n",
      "Epoch 6874\n",
      "-------------------------------\n",
      "tensor(60.5650)\n",
      "tensor(20.5697)\n",
      "tensor(43.3093)\n",
      "tensor(1.0643)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.287754\n",
      "Epoch 6875\n",
      "-------------------------------\n",
      "tensor(60.9992)\n",
      "tensor(19.1636)\n",
      "tensor(20.8514)\n",
      "tensor(0.5862)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.261097\n",
      "Epoch 6876\n",
      "-------------------------------\n",
      "tensor(34.7710)\n",
      "tensor(11.3857)\n",
      "tensor(9.8940)\n",
      "tensor(0.4603)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.204788\n",
      "Epoch 6877\n",
      "-------------------------------\n",
      "tensor(33.3582)\n",
      "tensor(10.3131)\n",
      "tensor(18.7781)\n",
      "tensor(0.3381)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.213543\n",
      "Epoch 6878\n",
      "-------------------------------\n",
      "tensor(20.0164)\n",
      "tensor(7.8509)\n",
      "tensor(7.4145)\n",
      "tensor(0.2641)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.197033\n",
      "Epoch 6879\n",
      "-------------------------------\n",
      "tensor(32.1642)\n",
      "tensor(11.1413)\n",
      "tensor(8.7031)\n",
      "tensor(0.0126)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.190868\n",
      "Epoch 6880\n",
      "-------------------------------\n",
      "tensor(42.5849)\n",
      "tensor(13.6127)\n",
      "tensor(11.2264)\n",
      "tensor(0.0934)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.192345\n",
      "Epoch 6881\n",
      "-------------------------------\n",
      "tensor(39.4112)\n",
      "tensor(11.6879)\n",
      "tensor(7.1633)\n",
      "tensor(0.0624)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.179325\n",
      "Epoch 6882\n",
      "-------------------------------\n",
      "tensor(14.3826)\n",
      "tensor(4.7455)\n",
      "tensor(1.4706)\n",
      "tensor(0.0013)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.175793\n",
      "Epoch 6883\n",
      "-------------------------------\n",
      "tensor(25.7129)\n",
      "tensor(7.5416)\n",
      "tensor(5.9461)\n",
      "tensor(0.0711)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.181179\n",
      "Epoch 6884\n",
      "-------------------------------\n",
      "tensor(33.7083)\n",
      "tensor(9.4230)\n",
      "tensor(10.5859)\n",
      "tensor(0.0735)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.189278\n",
      "Epoch 6885\n",
      "-------------------------------\n",
      "tensor(33.0128)\n",
      "tensor(9.8295)\n",
      "tensor(5.9257)\n",
      "tensor(0.1413)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.194252\n",
      "Epoch 6886\n",
      "-------------------------------\n",
      "tensor(38.9097)\n",
      "tensor(13.6415)\n",
      "tensor(11.4564)\n",
      "tensor(0.4843)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.179844\n",
      "Epoch 6887\n",
      "-------------------------------\n",
      "tensor(34.8801)\n",
      "tensor(12.6518)\n",
      "tensor(6.8758)\n",
      "tensor(0.1923)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.176037\n",
      "Epoch 6888\n",
      "-------------------------------\n",
      "tensor(34.4902)\n",
      "tensor(26.9429)\n",
      "tensor(17.4255)\n",
      "tensor(0.5584)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.205826\n",
      "Epoch 6889\n",
      "-------------------------------\n",
      "tensor(40.5589)\n",
      "tensor(12.5138)\n",
      "tensor(6.1585)\n",
      "tensor(0.1414)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.255562\n",
      "Epoch 6890\n",
      "-------------------------------\n",
      "tensor(41.1758)\n",
      "tensor(20.5675)\n",
      "tensor(27.3369)\n",
      "tensor(1.1112)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.239166\n",
      "Epoch 6891\n",
      "-------------------------------\n",
      "tensor(53.4474)\n",
      "tensor(15.9978)\n",
      "tensor(34.5705)\n",
      "tensor(0.9029)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.299709\n",
      "Epoch 6892\n",
      "-------------------------------\n",
      "tensor(64.6956)\n",
      "tensor(18.8189)\n",
      "tensor(42.7123)\n",
      "tensor(0.7426)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.343102\n",
      "Epoch 6893\n",
      "-------------------------------\n",
      "tensor(65.4744)\n",
      "tensor(22.6953)\n",
      "tensor(46.5508)\n",
      "tensor(1.1478)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.357216\n",
      "Epoch 6894\n",
      "-------------------------------\n",
      "tensor(51.5695)\n",
      "tensor(19.2838)\n",
      "tensor(41.6280)\n",
      "tensor(1.2695)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.308514\n",
      "Epoch 6895\n",
      "-------------------------------\n",
      "tensor(41.2772)\n",
      "tensor(13.2300)\n",
      "tensor(10.9111)\n",
      "tensor(0.3417)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.317055\n",
      "Epoch 6896\n",
      "-------------------------------\n",
      "tensor(52.5027)\n",
      "tensor(17.3354)\n",
      "tensor(25.4971)\n",
      "tensor(0.7192)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.295864\n",
      "Epoch 6897\n",
      "-------------------------------\n",
      "tensor(26.3054)\n",
      "tensor(9.0819)\n",
      "tensor(12.5720)\n",
      "tensor(0.4537)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.231987\n",
      "Epoch 6898\n",
      "-------------------------------\n",
      "tensor(33.6634)\n",
      "tensor(12.1834)\n",
      "tensor(20.0855)\n",
      "tensor(0.6680)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.243690\n",
      "Epoch 6899\n",
      "-------------------------------\n",
      "tensor(29.8119)\n",
      "tensor(11.5954)\n",
      "tensor(4.5236)\n",
      "tensor(0.2016)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.219616\n",
      "Epoch 6900\n",
      "-------------------------------\n",
      "tensor(19.8918)\n",
      "tensor(6.1184)\n",
      "tensor(7.8448)\n",
      "tensor(0.1643)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.212322\n",
      "Epoch 6901\n",
      "-------------------------------\n",
      "tensor(28.1719)\n",
      "tensor(9.1905)\n",
      "tensor(11.0113)\n",
      "tensor(0.2940)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.211964\n",
      "Epoch 6902\n",
      "-------------------------------\n",
      "tensor(26.4737)\n",
      "tensor(8.8179)\n",
      "tensor(9.1600)\n",
      "tensor(0.2766)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.206177\n",
      "Epoch 6903\n",
      "-------------------------------\n",
      "tensor(18.3598)\n",
      "tensor(6.3074)\n",
      "tensor(3.2594)\n",
      "tensor(0.1360)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.195602\n",
      "Epoch 6904\n",
      "-------------------------------\n",
      "tensor(18.8080)\n",
      "tensor(8.8057)\n",
      "tensor(8.6128)\n",
      "tensor(0.1351)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.186005\n",
      "Epoch 6905\n",
      "-------------------------------\n",
      "tensor(30.6146)\n",
      "tensor(9.9196)\n",
      "tensor(15.4804)\n",
      "tensor(0.3388)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.172325\n",
      "Epoch 6906\n",
      "-------------------------------\n",
      "tensor(44.5625)\n",
      "tensor(13.7648)\n",
      "tensor(1.6609)\n",
      "tensor(0.0731)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.154224\n",
      "Epoch 6907\n",
      "-------------------------------\n",
      "tensor(59.5826)\n",
      "tensor(17.8772)\n",
      "tensor(21.9412)\n",
      "tensor(0.2642)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.208797\n",
      "Epoch 6908\n",
      "-------------------------------\n",
      "tensor(43.5550)\n",
      "tensor(20.2890)\n",
      "tensor(9.2395)\n",
      "tensor(0.6152)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.183174\n",
      "Epoch 6909\n",
      "-------------------------------\n",
      "tensor(17.5434)\n",
      "tensor(10.5993)\n",
      "tensor(10.1703)\n",
      "tensor(0.5574)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.171082\n",
      "Epoch 6910\n",
      "-------------------------------\n",
      "tensor(26.5444)\n",
      "tensor(11.8022)\n",
      "tensor(12.9631)\n",
      "tensor(0.6997)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.186752\n",
      "Epoch 6911\n",
      "-------------------------------\n",
      "tensor(36.2131)\n",
      "tensor(10.9220)\n",
      "tensor(8.5868)\n",
      "tensor(0.1713)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.219227\n",
      "Epoch 6912\n",
      "-------------------------------\n",
      "tensor(29.3420)\n",
      "tensor(9.4939)\n",
      "tensor(8.6956)\n",
      "tensor(0.5396)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.211292\n",
      "Epoch 6913\n",
      "-------------------------------\n",
      "tensor(27.1130)\n",
      "tensor(11.2701)\n",
      "tensor(11.9705)\n",
      "tensor(0.6071)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.221317\n",
      "Epoch 6914\n",
      "-------------------------------\n",
      "tensor(26.7486)\n",
      "tensor(13.0056)\n",
      "tensor(11.9053)\n",
      "tensor(0.6849)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.183514\n",
      "Epoch 6915\n",
      "-------------------------------\n",
      "tensor(35.0290)\n",
      "tensor(13.5290)\n",
      "tensor(6.3789)\n",
      "tensor(0.4220)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.190979\n",
      "Epoch 6916\n",
      "-------------------------------\n",
      "tensor(39.6647)\n",
      "tensor(12.8211)\n",
      "tensor(5.0712)\n",
      "tensor(0.3030)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.182476\n",
      "Epoch 6917\n",
      "-------------------------------\n",
      "tensor(34.8432)\n",
      "tensor(11.0503)\n",
      "tensor(8.3862)\n",
      "tensor(0.4512)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.193604\n",
      "Epoch 6918\n",
      "-------------------------------\n",
      "tensor(37.5635)\n",
      "tensor(12.1551)\n",
      "tensor(5.4007)\n",
      "tensor(0.2974)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.184116\n",
      "Epoch 6919\n",
      "-------------------------------\n",
      "tensor(36.4761)\n",
      "tensor(11.3126)\n",
      "tensor(1.7932)\n",
      "tensor(0.0875)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.171806\n",
      "Epoch 6920\n",
      "-------------------------------\n",
      "tensor(22.6818)\n",
      "tensor(7.3363)\n",
      "tensor(1.6002)\n",
      "tensor(0.0651)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.175140\n",
      "Epoch 6921\n",
      "-------------------------------\n",
      "tensor(15.0509)\n",
      "tensor(5.9722)\n",
      "tensor(2.5543)\n",
      "tensor(0.1434)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.173626\n",
      "Epoch 6922\n",
      "-------------------------------\n",
      "tensor(14.9618)\n",
      "tensor(6.2352)\n",
      "tensor(3.1131)\n",
      "tensor(0.1792)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.173592\n",
      "Epoch 6923\n",
      "-------------------------------\n",
      "tensor(16.1662)\n",
      "tensor(8.9087)\n",
      "tensor(3.2886)\n",
      "tensor(0.1806)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.169800\n",
      "Epoch 6924\n",
      "-------------------------------\n",
      "tensor(15.9339)\n",
      "tensor(8.5988)\n",
      "tensor(2.7779)\n",
      "tensor(0.1230)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.160641\n",
      "Epoch 6925\n",
      "-------------------------------\n",
      "tensor(14.3529)\n",
      "tensor(4.7770)\n",
      "tensor(1.6803)\n",
      "tensor(0.0273)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.142593\n",
      "Epoch 6926\n",
      "-------------------------------\n",
      "tensor(32.4005)\n",
      "tensor(10.7396)\n",
      "tensor(2.7433)\n",
      "tensor(0.2197)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.139709\n",
      "Epoch 6927\n",
      "-------------------------------\n",
      "tensor(55.7387)\n",
      "tensor(18.6199)\n",
      "tensor(3.1068)\n",
      "tensor(0.2818)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.200478\n",
      "Epoch 6928\n",
      "-------------------------------\n",
      "tensor(49.8401)\n",
      "tensor(16.3155)\n",
      "tensor(9.2395)\n",
      "tensor(0.2569)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.231064\n",
      "Epoch 6929\n",
      "-------------------------------\n",
      "tensor(41.9506)\n",
      "tensor(19.9636)\n",
      "tensor(7.1311)\n",
      "tensor(0.3833)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.266151\n",
      "Epoch 6930\n",
      "-------------------------------\n",
      "tensor(30.2304)\n",
      "tensor(11.5525)\n",
      "tensor(13.5917)\n",
      "tensor(0.4275)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.246128\n",
      "Epoch 6931\n",
      "-------------------------------\n",
      "tensor(34.0800)\n",
      "tensor(9.7844)\n",
      "tensor(17.8754)\n",
      "tensor(0.1799)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.253597\n",
      "Epoch 6932\n",
      "-------------------------------\n",
      "tensor(36.2886)\n",
      "tensor(11.0415)\n",
      "tensor(19.9118)\n",
      "tensor(0.4198)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.287418\n",
      "Epoch 6933\n",
      "-------------------------------\n",
      "tensor(31.7600)\n",
      "tensor(18.9594)\n",
      "tensor(28.6063)\n",
      "tensor(0.9716)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.287041\n",
      "Epoch 6934\n",
      "-------------------------------\n",
      "tensor(32.5745)\n",
      "tensor(8.5971)\n",
      "tensor(18.4414)\n",
      "tensor(0.4841)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.250607\n",
      "Epoch 6935\n",
      "-------------------------------\n",
      "tensor(35.4917)\n",
      "tensor(11.0963)\n",
      "tensor(7.1198)\n",
      "tensor(0.0605)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.258732\n",
      "Epoch 6936\n",
      "-------------------------------\n",
      "tensor(41.7314)\n",
      "tensor(13.2132)\n",
      "tensor(8.9080)\n",
      "tensor(0.2454)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.236313\n",
      "Epoch 6937\n",
      "-------------------------------\n",
      "tensor(35.8886)\n",
      "tensor(10.4540)\n",
      "tensor(9.7205)\n",
      "tensor(0.0290)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.212391\n",
      "Epoch 6938\n",
      "-------------------------------\n",
      "tensor(32.3284)\n",
      "tensor(9.5055)\n",
      "tensor(8.1116)\n",
      "tensor(0.0502)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.183243\n",
      "Epoch 6939\n",
      "-------------------------------\n",
      "tensor(29.8966)\n",
      "tensor(9.0731)\n",
      "tensor(3.1422)\n",
      "tensor(0.1231)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.160110\n",
      "Epoch 6940\n",
      "-------------------------------\n",
      "tensor(34.0292)\n",
      "tensor(10.2209)\n",
      "tensor(5.3524)\n",
      "tensor(0.0878)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.166187\n",
      "Epoch 6941\n",
      "-------------------------------\n",
      "tensor(33.9099)\n",
      "tensor(10.2857)\n",
      "tensor(4.6332)\n",
      "tensor(0.0066)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.168598\n",
      "Epoch 6942\n",
      "-------------------------------\n",
      "tensor(33.1995)\n",
      "tensor(10.4097)\n",
      "tensor(2.7305)\n",
      "tensor(0.0797)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.158161\n",
      "Epoch 6943\n",
      "-------------------------------\n",
      "tensor(15.3334)\n",
      "tensor(6.3121)\n",
      "tensor(3.1525)\n",
      "tensor(0.1742)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.149719\n",
      "Epoch 6944\n",
      "-------------------------------\n",
      "tensor(17.0846)\n",
      "tensor(6.6636)\n",
      "tensor(6.4193)\n",
      "tensor(0.2336)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.147049\n",
      "Epoch 6945\n",
      "-------------------------------\n",
      "tensor(27.2949)\n",
      "tensor(8.3414)\n",
      "tensor(4.4780)\n",
      "tensor(0.1100)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.144669\n",
      "Epoch 6946\n",
      "-------------------------------\n",
      "tensor(31.7334)\n",
      "tensor(10.9881)\n",
      "tensor(5.7726)\n",
      "tensor(0.2680)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.123112\n",
      "Epoch 6947\n",
      "-------------------------------\n",
      "tensor(40.0730)\n",
      "tensor(14.5977)\n",
      "tensor(6.6920)\n",
      "tensor(0.4003)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.205769\n",
      "Epoch 6948\n",
      "-------------------------------\n",
      "tensor(42.4992)\n",
      "tensor(14.7163)\n",
      "tensor(7.7409)\n",
      "tensor(0.0968)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.235992\n",
      "Epoch 6949\n",
      "-------------------------------\n",
      "tensor(42.9927)\n",
      "tensor(21.4872)\n",
      "tensor(3.2583)\n",
      "tensor(0.2247)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.192451\n",
      "Epoch 6950\n",
      "-------------------------------\n",
      "tensor(31.5889)\n",
      "tensor(8.1074)\n",
      "tensor(11.9475)\n",
      "tensor(0.0428)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.153309\n",
      "Epoch 6951\n",
      "-------------------------------\n",
      "tensor(47.2671)\n",
      "tensor(14.5826)\n",
      "tensor(20.7830)\n",
      "tensor(0.4567)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.230549\n",
      "Epoch 6952\n",
      "-------------------------------\n",
      "tensor(49.9586)\n",
      "tensor(16.7364)\n",
      "tensor(29.8560)\n",
      "tensor(0.7942)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.207493\n",
      "Epoch 6953\n",
      "-------------------------------\n",
      "tensor(41.6440)\n",
      "tensor(15.3149)\n",
      "tensor(32.7579)\n",
      "tensor(0.9938)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.231857\n",
      "Epoch 6954\n",
      "-------------------------------\n",
      "tensor(34.7749)\n",
      "tensor(11.9998)\n",
      "tensor(22.3915)\n",
      "tensor(0.5814)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.215538\n",
      "Epoch 6955\n",
      "-------------------------------\n",
      "tensor(45.2928)\n",
      "tensor(13.3188)\n",
      "tensor(5.4394)\n",
      "tensor(0.1019)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.257832\n",
      "Epoch 6956\n",
      "-------------------------------\n",
      "tensor(41.7289)\n",
      "tensor(15.2467)\n",
      "tensor(9.2688)\n",
      "tensor(0.3315)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.222294\n",
      "Epoch 6957\n",
      "-------------------------------\n",
      "tensor(25.7735)\n",
      "tensor(9.0923)\n",
      "tensor(9.3168)\n",
      "tensor(0.1233)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.207603\n",
      "Epoch 6958\n",
      "-------------------------------\n",
      "tensor(20.4938)\n",
      "tensor(4.8173)\n",
      "tensor(5.3920)\n",
      "tensor(0.0780)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.203278\n",
      "Epoch 6959\n",
      "-------------------------------\n",
      "tensor(16.4524)\n",
      "tensor(5.1430)\n",
      "tensor(4.3136)\n",
      "tensor(0.0836)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.194160\n",
      "Epoch 6960\n",
      "-------------------------------\n",
      "tensor(18.1674)\n",
      "tensor(5.8280)\n",
      "tensor(7.4371)\n",
      "tensor(0.0207)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.186298\n",
      "Epoch 6961\n",
      "-------------------------------\n",
      "tensor(17.5182)\n",
      "tensor(6.0537)\n",
      "tensor(6.6930)\n",
      "tensor(0.0639)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.177704\n",
      "Epoch 6962\n",
      "-------------------------------\n",
      "tensor(15.8738)\n",
      "tensor(6.1679)\n",
      "tensor(4.7413)\n",
      "tensor(0.1438)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.167774\n",
      "Epoch 6963\n",
      "-------------------------------\n",
      "tensor(22.7037)\n",
      "tensor(8.1064)\n",
      "tensor(3.8890)\n",
      "tensor(0.2160)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.156128\n",
      "Epoch 6964\n",
      "-------------------------------\n",
      "tensor(24.2756)\n",
      "tensor(7.9103)\n",
      "tensor(6.4122)\n",
      "tensor(0.2220)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.136936\n",
      "Epoch 6965\n",
      "-------------------------------\n",
      "tensor(33.4278)\n",
      "tensor(9.8172)\n",
      "tensor(5.4666)\n",
      "tensor(0.0197)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.134823\n",
      "Epoch 6966\n",
      "-------------------------------\n",
      "tensor(38.5462)\n",
      "tensor(13.8893)\n",
      "tensor(5.9368)\n",
      "tensor(0.4292)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.144279\n",
      "Epoch 6967\n",
      "-------------------------------\n",
      "tensor(57.7063)\n",
      "tensor(21.8740)\n",
      "tensor(9.1203)\n",
      "tensor(0.5473)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.182281\n",
      "Epoch 6968\n",
      "-------------------------------\n",
      "tensor(48.0710)\n",
      "tensor(14.7868)\n",
      "tensor(15.1910)\n",
      "tensor(0.0692)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.211117\n",
      "Epoch 6969\n",
      "-------------------------------\n",
      "tensor(47.3429)\n",
      "tensor(23.2961)\n",
      "tensor(4.7232)\n",
      "tensor(0.1110)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.177246\n",
      "Epoch 6970\n",
      "-------------------------------\n",
      "tensor(23.0509)\n",
      "tensor(8.9599)\n",
      "tensor(11.7112)\n",
      "tensor(0.2657)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.172943\n",
      "Epoch 6971\n",
      "-------------------------------\n",
      "tensor(32.3872)\n",
      "tensor(10.1269)\n",
      "tensor(17.4864)\n",
      "tensor(0.3264)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.224121\n",
      "Epoch 6972\n",
      "-------------------------------\n",
      "tensor(35.0716)\n",
      "tensor(8.3231)\n",
      "tensor(19.8121)\n",
      "tensor(0.4796)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.238426\n",
      "Epoch 6973\n",
      "-------------------------------\n",
      "tensor(31.2090)\n",
      "tensor(17.9052)\n",
      "tensor(28.0869)\n",
      "tensor(0.8817)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.236629\n",
      "Epoch 6974\n",
      "-------------------------------\n",
      "tensor(40.5780)\n",
      "tensor(9.4066)\n",
      "tensor(18.3160)\n",
      "tensor(0.3406)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.220470\n",
      "Epoch 6975\n",
      "-------------------------------\n",
      "tensor(44.6339)\n",
      "tensor(14.2046)\n",
      "tensor(5.7198)\n",
      "tensor(0.1761)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.224007\n",
      "Epoch 6976\n",
      "-------------------------------\n",
      "tensor(34.5478)\n",
      "tensor(10.8924)\n",
      "tensor(7.6905)\n",
      "tensor(0.3246)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.174274\n",
      "Epoch 6977\n",
      "-------------------------------\n",
      "tensor(21.9984)\n",
      "tensor(7.2319)\n",
      "tensor(8.4178)\n",
      "tensor(0.0781)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.149960\n",
      "Epoch 6978\n",
      "-------------------------------\n",
      "tensor(32.2906)\n",
      "tensor(11.6224)\n",
      "tensor(6.3296)\n",
      "tensor(0.2123)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.171211\n",
      "Epoch 6979\n",
      "-------------------------------\n",
      "tensor(32.4952)\n",
      "tensor(10.8235)\n",
      "tensor(2.6087)\n",
      "tensor(0.1529)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.167767\n",
      "Epoch 6980\n",
      "-------------------------------\n",
      "tensor(27.7781)\n",
      "tensor(10.9357)\n",
      "tensor(3.3249)\n",
      "tensor(0.0937)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.146267\n",
      "Epoch 6981\n",
      "-------------------------------\n",
      "tensor(27.4073)\n",
      "tensor(10.7670)\n",
      "tensor(3.0048)\n",
      "tensor(0.0703)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.128345\n",
      "Epoch 6982\n",
      "-------------------------------\n",
      "tensor(23.2212)\n",
      "tensor(7.5840)\n",
      "tensor(1.9434)\n",
      "tensor(0.0649)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.120190\n",
      "Epoch 6983\n",
      "-------------------------------\n",
      "tensor(22.9131)\n",
      "tensor(7.4087)\n",
      "tensor(1.7928)\n",
      "tensor(0.0617)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.113052\n",
      "Epoch 6984\n",
      "-------------------------------\n",
      "tensor(16.6325)\n",
      "tensor(4.9084)\n",
      "tensor(3.2040)\n",
      "tensor(0.0319)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.101025\n",
      "Epoch 6985\n",
      "-------------------------------\n",
      "tensor(24.5271)\n",
      "tensor(7.4823)\n",
      "tensor(1.7299)\n",
      "tensor(0.0791)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.087334\n",
      "Epoch 6986\n",
      "-------------------------------\n",
      "tensor(34.1186)\n",
      "tensor(13.7970)\n",
      "tensor(4.8000)\n",
      "tensor(0.2390)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.137199\n",
      "Epoch 6987\n",
      "-------------------------------\n",
      "tensor(51.9340)\n",
      "tensor(17.2315)\n",
      "tensor(4.4927)\n",
      "tensor(0.2329)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.253887\n",
      "Epoch 6988\n",
      "-------------------------------\n",
      "tensor(51.5115)\n",
      "tensor(16.3191)\n",
      "tensor(7.2258)\n",
      "tensor(0.1738)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.230637\n",
      "Epoch 6989\n",
      "-------------------------------\n",
      "tensor(19.1443)\n",
      "tensor(5.2945)\n",
      "tensor(6.6441)\n",
      "tensor(0.4398)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.081409\n",
      "Epoch 6990\n",
      "-------------------------------\n",
      "tensor(49.8033)\n",
      "tensor(22.4483)\n",
      "tensor(8.3196)\n",
      "tensor(0.1107)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.196484\n",
      "Epoch 6991\n",
      "-------------------------------\n",
      "tensor(48.9274)\n",
      "tensor(20.5229)\n",
      "tensor(37.2403)\n",
      "tensor(1.1134)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.183670\n",
      "Epoch 6992\n",
      "-------------------------------\n",
      "tensor(74.5771)\n",
      "tensor(30.4417)\n",
      "tensor(61.9322)\n",
      "tensor(1.7611)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.263084\n",
      "Epoch 6993\n",
      "-------------------------------\n",
      "tensor(103.9735)\n",
      "tensor(27.7982)\n",
      "tensor(70.3254)\n",
      "tensor(1.5841)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.368919\n",
      "Epoch 6994\n",
      "-------------------------------\n",
      "tensor(93.6517)\n",
      "tensor(36.3796)\n",
      "tensor(68.6981)\n",
      "tensor(1.7853)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.297554\n",
      "Epoch 6995\n",
      "-------------------------------\n",
      "tensor(55.4428)\n",
      "tensor(15.4028)\n",
      "tensor(27.8885)\n",
      "tensor(0.5945)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.187016\n",
      "Epoch 6996\n",
      "-------------------------------\n",
      "tensor(54.9902)\n",
      "tensor(17.0199)\n",
      "tensor(29.3188)\n",
      "tensor(0.5383)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.141884\n",
      "Epoch 6997\n",
      "-------------------------------\n",
      "tensor(39.6865)\n",
      "tensor(10.8404)\n",
      "tensor(27.8689)\n",
      "tensor(0.8365)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.143078\n",
      "Epoch 6998\n",
      "-------------------------------\n",
      "tensor(38.4408)\n",
      "tensor(8.0960)\n",
      "tensor(23.7662)\n",
      "tensor(0.5104)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.142399\n",
      "Epoch 6999\n",
      "-------------------------------\n",
      "tensor(16.8247)\n",
      "tensor(9.8872)\n",
      "tensor(5.7953)\n",
      "tensor(0.3593)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.112137\n",
      "Epoch 7000\n",
      "-------------------------------\n",
      "tensor(37.1402)\n",
      "tensor(17.7286)\n",
      "tensor(19.0903)\n",
      "tensor(0.7523)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.120003\n",
      "Epoch 7001\n",
      "-------------------------------\n",
      "tensor(38.1934)\n",
      "tensor(17.3200)\n",
      "tensor(19.7218)\n",
      "tensor(0.7274)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.117962\n",
      "Epoch 7002\n",
      "-------------------------------\n",
      "tensor(35.1724)\n",
      "tensor(13.3586)\n",
      "tensor(13.5105)\n",
      "tensor(0.4916)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.097542\n",
      "Epoch 7003\n",
      "-------------------------------\n",
      "tensor(15.8122)\n",
      "tensor(5.4838)\n",
      "tensor(1.6865)\n",
      "tensor(0.0570)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.087608\n",
      "Epoch 7004\n",
      "-------------------------------\n",
      "tensor(34.7458)\n",
      "tensor(12.3376)\n",
      "tensor(16.5401)\n",
      "tensor(0.5587)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.102966\n",
      "Epoch 7005\n",
      "-------------------------------\n",
      "tensor(37.9526)\n",
      "tensor(17.2262)\n",
      "tensor(24.2900)\n",
      "tensor(0.9220)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.109146\n",
      "Epoch 7006\n",
      "-------------------------------\n",
      "tensor(27.7313)\n",
      "tensor(9.8024)\n",
      "tensor(4.0364)\n",
      "tensor(0.3372)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.069054\n",
      "Epoch 7007\n",
      "-------------------------------\n",
      "tensor(58.4882)\n",
      "tensor(15.1824)\n",
      "tensor(25.0583)\n",
      "tensor(0.5182)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.137363\n",
      "Epoch 7008\n",
      "-------------------------------\n",
      "tensor(44.5952)\n",
      "tensor(16.2585)\n",
      "tensor(12.0976)\n",
      "tensor(0.3016)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.109802\n",
      "Epoch 7009\n",
      "-------------------------------\n",
      "tensor(20.6463)\n",
      "tensor(7.6008)\n",
      "tensor(13.9032)\n",
      "tensor(0.5012)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.077988\n",
      "Epoch 7010\n",
      "-------------------------------\n",
      "tensor(36.9824)\n",
      "tensor(30.7863)\n",
      "tensor(27.5367)\n",
      "tensor(0.6670)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.147278\n",
      "Epoch 7011\n",
      "-------------------------------\n",
      "tensor(48.2619)\n",
      "tensor(15.3963)\n",
      "tensor(18.1204)\n",
      "tensor(0.5351)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.131157\n",
      "Epoch 7012\n",
      "-------------------------------\n",
      "tensor(27.6506)\n",
      "tensor(5.3983)\n",
      "tensor(12.4159)\n",
      "tensor(0.0882)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.086941\n",
      "Epoch 7013\n",
      "-------------------------------\n",
      "tensor(30.6266)\n",
      "tensor(11.2954)\n",
      "tensor(14.0963)\n",
      "tensor(0.2836)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.079601\n",
      "Epoch 7014\n",
      "-------------------------------\n",
      "tensor(21.4607)\n",
      "tensor(9.0515)\n",
      "tensor(8.8943)\n",
      "tensor(0.2778)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.056580\n",
      "Epoch 7015\n",
      "-------------------------------\n",
      "tensor(42.1559)\n",
      "tensor(13.5235)\n",
      "tensor(4.1140)\n",
      "tensor(0.1681)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.045261\n",
      "Epoch 7016\n",
      "-------------------------------\n",
      "tensor(39.6677)\n",
      "tensor(13.0722)\n",
      "tensor(4.2853)\n",
      "tensor(0.2986)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.072655\n",
      "Epoch 7017\n",
      "-------------------------------\n",
      "tensor(40.0752)\n",
      "tensor(12.6391)\n",
      "tensor(5.7620)\n",
      "tensor(0.1580)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.057697\n",
      "Epoch 7018\n",
      "-------------------------------\n",
      "tensor(18.7847)\n",
      "tensor(5.8544)\n",
      "tensor(4.1827)\n",
      "tensor(0.0016)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.051880\n",
      "Epoch 7019\n",
      "-------------------------------\n",
      "tensor(26.8164)\n",
      "tensor(9.0830)\n",
      "tensor(2.4604)\n",
      "tensor(0.1069)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.054756\n",
      "Epoch 7020\n",
      "-------------------------------\n",
      "tensor(16.8161)\n",
      "tensor(7.2538)\n",
      "tensor(3.5217)\n",
      "tensor(0.1757)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.053082\n",
      "Epoch 7021\n",
      "-------------------------------\n",
      "tensor(16.1537)\n",
      "tensor(7.3921)\n",
      "tensor(3.7000)\n",
      "tensor(0.2100)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.048447\n",
      "Epoch 7022\n",
      "-------------------------------\n",
      "tensor(15.3833)\n",
      "tensor(6.9434)\n",
      "tensor(3.4755)\n",
      "tensor(0.2172)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.042522\n",
      "Epoch 7023\n",
      "-------------------------------\n",
      "tensor(26.5568)\n",
      "tensor(10.9018)\n",
      "tensor(3.0421)\n",
      "tensor(0.1926)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.035267\n",
      "Epoch 7024\n",
      "-------------------------------\n",
      "tensor(33.4589)\n",
      "tensor(12.2319)\n",
      "tensor(2.7478)\n",
      "tensor(0.0954)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.028370\n",
      "Epoch 7025\n",
      "-------------------------------\n",
      "tensor(41.9480)\n",
      "tensor(13.2159)\n",
      "tensor(3.5864)\n",
      "tensor(0.1329)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.048809\n",
      "Epoch 7026\n",
      "-------------------------------\n",
      "tensor(37.5581)\n",
      "tensor(13.9913)\n",
      "tensor(5.8987)\n",
      "tensor(0.4872)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.055817\n",
      "Epoch 7027\n",
      "-------------------------------\n",
      "tensor(40.0291)\n",
      "tensor(16.1211)\n",
      "tensor(8.2197)\n",
      "tensor(0.5945)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.074715\n",
      "Epoch 7028\n",
      "-------------------------------\n",
      "tensor(40.0052)\n",
      "tensor(12.2369)\n",
      "tensor(1.8545)\n",
      "tensor(0.0059)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.094437\n",
      "Epoch 7029\n",
      "-------------------------------\n",
      "tensor(15.8395)\n",
      "tensor(8.9976)\n",
      "tensor(7.8834)\n",
      "tensor(0.4118)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.062515\n",
      "Epoch 7030\n",
      "-------------------------------\n",
      "tensor(42.2786)\n",
      "tensor(18.6986)\n",
      "tensor(9.9869)\n",
      "tensor(0.0816)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.106117\n",
      "Epoch 7031\n",
      "-------------------------------\n",
      "tensor(48.8869)\n",
      "tensor(12.3966)\n",
      "tensor(27.3920)\n",
      "tensor(0.2223)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.115036\n",
      "Epoch 7032\n",
      "-------------------------------\n",
      "tensor(74.5743)\n",
      "tensor(30.0544)\n",
      "tensor(50.0763)\n",
      "tensor(1.4241)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.206329\n",
      "Epoch 7033\n",
      "-------------------------------\n",
      "tensor(91.7752)\n",
      "tensor(26.9160)\n",
      "tensor(64.4279)\n",
      "tensor(1.6695)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.232613\n",
      "Epoch 7034\n",
      "-------------------------------\n",
      "tensor(89.9499)\n",
      "tensor(32.2518)\n",
      "tensor(65.3678)\n",
      "tensor(1.6592)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.201126\n",
      "Epoch 7035\n",
      "-------------------------------\n",
      "tensor(47.0645)\n",
      "tensor(13.2344)\n",
      "tensor(29.9958)\n",
      "tensor(0.5764)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.085289\n",
      "Epoch 7036\n",
      "-------------------------------\n",
      "tensor(46.4697)\n",
      "tensor(15.9400)\n",
      "tensor(21.2310)\n",
      "tensor(0.5156)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.065563\n",
      "Epoch 7037\n",
      "-------------------------------\n",
      "tensor(52.6017)\n",
      "tensor(15.9260)\n",
      "tensor(25.3569)\n",
      "tensor(0.5926)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.066174\n",
      "Epoch 7038\n",
      "-------------------------------\n",
      "tensor(26.2782)\n",
      "tensor(6.0582)\n",
      "tensor(14.6700)\n",
      "tensor(0.3281)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.050579\n",
      "Epoch 7039\n",
      "-------------------------------\n",
      "tensor(30.9342)\n",
      "tensor(11.0674)\n",
      "tensor(7.9989)\n",
      "tensor(0.2590)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.040550\n",
      "Epoch 7040\n",
      "-------------------------------\n",
      "tensor(29.6400)\n",
      "tensor(11.6436)\n",
      "tensor(16.2184)\n",
      "tensor(0.4664)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.046177\n",
      "Epoch 7041\n",
      "-------------------------------\n",
      "tensor(30.8948)\n",
      "tensor(11.2380)\n",
      "tensor(13.7684)\n",
      "tensor(0.3991)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.039898\n",
      "Epoch 7042\n",
      "-------------------------------\n",
      "tensor(27.7585)\n",
      "tensor(11.4480)\n",
      "tensor(6.9626)\n",
      "tensor(0.2150)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.030418\n",
      "Epoch 7043\n",
      "-------------------------------\n",
      "tensor(15.2050)\n",
      "tensor(5.0264)\n",
      "tensor(3.9359)\n",
      "tensor(0.0748)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.020386\n",
      "Epoch 7044\n",
      "-------------------------------\n",
      "tensor(39.1192)\n",
      "tensor(12.7901)\n",
      "tensor(16.1476)\n",
      "tensor(0.4074)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.034832\n",
      "Epoch 7045\n",
      "-------------------------------\n",
      "tensor(44.9132)\n",
      "tensor(16.8079)\n",
      "tensor(15.9935)\n",
      "tensor(0.4629)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.023338\n",
      "Epoch 7046\n",
      "-------------------------------\n",
      "tensor(42.9897)\n",
      "tensor(12.3952)\n",
      "tensor(9.7543)\n",
      "tensor(0.0409)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.009083\n",
      "Epoch 7047\n",
      "-------------------------------\n",
      "tensor(47.1072)\n",
      "tensor(11.4514)\n",
      "tensor(20.8295)\n",
      "tensor(0.2023)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.045197\n",
      "Epoch 7048\n",
      "-------------------------------\n",
      "tensor(37.8575)\n",
      "tensor(14.8559)\n",
      "tensor(24.1225)\n",
      "tensor(0.7759)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.029305\n",
      "Epoch 7049\n",
      "-------------------------------\n",
      "tensor(37.1267)\n",
      "tensor(11.5028)\n",
      "tensor(3.2633)\n",
      "tensor(0.1797)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.049423\n",
      "Epoch 7050\n",
      "-------------------------------\n",
      "tensor(38.9104)\n",
      "tensor(31.9865)\n",
      "tensor(29.8726)\n",
      "tensor(0.8318)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.098148\n",
      "Epoch 7051\n",
      "-------------------------------\n",
      "tensor(37.5912)\n",
      "tensor(19.5389)\n",
      "tensor(34.8676)\n",
      "tensor(1.3542)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.082737\n",
      "Epoch 7052\n",
      "-------------------------------\n",
      "tensor(42.5071)\n",
      "tensor(9.5821)\n",
      "tensor(25.5526)\n",
      "tensor(0.5692)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.059399\n",
      "Epoch 7053\n",
      "-------------------------------\n",
      "tensor(64.2208)\n",
      "tensor(20.1382)\n",
      "tensor(25.4164)\n",
      "tensor(0.2466)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.121464\n",
      "Epoch 7054\n",
      "-------------------------------\n",
      "tensor(49.3136)\n",
      "tensor(13.9402)\n",
      "tensor(22.1483)\n",
      "tensor(0.1953)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.053665\n",
      "Epoch 7055\n",
      "-------------------------------\n",
      "tensor(20.6437)\n",
      "tensor(10.1323)\n",
      "tensor(15.2085)\n",
      "tensor(0.8506)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.024723\n",
      "Epoch 7056\n",
      "-------------------------------\n",
      "tensor(42.3245)\n",
      "tensor(11.7195)\n",
      "tensor(11.7018)\n",
      "tensor(0.2029)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.093361\n",
      "Epoch 7057\n",
      "-------------------------------\n",
      "tensor(38.1860)\n",
      "tensor(18.1419)\n",
      "tensor(17.5095)\n",
      "tensor(0.8345)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.079624\n",
      "Epoch 7058\n",
      "-------------------------------\n",
      "tensor(20.8648)\n",
      "tensor(14.2156)\n",
      "tensor(15.4406)\n",
      "tensor(0.6769)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.038849\n",
      "Epoch 7059\n",
      "-------------------------------\n",
      "tensor(33.4127)\n",
      "tensor(10.7219)\n",
      "tensor(2.3424)\n",
      "tensor(0.1063)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.037937\n",
      "Epoch 7060\n",
      "-------------------------------\n",
      "tensor(35.2107)\n",
      "tensor(10.3570)\n",
      "tensor(8.4262)\n",
      "tensor(0.2339)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.032043\n",
      "Epoch 7061\n",
      "-------------------------------\n",
      "tensor(18.8549)\n",
      "tensor(5.2279)\n",
      "tensor(8.8874)\n",
      "tensor(0.3246)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.026161\n",
      "Epoch 7062\n",
      "-------------------------------\n",
      "tensor(26.8180)\n",
      "tensor(8.3399)\n",
      "tensor(6.3915)\n",
      "tensor(0.2850)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.022591\n",
      "Epoch 7063\n",
      "-------------------------------\n",
      "tensor(26.1764)\n",
      "tensor(7.9625)\n",
      "tensor(2.2762)\n",
      "tensor(0.1447)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.017559\n",
      "Epoch 7064\n",
      "-------------------------------\n",
      "tensor(19.8784)\n",
      "tensor(5.1551)\n",
      "tensor(6.9259)\n",
      "tensor(0.0903)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.011349\n",
      "Epoch 7065\n",
      "-------------------------------\n",
      "tensor(24.8515)\n",
      "tensor(7.3566)\n",
      "tensor(8.3465)\n",
      "tensor(0.2473)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.006184\n",
      "Epoch 7066\n",
      "-------------------------------\n",
      "tensor(43.1751)\n",
      "tensor(13.0353)\n",
      "tensor(6.9345)\n",
      "tensor(0.0494)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.025394\n",
      "Epoch 7067\n",
      "-------------------------------\n",
      "tensor(57.6372)\n",
      "tensor(18.8080)\n",
      "tensor(9.4667)\n",
      "tensor(0.2730)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.029659\n",
      "Epoch 7068\n",
      "-------------------------------\n",
      "tensor(43.2073)\n",
      "tensor(13.2542)\n",
      "tensor(16.2253)\n",
      "tensor(0.2531)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.058319\n",
      "Epoch 7069\n",
      "-------------------------------\n",
      "tensor(29.8520)\n",
      "tensor(13.1103)\n",
      "tensor(14.0381)\n",
      "tensor(0.6761)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.032494\n",
      "Epoch 7070\n",
      "-------------------------------\n",
      "tensor(37.2039)\n",
      "tensor(25.9577)\n",
      "tensor(7.6683)\n",
      "tensor(0.5666)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.068306\n",
      "Epoch 7071\n",
      "-------------------------------\n",
      "tensor(42.8660)\n",
      "tensor(18.6005)\n",
      "tensor(32.8297)\n",
      "tensor(1.0505)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.077557\n",
      "Epoch 7072\n",
      "-------------------------------\n",
      "tensor(79.2918)\n",
      "tensor(35.3288)\n",
      "tensor(61.3694)\n",
      "tensor(1.9651)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.214676\n",
      "Epoch 7073\n",
      "-------------------------------\n",
      "tensor(112.9441)\n",
      "tensor(26.2444)\n",
      "tensor(74.6547)\n",
      "tensor(1.4236)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.266647\n",
      "Epoch 7074\n",
      "-------------------------------\n",
      "tensor(115.6932)\n",
      "tensor(39.4215)\n",
      "tensor(81.1919)\n",
      "tensor(1.8226)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.304405\n",
      "Epoch 7075\n",
      "-------------------------------\n",
      "tensor(63.3521)\n",
      "tensor(21.7497)\n",
      "tensor(44.0269)\n",
      "tensor(1.2715)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.067287\n",
      "Epoch 7076\n",
      "-------------------------------\n",
      "tensor(50.5619)\n",
      "tensor(15.7405)\n",
      "tensor(26.2373)\n",
      "tensor(0.4866)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.050388\n",
      "Epoch 7077\n",
      "-------------------------------\n",
      "tensor(69.8928)\n",
      "tensor(23.5224)\n",
      "tensor(39.4738)\n",
      "tensor(1.2772)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.109230\n",
      "Epoch 7078\n",
      "-------------------------------\n",
      "tensor(43.0378)\n",
      "tensor(12.7069)\n",
      "tensor(20.6019)\n",
      "tensor(0.7143)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.038124\n",
      "Epoch 7079\n",
      "-------------------------------\n",
      "tensor(26.7300)\n",
      "tensor(11.0713)\n",
      "tensor(15.1925)\n",
      "tensor(0.3408)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.038303\n",
      "Epoch 7080\n",
      "-------------------------------\n",
      "tensor(34.1161)\n",
      "tensor(16.2177)\n",
      "tensor(24.9242)\n",
      "tensor(0.7485)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.052467\n",
      "Epoch 7081\n",
      "-------------------------------\n",
      "tensor(33.1468)\n",
      "tensor(15.5008)\n",
      "tensor(19.7833)\n",
      "tensor(0.7005)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.042889\n",
      "Epoch 7082\n",
      "-------------------------------\n",
      "tensor(26.7669)\n",
      "tensor(11.6455)\n",
      "tensor(9.3186)\n",
      "tensor(0.4544)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.028736\n",
      "Epoch 7083\n",
      "-------------------------------\n",
      "tensor(19.3064)\n",
      "tensor(5.9978)\n",
      "tensor(7.5965)\n",
      "tensor(0.0468)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.020985\n",
      "Epoch 7084\n",
      "-------------------------------\n",
      "tensor(33.7914)\n",
      "tensor(10.0642)\n",
      "tensor(23.2002)\n",
      "tensor(0.4487)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.028343\n",
      "Epoch 7085\n",
      "-------------------------------\n",
      "tensor(36.3732)\n",
      "tensor(13.8979)\n",
      "tensor(21.3284)\n",
      "tensor(0.5811)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.005135\n",
      "Epoch 7086\n",
      "-------------------------------\n",
      "tensor(39.6175)\n",
      "tensor(9.6249)\n",
      "tensor(13.1345)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.992790\n",
      "Epoch 7087\n",
      "-------------------------------\n",
      "tensor(64.8944)\n",
      "tensor(16.9233)\n",
      "tensor(23.8619)\n",
      "tensor(0.1869)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.043499\n",
      "Epoch 7088\n",
      "-------------------------------\n",
      "tensor(57.1181)\n",
      "tensor(24.9710)\n",
      "tensor(26.0899)\n",
      "tensor(1.0671)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.032379\n",
      "Epoch 7089\n",
      "-------------------------------\n",
      "tensor(30.8529)\n",
      "tensor(11.8610)\n",
      "tensor(3.3218)\n",
      "tensor(0.1541)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.038113\n",
      "Epoch 7090\n",
      "-------------------------------\n",
      "tensor(31.7478)\n",
      "tensor(33.4257)\n",
      "tensor(23.9633)\n",
      "tensor(0.8979)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.114605\n",
      "Epoch 7091\n",
      "-------------------------------\n",
      "tensor(34.2769)\n",
      "tensor(9.7184)\n",
      "tensor(20.3964)\n",
      "tensor(0.4607)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.116287\n",
      "Epoch 7092\n",
      "-------------------------------\n",
      "tensor(31.5110)\n",
      "tensor(8.1130)\n",
      "tensor(16.8301)\n",
      "tensor(0.2987)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.095985\n",
      "Epoch 7093\n",
      "-------------------------------\n",
      "tensor(33.4806)\n",
      "tensor(14.3183)\n",
      "tensor(19.1071)\n",
      "tensor(0.4700)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.089497\n",
      "Epoch 7094\n",
      "-------------------------------\n",
      "tensor(34.8743)\n",
      "tensor(8.8092)\n",
      "tensor(10.9452)\n",
      "tensor(0.2117)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.061489\n",
      "Epoch 7095\n",
      "-------------------------------\n",
      "tensor(30.1892)\n",
      "tensor(11.5787)\n",
      "tensor(4.9816)\n",
      "tensor(0.2839)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.046341\n",
      "Epoch 7096\n",
      "-------------------------------\n",
      "tensor(40.5483)\n",
      "tensor(14.4263)\n",
      "tensor(10.0089)\n",
      "tensor(0.4148)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.029655\n",
      "Epoch 7097\n",
      "-------------------------------\n",
      "tensor(31.5677)\n",
      "tensor(9.5847)\n",
      "tensor(6.3431)\n",
      "tensor(0.0566)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.005302\n",
      "Epoch 7098\n",
      "-------------------------------\n",
      "tensor(19.2818)\n",
      "tensor(7.6541)\n",
      "tensor(7.4281)\n",
      "tensor(0.2059)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.985863\n",
      "Epoch 7099\n",
      "-------------------------------\n",
      "tensor(33.3505)\n",
      "tensor(11.0103)\n",
      "tensor(2.3664)\n",
      "tensor(0.1470)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.019089\n",
      "Epoch 7100\n",
      "-------------------------------\n",
      "tensor(40.8161)\n",
      "tensor(12.5191)\n",
      "tensor(3.7553)\n",
      "tensor(0.0846)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.025379\n",
      "Epoch 7101\n",
      "-------------------------------\n",
      "tensor(40.5434)\n",
      "tensor(12.4429)\n",
      "tensor(3.3699)\n",
      "tensor(0.0590)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.008682\n",
      "Epoch 7102\n",
      "-------------------------------\n",
      "tensor(39.3133)\n",
      "tensor(12.2775)\n",
      "tensor(1.9827)\n",
      "tensor(0.0525)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.984943\n",
      "Epoch 7103\n",
      "-------------------------------\n",
      "tensor(26.7362)\n",
      "tensor(8.6684)\n",
      "tensor(3.4996)\n",
      "tensor(0.0586)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.978954\n",
      "Epoch 7104\n",
      "-------------------------------\n",
      "tensor(20.1205)\n",
      "tensor(5.6726)\n",
      "tensor(6.5940)\n",
      "tensor(0.0428)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.981834\n",
      "Epoch 7105\n",
      "-------------------------------\n",
      "tensor(17.6421)\n",
      "tensor(4.4254)\n",
      "tensor(3.4363)\n",
      "tensor(0.0831)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.978600\n",
      "Epoch 7106\n",
      "-------------------------------\n",
      "tensor(31.1475)\n",
      "tensor(10.7755)\n",
      "tensor(8.2105)\n",
      "tensor(0.2625)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.995121\n",
      "Epoch 7107\n",
      "-------------------------------\n",
      "tensor(45.4709)\n",
      "tensor(15.7209)\n",
      "tensor(6.4668)\n",
      "tensor(0.0921)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.054230\n",
      "Epoch 7108\n",
      "-------------------------------\n",
      "tensor(46.3750)\n",
      "tensor(14.5933)\n",
      "tensor(13.1434)\n",
      "tensor(0.1630)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.089905\n",
      "Epoch 7109\n",
      "-------------------------------\n",
      "tensor(30.9329)\n",
      "tensor(14.9519)\n",
      "tensor(10.5343)\n",
      "tensor(0.6362)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.046219\n",
      "Epoch 7110\n",
      "-------------------------------\n",
      "tensor(18.1393)\n",
      "tensor(4.6646)\n",
      "tensor(5.2291)\n",
      "tensor(0.1853)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.042717\n",
      "Epoch 7111\n",
      "-------------------------------\n",
      "tensor(28.4942)\n",
      "tensor(29.7379)\n",
      "tensor(16.6534)\n",
      "tensor(0.8591)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.083843\n",
      "Epoch 7112\n",
      "-------------------------------\n",
      "tensor(15.6702)\n",
      "tensor(4.9644)\n",
      "tensor(2.8229)\n",
      "tensor(0.2002)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.028950\n",
      "Epoch 7113\n",
      "-------------------------------\n",
      "tensor(27.9311)\n",
      "tensor(16.3657)\n",
      "tensor(15.8952)\n",
      "tensor(0.7995)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.031925\n",
      "Epoch 7114\n",
      "-------------------------------\n",
      "tensor(49.0703)\n",
      "tensor(12.3959)\n",
      "tensor(16.0873)\n",
      "tensor(0.2849)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.995422\n",
      "Epoch 7115\n",
      "-------------------------------\n",
      "tensor(42.6697)\n",
      "tensor(14.6605)\n",
      "tensor(10.7722)\n",
      "tensor(0.2707)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.964882\n",
      "Epoch 7116\n",
      "-------------------------------\n",
      "tensor(15.1006)\n",
      "tensor(4.9686)\n",
      "tensor(3.7467)\n",
      "tensor(0.2219)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.947025\n",
      "Epoch 7117\n",
      "-------------------------------\n",
      "tensor(33.5274)\n",
      "tensor(10.7208)\n",
      "tensor(5.8843)\n",
      "tensor(0.0767)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.950947\n",
      "Epoch 7118\n",
      "-------------------------------\n",
      "tensor(27.4797)\n",
      "tensor(9.5774)\n",
      "tensor(4.7379)\n",
      "tensor(0.1307)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.945591\n",
      "Epoch 7119\n",
      "-------------------------------\n",
      "tensor(17.1423)\n",
      "tensor(6.4591)\n",
      "tensor(2.4290)\n",
      "tensor(0.0853)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.939896\n",
      "Epoch 7120\n",
      "-------------------------------\n",
      "tensor(40.1548)\n",
      "tensor(12.3485)\n",
      "tensor(3.1996)\n",
      "tensor(0.0733)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.946239\n",
      "Epoch 7121\n",
      "-------------------------------\n",
      "tensor(30.6416)\n",
      "tensor(9.7579)\n",
      "tensor(2.5340)\n",
      "tensor(0.0781)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.935005\n",
      "Epoch 7122\n",
      "-------------------------------\n",
      "tensor(15.9465)\n",
      "tensor(6.0097)\n",
      "tensor(1.9341)\n",
      "tensor(0.0894)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.926830\n",
      "Epoch 7123\n",
      "-------------------------------\n",
      "tensor(26.0609)\n",
      "tensor(8.6037)\n",
      "tensor(2.6452)\n",
      "tensor(0.1010)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.929024\n",
      "Epoch 7124\n",
      "-------------------------------\n",
      "tensor(44.8658)\n",
      "tensor(13.9238)\n",
      "tensor(3.6451)\n",
      "tensor(0.0787)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.935246\n",
      "Epoch 7125\n",
      "-------------------------------\n",
      "tensor(36.9013)\n",
      "tensor(11.2900)\n",
      "tensor(4.3214)\n",
      "tensor(0.0548)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.944275\n",
      "Epoch 7126\n",
      "-------------------------------\n",
      "tensor(36.4313)\n",
      "tensor(12.5567)\n",
      "tensor(3.8117)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.937965\n",
      "Epoch 7127\n",
      "-------------------------------\n",
      "tensor(26.5789)\n",
      "tensor(10.8606)\n",
      "tensor(6.9007)\n",
      "tensor(0.4158)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.938515\n",
      "Epoch 7128\n",
      "-------------------------------\n",
      "tensor(31.4602)\n",
      "tensor(11.7642)\n",
      "tensor(2.9339)\n",
      "tensor(0.0366)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.965595\n",
      "Epoch 7129\n",
      "-------------------------------\n",
      "tensor(31.2445)\n",
      "tensor(11.8101)\n",
      "tensor(6.0817)\n",
      "tensor(0.2775)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.992783\n",
      "Epoch 7130\n",
      "-------------------------------\n",
      "tensor(27.8688)\n",
      "tensor(10.9736)\n",
      "tensor(6.9016)\n",
      "tensor(0.2578)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.000492\n",
      "Epoch 7131\n",
      "-------------------------------\n",
      "tensor(21.2556)\n",
      "tensor(6.3980)\n",
      "tensor(6.8274)\n",
      "tensor(0.0116)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.004517\n",
      "Epoch 7132\n",
      "-------------------------------\n",
      "tensor(39.6522)\n",
      "tensor(19.1651)\n",
      "tensor(8.4749)\n",
      "tensor(0.0406)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.999786\n",
      "Epoch 7133\n",
      "-------------------------------\n",
      "tensor(48.0397)\n",
      "tensor(11.7732)\n",
      "tensor(24.4161)\n",
      "tensor(0.4957)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.975971\n",
      "Epoch 7134\n",
      "-------------------------------\n",
      "tensor(55.8077)\n",
      "tensor(28.2456)\n",
      "tensor(37.1664)\n",
      "tensor(1.3226)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.037872\n",
      "Epoch 7135\n",
      "-------------------------------\n",
      "tensor(52.5325)\n",
      "tensor(12.0974)\n",
      "tensor(20.9796)\n",
      "tensor(0.1784)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.984882\n",
      "Epoch 7136\n",
      "-------------------------------\n",
      "tensor(34.2290)\n",
      "tensor(10.0027)\n",
      "tensor(6.1003)\n",
      "tensor(0.0647)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.943592\n",
      "Epoch 7137\n",
      "-------------------------------\n",
      "tensor(50.8042)\n",
      "tensor(14.8236)\n",
      "tensor(18.0806)\n",
      "tensor(0.3541)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.966320\n",
      "Epoch 7138\n",
      "-------------------------------\n",
      "tensor(23.3912)\n",
      "tensor(7.2453)\n",
      "tensor(3.7004)\n",
      "tensor(0.0132)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.954735\n",
      "Epoch 7139\n",
      "-------------------------------\n",
      "tensor(26.4680)\n",
      "tensor(10.8125)\n",
      "tensor(10.4313)\n",
      "tensor(0.2966)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.965286\n",
      "Epoch 7140\n",
      "-------------------------------\n",
      "tensor(22.3360)\n",
      "tensor(10.0403)\n",
      "tensor(11.6171)\n",
      "tensor(0.3190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.966988\n",
      "Epoch 7141\n",
      "-------------------------------\n",
      "tensor(19.1699)\n",
      "tensor(8.3202)\n",
      "tensor(7.2914)\n",
      "tensor(0.2161)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.959927\n",
      "Epoch 7142\n",
      "-------------------------------\n",
      "tensor(30.4275)\n",
      "tensor(10.0128)\n",
      "tensor(2.2412)\n",
      "tensor(0.0853)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.957001\n",
      "Epoch 7143\n",
      "-------------------------------\n",
      "tensor(31.7060)\n",
      "tensor(9.3398)\n",
      "tensor(5.4706)\n",
      "tensor(0.0717)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.952408\n",
      "Epoch 7144\n",
      "-------------------------------\n",
      "tensor(19.7617)\n",
      "tensor(5.5737)\n",
      "tensor(9.8926)\n",
      "tensor(0.2062)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.937992\n",
      "Epoch 7145\n",
      "-------------------------------\n",
      "tensor(38.4922)\n",
      "tensor(11.9771)\n",
      "tensor(5.8472)\n",
      "tensor(0.1319)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.946846\n",
      "Epoch 7146\n",
      "-------------------------------\n",
      "tensor(40.1807)\n",
      "tensor(11.3704)\n",
      "tensor(9.5289)\n",
      "tensor(0.1101)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.934708\n",
      "Epoch 7147\n",
      "-------------------------------\n",
      "tensor(42.2340)\n",
      "tensor(14.5519)\n",
      "tensor(8.3503)\n",
      "tensor(0.1114)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.951431\n",
      "Epoch 7148\n",
      "-------------------------------\n",
      "tensor(42.7278)\n",
      "tensor(19.8153)\n",
      "tensor(17.0157)\n",
      "tensor(0.7509)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.003975\n",
      "Epoch 7149\n",
      "-------------------------------\n",
      "tensor(40.4809)\n",
      "tensor(14.2159)\n",
      "tensor(5.6854)\n",
      "tensor(0.0230)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.998184\n",
      "Epoch 7150\n",
      "-------------------------------\n",
      "tensor(27.3775)\n",
      "tensor(11.9715)\n",
      "tensor(11.3236)\n",
      "tensor(0.3756)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.986465\n",
      "Epoch 7151\n",
      "-------------------------------\n",
      "tensor(56.4718)\n",
      "tensor(17.2528)\n",
      "tensor(20.6796)\n",
      "tensor(0.2905)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.042328\n",
      "Epoch 7152\n",
      "-------------------------------\n",
      "tensor(70.7105)\n",
      "tensor(20.5436)\n",
      "tensor(47.7023)\n",
      "tensor(0.9656)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.101006\n",
      "Epoch 7153\n",
      "-------------------------------\n",
      "tensor(91.1497)\n",
      "tensor(37.7913)\n",
      "tensor(75.6464)\n",
      "tensor(2.0205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.188129\n",
      "Epoch 7154\n",
      "-------------------------------\n",
      "tensor(107.8169)\n",
      "tensor(29.1930)\n",
      "tensor(72.7660)\n",
      "tensor(1.9304)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.220673\n",
      "Epoch 7155\n",
      "-------------------------------\n",
      "tensor(57.4998)\n",
      "tensor(25.7270)\n",
      "tensor(33.1540)\n",
      "tensor(1.0845)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.039120\n",
      "Epoch 7156\n",
      "-------------------------------\n",
      "tensor(43.5107)\n",
      "tensor(21.1480)\n",
      "tensor(30.7965)\n",
      "tensor(1.2209)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.015007\n",
      "Epoch 7157\n",
      "-------------------------------\n",
      "tensor(52.7196)\n",
      "tensor(16.6197)\n",
      "tensor(31.3587)\n",
      "tensor(0.6025)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.065968\n",
      "Epoch 7158\n",
      "-------------------------------\n",
      "tensor(39.8180)\n",
      "tensor(17.7520)\n",
      "tensor(23.8811)\n",
      "tensor(0.7604)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.055664\n",
      "Epoch 7159\n",
      "-------------------------------\n",
      "tensor(34.3422)\n",
      "tensor(11.1680)\n",
      "tensor(6.5055)\n",
      "tensor(0.1950)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.018330\n",
      "Epoch 7160\n",
      "-------------------------------\n",
      "tensor(33.3964)\n",
      "tensor(7.3296)\n",
      "tensor(17.0766)\n",
      "tensor(0.0952)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.017502\n",
      "Epoch 7161\n",
      "-------------------------------\n",
      "tensor(30.9165)\n",
      "tensor(6.6392)\n",
      "tensor(15.6603)\n",
      "tensor(0.1035)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.011040\n",
      "Epoch 7162\n",
      "-------------------------------\n",
      "tensor(22.5708)\n",
      "tensor(6.2947)\n",
      "tensor(8.6957)\n",
      "tensor(0.0115)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.996407\n",
      "Epoch 7163\n",
      "-------------------------------\n",
      "tensor(30.2647)\n",
      "tensor(10.4811)\n",
      "tensor(4.2242)\n",
      "tensor(0.2139)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.988876\n",
      "Epoch 7164\n",
      "-------------------------------\n",
      "tensor(38.5949)\n",
      "tensor(12.3092)\n",
      "tensor(18.0163)\n",
      "tensor(0.4065)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.989269\n",
      "Epoch 7165\n",
      "-------------------------------\n",
      "tensor(35.7981)\n",
      "tensor(6.9797)\n",
      "tensor(18.5081)\n",
      "tensor(0.2015)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.968960\n",
      "Epoch 7166\n",
      "-------------------------------\n",
      "tensor(27.8051)\n",
      "tensor(14.6739)\n",
      "tensor(13.8779)\n",
      "tensor(0.6568)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.939835\n",
      "Epoch 7167\n",
      "-------------------------------\n",
      "tensor(56.7195)\n",
      "tensor(24.9795)\n",
      "tensor(28.7930)\n",
      "tensor(0.8653)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.989731\n",
      "Epoch 7168\n",
      "-------------------------------\n",
      "tensor(61.3280)\n",
      "tensor(16.7457)\n",
      "tensor(27.0851)\n",
      "tensor(0.5960)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.002205\n",
      "Epoch 7169\n",
      "-------------------------------\n",
      "tensor(34.0533)\n",
      "tensor(12.8714)\n",
      "tensor(4.5589)\n",
      "tensor(0.1910)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.957886\n",
      "Epoch 7170\n",
      "-------------------------------\n",
      "tensor(64.0761)\n",
      "tensor(18.6032)\n",
      "tensor(34.9752)\n",
      "tensor(1.0082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.998917\n",
      "Epoch 7171\n",
      "-------------------------------\n",
      "tensor(80.3339)\n",
      "tensor(37.9664)\n",
      "tensor(65.5331)\n",
      "tensor(1.8111)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.131363\n",
      "Epoch 7172\n",
      "-------------------------------\n",
      "tensor(135.8132)\n",
      "tensor(22.9895)\n",
      "tensor(81.8268)\n",
      "tensor(1.6824)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.329880\n",
      "Epoch 7173\n",
      "-------------------------------\n",
      "tensor(157.2122)\n",
      "tensor(49.2250)\n",
      "tensor(114.9152)\n",
      "tensor(2.7244)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.479649\n",
      "Epoch 7174\n",
      "-------------------------------\n",
      "tensor(156.1056)\n",
      "tensor(65.3255)\n",
      "tensor(124.0172)\n",
      "tensor(3.4904)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.529259\n",
      "Epoch 7175\n",
      "-------------------------------\n",
      "tensor(97.5766)\n",
      "tensor(19.4469)\n",
      "tensor(59.0530)\n",
      "tensor(1.1637)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.152321\n",
      "Epoch 7176\n",
      "-------------------------------\n",
      "tensor(62.1760)\n",
      "tensor(12.1133)\n",
      "tensor(36.1633)\n",
      "tensor(0.7387)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.005928\n",
      "Epoch 7177\n",
      "-------------------------------\n",
      "tensor(81.2789)\n",
      "tensor(30.5105)\n",
      "tensor(61.7073)\n",
      "tensor(1.5571)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.065598\n",
      "Epoch 7178\n",
      "-------------------------------\n",
      "tensor(48.2215)\n",
      "tensor(15.6069)\n",
      "tensor(33.6352)\n",
      "tensor(0.7812)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.963181\n",
      "Epoch 7179\n",
      "-------------------------------\n",
      "tensor(40.0182)\n",
      "tensor(14.7084)\n",
      "tensor(19.7607)\n",
      "tensor(0.5772)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.941513\n",
      "Epoch 7180\n",
      "-------------------------------\n",
      "tensor(51.2128)\n",
      "tensor(20.1264)\n",
      "tensor(37.8028)\n",
      "tensor(1.0031)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.969601\n",
      "Epoch 7181\n",
      "-------------------------------\n",
      "tensor(43.0888)\n",
      "tensor(17.3946)\n",
      "tensor(31.1032)\n",
      "tensor(0.8208)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.947781\n",
      "Epoch 7182\n",
      "-------------------------------\n",
      "tensor(24.5903)\n",
      "tensor(10.5895)\n",
      "tensor(14.4794)\n",
      "tensor(0.3975)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.912132\n",
      "Epoch 7183\n",
      "-------------------------------\n",
      "tensor(45.9469)\n",
      "tensor(13.3439)\n",
      "tensor(10.6977)\n",
      "tensor(0.2232)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.905449\n",
      "Epoch 7184\n",
      "-------------------------------\n",
      "tensor(59.5592)\n",
      "tensor(17.4542)\n",
      "tensor(35.6567)\n",
      "tensor(0.8852)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.941120\n",
      "Epoch 7185\n",
      "-------------------------------\n",
      "tensor(55.8293)\n",
      "tensor(17.4909)\n",
      "tensor(30.5765)\n",
      "tensor(0.8124)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.925957\n",
      "Epoch 7186\n",
      "-------------------------------\n",
      "tensor(47.4276)\n",
      "tensor(14.0405)\n",
      "tensor(25.2616)\n",
      "tensor(0.5119)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.925117\n",
      "Epoch 7187\n",
      "-------------------------------\n",
      "tensor(66.6706)\n",
      "tensor(20.3453)\n",
      "tensor(39.6749)\n",
      "tensor(0.8479)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.964542\n",
      "Epoch 7188\n",
      "-------------------------------\n",
      "tensor(72.3641)\n",
      "tensor(28.0733)\n",
      "tensor(46.2194)\n",
      "tensor(1.3358)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.022896\n",
      "Epoch 7189\n",
      "-------------------------------\n",
      "tensor(26.5217)\n",
      "tensor(10.2177)\n",
      "tensor(6.0634)\n",
      "tensor(0.0413)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.955276\n",
      "Epoch 7190\n",
      "-------------------------------\n",
      "tensor(49.3045)\n",
      "tensor(18.6327)\n",
      "tensor(34.3382)\n",
      "tensor(0.9471)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.987495\n",
      "Epoch 7191\n",
      "-------------------------------\n",
      "tensor(93.8274)\n",
      "tensor(26.3915)\n",
      "tensor(64.0568)\n",
      "tensor(1.4065)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.161880\n",
      "Epoch 7192\n",
      "-------------------------------\n",
      "tensor(123.6163)\n",
      "tensor(41.9313)\n",
      "tensor(91.7569)\n",
      "tensor(2.2118)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.298725\n",
      "Epoch 7193\n",
      "-------------------------------\n",
      "tensor(169.8914)\n",
      "tensor(45.3802)\n",
      "tensor(113.1301)\n",
      "tensor(2.9553)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.519180\n",
      "Epoch 7194\n",
      "-------------------------------\n",
      "tensor(174.1772)\n",
      "tensor(48.7337)\n",
      "tensor(122.8673)\n",
      "tensor(2.6296)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.615307\n",
      "Epoch 7195\n",
      "-------------------------------\n",
      "tensor(91.5679)\n",
      "tensor(38.0856)\n",
      "tensor(73.4724)\n",
      "tensor(2.0601)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.169346\n",
      "Epoch 7196\n",
      "-------------------------------\n",
      "tensor(48.2083)\n",
      "tensor(18.5169)\n",
      "tensor(37.5374)\n",
      "tensor(0.7929)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.034920\n",
      "Epoch 7197\n",
      "-------------------------------\n",
      "tensor(87.2171)\n",
      "tensor(29.2381)\n",
      "tensor(62.9234)\n",
      "tensor(1.7952)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.134151\n",
      "Epoch 7198\n",
      "-------------------------------\n",
      "tensor(48.4414)\n",
      "tensor(15.2503)\n",
      "tensor(32.8006)\n",
      "tensor(0.8451)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.011524\n",
      "Epoch 7199\n",
      "-------------------------------\n",
      "tensor(30.0530)\n",
      "tensor(11.4252)\n",
      "tensor(22.8539)\n",
      "tensor(0.6782)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.974133\n",
      "Epoch 7200\n",
      "-------------------------------\n",
      "tensor(56.2991)\n",
      "tensor(19.9864)\n",
      "tensor(40.3862)\n",
      "tensor(1.1430)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.007736\n",
      "Epoch 7201\n",
      "-------------------------------\n",
      "tensor(47.7859)\n",
      "tensor(16.5860)\n",
      "tensor(32.4355)\n",
      "tensor(0.9266)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.981388\n",
      "Epoch 7202\n",
      "-------------------------------\n",
      "tensor(31.5233)\n",
      "tensor(10.2278)\n",
      "tensor(14.6124)\n",
      "tensor(0.4434)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.939335\n",
      "Epoch 7203\n",
      "-------------------------------\n",
      "tensor(23.6123)\n",
      "tensor(7.7314)\n",
      "tensor(11.7009)\n",
      "tensor(0.2723)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.930508\n",
      "Epoch 7204\n",
      "-------------------------------\n",
      "tensor(53.2866)\n",
      "tensor(19.4010)\n",
      "tensor(38.7268)\n",
      "tensor(1.0544)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.966282\n",
      "Epoch 7205\n",
      "-------------------------------\n",
      "tensor(55.1541)\n",
      "tensor(20.2589)\n",
      "tensor(34.3322)\n",
      "tensor(1.0608)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.974277\n",
      "Epoch 7206\n",
      "-------------------------------\n",
      "tensor(58.1879)\n",
      "tensor(17.0475)\n",
      "tensor(22.0808)\n",
      "tensor(0.4190)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.995884\n",
      "Epoch 7207\n",
      "-------------------------------\n",
      "tensor(62.7603)\n",
      "tensor(24.3009)\n",
      "tensor(39.6674)\n",
      "tensor(1.2416)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.027161\n",
      "Epoch 7208\n",
      "-------------------------------\n",
      "tensor(74.9256)\n",
      "tensor(22.4423)\n",
      "tensor(45.1951)\n",
      "tensor(0.6536)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.063881\n",
      "Epoch 7209\n",
      "-------------------------------\n",
      "tensor(35.8367)\n",
      "tensor(13.1468)\n",
      "tensor(3.9076)\n",
      "tensor(0.1849)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.015526\n",
      "Epoch 7210\n",
      "-------------------------------\n",
      "tensor(67.9822)\n",
      "tensor(15.3165)\n",
      "tensor(38.9032)\n",
      "tensor(0.5905)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.096218\n",
      "Epoch 7211\n",
      "-------------------------------\n",
      "tensor(102.0668)\n",
      "tensor(39.2124)\n",
      "tensor(78.4843)\n",
      "tensor(2.1506)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.285770\n",
      "Epoch 7212\n",
      "-------------------------------\n",
      "tensor(136.0982)\n",
      "tensor(49.5960)\n",
      "tensor(105.9197)\n",
      "tensor(3.0187)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.431694\n",
      "Epoch 7213\n",
      "-------------------------------\n",
      "tensor(164.9243)\n",
      "tensor(51.4391)\n",
      "tensor(120.3160)\n",
      "tensor(2.8320)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.576439\n",
      "Epoch 7214\n",
      "-------------------------------\n",
      "tensor(152.8180)\n",
      "tensor(49.1626)\n",
      "tensor(111.4385)\n",
      "tensor(2.4930)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.581726\n",
      "Epoch 7215\n",
      "-------------------------------\n",
      "tensor(69.1657)\n",
      "tensor(21.8997)\n",
      "tensor(45.9462)\n",
      "tensor(1.2469)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.160618\n",
      "Epoch 7216\n",
      "-------------------------------\n",
      "tensor(71.5858)\n",
      "tensor(18.6245)\n",
      "tensor(45.2738)\n",
      "tensor(0.8279)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.102383\n",
      "Epoch 7217\n",
      "-------------------------------\n",
      "tensor(66.9803)\n",
      "tensor(26.1841)\n",
      "tensor(52.9344)\n",
      "tensor(1.6175)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.100266\n",
      "Epoch 7218\n",
      "-------------------------------\n",
      "tensor(50.8949)\n",
      "tensor(17.1348)\n",
      "tensor(39.3097)\n",
      "tensor(1.0268)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.044212\n",
      "Epoch 7219\n",
      "-------------------------------\n",
      "tensor(20.1113)\n",
      "tensor(9.3229)\n",
      "tensor(10.6790)\n",
      "tensor(0.4254)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.980267\n",
      "Epoch 7220\n",
      "-------------------------------\n",
      "tensor(42.2555)\n",
      "tensor(18.1964)\n",
      "tensor(31.9462)\n",
      "tensor(1.0418)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.001144\n",
      "Epoch 7221\n",
      "-------------------------------\n",
      "tensor(39.8201)\n",
      "tensor(17.1727)\n",
      "tensor(29.8726)\n",
      "tensor(0.9855)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.985466\n",
      "Epoch 7222\n",
      "-------------------------------\n",
      "tensor(25.7731)\n",
      "tensor(11.5699)\n",
      "tensor(17.1341)\n",
      "tensor(0.6195)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.952396\n",
      "Epoch 7223\n",
      "-------------------------------\n",
      "tensor(31.4423)\n",
      "tensor(9.6911)\n",
      "tensor(6.0016)\n",
      "tensor(0.0205)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.935287\n",
      "Epoch 7224\n",
      "-------------------------------\n",
      "tensor(49.4286)\n",
      "tensor(17.2985)\n",
      "tensor(31.6025)\n",
      "tensor(0.8534)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.959164\n",
      "Epoch 7225\n",
      "-------------------------------\n",
      "tensor(59.0688)\n",
      "tensor(23.0388)\n",
      "tensor(34.8190)\n",
      "tensor(1.1585)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.921440\n",
      "Epoch 7226\n",
      "-------------------------------\n",
      "tensor(44.1139)\n",
      "tensor(13.6365)\n",
      "tensor(17.1442)\n",
      "tensor(0.0566)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.899773\n",
      "Epoch 7227\n",
      "-------------------------------\n",
      "tensor(72.4096)\n",
      "tensor(22.5055)\n",
      "tensor(41.4926)\n",
      "tensor(0.7700)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.000320\n",
      "Epoch 7228\n",
      "-------------------------------\n",
      "tensor(64.9728)\n",
      "tensor(20.2620)\n",
      "tensor(34.0181)\n",
      "tensor(0.8322)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.993317\n",
      "Epoch 7229\n",
      "-------------------------------\n",
      "tensor(37.2372)\n",
      "tensor(11.5095)\n",
      "tensor(4.1236)\n",
      "tensor(0.0481)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.946068\n",
      "Epoch 7230\n",
      "-------------------------------\n",
      "tensor(63.7847)\n",
      "tensor(25.5053)\n",
      "tensor(47.0960)\n",
      "tensor(1.1465)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.040852\n",
      "Epoch 7231\n",
      "-------------------------------\n",
      "tensor(102.2961)\n",
      "tensor(31.4958)\n",
      "tensor(75.2336)\n",
      "tensor(1.9397)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.177925\n",
      "Epoch 7232\n",
      "-------------------------------\n",
      "tensor(131.1579)\n",
      "tensor(44.4040)\n",
      "tensor(98.6878)\n",
      "tensor(2.5362)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.329483\n",
      "Epoch 7233\n",
      "-------------------------------\n",
      "tensor(154.6060)\n",
      "tensor(54.8690)\n",
      "tensor(116.6061)\n",
      "tensor(2.9161)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.509338\n",
      "Epoch 7234\n",
      "-------------------------------\n",
      "tensor(147.2151)\n",
      "tensor(40.9118)\n",
      "tensor(103.5940)\n",
      "tensor(2.2958)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.388378\n",
      "Epoch 7235\n",
      "-------------------------------\n",
      "tensor(72.1766)\n",
      "tensor(29.1350)\n",
      "tensor(45.0355)\n",
      "tensor(1.4388)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.021271\n",
      "Epoch 7236\n",
      "-------------------------------\n",
      "tensor(54.4896)\n",
      "tensor(19.7923)\n",
      "tensor(39.9514)\n",
      "tensor(1.1971)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.004234\n",
      "Epoch 7237\n",
      "-------------------------------\n",
      "tensor(60.0267)\n",
      "tensor(23.6794)\n",
      "tensor(45.1463)\n",
      "tensor(1.2075)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.020893\n",
      "Epoch 7238\n",
      "-------------------------------\n",
      "tensor(40.5730)\n",
      "tensor(20.1518)\n",
      "tensor(32.1909)\n",
      "tensor(1.0430)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.975391\n",
      "Epoch 7239\n",
      "-------------------------------\n",
      "tensor(24.2642)\n",
      "tensor(6.1971)\n",
      "tensor(10.7476)\n",
      "tensor(0.0071)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.932274\n",
      "Epoch 7240\n",
      "-------------------------------\n",
      "tensor(45.3493)\n",
      "tensor(10.8711)\n",
      "tensor(27.3982)\n",
      "tensor(0.4977)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.944069\n",
      "Epoch 7241\n",
      "-------------------------------\n",
      "tensor(40.6955)\n",
      "tensor(10.4980)\n",
      "tensor(24.5321)\n",
      "tensor(0.4987)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.927505\n",
      "Epoch 7242\n",
      "-------------------------------\n",
      "tensor(23.0980)\n",
      "tensor(6.1152)\n",
      "tensor(13.0789)\n",
      "tensor(0.2810)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.899292\n",
      "Epoch 7243\n",
      "-------------------------------\n",
      "tensor(18.0193)\n",
      "tensor(5.7111)\n",
      "tensor(5.4162)\n",
      "tensor(0.1024)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.881557\n",
      "Epoch 7244\n",
      "-------------------------------\n",
      "tensor(41.5587)\n",
      "tensor(11.7076)\n",
      "tensor(26.1098)\n",
      "tensor(0.5380)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.888470\n",
      "Epoch 7245\n",
      "-------------------------------\n",
      "tensor(50.7049)\n",
      "tensor(13.4633)\n",
      "tensor(25.5006)\n",
      "tensor(0.4744)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.908760\n",
      "Epoch 7246\n",
      "-------------------------------\n",
      "tensor(54.1644)\n",
      "tensor(19.7135)\n",
      "tensor(15.9110)\n",
      "tensor(0.5780)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.926456\n",
      "Epoch 7247\n",
      "-------------------------------\n",
      "tensor(74.3022)\n",
      "tensor(28.9785)\n",
      "tensor(31.0625)\n",
      "tensor(1.0488)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.978661\n",
      "Epoch 7248\n",
      "-------------------------------\n",
      "tensor(67.4373)\n",
      "tensor(19.1323)\n",
      "tensor(39.0094)\n",
      "tensor(0.4799)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.959202\n",
      "Epoch 7249\n",
      "-------------------------------\n",
      "tensor(28.0839)\n",
      "tensor(10.0145)\n",
      "tensor(9.1725)\n",
      "tensor(0.2834)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.892914\n",
      "Epoch 7250\n",
      "-------------------------------\n",
      "tensor(48.7537)\n",
      "tensor(8.8388)\n",
      "tensor(25.8425)\n",
      "tensor(0.0288)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.950878\n",
      "Epoch 7251\n",
      "-------------------------------\n",
      "tensor(74.6072)\n",
      "tensor(35.1804)\n",
      "tensor(61.4286)\n",
      "tensor(1.9272)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.058006\n",
      "Epoch 7252\n",
      "-------------------------------\n",
      "tensor(103.8600)\n",
      "tensor(40.1338)\n",
      "tensor(85.4243)\n",
      "tensor(2.5169)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.161896\n",
      "Epoch 7253\n",
      "-------------------------------\n",
      "tensor(135.3558)\n",
      "tensor(36.9164)\n",
      "tensor(93.7398)\n",
      "tensor(2.0733)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.293354\n",
      "Epoch 7254\n",
      "-------------------------------\n",
      "tensor(124.1043)\n",
      "tensor(45.2071)\n",
      "tensor(87.5809)\n",
      "tensor(2.1348)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.193878\n",
      "Epoch 7255\n",
      "-------------------------------\n",
      "tensor(59.7576)\n",
      "tensor(14.8981)\n",
      "tensor(38.5178)\n",
      "tensor(0.8405)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.946247\n",
      "Epoch 7256\n",
      "-------------------------------\n",
      "tensor(44.1293)\n",
      "tensor(12.1389)\n",
      "tensor(27.5460)\n",
      "tensor(0.5582)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.908741\n",
      "Epoch 7257\n",
      "-------------------------------\n",
      "tensor(69.1364)\n",
      "tensor(21.0314)\n",
      "tensor(41.2632)\n",
      "tensor(1.0145)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.942085\n",
      "Epoch 7258\n",
      "-------------------------------\n",
      "tensor(56.3004)\n",
      "tensor(15.9636)\n",
      "tensor(21.1781)\n",
      "tensor(0.4831)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.896729\n",
      "Epoch 7259\n",
      "-------------------------------\n",
      "tensor(28.5594)\n",
      "tensor(11.5991)\n",
      "tensor(17.2541)\n",
      "tensor(0.4151)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.883480\n",
      "Epoch 7260\n",
      "-------------------------------\n",
      "tensor(40.0015)\n",
      "tensor(15.7731)\n",
      "tensor(27.5200)\n",
      "tensor(0.6719)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.902008\n",
      "Epoch 7261\n",
      "-------------------------------\n",
      "tensor(32.0431)\n",
      "tensor(13.0004)\n",
      "tensor(20.8365)\n",
      "tensor(0.5248)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.885216\n",
      "Epoch 7262\n",
      "-------------------------------\n",
      "tensor(24.2578)\n",
      "tensor(9.2153)\n",
      "tensor(8.0298)\n",
      "tensor(0.2302)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.865967\n",
      "Epoch 7263\n",
      "-------------------------------\n",
      "tensor(20.6577)\n",
      "tensor(5.4568)\n",
      "tensor(9.9693)\n",
      "tensor(0.1819)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.856636\n",
      "Epoch 7264\n",
      "-------------------------------\n",
      "tensor(48.8482)\n",
      "tensor(14.1189)\n",
      "tensor(27.6264)\n",
      "tensor(0.5830)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.873444\n",
      "Epoch 7265\n",
      "-------------------------------\n",
      "tensor(47.9174)\n",
      "tensor(14.9147)\n",
      "tensor(21.6447)\n",
      "tensor(0.4521)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.856815\n",
      "Epoch 7266\n",
      "-------------------------------\n",
      "tensor(51.0384)\n",
      "tensor(15.6720)\n",
      "tensor(21.3175)\n",
      "tensor(0.4371)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.871124\n",
      "Epoch 7267\n",
      "-------------------------------\n",
      "tensor(61.0490)\n",
      "tensor(16.8297)\n",
      "tensor(28.8613)\n",
      "tensor(0.3126)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.859028\n",
      "Epoch 7268\n",
      "-------------------------------\n",
      "tensor(52.9532)\n",
      "tensor(26.2260)\n",
      "tensor(38.7773)\n",
      "tensor(1.4386)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.938988\n",
      "Epoch 7269\n",
      "-------------------------------\n",
      "tensor(39.6139)\n",
      "tensor(12.3944)\n",
      "tensor(4.0394)\n",
      "tensor(0.1064)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.960655\n",
      "Epoch 7270\n",
      "-------------------------------\n",
      "tensor(50.8537)\n",
      "tensor(24.4049)\n",
      "tensor(36.9939)\n",
      "tensor(1.2144)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.958138\n",
      "Epoch 7271\n",
      "-------------------------------\n",
      "tensor(80.6372)\n",
      "tensor(20.6714)\n",
      "tensor(57.4905)\n",
      "tensor(1.4360)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.046986\n",
      "Epoch 7272\n",
      "-------------------------------\n",
      "tensor(102.8549)\n",
      "tensor(36.8823)\n",
      "tensor(76.7626)\n",
      "tensor(1.8570)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.161201\n",
      "Epoch 7273\n",
      "-------------------------------\n",
      "tensor(117.4905)\n",
      "tensor(38.8302)\n",
      "tensor(89.8716)\n",
      "tensor(2.1767)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.213005\n",
      "Epoch 7274\n",
      "-------------------------------\n",
      "tensor(112.5651)\n",
      "tensor(34.4093)\n",
      "tensor(79.7726)\n",
      "tensor(2.0723)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.221394\n",
      "Epoch 7275\n",
      "-------------------------------\n",
      "tensor(46.6957)\n",
      "tensor(22.6856)\n",
      "tensor(29.8459)\n",
      "tensor(1.1140)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.958340\n",
      "Epoch 7276\n",
      "-------------------------------\n",
      "tensor(53.0319)\n",
      "tensor(25.0482)\n",
      "tensor(40.5322)\n",
      "tensor(1.4315)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.949093\n",
      "Epoch 7277\n",
      "-------------------------------\n",
      "tensor(49.2700)\n",
      "tensor(14.4979)\n",
      "tensor(32.1932)\n",
      "tensor(0.7018)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.944019\n",
      "Epoch 7278\n",
      "-------------------------------\n",
      "tensor(38.2218)\n",
      "tensor(16.6626)\n",
      "tensor(28.4328)\n",
      "tensor(0.9103)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.923817\n",
      "Epoch 7279\n",
      "-------------------------------\n",
      "tensor(34.8058)\n",
      "tensor(10.5750)\n",
      "tensor(6.8065)\n",
      "tensor(0.1848)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.911213\n",
      "Epoch 7280\n",
      "-------------------------------\n",
      "tensor(45.1729)\n",
      "tensor(11.4965)\n",
      "tensor(20.1437)\n",
      "tensor(0.2520)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.913986\n",
      "Epoch 7281\n",
      "-------------------------------\n",
      "tensor(35.8318)\n",
      "tensor(9.1728)\n",
      "tensor(18.9826)\n",
      "tensor(0.3268)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.900894\n",
      "Epoch 7282\n",
      "-------------------------------\n",
      "tensor(27.7804)\n",
      "tensor(7.6934)\n",
      "tensor(10.9636)\n",
      "tensor(0.2241)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.878834\n",
      "Epoch 7283\n",
      "-------------------------------\n",
      "tensor(26.9868)\n",
      "tensor(8.3625)\n",
      "tensor(3.3323)\n",
      "tensor(0.0117)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.869705\n",
      "Epoch 7284\n",
      "-------------------------------\n",
      "tensor(39.7477)\n",
      "tensor(11.4661)\n",
      "tensor(20.0492)\n",
      "tensor(0.3011)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.876591\n",
      "Epoch 7285\n",
      "-------------------------------\n",
      "tensor(40.3816)\n",
      "tensor(9.9779)\n",
      "tensor(21.1588)\n",
      "tensor(0.2436)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.863342\n",
      "Epoch 7286\n",
      "-------------------------------\n",
      "tensor(24.9729)\n",
      "tensor(12.1814)\n",
      "tensor(12.8460)\n",
      "tensor(0.4808)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.823372\n",
      "Epoch 7287\n",
      "-------------------------------\n",
      "tensor(64.3816)\n",
      "tensor(20.9875)\n",
      "tensor(28.8023)\n",
      "tensor(0.5833)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.921074\n",
      "Epoch 7288\n",
      "-------------------------------\n",
      "tensor(65.8376)\n",
      "tensor(21.1118)\n",
      "tensor(32.9601)\n",
      "tensor(0.8319)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.988098\n",
      "Epoch 7289\n",
      "-------------------------------\n",
      "tensor(40.2546)\n",
      "tensor(13.3430)\n",
      "tensor(9.2043)\n",
      "tensor(0.4700)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.838924\n",
      "Epoch 7290\n",
      "-------------------------------\n",
      "tensor(54.9723)\n",
      "tensor(16.7459)\n",
      "tensor(37.4825)\n",
      "tensor(1.1968)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.901588\n",
      "Epoch 7291\n",
      "-------------------------------\n",
      "tensor(70.3070)\n",
      "tensor(37.9291)\n",
      "tensor(62.6900)\n",
      "tensor(2.1770)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.020477\n",
      "Epoch 7292\n",
      "-------------------------------\n",
      "tensor(108.5744)\n",
      "tensor(28.5430)\n",
      "tensor(73.8681)\n",
      "tensor(1.4186)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.124367\n",
      "Epoch 7293\n",
      "-------------------------------\n",
      "tensor(127.9751)\n",
      "tensor(33.7028)\n",
      "tensor(87.0427)\n",
      "tensor(1.6361)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.236069\n",
      "Epoch 7294\n",
      "-------------------------------\n",
      "tensor(112.6973)\n",
      "tensor(47.9694)\n",
      "tensor(89.0889)\n",
      "tensor(2.7086)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.228127\n",
      "Epoch 7295\n",
      "-------------------------------\n",
      "tensor(61.8148)\n",
      "tensor(17.8615)\n",
      "tensor(38.8818)\n",
      "tensor(1.0318)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.981735\n",
      "Epoch 7296\n",
      "-------------------------------\n",
      "tensor(45.7681)\n",
      "tensor(16.9849)\n",
      "tensor(33.4672)\n",
      "tensor(1.0558)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.920582\n",
      "Epoch 7297\n",
      "-------------------------------\n",
      "tensor(63.2478)\n",
      "tensor(19.7603)\n",
      "tensor(41.3756)\n",
      "tensor(0.9479)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.934143\n",
      "Epoch 7298\n",
      "-------------------------------\n",
      "tensor(35.3176)\n",
      "tensor(12.0702)\n",
      "tensor(26.5912)\n",
      "tensor(0.7816)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.878342\n",
      "Epoch 7299\n",
      "-------------------------------\n",
      "tensor(31.7555)\n",
      "tensor(9.4939)\n",
      "tensor(10.7594)\n",
      "tensor(0.0968)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.861176\n",
      "Epoch 7300\n",
      "-------------------------------\n",
      "tensor(43.3186)\n",
      "tensor(13.7615)\n",
      "tensor(24.4351)\n",
      "tensor(0.4722)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.867718\n",
      "Epoch 7301\n",
      "-------------------------------\n",
      "tensor(39.1041)\n",
      "tensor(12.8119)\n",
      "tensor(21.1793)\n",
      "tensor(0.4370)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.845818\n",
      "Epoch 7302\n",
      "-------------------------------\n",
      "tensor(22.4166)\n",
      "tensor(7.6467)\n",
      "tensor(10.8098)\n",
      "tensor(0.2272)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.824932\n",
      "Epoch 7303\n",
      "-------------------------------\n",
      "tensor(19.9710)\n",
      "tensor(5.8504)\n",
      "tensor(6.0760)\n",
      "tensor(0.1191)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.811382\n",
      "Epoch 7304\n",
      "-------------------------------\n",
      "tensor(42.0033)\n",
      "tensor(11.8433)\n",
      "tensor(25.0876)\n",
      "tensor(0.4975)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.819939\n",
      "Epoch 7305\n",
      "-------------------------------\n",
      "tensor(56.6280)\n",
      "tensor(16.0513)\n",
      "tensor(23.7936)\n",
      "tensor(0.3993)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.837776\n",
      "Epoch 7306\n",
      "-------------------------------\n",
      "tensor(44.6411)\n",
      "tensor(15.3194)\n",
      "tensor(18.2999)\n",
      "tensor(0.4690)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.851574\n",
      "Epoch 7307\n",
      "-------------------------------\n",
      "tensor(67.2648)\n",
      "tensor(20.9596)\n",
      "tensor(31.8031)\n",
      "tensor(0.4464)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.855782\n",
      "Epoch 7308\n",
      "-------------------------------\n",
      "tensor(42.3105)\n",
      "tensor(21.7801)\n",
      "tensor(31.5244)\n",
      "tensor(1.3849)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.874794\n",
      "Epoch 7309\n",
      "-------------------------------\n",
      "tensor(28.1594)\n",
      "tensor(7.7862)\n",
      "tensor(8.4361)\n",
      "tensor(0.1824)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.896725\n",
      "Epoch 7310\n",
      "-------------------------------\n",
      "tensor(53.2818)\n",
      "tensor(30.0368)\n",
      "tensor(44.7514)\n",
      "tensor(1.8041)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.987122\n",
      "Epoch 7311\n",
      "-------------------------------\n",
      "tensor(82.0556)\n",
      "tensor(25.0421)\n",
      "tensor(55.7889)\n",
      "tensor(1.3931)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.977623\n",
      "Epoch 7312\n",
      "-------------------------------\n",
      "tensor(106.9451)\n",
      "tensor(26.0230)\n",
      "tensor(69.6529)\n",
      "tensor(1.0254)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.096817\n",
      "Epoch 7313\n",
      "-------------------------------\n",
      "tensor(117.3686)\n",
      "tensor(40.7507)\n",
      "tensor(89.7547)\n",
      "tensor(2.2151)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.174885\n",
      "Epoch 7314\n",
      "-------------------------------\n",
      "tensor(110.6565)\n",
      "tensor(41.2475)\n",
      "tensor(85.7424)\n",
      "tensor(2.6636)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.164810\n",
      "Epoch 7315\n",
      "-------------------------------\n",
      "tensor(48.7380)\n",
      "tensor(19.0807)\n",
      "tensor(33.2527)\n",
      "tensor(0.9459)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.911438\n",
      "Epoch 7316\n",
      "-------------------------------\n",
      "tensor(47.5617)\n",
      "tensor(23.3656)\n",
      "tensor(40.0869)\n",
      "tensor(1.5297)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.901375\n",
      "Epoch 7317\n",
      "-------------------------------\n",
      "tensor(53.7021)\n",
      "tensor(14.0491)\n",
      "tensor(33.6465)\n",
      "tensor(0.5354)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.888100\n",
      "Epoch 7318\n",
      "-------------------------------\n",
      "tensor(45.5794)\n",
      "tensor(17.1322)\n",
      "tensor(27.3476)\n",
      "tensor(0.8063)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.855503\n",
      "Epoch 7319\n",
      "-------------------------------\n",
      "tensor(51.0910)\n",
      "tensor(14.6169)\n",
      "tensor(6.6488)\n",
      "tensor(0.1900)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.881287\n",
      "Epoch 7320\n",
      "-------------------------------\n",
      "tensor(52.8221)\n",
      "tensor(14.0108)\n",
      "tensor(18.0952)\n",
      "tensor(0.2028)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.888943\n",
      "Epoch 7321\n",
      "-------------------------------\n",
      "tensor(41.8375)\n",
      "tensor(11.5421)\n",
      "tensor(16.4096)\n",
      "tensor(0.2844)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.865971\n",
      "Epoch 7322\n",
      "-------------------------------\n",
      "tensor(35.9646)\n",
      "tensor(10.6133)\n",
      "tensor(8.7723)\n",
      "tensor(0.2093)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.839458\n",
      "Epoch 7323\n",
      "-------------------------------\n",
      "tensor(25.1307)\n",
      "tensor(7.4660)\n",
      "tensor(4.8044)\n",
      "tensor(0.0212)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.827118\n",
      "Epoch 7324\n",
      "-------------------------------\n",
      "tensor(35.1162)\n",
      "tensor(9.0631)\n",
      "tensor(19.2123)\n",
      "tensor(0.2153)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.847973\n",
      "Epoch 7325\n",
      "-------------------------------\n",
      "tensor(34.7465)\n",
      "tensor(11.4222)\n",
      "tensor(17.4273)\n",
      "tensor(0.1552)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.849304\n",
      "Epoch 7326\n",
      "-------------------------------\n",
      "tensor(34.1704)\n",
      "tensor(13.3954)\n",
      "tensor(14.2715)\n",
      "tensor(0.4251)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.836845\n",
      "Epoch 7327\n",
      "-------------------------------\n",
      "tensor(54.1429)\n",
      "tensor(16.0132)\n",
      "tensor(23.0088)\n",
      "tensor(0.3784)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.857193\n",
      "Epoch 7328\n",
      "-------------------------------\n",
      "tensor(50.8152)\n",
      "tensor(15.7513)\n",
      "tensor(31.1525)\n",
      "tensor(0.8100)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.888710\n",
      "Epoch 7329\n",
      "-------------------------------\n",
      "tensor(22.5743)\n",
      "tensor(9.5304)\n",
      "tensor(10.0636)\n",
      "tensor(0.5313)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.846390\n",
      "Epoch 7330\n",
      "-------------------------------\n",
      "tensor(47.0773)\n",
      "tensor(16.2544)\n",
      "tensor(23.5675)\n",
      "tensor(0.7410)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.899467\n",
      "Epoch 7331\n",
      "-------------------------------\n",
      "tensor(69.3869)\n",
      "tensor(27.0806)\n",
      "tensor(51.2489)\n",
      "tensor(1.3854)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.995785\n",
      "Epoch 7332\n",
      "-------------------------------\n",
      "tensor(108.5652)\n",
      "tensor(31.6745)\n",
      "tensor(72.4638)\n",
      "tensor(1.6865)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.118294\n",
      "Epoch 7333\n",
      "-------------------------------\n",
      "tensor(128.7958)\n",
      "tensor(42.3829)\n",
      "tensor(94.4184)\n",
      "tensor(2.1388)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.156502\n",
      "Epoch 7334\n",
      "-------------------------------\n",
      "tensor(124.4613)\n",
      "tensor(41.3751)\n",
      "tensor(91.8772)\n",
      "tensor(2.4151)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.177330\n",
      "Epoch 7335\n",
      "-------------------------------\n",
      "tensor(63.5793)\n",
      "tensor(22.4124)\n",
      "tensor(43.1438)\n",
      "tensor(1.1739)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.919472\n",
      "Epoch 7336\n",
      "-------------------------------\n",
      "tensor(60.5096)\n",
      "tensor(19.7712)\n",
      "tensor(35.3533)\n",
      "tensor(0.9288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.888443\n",
      "Epoch 7337\n",
      "-------------------------------\n",
      "tensor(54.2188)\n",
      "tensor(20.4398)\n",
      "tensor(40.8354)\n",
      "tensor(1.2010)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.892086\n",
      "Epoch 7338\n",
      "-------------------------------\n",
      "tensor(46.9151)\n",
      "tensor(18.8963)\n",
      "tensor(30.4767)\n",
      "tensor(0.9458)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.863430\n",
      "Epoch 7339\n",
      "-------------------------------\n",
      "tensor(27.8301)\n",
      "tensor(10.6516)\n",
      "tensor(8.2938)\n",
      "tensor(0.1027)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.832726\n",
      "Epoch 7340\n",
      "-------------------------------\n",
      "tensor(38.1080)\n",
      "tensor(12.1432)\n",
      "tensor(24.3311)\n",
      "tensor(0.5896)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.846169\n",
      "Epoch 7341\n",
      "-------------------------------\n",
      "tensor(34.3820)\n",
      "tensor(11.7748)\n",
      "tensor(22.0964)\n",
      "tensor(0.5921)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.838100\n",
      "Epoch 7342\n",
      "-------------------------------\n",
      "tensor(31.0194)\n",
      "tensor(10.4420)\n",
      "tensor(11.8399)\n",
      "tensor(0.3720)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.822971\n",
      "Epoch 7343\n",
      "-------------------------------\n",
      "tensor(28.3064)\n",
      "tensor(8.6805)\n",
      "tensor(5.9006)\n",
      "tensor(0.0326)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.810360\n",
      "Epoch 7344\n",
      "-------------------------------\n",
      "tensor(46.5650)\n",
      "tensor(14.3662)\n",
      "tensor(24.8278)\n",
      "tensor(0.5391)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.836864\n",
      "Epoch 7345\n",
      "-------------------------------\n",
      "tensor(55.4701)\n",
      "tensor(17.8315)\n",
      "tensor(24.4410)\n",
      "tensor(0.6371)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.831726\n",
      "Epoch 7346\n",
      "-------------------------------\n",
      "tensor(45.4359)\n",
      "tensor(13.2324)\n",
      "tensor(16.5961)\n",
      "tensor(0.1502)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.809044\n",
      "Epoch 7347\n",
      "-------------------------------\n",
      "tensor(52.9286)\n",
      "tensor(15.6136)\n",
      "tensor(32.5425)\n",
      "tensor(0.4357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.840603\n",
      "Epoch 7348\n",
      "-------------------------------\n",
      "tensor(55.4550)\n",
      "tensor(16.0933)\n",
      "tensor(38.2600)\n",
      "tensor(0.9194)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.876457\n",
      "Epoch 7349\n",
      "-------------------------------\n",
      "tensor(29.4857)\n",
      "tensor(12.5862)\n",
      "tensor(9.1117)\n",
      "tensor(0.5797)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.858929\n",
      "Epoch 7350\n",
      "-------------------------------\n",
      "tensor(57.5635)\n",
      "tensor(19.5929)\n",
      "tensor(36.5180)\n",
      "tensor(0.9411)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.897339\n",
      "Epoch 7351\n",
      "-------------------------------\n",
      "tensor(87.8772)\n",
      "tensor(36.9750)\n",
      "tensor(66.3487)\n",
      "tensor(2.2280)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.007015\n",
      "Epoch 7352\n",
      "-------------------------------\n",
      "tensor(121.6190)\n",
      "tensor(38.6052)\n",
      "tensor(86.4968)\n",
      "tensor(1.9034)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.097546\n",
      "Epoch 7353\n",
      "-------------------------------\n",
      "tensor(135.6374)\n",
      "tensor(39.9759)\n",
      "tensor(95.7616)\n",
      "tensor(2.1810)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.183540\n",
      "Epoch 7354\n",
      "-------------------------------\n",
      "tensor(114.1689)\n",
      "tensor(43.6788)\n",
      "tensor(88.9098)\n",
      "tensor(2.4095)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.110661\n",
      "Epoch 7355\n",
      "-------------------------------\n",
      "tensor(63.2889)\n",
      "tensor(19.4141)\n",
      "tensor(36.4302)\n",
      "tensor(0.9572)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.899353\n",
      "Epoch 7356\n",
      "-------------------------------\n",
      "tensor(43.0071)\n",
      "tensor(16.5088)\n",
      "tensor(33.5849)\n",
      "tensor(1.0903)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.879993\n",
      "Epoch 7357\n",
      "-------------------------------\n",
      "tensor(57.8669)\n",
      "tensor(17.4393)\n",
      "tensor(35.7882)\n",
      "tensor(0.7827)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.912117\n",
      "Epoch 7358\n",
      "-------------------------------\n",
      "tensor(39.6680)\n",
      "tensor(14.4843)\n",
      "tensor(25.5846)\n",
      "tensor(0.6673)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.871548\n",
      "Epoch 7359\n",
      "-------------------------------\n",
      "tensor(19.1897)\n",
      "tensor(6.7123)\n",
      "tensor(10.2816)\n",
      "tensor(0.1452)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.840008\n",
      "Epoch 7360\n",
      "-------------------------------\n",
      "tensor(34.0931)\n",
      "tensor(11.5913)\n",
      "tensor(24.6067)\n",
      "tensor(0.5038)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.849476\n",
      "Epoch 7361\n",
      "-------------------------------\n",
      "tensor(31.1419)\n",
      "tensor(10.9063)\n",
      "tensor(22.4230)\n",
      "tensor(0.4717)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.836849\n",
      "Epoch 7362\n",
      "-------------------------------\n",
      "tensor(34.6867)\n",
      "tensor(11.2223)\n",
      "tensor(12.8623)\n",
      "tensor(0.2644)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.818321\n",
      "Epoch 7363\n",
      "-------------------------------\n",
      "tensor(30.6880)\n",
      "tensor(9.2573)\n",
      "tensor(4.1011)\n",
      "tensor(0.0800)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.810658\n",
      "Epoch 7364\n",
      "-------------------------------\n",
      "tensor(45.1356)\n",
      "tensor(14.5555)\n",
      "tensor(23.1448)\n",
      "tensor(0.4572)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.820950\n",
      "Epoch 7365\n",
      "-------------------------------\n",
      "tensor(41.5195)\n",
      "tensor(9.9490)\n",
      "tensor(24.1569)\n",
      "tensor(0.3597)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.796150\n",
      "Epoch 7366\n",
      "-------------------------------\n",
      "tensor(46.1561)\n",
      "tensor(15.6587)\n",
      "tensor(14.9676)\n",
      "tensor(0.5602)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.802799\n",
      "Epoch 7367\n",
      "-------------------------------\n",
      "tensor(57.7412)\n",
      "tensor(17.7524)\n",
      "tensor(28.7219)\n",
      "tensor(0.7649)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.867012\n",
      "Epoch 7368\n",
      "-------------------------------\n",
      "tensor(53.0080)\n",
      "tensor(20.6623)\n",
      "tensor(34.0903)\n",
      "tensor(0.9237)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.878670\n",
      "Epoch 7369\n",
      "-------------------------------\n",
      "tensor(40.0811)\n",
      "tensor(14.4588)\n",
      "tensor(2.0854)\n",
      "tensor(0.0523)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.828499\n",
      "Epoch 7370\n",
      "-------------------------------\n",
      "tensor(46.8527)\n",
      "tensor(14.1873)\n",
      "tensor(28.0306)\n",
      "tensor(0.8576)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.834835\n",
      "Epoch 7371\n",
      "-------------------------------\n",
      "tensor(70.0577)\n",
      "tensor(24.1814)\n",
      "tensor(49.2948)\n",
      "tensor(1.1937)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.890221\n",
      "Epoch 7372\n",
      "-------------------------------\n",
      "tensor(101.5052)\n",
      "tensor(32.2054)\n",
      "tensor(65.4946)\n",
      "tensor(1.5326)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.024681\n",
      "Epoch 7373\n",
      "-------------------------------\n",
      "tensor(118.2373)\n",
      "tensor(34.6161)\n",
      "tensor(82.9039)\n",
      "tensor(1.7901)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.079674\n",
      "Epoch 7374\n",
      "-------------------------------\n",
      "tensor(105.8864)\n",
      "tensor(39.9191)\n",
      "tensor(83.0385)\n",
      "tensor(2.3310)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.078796\n",
      "Epoch 7375\n",
      "-------------------------------\n",
      "tensor(50.6654)\n",
      "tensor(18.8116)\n",
      "tensor(36.8037)\n",
      "tensor(1.0401)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.896317\n",
      "Epoch 7376\n",
      "-------------------------------\n",
      "tensor(42.8833)\n",
      "tensor(18.0972)\n",
      "tensor(30.4507)\n",
      "tensor(1.0197)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.886189\n",
      "Epoch 7377\n",
      "-------------------------------\n",
      "tensor(54.0435)\n",
      "tensor(16.6798)\n",
      "tensor(37.0896)\n",
      "tensor(0.8478)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.880955\n",
      "Epoch 7378\n",
      "-------------------------------\n",
      "tensor(45.4285)\n",
      "tensor(15.6297)\n",
      "tensor(24.6344)\n",
      "tensor(0.7297)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.840252\n",
      "Epoch 7379\n",
      "-------------------------------\n",
      "tensor(25.0083)\n",
      "tensor(5.6182)\n",
      "tensor(10.1726)\n",
      "tensor(0.0371)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.811157\n",
      "Epoch 7380\n",
      "-------------------------------\n",
      "tensor(38.0522)\n",
      "tensor(12.3421)\n",
      "tensor(21.9058)\n",
      "tensor(0.3632)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.818649\n",
      "Epoch 7381\n",
      "-------------------------------\n",
      "tensor(36.8432)\n",
      "tensor(12.7212)\n",
      "tensor(18.0917)\n",
      "tensor(0.3299)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.805843\n",
      "Epoch 7382\n",
      "-------------------------------\n",
      "tensor(39.7211)\n",
      "tensor(12.1271)\n",
      "tensor(8.1024)\n",
      "tensor(0.1436)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.786381\n",
      "Epoch 7383\n",
      "-------------------------------\n",
      "tensor(16.2757)\n",
      "tensor(5.3600)\n",
      "tensor(6.4248)\n",
      "tensor(0.1575)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.773415\n",
      "Epoch 7384\n",
      "-------------------------------\n",
      "tensor(32.0673)\n",
      "tensor(9.9630)\n",
      "tensor(22.4419)\n",
      "tensor(0.4723)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.778072\n",
      "Epoch 7385\n",
      "-------------------------------\n",
      "tensor(31.7910)\n",
      "tensor(9.2314)\n",
      "tensor(21.4198)\n",
      "tensor(0.3617)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.752899\n",
      "Epoch 7386\n",
      "-------------------------------\n",
      "tensor(40.0436)\n",
      "tensor(13.1596)\n",
      "tensor(13.9722)\n",
      "tensor(0.4889)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.784706\n",
      "Epoch 7387\n",
      "-------------------------------\n",
      "tensor(61.7203)\n",
      "tensor(18.8041)\n",
      "tensor(28.9402)\n",
      "tensor(0.5853)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.842674\n",
      "Epoch 7388\n",
      "-------------------------------\n",
      "tensor(55.6652)\n",
      "tensor(22.6615)\n",
      "tensor(25.2782)\n",
      "tensor(1.1535)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.849960\n",
      "Epoch 7389\n",
      "-------------------------------\n",
      "tensor(38.2103)\n",
      "tensor(12.4023)\n",
      "tensor(8.0613)\n",
      "tensor(0.4598)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.867928\n",
      "Epoch 7390\n",
      "-------------------------------\n",
      "tensor(42.8934)\n",
      "tensor(24.1425)\n",
      "tensor(30.9813)\n",
      "tensor(1.1932)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.883675\n",
      "Epoch 7391\n",
      "-------------------------------\n",
      "tensor(62.2219)\n",
      "tensor(13.6361)\n",
      "tensor(39.3559)\n",
      "tensor(0.5470)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.865395\n",
      "Epoch 7392\n",
      "-------------------------------\n",
      "tensor(79.2555)\n",
      "tensor(23.7650)\n",
      "tensor(52.3812)\n",
      "tensor(1.0598)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.958897\n",
      "Epoch 7393\n",
      "-------------------------------\n",
      "tensor(75.2989)\n",
      "tensor(34.0283)\n",
      "tensor(63.5112)\n",
      "tensor(2.0682)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.989140\n",
      "Epoch 7394\n",
      "-------------------------------\n",
      "tensor(71.1019)\n",
      "tensor(21.5348)\n",
      "tensor(50.0358)\n",
      "tensor(1.3386)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.954796\n",
      "Epoch 7395\n",
      "-------------------------------\n",
      "tensor(32.2766)\n",
      "tensor(10.8632)\n",
      "tensor(21.4234)\n",
      "tensor(0.1392)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.858124\n",
      "Epoch 7396\n",
      "-------------------------------\n",
      "tensor(31.0585)\n",
      "tensor(12.6059)\n",
      "tensor(23.8007)\n",
      "tensor(0.5530)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.825871\n",
      "Epoch 7397\n",
      "-------------------------------\n",
      "tensor(39.9038)\n",
      "tensor(9.2171)\n",
      "tensor(19.5867)\n",
      "tensor(0.2335)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.801426\n",
      "Epoch 7398\n",
      "-------------------------------\n",
      "tensor(44.3213)\n",
      "tensor(12.5631)\n",
      "tensor(15.5923)\n",
      "tensor(0.1077)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.782631\n",
      "Epoch 7399\n",
      "-------------------------------\n",
      "tensor(33.6115)\n",
      "tensor(10.9221)\n",
      "tensor(5.1683)\n",
      "tensor(0.3014)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.736145\n",
      "Epoch 7400\n",
      "-------------------------------\n",
      "tensor(36.6457)\n",
      "tensor(11.5850)\n",
      "tensor(12.6895)\n",
      "tensor(0.4113)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.738956\n",
      "Epoch 7401\n",
      "-------------------------------\n",
      "tensor(36.6480)\n",
      "tensor(11.0094)\n",
      "tensor(11.5192)\n",
      "tensor(0.3146)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.735580\n",
      "Epoch 7402\n",
      "-------------------------------\n",
      "tensor(28.5914)\n",
      "tensor(8.4466)\n",
      "tensor(6.2553)\n",
      "tensor(0.1334)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.720608\n",
      "Epoch 7403\n",
      "-------------------------------\n",
      "tensor(24.8303)\n",
      "tensor(8.4739)\n",
      "tensor(3.2112)\n",
      "tensor(0.1407)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.724846\n",
      "Epoch 7404\n",
      "-------------------------------\n",
      "tensor(30.6365)\n",
      "tensor(11.8417)\n",
      "tensor(13.5054)\n",
      "tensor(0.4471)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.731125\n",
      "Epoch 7405\n",
      "-------------------------------\n",
      "tensor(34.4032)\n",
      "tensor(12.5834)\n",
      "tensor(13.9968)\n",
      "tensor(0.4561)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.717903\n",
      "Epoch 7406\n",
      "-------------------------------\n",
      "tensor(39.1918)\n",
      "tensor(11.9934)\n",
      "tensor(5.4914)\n",
      "tensor(0.1429)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.720287\n",
      "Epoch 7407\n",
      "-------------------------------\n",
      "tensor(52.4278)\n",
      "tensor(19.3744)\n",
      "tensor(15.1765)\n",
      "tensor(0.5933)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.761841\n",
      "Epoch 7408\n",
      "-------------------------------\n",
      "tensor(45.5757)\n",
      "tensor(14.8302)\n",
      "tensor(18.2110)\n",
      "tensor(0.0256)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.751144\n",
      "Epoch 7409\n",
      "-------------------------------\n",
      "tensor(17.2517)\n",
      "tensor(8.5347)\n",
      "tensor(3.1387)\n",
      "tensor(0.1568)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.748245\n",
      "Epoch 7410\n",
      "-------------------------------\n",
      "tensor(47.5381)\n",
      "tensor(11.9126)\n",
      "tensor(17.5886)\n",
      "tensor(0.0331)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.793823\n",
      "Epoch 7411\n",
      "-------------------------------\n",
      "tensor(58.8799)\n",
      "tensor(20.6305)\n",
      "tensor(40.1480)\n",
      "tensor(1.0137)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.881046\n",
      "Epoch 7412\n",
      "-------------------------------\n",
      "tensor(75.3076)\n",
      "tensor(30.7083)\n",
      "tensor(60.9869)\n",
      "tensor(1.9587)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.951103\n",
      "Epoch 7413\n",
      "-------------------------------\n",
      "tensor(95.7405)\n",
      "tensor(30.8148)\n",
      "tensor(69.4329)\n",
      "tensor(1.7063)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.008099\n",
      "Epoch 7414\n",
      "-------------------------------\n",
      "tensor(97.5453)\n",
      "tensor(28.9933)\n",
      "tensor(67.9763)\n",
      "tensor(1.3154)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.982475\n",
      "Epoch 7415\n",
      "-------------------------------\n",
      "tensor(53.5533)\n",
      "tensor(16.5176)\n",
      "tensor(34.0049)\n",
      "tensor(0.8764)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.843018\n",
      "Epoch 7416\n",
      "-------------------------------\n",
      "tensor(42.8528)\n",
      "tensor(9.6930)\n",
      "tensor(20.7483)\n",
      "tensor(0.1527)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.784077\n",
      "Epoch 7417\n",
      "-------------------------------\n",
      "tensor(46.3550)\n",
      "tensor(19.4057)\n",
      "tensor(36.0396)\n",
      "tensor(1.1897)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.782967\n",
      "Epoch 7418\n",
      "-------------------------------\n",
      "tensor(49.2486)\n",
      "tensor(15.6179)\n",
      "tensor(19.7101)\n",
      "tensor(0.5261)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.768650\n",
      "Epoch 7419\n",
      "-------------------------------\n",
      "tensor(46.4834)\n",
      "tensor(15.0768)\n",
      "tensor(12.6683)\n",
      "tensor(0.3961)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.738316\n",
      "Epoch 7420\n",
      "-------------------------------\n",
      "tensor(41.2497)\n",
      "tensor(15.1608)\n",
      "tensor(23.9989)\n",
      "tensor(0.6713)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.720737\n",
      "Epoch 7421\n",
      "-------------------------------\n",
      "tensor(30.5999)\n",
      "tensor(11.9235)\n",
      "tensor(20.0234)\n",
      "tensor(0.5452)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.708599\n",
      "Epoch 7422\n",
      "-------------------------------\n",
      "tensor(20.0253)\n",
      "tensor(7.7639)\n",
      "tensor(9.3949)\n",
      "tensor(0.2590)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.695255\n",
      "Epoch 7423\n",
      "-------------------------------\n",
      "tensor(20.2226)\n",
      "tensor(5.9593)\n",
      "tensor(7.0377)\n",
      "tensor(0.1657)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.690395\n",
      "Epoch 7424\n",
      "-------------------------------\n",
      "tensor(46.0202)\n",
      "tensor(14.1551)\n",
      "tensor(24.7344)\n",
      "tensor(0.6219)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.711842\n",
      "Epoch 7425\n",
      "-------------------------------\n",
      "tensor(47.9222)\n",
      "tensor(15.2997)\n",
      "tensor(21.8734)\n",
      "tensor(0.5808)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.717888\n",
      "Epoch 7426\n",
      "-------------------------------\n",
      "tensor(55.2629)\n",
      "tensor(16.9047)\n",
      "tensor(16.7561)\n",
      "tensor(0.3077)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.700653\n",
      "Epoch 7427\n",
      "-------------------------------\n",
      "tensor(58.8143)\n",
      "tensor(17.4696)\n",
      "tensor(33.9989)\n",
      "tensor(0.5084)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.736740\n",
      "Epoch 7428\n",
      "-------------------------------\n",
      "tensor(50.3902)\n",
      "tensor(20.0624)\n",
      "tensor(37.2850)\n",
      "tensor(1.1640)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.765182\n",
      "Epoch 7429\n",
      "-------------------------------\n",
      "tensor(23.3407)\n",
      "tensor(10.4504)\n",
      "tensor(3.6525)\n",
      "tensor(0.2091)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.736443\n",
      "Epoch 7430\n",
      "-------------------------------\n",
      "tensor(50.6014)\n",
      "tensor(22.9152)\n",
      "tensor(40.5367)\n",
      "tensor(1.3063)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.795292\n",
      "Epoch 7431\n",
      "-------------------------------\n",
      "tensor(95.0214)\n",
      "tensor(32.6738)\n",
      "tensor(68.1560)\n",
      "tensor(1.8319)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.959877\n",
      "Epoch 7432\n",
      "-------------------------------\n",
      "tensor(130.9724)\n",
      "tensor(37.8722)\n",
      "tensor(91.3819)\n",
      "tensor(1.8560)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.071125\n",
      "Epoch 7433\n",
      "-------------------------------\n",
      "tensor(142.2544)\n",
      "tensor(48.9997)\n",
      "tensor(109.2737)\n",
      "tensor(2.8392)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.168217\n",
      "Epoch 7434\n",
      "-------------------------------\n",
      "tensor(127.6389)\n",
      "tensor(47.5379)\n",
      "tensor(99.5826)\n",
      "tensor(2.8059)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.088058\n",
      "Epoch 7435\n",
      "-------------------------------\n",
      "tensor(66.7120)\n",
      "tensor(19.6239)\n",
      "tensor(41.2312)\n",
      "tensor(0.8448)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.792477\n",
      "Epoch 7436\n",
      "-------------------------------\n",
      "tensor(50.9098)\n",
      "tensor(19.9301)\n",
      "tensor(38.2298)\n",
      "tensor(1.1983)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.763702\n",
      "Epoch 7437\n",
      "-------------------------------\n",
      "tensor(68.8909)\n",
      "tensor(19.2743)\n",
      "tensor(41.7659)\n",
      "tensor(0.7321)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.816074\n",
      "Epoch 7438\n",
      "-------------------------------\n",
      "tensor(53.7859)\n",
      "tensor(15.9696)\n",
      "tensor(29.2490)\n",
      "tensor(0.5530)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.745640\n",
      "Epoch 7439\n",
      "-------------------------------\n",
      "tensor(40.2348)\n",
      "tensor(12.4945)\n",
      "tensor(9.9096)\n",
      "tensor(0.2893)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.703083\n",
      "Epoch 7440\n",
      "-------------------------------\n",
      "tensor(50.7237)\n",
      "tensor(15.3432)\n",
      "tensor(25.2700)\n",
      "tensor(0.6095)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.709732\n",
      "Epoch 7441\n",
      "-------------------------------\n",
      "tensor(48.2633)\n",
      "tensor(14.2804)\n",
      "tensor(22.1713)\n",
      "tensor(0.5146)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.695724\n",
      "Epoch 7442\n",
      "-------------------------------\n",
      "tensor(27.0958)\n",
      "tensor(7.8563)\n",
      "tensor(11.0331)\n",
      "tensor(0.2425)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.676579\n",
      "Epoch 7443\n",
      "-------------------------------\n",
      "tensor(24.4740)\n",
      "tensor(8.2990)\n",
      "tensor(6.4886)\n",
      "tensor(0.1803)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.676941\n",
      "Epoch 7444\n",
      "-------------------------------\n",
      "tensor(46.5816)\n",
      "tensor(15.8739)\n",
      "tensor(25.7903)\n",
      "tensor(0.6443)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.719120\n",
      "Epoch 7445\n",
      "-------------------------------\n",
      "tensor(46.7103)\n",
      "tensor(15.2077)\n",
      "tensor(25.0931)\n",
      "tensor(0.5717)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.721535\n",
      "Epoch 7446\n",
      "-------------------------------\n",
      "tensor(24.1765)\n",
      "tensor(8.3648)\n",
      "tensor(14.4989)\n",
      "tensor(0.4850)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.695686\n",
      "Epoch 7447\n",
      "-------------------------------\n",
      "tensor(50.7726)\n",
      "tensor(18.5698)\n",
      "tensor(34.0187)\n",
      "tensor(0.9049)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.738628\n",
      "Epoch 7448\n",
      "-------------------------------\n",
      "tensor(48.3071)\n",
      "tensor(17.5167)\n",
      "tensor(32.0309)\n",
      "tensor(0.8556)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.735199\n",
      "Epoch 7449\n",
      "-------------------------------\n",
      "tensor(41.2467)\n",
      "tensor(14.1862)\n",
      "tensor(3.1429)\n",
      "tensor(0.1028)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.754036\n",
      "Epoch 7450\n",
      "-------------------------------\n",
      "tensor(52.9361)\n",
      "tensor(20.1565)\n",
      "tensor(32.9235)\n",
      "tensor(1.1046)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.744484\n",
      "Epoch 7451\n",
      "-------------------------------\n",
      "tensor(79.0034)\n",
      "tensor(25.2283)\n",
      "tensor(54.6305)\n",
      "tensor(1.1752)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.828896\n",
      "Epoch 7452\n",
      "-------------------------------\n",
      "tensor(115.0144)\n",
      "tensor(32.2794)\n",
      "tensor(73.3251)\n",
      "tensor(1.5818)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.947849\n",
      "Epoch 7453\n",
      "-------------------------------\n",
      "tensor(130.9935)\n",
      "tensor(45.5626)\n",
      "tensor(98.3522)\n",
      "tensor(2.4020)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.120098\n",
      "Epoch 7454\n",
      "-------------------------------\n",
      "tensor(128.9585)\n",
      "tensor(43.5178)\n",
      "tensor(98.9073)\n",
      "tensor(2.6394)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.119198\n",
      "Epoch 7455\n",
      "-------------------------------\n",
      "tensor(65.1310)\n",
      "tensor(22.3292)\n",
      "tensor(47.0147)\n",
      "tensor(1.2136)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.838203\n",
      "Epoch 7456\n",
      "-------------------------------\n",
      "tensor(53.4115)\n",
      "tensor(18.1553)\n",
      "tensor(33.0476)\n",
      "tensor(0.9927)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.811722\n",
      "Epoch 7457\n",
      "-------------------------------\n",
      "tensor(71.7002)\n",
      "tensor(25.0327)\n",
      "tensor(45.5203)\n",
      "tensor(1.1465)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.805126\n",
      "Epoch 7458\n",
      "-------------------------------\n",
      "tensor(32.5506)\n",
      "tensor(15.0715)\n",
      "tensor(26.3592)\n",
      "tensor(0.8757)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.752010\n",
      "Epoch 7459\n",
      "-------------------------------\n",
      "tensor(35.3692)\n",
      "tensor(9.0332)\n",
      "tensor(12.2004)\n",
      "tensor(0.0636)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.739521\n",
      "Epoch 7460\n",
      "-------------------------------\n",
      "tensor(41.8560)\n",
      "tensor(10.2891)\n",
      "tensor(24.8237)\n",
      "tensor(0.4427)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.749313\n",
      "Epoch 7461\n",
      "-------------------------------\n",
      "tensor(34.5959)\n",
      "tensor(9.1702)\n",
      "tensor(19.9624)\n",
      "tensor(0.3981)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.733376\n",
      "Epoch 7462\n",
      "-------------------------------\n",
      "tensor(20.5886)\n",
      "tensor(6.0207)\n",
      "tensor(8.2383)\n",
      "tensor(0.1800)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.711594\n",
      "Epoch 7463\n",
      "-------------------------------\n",
      "tensor(30.3825)\n",
      "tensor(9.5633)\n",
      "tensor(9.5206)\n",
      "tensor(0.1626)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.703808\n",
      "Epoch 7464\n",
      "-------------------------------\n",
      "tensor(48.6424)\n",
      "tensor(14.5058)\n",
      "tensor(26.8086)\n",
      "tensor(0.5074)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.726368\n",
      "Epoch 7465\n",
      "-------------------------------\n",
      "tensor(52.8618)\n",
      "tensor(15.1943)\n",
      "tensor(21.6208)\n",
      "tensor(0.3673)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.706734\n",
      "Epoch 7466\n",
      "-------------------------------\n",
      "tensor(41.6111)\n",
      "tensor(15.3402)\n",
      "tensor(21.3918)\n",
      "tensor(0.4941)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.689129\n",
      "Epoch 7467\n",
      "-------------------------------\n",
      "tensor(51.1214)\n",
      "tensor(16.8260)\n",
      "tensor(29.3256)\n",
      "tensor(0.4292)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.703407\n",
      "Epoch 7468\n",
      "-------------------------------\n",
      "tensor(70.5815)\n",
      "tensor(23.4282)\n",
      "tensor(38.4495)\n",
      "tensor(1.1880)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.802094\n",
      "Epoch 7469\n",
      "-------------------------------\n",
      "tensor(46.2265)\n",
      "tensor(16.2034)\n",
      "tensor(14.7084)\n",
      "tensor(0.3291)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.727249\n",
      "Epoch 7470\n",
      "-------------------------------\n",
      "tensor(38.1485)\n",
      "tensor(16.7239)\n",
      "tensor(27.4598)\n",
      "tensor(0.7090)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.709732\n",
      "Epoch 7471\n",
      "-------------------------------\n",
      "tensor(88.3931)\n",
      "tensor(24.8886)\n",
      "tensor(59.6185)\n",
      "tensor(1.3236)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.893299\n",
      "Epoch 7472\n",
      "-------------------------------\n",
      "tensor(124.1442)\n",
      "tensor(40.5421)\n",
      "tensor(91.0896)\n",
      "tensor(2.2084)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.045822\n",
      "Epoch 7473\n",
      "-------------------------------\n",
      "tensor(155.7252)\n",
      "tensor(57.0843)\n",
      "tensor(119.6730)\n",
      "tensor(3.1596)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.295200\n",
      "Epoch 7474\n",
      "-------------------------------\n",
      "tensor(159.5687)\n",
      "tensor(47.9821)\n",
      "tensor(115.4332)\n",
      "tensor(2.7257)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.266228\n",
      "Epoch 7475\n",
      "-------------------------------\n",
      "tensor(82.0592)\n",
      "tensor(29.1011)\n",
      "tensor(59.6214)\n",
      "tensor(1.5127)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.848175\n",
      "Epoch 7476\n",
      "-------------------------------\n",
      "tensor(54.6018)\n",
      "tensor(17.9227)\n",
      "tensor(37.8419)\n",
      "tensor(0.9347)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.752460\n",
      "Epoch 7477\n",
      "-------------------------------\n",
      "tensor(80.7278)\n",
      "tensor(27.8959)\n",
      "tensor(56.2359)\n",
      "tensor(1.4092)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.818760\n",
      "Epoch 7478\n",
      "-------------------------------\n",
      "tensor(56.1857)\n",
      "tensor(19.2512)\n",
      "tensor(33.6843)\n",
      "tensor(0.7652)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.731491\n",
      "Epoch 7479\n",
      "-------------------------------\n",
      "tensor(36.3532)\n",
      "tensor(10.8383)\n",
      "tensor(15.8804)\n",
      "tensor(0.5038)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.701580\n",
      "Epoch 7480\n",
      "-------------------------------\n",
      "tensor(50.2838)\n",
      "tensor(15.2209)\n",
      "tensor(33.3911)\n",
      "tensor(0.9243)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.730793\n",
      "Epoch 7481\n",
      "-------------------------------\n",
      "tensor(45.6826)\n",
      "tensor(15.0036)\n",
      "tensor(28.4028)\n",
      "tensor(0.7586)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.711742\n",
      "Epoch 7482\n",
      "-------------------------------\n",
      "tensor(24.6187)\n",
      "tensor(9.2955)\n",
      "tensor(13.8806)\n",
      "tensor(0.3503)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.684319\n",
      "Epoch 7483\n",
      "-------------------------------\n",
      "tensor(19.1175)\n",
      "tensor(7.8993)\n",
      "tensor(8.4906)\n",
      "tensor(0.2690)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.674217\n",
      "Epoch 7484\n",
      "-------------------------------\n",
      "tensor(50.1854)\n",
      "tensor(18.9327)\n",
      "tensor(32.8799)\n",
      "tensor(0.9646)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.716747\n",
      "Epoch 7485\n",
      "-------------------------------\n",
      "tensor(49.4900)\n",
      "tensor(17.8160)\n",
      "tensor(31.6541)\n",
      "tensor(0.9540)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.700027\n",
      "Epoch 7486\n",
      "-------------------------------\n",
      "tensor(35.5458)\n",
      "tensor(12.5141)\n",
      "tensor(19.0037)\n",
      "tensor(0.4471)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.645748\n",
      "Epoch 7487\n",
      "-------------------------------\n",
      "tensor(72.5303)\n",
      "tensor(28.0354)\n",
      "tensor(39.6604)\n",
      "tensor(1.2195)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.811954\n",
      "Epoch 7488\n",
      "-------------------------------\n",
      "tensor(77.9973)\n",
      "tensor(21.6909)\n",
      "tensor(42.1961)\n",
      "tensor(0.4907)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.792500\n",
      "Epoch 7489\n",
      "-------------------------------\n",
      "tensor(27.0034)\n",
      "tensor(8.1138)\n",
      "tensor(6.5027)\n",
      "tensor(0.3837)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.683403\n",
      "Epoch 7490\n",
      "-------------------------------\n",
      "tensor(72.9248)\n",
      "tensor(14.6873)\n",
      "tensor(46.2586)\n",
      "tensor(0.6561)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.827663\n",
      "Epoch 7491\n",
      "-------------------------------\n",
      "tensor(106.2789)\n",
      "tensor(48.1877)\n",
      "tensor(88.3977)\n",
      "tensor(2.7748)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.042473\n",
      "Epoch 7492\n",
      "-------------------------------\n",
      "tensor(146.7538)\n",
      "tensor(52.2956)\n",
      "tensor(112.5728)\n",
      "tensor(3.2051)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.262104\n",
      "Epoch 7493\n",
      "-------------------------------\n",
      "tensor(181.9853)\n",
      "tensor(52.1474)\n",
      "tensor(128.4185)\n",
      "tensor(2.6557)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.458904\n",
      "Epoch 7494\n",
      "-------------------------------\n",
      "tensor(172.1836)\n",
      "tensor(56.3117)\n",
      "tensor(126.9282)\n",
      "tensor(2.8485)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.417915\n",
      "Epoch 7495\n",
      "-------------------------------\n",
      "tensor(82.7696)\n",
      "tensor(30.3623)\n",
      "tensor(62.9174)\n",
      "tensor(1.9333)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.907043\n",
      "Epoch 7496\n",
      "-------------------------------\n",
      "tensor(64.2740)\n",
      "tensor(17.0579)\n",
      "tensor(40.7170)\n",
      "tensor(0.8895)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.812386\n",
      "Epoch 7497\n",
      "-------------------------------\n",
      "tensor(80.2570)\n",
      "tensor(33.7255)\n",
      "tensor(64.0929)\n",
      "tensor(2.0095)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.850525\n",
      "Epoch 7498\n",
      "-------------------------------\n",
      "tensor(54.1083)\n",
      "tensor(21.7120)\n",
      "tensor(36.7768)\n",
      "tensor(1.2322)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.781872\n",
      "Epoch 7499\n",
      "-------------------------------\n",
      "tensor(42.5814)\n",
      "tensor(13.0310)\n",
      "tensor(18.9812)\n",
      "tensor(0.3501)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.740540\n",
      "Epoch 7500\n",
      "-------------------------------\n",
      "tensor(58.4256)\n",
      "tensor(20.5167)\n",
      "tensor(37.8622)\n",
      "tensor(0.9672)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.750797\n",
      "Epoch 7501\n",
      "-------------------------------\n",
      "tensor(44.1367)\n",
      "tensor(17.3500)\n",
      "tensor(31.8412)\n",
      "tensor(0.8856)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.709225\n",
      "Epoch 7502\n",
      "-------------------------------\n",
      "tensor(32.2826)\n",
      "tensor(12.7897)\n",
      "tensor(15.4275)\n",
      "tensor(0.5163)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.675148\n",
      "Epoch 7503\n",
      "-------------------------------\n",
      "tensor(35.4312)\n",
      "tensor(10.1193)\n",
      "tensor(9.9387)\n",
      "tensor(0.0870)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.673672\n",
      "Epoch 7504\n",
      "-------------------------------\n",
      "tensor(53.6791)\n",
      "tensor(15.2426)\n",
      "tensor(36.4719)\n",
      "tensor(0.7912)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.706745\n",
      "Epoch 7505\n",
      "-------------------------------\n",
      "tensor(54.0346)\n",
      "tensor(16.7063)\n",
      "tensor(33.6433)\n",
      "tensor(0.8137)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.698700\n",
      "Epoch 7506\n",
      "-------------------------------\n",
      "tensor(44.3350)\n",
      "tensor(12.8872)\n",
      "tensor(22.7917)\n",
      "tensor(0.4412)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.666687\n",
      "Epoch 7507\n",
      "-------------------------------\n",
      "tensor(70.3886)\n",
      "tensor(19.5089)\n",
      "tensor(42.8274)\n",
      "tensor(0.8305)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.760738\n",
      "Epoch 7508\n",
      "-------------------------------\n",
      "tensor(74.8045)\n",
      "tensor(28.8058)\n",
      "tensor(47.2786)\n",
      "tensor(1.3924)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.820583\n",
      "Epoch 7509\n",
      "-------------------------------\n",
      "tensor(44.2179)\n",
      "tensor(15.5595)\n",
      "tensor(4.5767)\n",
      "tensor(0.0200)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.685059\n",
      "Epoch 7510\n",
      "-------------------------------\n",
      "tensor(65.0613)\n",
      "tensor(23.3970)\n",
      "tensor(47.1633)\n",
      "tensor(1.0859)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.736469\n",
      "Epoch 7511\n",
      "-------------------------------\n",
      "tensor(121.6667)\n",
      "tensor(33.5460)\n",
      "tensor(87.9276)\n",
      "tensor(1.9725)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.976894\n",
      "Epoch 7512\n",
      "-------------------------------\n",
      "tensor(163.2116)\n",
      "tensor(59.8628)\n",
      "tensor(126.9163)\n",
      "tensor(3.3856)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.299347\n",
      "Epoch 7513\n",
      "-------------------------------\n",
      "tensor(197.9187)\n",
      "tensor(70.2208)\n",
      "tensor(149.7772)\n",
      "tensor(4.0547)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.557423\n",
      "Epoch 7514\n",
      "-------------------------------\n",
      "tensor(194.9098)\n",
      "tensor(57.8704)\n",
      "tensor(139.9994)\n",
      "tensor(3.0584)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.484222\n",
      "Epoch 7515\n",
      "-------------------------------\n",
      "tensor(97.1435)\n",
      "tensor(31.7909)\n",
      "tensor(70.6109)\n",
      "tensor(1.6180)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.876301\n",
      "Epoch 7516\n",
      "-------------------------------\n",
      "tensor(69.0772)\n",
      "tensor(21.0254)\n",
      "tensor(46.0993)\n",
      "tensor(0.8170)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.791145\n",
      "Epoch 7517\n",
      "-------------------------------\n",
      "tensor(93.6798)\n",
      "tensor(31.1379)\n",
      "tensor(68.2938)\n",
      "tensor(1.7644)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.862164\n",
      "Epoch 7518\n",
      "-------------------------------\n",
      "tensor(60.7121)\n",
      "tensor(17.3411)\n",
      "tensor(37.2062)\n",
      "tensor(0.7345)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.721661\n",
      "Epoch 7519\n",
      "-------------------------------\n",
      "tensor(42.7567)\n",
      "tensor(16.4129)\n",
      "tensor(23.6585)\n",
      "tensor(0.8563)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.678059\n",
      "Epoch 7520\n",
      "-------------------------------\n",
      "tensor(62.2773)\n",
      "tensor(23.0144)\n",
      "tensor(42.8095)\n",
      "tensor(1.3099)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.714394\n",
      "Epoch 7521\n",
      "-------------------------------\n",
      "tensor(43.6716)\n",
      "tensor(17.6827)\n",
      "tensor(34.4696)\n",
      "tensor(1.0403)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.688591\n",
      "Epoch 7522\n",
      "-------------------------------\n",
      "tensor(23.3083)\n",
      "tensor(7.8542)\n",
      "tensor(15.7467)\n",
      "tensor(0.4864)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.652363\n",
      "Epoch 7523\n",
      "-------------------------------\n",
      "tensor(33.6000)\n",
      "tensor(11.1937)\n",
      "tensor(11.6413)\n",
      "tensor(0.3219)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.644524\n",
      "Epoch 7524\n",
      "-------------------------------\n",
      "tensor(54.3446)\n",
      "tensor(21.4770)\n",
      "tensor(41.1544)\n",
      "tensor(1.2087)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.692463\n",
      "Epoch 7525\n",
      "-------------------------------\n",
      "tensor(50.4107)\n",
      "tensor(22.1559)\n",
      "tensor(37.2390)\n",
      "tensor(1.2301)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.672333\n",
      "Epoch 7526\n",
      "-------------------------------\n",
      "tensor(59.3016)\n",
      "tensor(16.5590)\n",
      "tensor(25.1997)\n",
      "tensor(0.4019)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.659687\n",
      "Epoch 7527\n",
      "-------------------------------\n",
      "tensor(81.3182)\n",
      "tensor(29.0554)\n",
      "tensor(42.3306)\n",
      "tensor(1.3173)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.715553\n",
      "Epoch 7528\n",
      "-------------------------------\n",
      "tensor(92.8039)\n",
      "tensor(22.7163)\n",
      "tensor(57.8112)\n",
      "tensor(0.5971)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.805470\n",
      "Epoch 7529\n",
      "-------------------------------\n",
      "tensor(39.4908)\n",
      "tensor(11.5905)\n",
      "tensor(19.9277)\n",
      "tensor(0.6285)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.668568\n",
      "Epoch 7530\n",
      "-------------------------------\n",
      "tensor(52.3529)\n",
      "tensor(10.9036)\n",
      "tensor(29.4184)\n",
      "tensor(0.1483)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.721798\n",
      "Epoch 7531\n",
      "-------------------------------\n",
      "tensor(95.6456)\n",
      "tensor(38.3090)\n",
      "tensor(75.4444)\n",
      "tensor(2.2199)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.887875\n",
      "Epoch 7532\n",
      "-------------------------------\n",
      "tensor(141.7888)\n",
      "tensor(59.1515)\n",
      "tensor(115.8761)\n",
      "tensor(3.5783)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.201027\n",
      "Epoch 7533\n",
      "-------------------------------\n",
      "tensor(189.9078)\n",
      "tensor(54.6606)\n",
      "tensor(135.9476)\n",
      "tensor(2.9385)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.446590\n",
      "Epoch 7534\n",
      "-------------------------------\n",
      "tensor(173.8503)\n",
      "tensor(56.1305)\n",
      "tensor(125.6565)\n",
      "tensor(2.9536)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.354790\n",
      "Epoch 7535\n",
      "-------------------------------\n",
      "tensor(75.8400)\n",
      "tensor(30.1175)\n",
      "tensor(57.7113)\n",
      "tensor(1.7020)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.882168\n",
      "Epoch 7536\n",
      "-------------------------------\n",
      "tensor(66.8077)\n",
      "tensor(25.1440)\n",
      "tensor(48.2945)\n",
      "tensor(1.1755)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.829910\n",
      "Epoch 7537\n",
      "-------------------------------\n",
      "tensor(77.9364)\n",
      "tensor(25.1838)\n",
      "tensor(60.1390)\n",
      "tensor(1.6369)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.871307\n",
      "Epoch 7538\n",
      "-------------------------------\n",
      "tensor(57.8999)\n",
      "tensor(17.9073)\n",
      "tensor(40.3631)\n",
      "tensor(1.0336)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.802029\n",
      "Epoch 7539\n",
      "-------------------------------\n",
      "tensor(33.2039)\n",
      "tensor(11.8779)\n",
      "tensor(15.4594)\n",
      "tensor(0.4635)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.739872\n",
      "Epoch 7540\n",
      "-------------------------------\n",
      "tensor(54.7336)\n",
      "tensor(19.5172)\n",
      "tensor(37.1464)\n",
      "tensor(1.0378)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.765671\n",
      "Epoch 7541\n",
      "-------------------------------\n",
      "tensor(51.8715)\n",
      "tensor(17.8638)\n",
      "tensor(32.8082)\n",
      "tensor(0.9174)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.747505\n",
      "Epoch 7542\n",
      "-------------------------------\n",
      "tensor(37.9762)\n",
      "tensor(12.3199)\n",
      "tensor(17.2906)\n",
      "tensor(0.4921)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.714855\n",
      "Epoch 7543\n",
      "-------------------------------\n",
      "tensor(16.8738)\n",
      "tensor(6.2267)\n",
      "tensor(8.1284)\n",
      "tensor(0.2029)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.691074\n",
      "Epoch 7544\n",
      "-------------------------------\n",
      "tensor(47.3709)\n",
      "tensor(19.5200)\n",
      "tensor(37.7990)\n",
      "tensor(1.0292)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.726410\n",
      "Epoch 7545\n",
      "-------------------------------\n",
      "tensor(55.3155)\n",
      "tensor(22.6747)\n",
      "tensor(39.8855)\n",
      "tensor(1.1404)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.709118\n",
      "Epoch 7546\n",
      "-------------------------------\n",
      "tensor(34.9104)\n",
      "tensor(8.1278)\n",
      "tensor(18.1322)\n",
      "tensor(0.2970)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.640076\n",
      "Epoch 7547\n",
      "-------------------------------\n",
      "tensor(71.3023)\n",
      "tensor(21.7234)\n",
      "tensor(46.3484)\n",
      "tensor(1.1296)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.701492\n",
      "Epoch 7548\n",
      "-------------------------------\n",
      "tensor(73.2414)\n",
      "tensor(23.7173)\n",
      "tensor(40.6909)\n",
      "tensor(1.0224)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.855354\n",
      "Epoch 7549\n",
      "-------------------------------\n",
      "tensor(53.2141)\n",
      "tensor(17.6070)\n",
      "tensor(5.0038)\n",
      "tensor(0.3123)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.873371\n",
      "Epoch 7550\n",
      "-------------------------------\n",
      "tensor(80.9564)\n",
      "tensor(27.5670)\n",
      "tensor(53.5169)\n",
      "tensor(0.8814)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.843727\n",
      "Epoch 7551\n",
      "-------------------------------\n",
      "tensor(116.4198)\n",
      "tensor(29.3433)\n",
      "tensor(82.0399)\n",
      "tensor(1.9418)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.923828\n",
      "Epoch 7552\n",
      "-------------------------------\n",
      "tensor(138.3041)\n",
      "tensor(60.6867)\n",
      "tensor(115.1423)\n",
      "tensor(3.5075)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.151752\n",
      "Epoch 7553\n",
      "-------------------------------\n",
      "tensor(179.4680)\n",
      "tensor(59.4718)\n",
      "tensor(130.5673)\n",
      "tensor(3.1918)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.415241\n",
      "Epoch 7554\n",
      "-------------------------------\n",
      "tensor(179.7735)\n",
      "tensor(45.1239)\n",
      "tensor(121.9976)\n",
      "tensor(2.2622)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.369579\n",
      "Epoch 7555\n",
      "-------------------------------\n",
      "tensor(83.0845)\n",
      "tensor(37.7610)\n",
      "tensor(68.3722)\n",
      "tensor(2.0110)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.900368\n",
      "Epoch 7556\n",
      "-------------------------------\n",
      "tensor(63.0350)\n",
      "tensor(20.5665)\n",
      "tensor(41.9498)\n",
      "tensor(0.6973)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.797600\n",
      "Epoch 7557\n",
      "-------------------------------\n",
      "tensor(84.9190)\n",
      "tensor(30.9663)\n",
      "tensor(64.1886)\n",
      "tensor(1.9393)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.870094\n",
      "Epoch 7558\n",
      "-------------------------------\n",
      "tensor(49.6817)\n",
      "tensor(16.6160)\n",
      "tensor(34.2880)\n",
      "tensor(0.9032)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.749924\n",
      "Epoch 7559\n",
      "-------------------------------\n",
      "tensor(36.5463)\n",
      "tensor(13.0469)\n",
      "tensor(22.8574)\n",
      "tensor(0.7213)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.713409\n",
      "Epoch 7560\n",
      "-------------------------------\n",
      "tensor(55.4420)\n",
      "tensor(20.1035)\n",
      "tensor(40.9119)\n",
      "tensor(1.2328)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.740875\n",
      "Epoch 7561\n",
      "-------------------------------\n",
      "tensor(45.1766)\n",
      "tensor(16.1683)\n",
      "tensor(33.0879)\n",
      "tensor(1.0191)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.707863\n",
      "Epoch 7562\n",
      "-------------------------------\n",
      "tensor(28.1051)\n",
      "tensor(9.2062)\n",
      "tensor(15.1460)\n",
      "tensor(0.5131)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.670517\n",
      "Epoch 7563\n",
      "-------------------------------\n",
      "tensor(23.5917)\n",
      "tensor(7.8293)\n",
      "tensor(11.3675)\n",
      "tensor(0.2454)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.657318\n",
      "Epoch 7564\n",
      "-------------------------------\n",
      "tensor(53.0611)\n",
      "tensor(20.3415)\n",
      "tensor(38.7235)\n",
      "tensor(1.0999)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.690205\n",
      "Epoch 7565\n",
      "-------------------------------\n",
      "tensor(52.2644)\n",
      "tensor(20.9184)\n",
      "tensor(34.5331)\n",
      "tensor(1.1667)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.650829\n",
      "Epoch 7566\n",
      "-------------------------------\n",
      "tensor(51.8717)\n",
      "tensor(14.4278)\n",
      "tensor(23.0744)\n",
      "tensor(0.3142)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.610378\n",
      "Epoch 7567\n",
      "-------------------------------\n",
      "tensor(74.2270)\n",
      "tensor(27.3322)\n",
      "tensor(38.3017)\n",
      "tensor(1.1445)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.791313\n",
      "Epoch 7568\n",
      "-------------------------------\n",
      "tensor(84.7359)\n",
      "tensor(23.2992)\n",
      "tensor(49.6931)\n",
      "tensor(0.4640)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.911152\n",
      "Epoch 7569\n",
      "-------------------------------\n",
      "tensor(45.3434)\n",
      "tensor(15.6375)\n",
      "tensor(14.0592)\n",
      "tensor(0.6820)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.749420\n",
      "Epoch 7570\n",
      "-------------------------------\n",
      "tensor(50.5472)\n",
      "tensor(9.4173)\n",
      "tensor(29.0540)\n",
      "tensor(0.1603)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.671165\n",
      "Epoch 7571\n",
      "-------------------------------\n",
      "tensor(83.7186)\n",
      "tensor(40.5768)\n",
      "tensor(71.2991)\n",
      "tensor(2.3967)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.832302\n",
      "Epoch 7572\n",
      "-------------------------------\n",
      "tensor(124.1079)\n",
      "tensor(47.4776)\n",
      "tensor(98.1831)\n",
      "tensor(2.8867)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.028797\n",
      "Epoch 7573\n",
      "-------------------------------\n",
      "tensor(164.9003)\n",
      "tensor(43.8449)\n",
      "tensor(113.6550)\n",
      "tensor(2.2151)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.233253\n",
      "Epoch 7574\n",
      "-------------------------------\n",
      "tensor(152.6749)\n",
      "tensor(52.3565)\n",
      "tensor(114.7626)\n",
      "tensor(2.6737)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.210682\n",
      "Epoch 7575\n",
      "-------------------------------\n",
      "tensor(75.4833)\n",
      "tensor(28.0674)\n",
      "tensor(57.1165)\n",
      "tensor(1.8093)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.807693\n",
      "Epoch 7576\n",
      "-------------------------------\n",
      "tensor(56.7175)\n",
      "tensor(16.9414)\n",
      "tensor(38.3595)\n",
      "tensor(0.9032)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.725220\n",
      "Epoch 7577\n",
      "-------------------------------\n",
      "tensor(73.2650)\n",
      "tensor(28.6068)\n",
      "tensor(58.7105)\n",
      "tensor(1.7806)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.764584\n",
      "Epoch 7578\n",
      "-------------------------------\n",
      "tensor(46.8945)\n",
      "tensor(17.5670)\n",
      "tensor(34.8925)\n",
      "tensor(1.1095)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.667198\n",
      "Epoch 7579\n",
      "-------------------------------\n",
      "tensor(42.9687)\n",
      "tensor(13.8737)\n",
      "tensor(16.5749)\n",
      "tensor(0.3639)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.623234\n",
      "Epoch 7580\n",
      "-------------------------------\n",
      "tensor(52.0658)\n",
      "tensor(18.9565)\n",
      "tensor(35.5101)\n",
      "tensor(0.9427)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.642712\n",
      "Epoch 7581\n",
      "-------------------------------\n",
      "tensor(41.9858)\n",
      "tensor(16.5416)\n",
      "tensor(30.3460)\n",
      "tensor(0.8624)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.621880\n",
      "Epoch 7582\n",
      "-------------------------------\n",
      "tensor(32.1636)\n",
      "tensor(12.2691)\n",
      "tensor(14.9518)\n",
      "tensor(0.4987)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.589359\n",
      "Epoch 7583\n",
      "-------------------------------\n",
      "tensor(26.0444)\n",
      "tensor(10.0982)\n",
      "tensor(9.2441)\n",
      "tensor(0.0984)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.579208\n",
      "Epoch 7584\n",
      "-------------------------------\n",
      "tensor(61.7276)\n",
      "tensor(19.8151)\n",
      "tensor(35.0007)\n",
      "tensor(0.7958)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.622547\n",
      "Epoch 7585\n",
      "-------------------------------\n",
      "tensor(60.1747)\n",
      "tensor(19.9247)\n",
      "tensor(31.1762)\n",
      "tensor(0.8472)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.654297\n",
      "Epoch 7586\n",
      "-------------------------------\n",
      "tensor(48.4520)\n",
      "tensor(12.7841)\n",
      "tensor(23.8742)\n",
      "tensor(0.2896)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.634987\n",
      "Epoch 7587\n",
      "-------------------------------\n",
      "tensor(69.6006)\n",
      "tensor(19.8350)\n",
      "tensor(38.7826)\n",
      "tensor(0.6349)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.670803\n",
      "Epoch 7588\n",
      "-------------------------------\n",
      "tensor(65.6325)\n",
      "tensor(22.6512)\n",
      "tensor(43.3428)\n",
      "tensor(1.2424)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.673580\n",
      "Epoch 7589\n",
      "-------------------------------\n",
      "tensor(33.9659)\n",
      "tensor(11.7827)\n",
      "tensor(5.7465)\n",
      "tensor(0.2881)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.605114\n",
      "Epoch 7590\n",
      "-------------------------------\n",
      "tensor(55.3724)\n",
      "tensor(23.0020)\n",
      "tensor(42.2464)\n",
      "tensor(1.1797)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.698685\n",
      "Epoch 7591\n",
      "-------------------------------\n",
      "tensor(105.9768)\n",
      "tensor(34.7749)\n",
      "tensor(78.9321)\n",
      "tensor(2.0084)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.890320\n",
      "Epoch 7592\n",
      "-------------------------------\n",
      "tensor(150.1193)\n",
      "tensor(48.3097)\n",
      "tensor(110.5049)\n",
      "tensor(2.6523)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.136196\n",
      "Epoch 7593\n",
      "-------------------------------\n",
      "tensor(179.1883)\n",
      "tensor(62.4085)\n",
      "tensor(136.5848)\n",
      "tensor(3.4850)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.376411\n",
      "Epoch 7594\n",
      "-------------------------------\n",
      "tensor(175.0301)\n",
      "tensor(56.0128)\n",
      "tensor(129.5714)\n",
      "tensor(3.1835)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.293091\n",
      "Epoch 7595\n",
      "-------------------------------\n",
      "tensor(82.5059)\n",
      "tensor(29.7623)\n",
      "tensor(61.9299)\n",
      "tensor(1.5936)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.796608\n",
      "Epoch 7596\n",
      "-------------------------------\n",
      "tensor(73.4596)\n",
      "tensor(24.1099)\n",
      "tensor(46.6636)\n",
      "tensor(1.1883)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.769131\n",
      "Epoch 7597\n",
      "-------------------------------\n",
      "tensor(90.1499)\n",
      "tensor(30.0398)\n",
      "tensor(63.2353)\n",
      "tensor(1.4980)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.794342\n",
      "Epoch 7598\n",
      "-------------------------------\n",
      "tensor(57.5003)\n",
      "tensor(19.1046)\n",
      "tensor(39.6591)\n",
      "tensor(0.8741)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.675259\n",
      "Epoch 7599\n",
      "-------------------------------\n",
      "tensor(40.0680)\n",
      "tensor(11.7510)\n",
      "tensor(19.2393)\n",
      "tensor(0.5166)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.624443\n",
      "Epoch 7600\n",
      "-------------------------------\n",
      "tensor(65.3792)\n",
      "tensor(19.4095)\n",
      "tensor(39.4478)\n",
      "tensor(0.9644)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.689838\n",
      "Epoch 7601\n",
      "-------------------------------\n",
      "tensor(54.0079)\n",
      "tensor(15.6774)\n",
      "tensor(32.6172)\n",
      "tensor(0.7735)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.671577\n",
      "Epoch 7602\n",
      "-------------------------------\n",
      "tensor(37.3246)\n",
      "tensor(10.6369)\n",
      "tensor(15.0221)\n",
      "tensor(0.3277)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.631207\n",
      "Epoch 7603\n",
      "-------------------------------\n",
      "tensor(22.3865)\n",
      "tensor(8.5746)\n",
      "tensor(11.5458)\n",
      "tensor(0.3266)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.607964\n",
      "Epoch 7604\n",
      "-------------------------------\n",
      "tensor(55.3804)\n",
      "tensor(19.4109)\n",
      "tensor(39.4952)\n",
      "tensor(1.0202)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.657890\n",
      "Epoch 7605\n",
      "-------------------------------\n",
      "tensor(49.5968)\n",
      "tensor(17.8550)\n",
      "tensor(34.1545)\n",
      "tensor(0.8837)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.630367\n",
      "Epoch 7606\n",
      "-------------------------------\n",
      "tensor(57.9220)\n",
      "tensor(20.3949)\n",
      "tensor(27.8169)\n",
      "tensor(0.6744)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.620094\n",
      "Epoch 7607\n",
      "-------------------------------\n",
      "tensor(71.9225)\n",
      "tensor(27.0419)\n",
      "tensor(41.9963)\n",
      "tensor(1.2292)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.647804\n",
      "Epoch 7608\n",
      "-------------------------------\n",
      "tensor(86.7913)\n",
      "tensor(22.5610)\n",
      "tensor(55.3198)\n",
      "tensor(0.9529)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.765530\n",
      "Epoch 7609\n",
      "-------------------------------\n",
      "tensor(35.9961)\n",
      "tensor(11.9058)\n",
      "tensor(17.3510)\n",
      "tensor(0.5641)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.665752\n",
      "Epoch 7610\n",
      "-------------------------------\n",
      "tensor(67.3289)\n",
      "tensor(15.9145)\n",
      "tensor(32.0990)\n",
      "tensor(0.4190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.666893\n",
      "Epoch 7611\n",
      "-------------------------------\n",
      "tensor(112.7658)\n",
      "tensor(41.9244)\n",
      "tensor(85.7191)\n",
      "tensor(2.1732)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.908901\n",
      "Epoch 7612\n",
      "-------------------------------\n",
      "tensor(167.9360)\n",
      "tensor(56.8149)\n",
      "tensor(130.6929)\n",
      "tensor(3.5021)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.273560\n",
      "Epoch 7613\n",
      "-------------------------------\n",
      "tensor(215.0882)\n",
      "tensor(73.4251)\n",
      "tensor(162.9549)\n",
      "tensor(4.1757)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.655796\n",
      "Epoch 7614\n",
      "-------------------------------\n",
      "tensor(219.1640)\n",
      "tensor(72.6380)\n",
      "tensor(161.7134)\n",
      "tensor(3.7912)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.672115\n",
      "Epoch 7615\n",
      "-------------------------------\n",
      "tensor(119.6614)\n",
      "tensor(33.2975)\n",
      "tensor(82.8077)\n",
      "tensor(1.8157)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.903057\n",
      "Epoch 7616\n",
      "-------------------------------\n",
      "tensor(79.2104)\n",
      "tensor(19.9548)\n",
      "tensor(47.6680)\n",
      "tensor(0.7757)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.770866\n",
      "Epoch 7617\n",
      "-------------------------------\n",
      "tensor(108.8148)\n",
      "tensor(39.2036)\n",
      "tensor(80.0945)\n",
      "tensor(2.2213)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.896683\n",
      "Epoch 7618\n",
      "-------------------------------\n",
      "tensor(68.4402)\n",
      "tensor(20.8730)\n",
      "tensor(40.7379)\n",
      "tensor(1.0083)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.671146\n",
      "Epoch 7619\n",
      "-------------------------------\n",
      "tensor(41.6378)\n",
      "tensor(17.4672)\n",
      "tensor(30.2838)\n",
      "tensor(0.8651)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.646782\n",
      "Epoch 7620\n",
      "-------------------------------\n",
      "tensor(65.7178)\n",
      "tensor(26.5699)\n",
      "tensor(50.3560)\n",
      "tensor(1.4097)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.708580\n",
      "Epoch 7621\n",
      "-------------------------------\n",
      "tensor(51.8186)\n",
      "tensor(21.7578)\n",
      "tensor(38.8058)\n",
      "tensor(1.1266)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.668568\n",
      "Epoch 7622\n",
      "-------------------------------\n",
      "tensor(40.5572)\n",
      "tensor(15.0656)\n",
      "tensor(15.7246)\n",
      "tensor(0.5327)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.625664\n",
      "Epoch 7623\n",
      "-------------------------------\n",
      "tensor(40.5432)\n",
      "tensor(11.2129)\n",
      "tensor(16.3568)\n",
      "tensor(0.3267)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.624638\n",
      "Epoch 7624\n",
      "-------------------------------\n",
      "tensor(62.7470)\n",
      "tensor(20.2865)\n",
      "tensor(47.8689)\n",
      "tensor(1.2377)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.681534\n",
      "Epoch 7625\n",
      "-------------------------------\n",
      "tensor(49.9043)\n",
      "tensor(18.7758)\n",
      "tensor(40.1386)\n",
      "tensor(1.1646)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.640049\n",
      "Epoch 7626\n",
      "-------------------------------\n",
      "tensor(51.8147)\n",
      "tensor(13.4905)\n",
      "tensor(30.4725)\n",
      "tensor(0.5891)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.604675\n",
      "Epoch 7627\n",
      "-------------------------------\n",
      "tensor(77.8137)\n",
      "tensor(22.1643)\n",
      "tensor(49.0618)\n",
      "tensor(1.1836)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.688801\n",
      "Epoch 7628\n",
      "-------------------------------\n",
      "tensor(93.4518)\n",
      "tensor(34.2520)\n",
      "tensor(57.9568)\n",
      "tensor(1.5150)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.769768\n",
      "Epoch 7629\n",
      "-------------------------------\n",
      "tensor(45.5426)\n",
      "tensor(15.7355)\n",
      "tensor(12.5878)\n",
      "tensor(0.1172)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.552841\n",
      "Epoch 7630\n",
      "-------------------------------\n",
      "tensor(63.7340)\n",
      "tensor(22.8005)\n",
      "tensor(44.1067)\n",
      "tensor(0.8598)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.663494\n",
      "Epoch 7631\n",
      "-------------------------------\n",
      "tensor(135.7463)\n",
      "tensor(32.7559)\n",
      "tensor(89.7812)\n",
      "tensor(1.7012)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.991913\n",
      "Epoch 7632\n",
      "-------------------------------\n",
      "tensor(179.0868)\n",
      "tensor(68.6862)\n",
      "tensor(141.7815)\n",
      "tensor(3.8601)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.376244\n",
      "Epoch 7633\n",
      "-------------------------------\n",
      "tensor(223.1426)\n",
      "tensor(79.9648)\n",
      "tensor(176.3452)\n",
      "tensor(4.8230)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.778053\n",
      "Epoch 7634\n",
      "-------------------------------\n",
      "tensor(224.7592)\n",
      "tensor(68.6018)\n",
      "tensor(164.3876)\n",
      "tensor(3.8424)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.710426\n",
      "Epoch 7635\n",
      "-------------------------------\n",
      "tensor(108.0067)\n",
      "tensor(37.8363)\n",
      "tensor(79.6637)\n",
      "tensor(1.8197)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.898243\n",
      "Epoch 7636\n",
      "-------------------------------\n",
      "tensor(92.7133)\n",
      "tensor(31.4280)\n",
      "tensor(58.4858)\n",
      "tensor(1.3210)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.771481\n",
      "Epoch 7637\n",
      "-------------------------------\n",
      "tensor(116.2039)\n",
      "tensor(33.3421)\n",
      "tensor(81.5161)\n",
      "tensor(1.7846)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.869576\n",
      "Epoch 7638\n",
      "-------------------------------\n",
      "tensor(70.7043)\n",
      "tensor(19.4348)\n",
      "tensor(44.3478)\n",
      "tensor(0.7932)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.682774\n",
      "Epoch 7639\n",
      "-------------------------------\n",
      "tensor(36.3585)\n",
      "tensor(13.5208)\n",
      "tensor(28.6850)\n",
      "tensor(0.9096)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.617195\n",
      "Epoch 7640\n",
      "-------------------------------\n",
      "tensor(67.8550)\n",
      "tensor(21.9254)\n",
      "tensor(51.9127)\n",
      "tensor(1.3414)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.682392\n",
      "Epoch 7641\n",
      "-------------------------------\n",
      "tensor(57.1398)\n",
      "tensor(16.8418)\n",
      "tensor(41.9767)\n",
      "tensor(0.9800)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.643684\n",
      "Epoch 7642\n",
      "-------------------------------\n",
      "tensor(36.5537)\n",
      "tensor(9.7008)\n",
      "tensor(18.9088)\n",
      "tensor(0.3199)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.586189\n",
      "Epoch 7643\n",
      "-------------------------------\n",
      "tensor(35.4597)\n",
      "tensor(13.8915)\n",
      "tensor(15.9570)\n",
      "tensor(0.5951)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.570164\n",
      "Epoch 7644\n",
      "-------------------------------\n",
      "tensor(72.5202)\n",
      "tensor(27.4633)\n",
      "tensor(52.6361)\n",
      "tensor(1.5121)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.661339\n",
      "Epoch 7645\n",
      "-------------------------------\n",
      "tensor(69.4357)\n",
      "tensor(23.9706)\n",
      "tensor(47.6373)\n",
      "tensor(1.2426)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.626259\n",
      "Epoch 7646\n",
      "-------------------------------\n",
      "tensor(58.4332)\n",
      "tensor(21.2303)\n",
      "tensor(31.5694)\n",
      "tensor(1.0097)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.603180\n",
      "Epoch 7647\n",
      "-------------------------------\n",
      "tensor(87.2040)\n",
      "tensor(34.4979)\n",
      "tensor(59.8699)\n",
      "tensor(1.9278)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.720528\n",
      "Epoch 7648\n",
      "-------------------------------\n",
      "tensor(96.2452)\n",
      "tensor(29.6368)\n",
      "tensor(66.0704)\n",
      "tensor(1.3574)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.790768\n",
      "Epoch 7649\n",
      "-------------------------------\n",
      "tensor(26.6901)\n",
      "tensor(10.0653)\n",
      "tensor(7.9679)\n",
      "tensor(0.0519)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.633259\n",
      "Epoch 7650\n",
      "-------------------------------\n",
      "tensor(84.3001)\n",
      "tensor(22.4265)\n",
      "tensor(54.3142)\n",
      "tensor(0.9956)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.771194\n",
      "Epoch 7651\n",
      "-------------------------------\n",
      "tensor(148.5141)\n",
      "tensor(47.8038)\n",
      "tensor(108.6703)\n",
      "tensor(2.4775)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.068798\n",
      "Epoch 7652\n",
      "-------------------------------\n",
      "tensor(209.1340)\n",
      "tensor(72.9995)\n",
      "tensor(161.7574)\n",
      "tensor(4.2724)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.555672\n",
      "Epoch 7653\n",
      "-------------------------------\n",
      "tensor(264.2991)\n",
      "tensor(89.4346)\n",
      "tensor(201.9926)\n",
      "tensor(5.1976)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.167374\n",
      "Epoch 7654\n",
      "-------------------------------\n",
      "tensor(262.4460)\n",
      "tensor(88.1414)\n",
      "tensor(195.0697)\n",
      "tensor(4.8441)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.062359\n",
      "Epoch 7655\n",
      "-------------------------------\n",
      "tensor(137.9144)\n",
      "tensor(40.7204)\n",
      "tensor(97.8105)\n",
      "tensor(2.0603)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.967834\n",
      "Epoch 7656\n",
      "-------------------------------\n",
      "tensor(82.9782)\n",
      "tensor(26.3322)\n",
      "tensor(58.1229)\n",
      "tensor(1.2934)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.731480\n",
      "Epoch 7657\n",
      "-------------------------------\n",
      "tensor(138.1922)\n",
      "tensor(41.7522)\n",
      "tensor(93.8387)\n",
      "tensor(2.1576)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.952789\n",
      "Epoch 7658\n",
      "-------------------------------\n",
      "tensor(66.0234)\n",
      "tensor(18.5054)\n",
      "tensor(45.3655)\n",
      "tensor(0.9620)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.651447\n",
      "Epoch 7659\n",
      "-------------------------------\n",
      "tensor(48.2595)\n",
      "tensor(18.6094)\n",
      "tensor(34.9949)\n",
      "tensor(0.9632)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.617455\n",
      "Epoch 7660\n",
      "-------------------------------\n",
      "tensor(77.2021)\n",
      "tensor(27.1304)\n",
      "tensor(56.8694)\n",
      "tensor(1.4521)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.691395\n",
      "Epoch 7661\n",
      "-------------------------------\n",
      "tensor(62.9770)\n",
      "tensor(21.4637)\n",
      "tensor(43.0330)\n",
      "tensor(1.0762)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.637814\n",
      "Epoch 7662\n",
      "-------------------------------\n",
      "tensor(41.2727)\n",
      "tensor(13.1756)\n",
      "tensor(16.4118)\n",
      "tensor(0.3873)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.575226\n",
      "Epoch 7663\n",
      "-------------------------------\n",
      "tensor(42.6818)\n",
      "tensor(13.9911)\n",
      "tensor(20.3329)\n",
      "tensor(0.5634)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.566307\n",
      "Epoch 7664\n",
      "-------------------------------\n",
      "tensor(76.9953)\n",
      "tensor(27.0107)\n",
      "tensor(55.9496)\n",
      "tensor(1.4938)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.668758\n",
      "Epoch 7665\n",
      "-------------------------------\n",
      "tensor(70.6942)\n",
      "tensor(24.9779)\n",
      "tensor(44.8492)\n",
      "tensor(1.2313)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.627357\n",
      "Epoch 7666\n",
      "-------------------------------\n",
      "tensor(61.8003)\n",
      "tensor(17.6832)\n",
      "tensor(40.5163)\n",
      "tensor(0.8897)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.623188\n",
      "Epoch 7667\n",
      "-------------------------------\n",
      "tensor(81.7768)\n",
      "tensor(26.5780)\n",
      "tensor(55.2187)\n",
      "tensor(1.3568)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.677399\n",
      "Epoch 7668\n",
      "-------------------------------\n",
      "tensor(104.7399)\n",
      "tensor(35.7935)\n",
      "tensor(71.4493)\n",
      "tensor(1.8249)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.801395\n",
      "Epoch 7669\n",
      "-------------------------------\n",
      "tensor(34.6985)\n",
      "tensor(12.3380)\n",
      "tensor(19.4791)\n",
      "tensor(0.2576)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.585114\n",
      "Epoch 7670\n",
      "-------------------------------\n",
      "tensor(54.3936)\n",
      "tensor(21.8125)\n",
      "tensor(40.5184)\n",
      "tensor(1.0761)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.636333\n",
      "Epoch 7671\n",
      "-------------------------------\n",
      "tensor(130.4834)\n",
      "tensor(36.6928)\n",
      "tensor(90.4016)\n",
      "tensor(1.9141)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.899738\n",
      "Epoch 7672\n",
      "-------------------------------\n",
      "tensor(190.4998)\n",
      "tensor(62.3106)\n",
      "tensor(141.7213)\n",
      "tensor(3.4224)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.318073\n",
      "Epoch 7673\n",
      "-------------------------------\n",
      "tensor(249.8562)\n",
      "tensor(88.1582)\n",
      "tensor(187.3097)\n",
      "tensor(4.9186)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.964695\n",
      "Epoch 7674\n",
      "-------------------------------\n",
      "tensor(259.5257)\n",
      "tensor(81.4812)\n",
      "tensor(192.6198)\n",
      "tensor(4.4945)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.041832\n",
      "Epoch 7675\n",
      "-------------------------------\n",
      "tensor(134.5826)\n",
      "tensor(45.4576)\n",
      "tensor(102.4387)\n",
      "tensor(2.6440)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.945469\n",
      "Epoch 7676\n",
      "-------------------------------\n",
      "tensor(88.2112)\n",
      "tensor(24.8639)\n",
      "tensor(58.3920)\n",
      "tensor(1.2577)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.690315\n",
      "Epoch 7677\n",
      "-------------------------------\n",
      "tensor(128.6426)\n",
      "tensor(47.5102)\n",
      "tensor(99.1263)\n",
      "tensor(2.6438)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.931805\n",
      "Epoch 7678\n",
      "-------------------------------\n",
      "tensor(67.9390)\n",
      "tensor(25.0411)\n",
      "tensor(49.4158)\n",
      "tensor(1.3007)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.665035\n",
      "Epoch 7679\n",
      "-------------------------------\n",
      "tensor(61.5946)\n",
      "tensor(19.1861)\n",
      "tensor(35.5771)\n",
      "tensor(0.9060)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.615654\n",
      "Epoch 7680\n",
      "-------------------------------\n",
      "tensor(87.8886)\n",
      "tensor(29.2109)\n",
      "tensor(59.4950)\n",
      "tensor(1.5691)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.685452\n",
      "Epoch 7681\n",
      "-------------------------------\n",
      "tensor(57.6477)\n",
      "tensor(20.3555)\n",
      "tensor(45.4752)\n",
      "tensor(1.2553)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.622589\n",
      "Epoch 7682\n",
      "-------------------------------\n",
      "tensor(25.1845)\n",
      "tensor(9.2127)\n",
      "tensor(18.2459)\n",
      "tensor(0.5702)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.558979\n",
      "Epoch 7683\n",
      "-------------------------------\n",
      "tensor(36.3566)\n",
      "tensor(10.8505)\n",
      "tensor(19.6745)\n",
      "tensor(0.4151)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.565105\n",
      "Epoch 7684\n",
      "-------------------------------\n",
      "tensor(79.4419)\n",
      "tensor(26.0098)\n",
      "tensor(56.7780)\n",
      "tensor(1.4282)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.668091\n",
      "Epoch 7685\n",
      "-------------------------------\n",
      "tensor(66.1095)\n",
      "tensor(22.2628)\n",
      "tensor(45.2179)\n",
      "tensor(1.2491)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.599297\n",
      "Epoch 7686\n",
      "-------------------------------\n",
      "tensor(63.7574)\n",
      "tensor(20.8108)\n",
      "tensor(40.4888)\n",
      "tensor(0.8847)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.572880\n",
      "Epoch 7687\n",
      "-------------------------------\n",
      "tensor(93.9155)\n",
      "tensor(33.4965)\n",
      "tensor(57.0118)\n",
      "tensor(1.5151)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.705189\n",
      "Epoch 7688\n",
      "-------------------------------\n",
      "tensor(116.4701)\n",
      "tensor(33.2912)\n",
      "tensor(79.1266)\n",
      "tensor(1.4387)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.837433\n",
      "Epoch 7689\n",
      "-------------------------------\n",
      "tensor(50.2645)\n",
      "tensor(18.0354)\n",
      "tensor(26.8237)\n",
      "tensor(0.8879)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.577263\n",
      "Epoch 7690\n",
      "-------------------------------\n",
      "tensor(64.1969)\n",
      "tensor(12.8838)\n",
      "tensor(37.4569)\n",
      "tensor(0.5095)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.638515\n",
      "Epoch 7691\n",
      "-------------------------------\n",
      "tensor(124.6601)\n",
      "tensor(50.9465)\n",
      "tensor(99.7631)\n",
      "tensor(2.8560)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.933197\n",
      "Epoch 7692\n",
      "-------------------------------\n",
      "tensor(195.8080)\n",
      "tensor(68.1221)\n",
      "tensor(148.0514)\n",
      "tensor(3.9568)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.478214\n",
      "Epoch 7693\n",
      "-------------------------------\n",
      "tensor(256.3502)\n",
      "tensor(77.5547)\n",
      "tensor(187.3826)\n",
      "tensor(4.1905)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.017204\n",
      "Epoch 7694\n",
      "-------------------------------\n",
      "tensor(247.4085)\n",
      "tensor(84.9131)\n",
      "tensor(188.6095)\n",
      "tensor(4.7280)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.000076\n",
      "Epoch 7695\n",
      "-------------------------------\n",
      "tensor(125.2090)\n",
      "tensor(44.2269)\n",
      "tensor(95.2846)\n",
      "tensor(2.6332)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.008247\n",
      "Epoch 7696\n",
      "-------------------------------\n",
      "tensor(82.8966)\n",
      "tensor(28.5034)\n",
      "tensor(60.3765)\n",
      "tensor(1.5933)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.799110\n",
      "Epoch 7697\n",
      "-------------------------------\n",
      "tensor(122.5979)\n",
      "tensor(42.1220)\n",
      "tensor(92.4287)\n",
      "tensor(2.4240)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.976505\n",
      "Epoch 7698\n",
      "-------------------------------\n",
      "tensor(70.4515)\n",
      "tensor(24.8571)\n",
      "tensor(51.2704)\n",
      "tensor(1.4397)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.732376\n",
      "Epoch 7699\n",
      "-------------------------------\n",
      "tensor(46.2139)\n",
      "tensor(14.1401)\n",
      "tensor(29.6227)\n",
      "tensor(0.6415)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.654327\n",
      "Epoch 7700\n",
      "-------------------------------\n",
      "tensor(76.4949)\n",
      "tensor(25.5481)\n",
      "tensor(55.0140)\n",
      "tensor(1.3675)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.728439\n",
      "Epoch 7701\n",
      "-------------------------------\n",
      "tensor(61.2157)\n",
      "tensor(21.2377)\n",
      "tensor(43.8657)\n",
      "tensor(1.1543)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.675171\n",
      "Epoch 7702\n",
      "-------------------------------\n",
      "tensor(37.8434)\n",
      "tensor(13.1338)\n",
      "tensor(18.6440)\n",
      "tensor(0.5663)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.607620\n",
      "Epoch 7703\n",
      "-------------------------------\n",
      "tensor(38.5779)\n",
      "tensor(11.6890)\n",
      "tensor(18.1488)\n",
      "tensor(0.3242)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.594242\n",
      "Epoch 7704\n",
      "-------------------------------\n",
      "tensor(76.4019)\n",
      "tensor(25.6790)\n",
      "tensor(55.0970)\n",
      "tensor(1.2752)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.686878\n",
      "Epoch 7705\n",
      "-------------------------------\n",
      "tensor(68.4602)\n",
      "tensor(24.3109)\n",
      "tensor(46.7976)\n",
      "tensor(1.1590)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.638100\n",
      "Epoch 7706\n",
      "-------------------------------\n",
      "tensor(58.7009)\n",
      "tensor(16.1810)\n",
      "tensor(36.8390)\n",
      "tensor(0.7655)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.581074\n",
      "Epoch 7707\n",
      "-------------------------------\n",
      "tensor(87.3839)\n",
      "tensor(25.1577)\n",
      "tensor(58.1108)\n",
      "tensor(1.2369)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.680378\n",
      "Epoch 7708\n",
      "-------------------------------\n",
      "tensor(104.8648)\n",
      "tensor(36.2299)\n",
      "tensor(69.4328)\n",
      "tensor(1.8601)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.927975\n",
      "Epoch 7709\n",
      "-------------------------------\n",
      "tensor(50.7059)\n",
      "tensor(16.3753)\n",
      "tensor(13.1422)\n",
      "tensor(0.2074)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.735378\n",
      "Epoch 7710\n",
      "-------------------------------\n",
      "tensor(84.4267)\n",
      "tensor(29.9060)\n",
      "tensor(59.5820)\n",
      "tensor(1.2617)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.764404\n",
      "Epoch 7711\n",
      "-------------------------------\n",
      "tensor(156.0092)\n",
      "tensor(45.3959)\n",
      "tensor(111.9450)\n",
      "tensor(2.7318)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.073399\n",
      "Epoch 7712\n",
      "-------------------------------\n",
      "tensor(213.8631)\n",
      "tensor(80.0209)\n",
      "tensor(167.3173)\n",
      "tensor(4.4434)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.635265\n",
      "Epoch 7713\n",
      "-------------------------------\n",
      "tensor(278.6821)\n",
      "tensor(91.5304)\n",
      "tensor(208.1625)\n",
      "tensor(5.1117)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.323616\n",
      "Epoch 7714\n",
      "-------------------------------\n",
      "tensor(281.9799)\n",
      "tensor(86.0680)\n",
      "tensor(206.9889)\n",
      "tensor(4.7631)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.333775\n",
      "Epoch 7715\n",
      "-------------------------------\n",
      "tensor(134.3878)\n",
      "tensor(53.2332)\n",
      "tensor(110.1769)\n",
      "tensor(3.0754)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.080456\n",
      "Epoch 7716\n",
      "-------------------------------\n",
      "tensor(89.4422)\n",
      "tensor(31.6152)\n",
      "tensor(69.3984)\n",
      "tensor(1.6175)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.826550\n",
      "Epoch 7717\n",
      "-------------------------------\n",
      "tensor(134.9327)\n",
      "tensor(46.9129)\n",
      "tensor(102.6795)\n",
      "tensor(2.8325)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.031895\n",
      "Epoch 7718\n",
      "-------------------------------\n",
      "tensor(74.9656)\n",
      "tensor(26.0693)\n",
      "tensor(55.4044)\n",
      "tensor(1.5394)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.725613\n",
      "Epoch 7719\n",
      "-------------------------------\n",
      "tensor(49.8495)\n",
      "tensor(17.1513)\n",
      "tensor(35.7395)\n",
      "tensor(0.8870)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.636127\n",
      "Epoch 7720\n",
      "-------------------------------\n",
      "tensor(83.7056)\n",
      "tensor(29.5471)\n",
      "tensor(64.3426)\n",
      "tensor(1.6963)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.724564\n",
      "Epoch 7721\n",
      "-------------------------------\n",
      "tensor(67.6005)\n",
      "tensor(24.3611)\n",
      "tensor(51.8147)\n",
      "tensor(1.4179)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.653809\n",
      "Epoch 7722\n",
      "-------------------------------\n",
      "tensor(30.1545)\n",
      "tensor(11.5504)\n",
      "tensor(23.2516)\n",
      "tensor(0.7048)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.562496\n",
      "Epoch 7723\n",
      "-------------------------------\n",
      "tensor(32.2155)\n",
      "tensor(9.2682)\n",
      "tensor(18.4039)\n",
      "tensor(0.3685)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.546474\n",
      "Epoch 7724\n",
      "-------------------------------\n",
      "tensor(88.6263)\n",
      "tensor(29.4839)\n",
      "tensor(61.4779)\n",
      "tensor(1.5374)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.674583\n",
      "Epoch 7725\n",
      "-------------------------------\n",
      "tensor(80.8581)\n",
      "tensor(27.8032)\n",
      "tensor(55.5932)\n",
      "tensor(1.4502)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.642746\n",
      "Epoch 7726\n",
      "-------------------------------\n",
      "tensor(61.4447)\n",
      "tensor(19.0573)\n",
      "tensor(37.0113)\n",
      "tensor(0.9256)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.539993\n",
      "Epoch 7727\n",
      "-------------------------------\n",
      "tensor(101.4329)\n",
      "tensor(33.2023)\n",
      "tensor(67.7762)\n",
      "tensor(1.8161)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.717316\n",
      "Epoch 7728\n",
      "-------------------------------\n",
      "tensor(111.3552)\n",
      "tensor(37.9901)\n",
      "tensor(79.6970)\n",
      "tensor(1.7568)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.761131\n",
      "Epoch 7729\n",
      "-------------------------------\n",
      "tensor(20.3582)\n",
      "tensor(8.6988)\n",
      "tensor(6.4728)\n",
      "tensor(0.2543)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.498978\n",
      "Epoch 7730\n",
      "-------------------------------\n",
      "tensor(113.9093)\n",
      "tensor(30.6388)\n",
      "tensor(73.2841)\n",
      "tensor(1.4565)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.782352\n",
      "Epoch 7731\n",
      "-------------------------------\n",
      "tensor(191.4875)\n",
      "tensor(65.8291)\n",
      "tensor(145.3821)\n",
      "tensor(3.6789)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.344906\n",
      "Epoch 7732\n",
      "-------------------------------\n",
      "tensor(262.2179)\n",
      "tensor(94.1517)\n",
      "tensor(207.1717)\n",
      "tensor(5.5430)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.132050\n",
      "Epoch 7733\n",
      "-------------------------------\n",
      "tensor(331.5043)\n",
      "tensor(108.1496)\n",
      "tensor(248.6999)\n",
      "tensor(6.2084)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.957714\n",
      "Epoch 7734\n",
      "-------------------------------\n",
      "tensor(329.0077)\n",
      "tensor(108.3614)\n",
      "tensor(244.6358)\n",
      "tensor(5.7649)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.934162\n",
      "Epoch 7735\n",
      "-------------------------------\n",
      "tensor(172.1445)\n",
      "tensor(52.9742)\n",
      "tensor(125.2506)\n",
      "tensor(2.9433)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.197258\n",
      "Epoch 7736\n",
      "-------------------------------\n",
      "tensor(109.1642)\n",
      "tensor(30.1906)\n",
      "tensor(74.5021)\n",
      "tensor(1.4305)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.790409\n",
      "Epoch 7737\n",
      "-------------------------------\n",
      "tensor(163.9648)\n",
      "tensor(54.9381)\n",
      "tensor(124.1649)\n",
      "tensor(3.1930)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.115833\n",
      "Epoch 7738\n",
      "-------------------------------\n",
      "tensor(85.6640)\n",
      "tensor(24.5600)\n",
      "tensor(62.5443)\n",
      "tensor(1.3592)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.681713\n",
      "Epoch 7739\n",
      "-------------------------------\n",
      "tensor(56.3030)\n",
      "tensor(24.7400)\n",
      "tensor(44.6113)\n",
      "tensor(1.4135)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.588329\n",
      "Epoch 7740\n",
      "-------------------------------\n",
      "tensor(96.7275)\n",
      "tensor(37.4724)\n",
      "tensor(75.5634)\n",
      "tensor(2.1621)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.714844\n",
      "Epoch 7741\n",
      "-------------------------------\n",
      "tensor(79.0180)\n",
      "tensor(29.8492)\n",
      "tensor(58.7616)\n",
      "tensor(1.6738)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.622711\n",
      "Epoch 7742\n",
      "-------------------------------\n",
      "tensor(46.6707)\n",
      "tensor(16.4520)\n",
      "tensor(24.3641)\n",
      "tensor(0.7195)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.510719\n",
      "Epoch 7743\n",
      "-------------------------------\n",
      "tensor(57.2783)\n",
      "tensor(18.1356)\n",
      "tensor(24.1620)\n",
      "tensor(0.6356)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.510426\n",
      "Epoch 7744\n",
      "-------------------------------\n",
      "tensor(100.1504)\n",
      "tensor(36.6072)\n",
      "tensor(72.1522)\n",
      "tensor(2.0692)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.685112\n",
      "Epoch 7745\n",
      "-------------------------------\n",
      "tensor(77.5221)\n",
      "tensor(32.3012)\n",
      "tensor(60.1650)\n",
      "tensor(1.9940)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.598209\n",
      "Epoch 7746\n",
      "-------------------------------\n",
      "tensor(75.4774)\n",
      "tensor(18.9178)\n",
      "tensor(48.7060)\n",
      "tensor(0.7731)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.588711\n",
      "Epoch 7747\n",
      "-------------------------------\n",
      "tensor(97.8749)\n",
      "tensor(35.0733)\n",
      "tensor(72.1710)\n",
      "tensor(1.9550)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.722000\n",
      "Epoch 7748\n",
      "-------------------------------\n",
      "tensor(130.7542)\n",
      "tensor(37.0526)\n",
      "tensor(88.4980)\n",
      "tensor(1.7084)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.890079\n",
      "Epoch 7749\n",
      "-------------------------------\n",
      "tensor(44.8397)\n",
      "tensor(16.2831)\n",
      "tensor(21.9723)\n",
      "tensor(0.4929)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.529964\n",
      "Epoch 7750\n",
      "-------------------------------\n",
      "tensor(92.6408)\n",
      "tensor(24.5460)\n",
      "tensor(60.4744)\n",
      "tensor(0.8795)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.695232\n",
      "Epoch 7751\n",
      "-------------------------------\n",
      "tensor(182.5922)\n",
      "tensor(57.9678)\n",
      "tensor(134.4396)\n",
      "tensor(3.2876)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.241764\n",
      "Epoch 7752\n",
      "-------------------------------\n",
      "tensor(261.6054)\n",
      "tensor(95.7408)\n",
      "tensor(205.2699)\n",
      "tensor(5.6096)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.082951\n",
      "Epoch 7753\n",
      "-------------------------------\n",
      "tensor(342.5877)\n",
      "tensor(115.0902)\n",
      "tensor(258.7388)\n",
      "tensor(6.5028)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.038616\n",
      "Epoch 7754\n",
      "-------------------------------\n",
      "tensor(349.8951)\n",
      "tensor(108.2147)\n",
      "tensor(258.4731)\n",
      "tensor(5.8254)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.163681\n",
      "Epoch 7755\n",
      "-------------------------------\n",
      "tensor(176.9296)\n",
      "tensor(62.0211)\n",
      "tensor(134.5892)\n",
      "tensor(3.6170)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.230225\n",
      "Epoch 7756\n",
      "-------------------------------\n",
      "tensor(112.2602)\n",
      "tensor(32.4836)\n",
      "tensor(80.3298)\n",
      "tensor(1.6866)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.825771\n",
      "Epoch 7757\n",
      "-------------------------------\n",
      "tensor(163.0836)\n",
      "tensor(61.6711)\n",
      "tensor(128.4584)\n",
      "tensor(3.6365)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.151627\n",
      "Epoch 7758\n",
      "-------------------------------\n",
      "tensor(89.2222)\n",
      "tensor(33.0366)\n",
      "tensor(68.0357)\n",
      "tensor(1.9024)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.708004\n",
      "Epoch 7759\n",
      "-------------------------------\n",
      "tensor(68.1713)\n",
      "tensor(22.7942)\n",
      "tensor(44.0268)\n",
      "tensor(1.1243)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.626335\n",
      "Epoch 7760\n",
      "-------------------------------\n",
      "tensor(106.0233)\n",
      "tensor(37.9505)\n",
      "tensor(78.4352)\n",
      "tensor(2.1287)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.769810\n",
      "Epoch 7761\n",
      "-------------------------------\n",
      "tensor(85.9237)\n",
      "tensor(31.9477)\n",
      "tensor(62.6188)\n",
      "tensor(1.7907)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.659786\n",
      "Epoch 7762\n",
      "-------------------------------\n",
      "tensor(32.2026)\n",
      "tensor(14.2846)\n",
      "tensor(27.2064)\n",
      "tensor(0.9125)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.535862\n",
      "Epoch 7763\n",
      "-------------------------------\n",
      "tensor(40.8405)\n",
      "tensor(10.1470)\n",
      "tensor(23.8184)\n",
      "tensor(0.4096)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.536762\n",
      "Epoch 7764\n",
      "-------------------------------\n",
      "tensor(104.0594)\n",
      "tensor(33.3678)\n",
      "tensor(74.9825)\n",
      "tensor(1.8429)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.726486\n",
      "Epoch 7765\n",
      "-------------------------------\n",
      "tensor(87.4308)\n",
      "tensor(30.5337)\n",
      "tensor(63.6272)\n",
      "tensor(1.7661)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.658478\n",
      "Epoch 7766\n",
      "-------------------------------\n",
      "tensor(75.2909)\n",
      "tensor(23.2953)\n",
      "tensor(50.3000)\n",
      "tensor(0.9988)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.596756\n",
      "Epoch 7767\n",
      "-------------------------------\n",
      "tensor(115.8601)\n",
      "tensor(38.3088)\n",
      "tensor(79.8927)\n",
      "tensor(1.9607)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.784096\n",
      "Epoch 7768\n",
      "-------------------------------\n",
      "tensor(139.9747)\n",
      "tensor(43.7545)\n",
      "tensor(100.0371)\n",
      "tensor(2.0873)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.942535\n",
      "Epoch 7769\n",
      "-------------------------------\n",
      "tensor(33.3413)\n",
      "tensor(11.7855)\n",
      "tensor(18.8468)\n",
      "tensor(0.8682)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.528194\n",
      "Epoch 7770\n",
      "-------------------------------\n",
      "tensor(116.1617)\n",
      "tensor(30.0234)\n",
      "tensor(79.7511)\n",
      "tensor(1.6102)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.757378\n",
      "Epoch 7771\n",
      "-------------------------------\n",
      "tensor(208.9617)\n",
      "tensor(79.4531)\n",
      "tensor(166.2580)\n",
      "tensor(4.6518)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.554245\n",
      "Epoch 7772\n",
      "-------------------------------\n",
      "tensor(302.3533)\n",
      "tensor(107.2287)\n",
      "tensor(233.9399)\n",
      "tensor(6.2556)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.615486\n",
      "Epoch 7773\n",
      "-------------------------------\n",
      "tensor(388.1879)\n",
      "tensor(119.4722)\n",
      "tensor(285.9356)\n",
      "tensor(6.5086)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.767906\n",
      "Epoch 7774\n",
      "-------------------------------\n",
      "tensor(376.8067)\n",
      "tensor(125.1096)\n",
      "tensor(285.2088)\n",
      "tensor(6.9310)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.672268\n",
      "Epoch 7775\n",
      "-------------------------------\n",
      "tensor(187.5773)\n",
      "tensor(67.6352)\n",
      "tensor(146.5105)\n",
      "tensor(4.0618)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.355850\n",
      "Epoch 7776\n",
      "-------------------------------\n",
      "tensor(130.4293)\n",
      "tensor(41.8549)\n",
      "tensor(95.2154)\n",
      "tensor(2.2213)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.903137\n",
      "Epoch 7777\n",
      "-------------------------------\n",
      "tensor(185.3091)\n",
      "tensor(65.7520)\n",
      "tensor(144.9716)\n",
      "tensor(3.9777)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.315941\n",
      "Epoch 7778\n",
      "-------------------------------\n",
      "tensor(102.1177)\n",
      "tensor(35.6242)\n",
      "tensor(80.5183)\n",
      "tensor(2.2172)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.769955\n",
      "Epoch 7779\n",
      "-------------------------------\n",
      "tensor(63.8087)\n",
      "tensor(22.6420)\n",
      "tensor(46.3394)\n",
      "tensor(1.1825)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.606682\n",
      "Epoch 7780\n",
      "-------------------------------\n",
      "tensor(114.0202)\n",
      "tensor(41.3099)\n",
      "tensor(87.1752)\n",
      "tensor(2.3449)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.797665\n",
      "Epoch 7781\n",
      "-------------------------------\n",
      "tensor(93.8451)\n",
      "tensor(34.7239)\n",
      "tensor(70.4563)\n",
      "tensor(1.9798)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.688541\n",
      "Epoch 7782\n",
      "-------------------------------\n",
      "tensor(52.7257)\n",
      "tensor(19.4781)\n",
      "tensor(31.8440)\n",
      "tensor(0.9971)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.539295\n",
      "Epoch 7783\n",
      "-------------------------------\n",
      "tensor(40.0165)\n",
      "tensor(11.9307)\n",
      "tensor(25.1392)\n",
      "tensor(0.5027)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.519859\n",
      "Epoch 7784\n",
      "-------------------------------\n",
      "tensor(114.1371)\n",
      "tensor(38.8466)\n",
      "tensor(85.0373)\n",
      "tensor(2.1648)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.750637\n",
      "Epoch 7785\n",
      "-------------------------------\n",
      "tensor(95.1575)\n",
      "tensor(36.8806)\n",
      "tensor(75.2573)\n",
      "tensor(2.1464)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.676662\n",
      "Epoch 7786\n",
      "-------------------------------\n",
      "tensor(83.2898)\n",
      "tensor(23.6375)\n",
      "tensor(53.7404)\n",
      "tensor(0.9666)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.591553\n",
      "Epoch 7787\n",
      "-------------------------------\n",
      "tensor(130.5699)\n",
      "tensor(41.0922)\n",
      "tensor(93.7363)\n",
      "tensor(2.1863)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.852501\n",
      "Epoch 7788\n",
      "-------------------------------\n",
      "tensor(143.8568)\n",
      "tensor(45.9921)\n",
      "tensor(102.9376)\n",
      "tensor(2.5049)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.040634\n",
      "Epoch 7789\n",
      "-------------------------------\n",
      "tensor(40.9095)\n",
      "tensor(14.2760)\n",
      "tensor(11.2904)\n",
      "tensor(0.3458)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.622616\n",
      "Epoch 7790\n",
      "-------------------------------\n",
      "tensor(126.9058)\n",
      "tensor(44.6315)\n",
      "tensor(94.2665)\n",
      "tensor(2.2192)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.908478\n",
      "Epoch 7791\n",
      "-------------------------------\n",
      "tensor(223.0638)\n",
      "tensor(72.8163)\n",
      "tensor(170.7732)\n",
      "tensor(4.3994)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.617420\n",
      "Epoch 7792\n",
      "-------------------------------\n",
      "tensor(309.2698)\n",
      "tensor(109.8554)\n",
      "tensor(238.8959)\n",
      "tensor(6.2917)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.689041\n",
      "Epoch 7793\n",
      "-------------------------------\n",
      "tensor(391.6632)\n",
      "tensor(129.1111)\n",
      "tensor(293.9875)\n",
      "tensor(7.0735)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.874821\n",
      "Epoch 7794\n",
      "-------------------------------\n",
      "tensor(392.7698)\n",
      "tensor(121.5765)\n",
      "tensor(290.1967)\n",
      "tensor(6.7868)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 38.883804\n",
      "Epoch 7795\n",
      "-------------------------------\n",
      "tensor(194.3609)\n",
      "tensor(73.6493)\n",
      "tensor(154.5501)\n",
      "tensor(4.2747)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.455566\n",
      "Epoch 7796\n",
      "-------------------------------\n",
      "tensor(122.0334)\n",
      "tensor(40.5808)\n",
      "tensor(92.3641)\n",
      "tensor(2.1837)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.888737\n",
      "Epoch 7797\n",
      "-------------------------------\n",
      "tensor(185.1300)\n",
      "tensor(67.6134)\n",
      "tensor(144.2740)\n",
      "tensor(4.0208)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.314411\n",
      "Epoch 7798\n",
      "-------------------------------\n",
      "tensor(99.2865)\n",
      "tensor(37.9785)\n",
      "tensor(77.1829)\n",
      "tensor(2.2483)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.739269\n",
      "Epoch 7799\n",
      "-------------------------------\n",
      "tensor(65.2015)\n",
      "tensor(20.5080)\n",
      "tensor(49.2711)\n",
      "tensor(1.1086)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.585442\n",
      "Epoch 7800\n",
      "-------------------------------\n",
      "tensor(115.8599)\n",
      "tensor(39.1346)\n",
      "tensor(88.8041)\n",
      "tensor(2.2302)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.779697\n",
      "Epoch 7801\n",
      "-------------------------------\n",
      "tensor(92.4308)\n",
      "tensor(32.2814)\n",
      "tensor(71.3148)\n",
      "tensor(1.8549)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.662373\n",
      "Epoch 7802\n",
      "-------------------------------\n",
      "tensor(43.9365)\n",
      "tensor(16.1781)\n",
      "tensor(31.7159)\n",
      "tensor(0.8865)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.498631\n",
      "Epoch 7803\n",
      "-------------------------------\n",
      "tensor(50.8060)\n",
      "tensor(14.9890)\n",
      "tensor(26.1161)\n",
      "tensor(0.5647)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.487015\n",
      "Epoch 7804\n",
      "-------------------------------\n",
      "tensor(120.6216)\n",
      "tensor(38.3552)\n",
      "tensor(86.7179)\n",
      "tensor(2.1028)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.737022\n",
      "Epoch 7805\n",
      "-------------------------------\n",
      "tensor(108.6991)\n",
      "tensor(34.4436)\n",
      "tensor(76.9929)\n",
      "tensor(1.8642)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.659996\n",
      "Epoch 7806\n",
      "-------------------------------\n",
      "tensor(82.7549)\n",
      "tensor(28.5687)\n",
      "tensor(54.1983)\n",
      "tensor(1.4548)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.576500\n",
      "Epoch 7807\n",
      "-------------------------------\n",
      "tensor(133.2553)\n",
      "tensor(45.2470)\n",
      "tensor(97.0532)\n",
      "tensor(2.5674)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.844765\n",
      "Epoch 7808\n",
      "-------------------------------\n",
      "tensor(148.9697)\n",
      "tensor(51.5018)\n",
      "tensor(111.7241)\n",
      "tensor(2.7056)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 35.972839\n",
      "Epoch 7809\n",
      "-------------------------------\n",
      "tensor(32.3149)\n",
      "tensor(10.7099)\n",
      "tensor(12.3139)\n",
      "tensor(0.2335)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.507633\n",
      "Epoch 7810\n",
      "-------------------------------\n",
      "tensor(132.1715)\n",
      "tensor(39.8417)\n",
      "tensor(91.4251)\n",
      "tensor(2.1548)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.864098\n",
      "Epoch 7811\n",
      "-------------------------------\n",
      "tensor(245.1525)\n",
      "tensor(80.5751)\n",
      "tensor(183.1303)\n",
      "tensor(4.3472)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.845867\n",
      "Epoch 7812\n",
      "-------------------------------\n",
      "tensor(355.6213)\n",
      "tensor(119.8958)\n",
      "tensor(270.9434)\n",
      "tensor(6.8111)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.322689\n",
      "Epoch 7813\n",
      "-------------------------------\n",
      "tensor(455.6681)\n",
      "tensor(151.0549)\n",
      "tensor(344.9355)\n",
      "tensor(8.6245)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.100712\n",
      "Epoch 7814\n",
      "-------------------------------\n",
      "tensor(472.5205)\n",
      "tensor(149.8219)\n",
      "tensor(350.5080)\n",
      "tensor(8.7412)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.247105\n",
      "Epoch 7815\n",
      "-------------------------------\n",
      "tensor(271.1874)\n",
      "tensor(79.9207)\n",
      "tensor(196.2341)\n",
      "tensor(4.3504)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.085327\n",
      "Epoch 7816\n",
      "-------------------------------\n",
      "tensor(124.6961)\n",
      "tensor(34.1302)\n",
      "tensor(86.6130)\n",
      "tensor(1.8111)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.816433\n",
      "Epoch 7817\n",
      "-------------------------------\n",
      "tensor(240.5166)\n",
      "tensor(79.9484)\n",
      "tensor(180.5975)\n",
      "tensor(4.3708)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.715286\n",
      "Epoch 7818\n",
      "-------------------------------\n",
      "tensor(109.6522)\n",
      "tensor(32.9794)\n",
      "tensor(75.3211)\n",
      "tensor(1.5973)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.683712\n",
      "Epoch 7819\n",
      "-------------------------------\n",
      "tensor(97.7550)\n",
      "tensor(35.4894)\n",
      "tensor(74.5784)\n",
      "tensor(2.0289)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.663849\n",
      "Epoch 7820\n",
      "-------------------------------\n",
      "tensor(144.1628)\n",
      "tensor(49.6779)\n",
      "tensor(108.8420)\n",
      "tensor(2.7729)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.898094\n",
      "Epoch 7821\n",
      "-------------------------------\n",
      "tensor(106.2033)\n",
      "tensor(36.0543)\n",
      "tensor(77.9603)\n",
      "tensor(1.9332)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.681190\n",
      "Epoch 7822\n",
      "-------------------------------\n",
      "tensor(45.7140)\n",
      "tensor(14.7705)\n",
      "tensor(25.4247)\n",
      "tensor(0.5680)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.466743\n",
      "Epoch 7823\n",
      "-------------------------------\n",
      "tensor(62.6056)\n",
      "tensor(21.3054)\n",
      "tensor(44.2658)\n",
      "tensor(1.2180)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.493977\n",
      "Epoch 7824\n",
      "-------------------------------\n",
      "tensor(151.3696)\n",
      "tensor(51.2002)\n",
      "tensor(107.7348)\n",
      "tensor(2.8302)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.882610\n",
      "Epoch 7825\n",
      "-------------------------------\n",
      "tensor(106.5192)\n",
      "tensor(36.8229)\n",
      "tensor(76.0466)\n",
      "tensor(2.0812)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.652893\n",
      "Epoch 7826\n",
      "-------------------------------\n",
      "tensor(118.7136)\n",
      "tensor(38.6632)\n",
      "tensor(86.0118)\n",
      "tensor(1.9925)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.716305\n",
      "Epoch 7827\n",
      "-------------------------------\n",
      "tensor(133.7762)\n",
      "tensor(46.3983)\n",
      "tensor(100.4307)\n",
      "tensor(2.5411)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.817608\n",
      "Epoch 7828\n",
      "-------------------------------\n",
      "tensor(201.0694)\n",
      "tensor(63.3420)\n",
      "tensor(146.4704)\n",
      "tensor(3.4367)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.346424\n",
      "Epoch 7829\n",
      "-------------------------------\n",
      "tensor(76.1709)\n",
      "tensor(26.5941)\n",
      "tensor(51.4615)\n",
      "tensor(1.2717)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.565762\n",
      "Epoch 7830\n",
      "-------------------------------\n",
      "tensor(110.3086)\n",
      "tensor(34.4018)\n",
      "tensor(76.3611)\n",
      "tensor(1.5963)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.774521\n",
      "Epoch 7831\n",
      "-------------------------------\n",
      "tensor(244.5633)\n",
      "tensor(81.7944)\n",
      "tensor(186.5353)\n",
      "tensor(4.7235)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 36.838051\n",
      "Epoch 7832\n",
      "-------------------------------\n",
      "tensor(370.4898)\n",
      "tensor(129.4083)\n",
      "tensor(285.6860)\n",
      "tensor(7.4836)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.645714\n",
      "Epoch 7833\n",
      "-------------------------------\n",
      "tensor(484.1425)\n",
      "tensor(160.4837)\n",
      "tensor(368.4049)\n",
      "tensor(9.0574)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.716228\n",
      "Epoch 7834\n",
      "-------------------------------\n",
      "tensor(493.4953)\n",
      "tensor(159.9279)\n",
      "tensor(370.1321)\n",
      "tensor(8.9450)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 40.879673\n",
      "Epoch 7835\n",
      "-------------------------------\n",
      "tensor(258.3189)\n",
      "tensor(89.0688)\n",
      "tensor(197.7303)\n",
      "tensor(5.0694)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.052265\n",
      "Epoch 7836\n",
      "-------------------------------\n",
      "tensor(150.5077)\n",
      "tensor(48.7817)\n",
      "tensor(111.4045)\n",
      "tensor(2.6058)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.014709\n",
      "Epoch 7837\n",
      "-------------------------------\n",
      "tensor(242.3254)\n",
      "tensor(83.4065)\n",
      "tensor(185.3038)\n",
      "tensor(4.8593)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 36.835205\n",
      "Epoch 7838\n",
      "-------------------------------\n",
      "tensor(125.5881)\n",
      "tensor(42.4942)\n",
      "tensor(93.3872)\n",
      "tensor(2.4326)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.854404\n",
      "Epoch 7839\n",
      "-------------------------------\n",
      "tensor(87.1903)\n",
      "tensor(30.2547)\n",
      "tensor(66.9558)\n",
      "tensor(1.6922)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.653275\n",
      "Epoch 7840\n",
      "-------------------------------\n",
      "tensor(147.9986)\n",
      "tensor(51.2388)\n",
      "tensor(114.1239)\n",
      "tensor(2.9138)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 35.961399\n",
      "Epoch 7841\n",
      "-------------------------------\n",
      "tensor(116.3923)\n",
      "tensor(40.6833)\n",
      "tensor(89.3671)\n",
      "tensor(2.2966)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.762074\n",
      "Epoch 7842\n",
      "-------------------------------\n",
      "tensor(46.5764)\n",
      "tensor(17.3274)\n",
      "tensor(37.3954)\n",
      "tensor(0.9804)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.508442\n",
      "Epoch 7843\n",
      "-------------------------------\n",
      "tensor(53.8050)\n",
      "tensor(16.7884)\n",
      "tensor(36.3633)\n",
      "tensor(0.9076)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.506184\n",
      "Epoch 7844\n",
      "-------------------------------\n",
      "tensor(147.8024)\n",
      "tensor(49.1619)\n",
      "tensor(110.4601)\n",
      "tensor(2.8407)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 35.905910\n",
      "Epoch 7845\n",
      "-------------------------------\n",
      "tensor(123.2843)\n",
      "tensor(42.3255)\n",
      "tensor(91.3266)\n",
      "tensor(2.4476)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.763691\n",
      "Epoch 7846\n",
      "-------------------------------\n",
      "tensor(112.2169)\n",
      "tensor(37.7252)\n",
      "tensor(73.6249)\n",
      "tensor(1.8057)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.707489\n",
      "Epoch 7847\n",
      "-------------------------------\n",
      "tensor(156.4659)\n",
      "tensor(55.9238)\n",
      "tensor(114.3619)\n",
      "tensor(3.1989)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 35.961151\n",
      "Epoch 7848\n",
      "-------------------------------\n",
      "tensor(195.4229)\n",
      "tensor(62.1898)\n",
      "tensor(144.1593)\n",
      "tensor(3.0934)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.251175\n",
      "Epoch 7849\n",
      "-------------------------------\n",
      "tensor(44.6685)\n",
      "tensor(10.3532)\n",
      "tensor(24.3124)\n",
      "tensor(0.6485)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.517487\n",
      "Epoch 7850\n",
      "-------------------------------\n",
      "tensor(157.0501)\n",
      "tensor(39.4769)\n",
      "tensor(106.9752)\n",
      "tensor(2.0650)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.013359\n",
      "Epoch 7851\n",
      "-------------------------------\n",
      "tensor(294.4738)\n",
      "tensor(102.7200)\n",
      "tensor(225.9444)\n",
      "tensor(5.7583)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.484840\n",
      "Epoch 7852\n",
      "-------------------------------\n",
      "tensor(427.7391)\n",
      "tensor(152.2055)\n",
      "tensor(334.7871)\n",
      "tensor(8.8320)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.708107\n",
      "Epoch 7853\n",
      "-------------------------------\n",
      "tensor(557.8668)\n",
      "tensor(177.9946)\n",
      "tensor(417.2775)\n",
      "tensor(10.1522)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.280762\n",
      "Epoch 7854\n",
      "-------------------------------\n",
      "tensor(557.3588)\n",
      "tensor(188.1776)\n",
      "tensor(423.8476)\n",
      "tensor(10.3090)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.464870\n",
      "Epoch 7855\n",
      "-------------------------------\n",
      "tensor(296.3483)\n",
      "tensor(95.4892)\n",
      "tensor(222.8990)\n",
      "tensor(5.5032)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.456467\n",
      "Epoch 7856\n",
      "-------------------------------\n",
      "tensor(172.8315)\n",
      "tensor(52.6582)\n",
      "tensor(125.1081)\n",
      "tensor(2.7061)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.093281\n",
      "Epoch 7857\n",
      "-------------------------------\n",
      "tensor(276.9047)\n",
      "tensor(93.8562)\n",
      "tensor(208.9635)\n",
      "tensor(5.5313)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.158535\n",
      "Epoch 7858\n",
      "-------------------------------\n",
      "tensor(143.8703)\n",
      "tensor(44.4365)\n",
      "tensor(103.0167)\n",
      "tensor(2.6599)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.897026\n",
      "Epoch 7859\n",
      "-------------------------------\n",
      "tensor(100.3508)\n",
      "tensor(39.7220)\n",
      "tensor(77.7996)\n",
      "tensor(2.0219)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.662136\n",
      "Epoch 7860\n",
      "-------------------------------\n",
      "tensor(165.3000)\n",
      "tensor(63.2408)\n",
      "tensor(129.3679)\n",
      "tensor(3.4068)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.047409\n",
      "Epoch 7861\n",
      "-------------------------------\n",
      "tensor(126.4780)\n",
      "tensor(50.3670)\n",
      "tensor(99.7939)\n",
      "tensor(2.7065)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.787586\n",
      "Epoch 7862\n",
      "-------------------------------\n",
      "tensor(50.4024)\n",
      "tensor(24.5345)\n",
      "tensor(40.1322)\n",
      "tensor(1.2189)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.470104\n",
      "Epoch 7863\n",
      "-------------------------------\n",
      "tensor(78.8992)\n",
      "tensor(20.9528)\n",
      "tensor(43.8100)\n",
      "tensor(0.9020)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.497330\n",
      "Epoch 7864\n",
      "-------------------------------\n",
      "tensor(176.3755)\n",
      "tensor(56.2938)\n",
      "tensor(125.1883)\n",
      "tensor(3.0794)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.023922\n",
      "Epoch 7865\n",
      "-------------------------------\n",
      "tensor(133.5434)\n",
      "tensor(48.1090)\n",
      "tensor(98.1835)\n",
      "tensor(2.7089)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.766506\n",
      "Epoch 7866\n",
      "-------------------------------\n",
      "tensor(136.7873)\n",
      "tensor(36.0216)\n",
      "tensor(93.0733)\n",
      "tensor(1.7607)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.797382\n",
      "Epoch 7867\n",
      "-------------------------------\n",
      "tensor(171.0724)\n",
      "tensor(48.6076)\n",
      "tensor(121.1158)\n",
      "tensor(2.6711)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.039036\n",
      "Epoch 7868\n",
      "-------------------------------\n",
      "tensor(233.1627)\n",
      "tensor(77.8442)\n",
      "tensor(173.6032)\n",
      "tensor(4.0413)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.639664\n",
      "Epoch 7869\n",
      "-------------------------------\n",
      "tensor(76.9994)\n",
      "tensor(27.1232)\n",
      "tensor(55.8944)\n",
      "tensor(1.6814)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.603378\n",
      "Epoch 7870\n",
      "-------------------------------\n",
      "tensor(129.9591)\n",
      "tensor(39.3432)\n",
      "tensor(92.1229)\n",
      "tensor(1.9940)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.834343\n",
      "Epoch 7871\n",
      "-------------------------------\n",
      "tensor(287.1212)\n",
      "tensor(96.9321)\n",
      "tensor(223.0895)\n",
      "tensor(5.8005)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.317806\n",
      "Epoch 7872\n",
      "-------------------------------\n",
      "tensor(430.0226)\n",
      "tensor(153.9302)\n",
      "tensor(335.2443)\n",
      "tensor(8.9406)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.638042\n",
      "Epoch 7873\n",
      "-------------------------------\n",
      "tensor(560.4587)\n",
      "tensor(182.7026)\n",
      "tensor(420.6921)\n",
      "tensor(10.1627)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.375786\n",
      "Epoch 7874\n",
      "-------------------------------\n",
      "tensor(558.0665)\n",
      "tensor(178.0560)\n",
      "tensor(418.9262)\n",
      "tensor(9.8871)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 42.264332\n",
      "Epoch 7875\n",
      "-------------------------------\n",
      "tensor(272.6349)\n",
      "tensor(99.9309)\n",
      "tensor(213.5321)\n",
      "tensor(5.9250)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.137184\n",
      "Epoch 7876\n",
      "-------------------------------\n",
      "tensor(187.8385)\n",
      "tensor(60.0614)\n",
      "tensor(136.5851)\n",
      "tensor(3.2765)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.147873\n",
      "Epoch 7877\n",
      "-------------------------------\n",
      "tensor(267.8752)\n",
      "tensor(97.5821)\n",
      "tensor(209.6463)\n",
      "tensor(5.7080)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.072727\n",
      "Epoch 7878\n",
      "-------------------------------\n",
      "tensor(145.8769)\n",
      "tensor(55.2744)\n",
      "tensor(114.3494)\n",
      "tensor(3.2222)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.928711\n",
      "Epoch 7879\n",
      "-------------------------------\n",
      "tensor(96.8319)\n",
      "tensor(29.8227)\n",
      "tensor(70.1841)\n",
      "tensor(1.6115)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.645424\n",
      "Epoch 7880\n",
      "-------------------------------\n",
      "tensor(168.0032)\n",
      "tensor(56.4628)\n",
      "tensor(128.2631)\n",
      "tensor(3.2393)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.064182\n",
      "Epoch 7881\n",
      "-------------------------------\n",
      "tensor(133.5983)\n",
      "tensor(46.4766)\n",
      "tensor(102.9948)\n",
      "tensor(2.6998)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.826347\n",
      "Epoch 7882\n",
      "-------------------------------\n",
      "tensor(59.9148)\n",
      "tensor(22.3433)\n",
      "tensor(45.2358)\n",
      "tensor(1.2966)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.501080\n",
      "Epoch 7883\n",
      "-------------------------------\n",
      "tensor(61.3173)\n",
      "tensor(17.4186)\n",
      "tensor(38.9895)\n",
      "tensor(0.8014)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.488491\n",
      "Epoch 7884\n",
      "-------------------------------\n",
      "tensor(170.5893)\n",
      "tensor(54.4454)\n",
      "tensor(125.6998)\n",
      "tensor(3.0298)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.038773\n",
      "Epoch 7885\n",
      "-------------------------------\n",
      "tensor(144.1850)\n",
      "tensor(47.1628)\n",
      "tensor(106.8952)\n",
      "tensor(2.7043)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.863495\n",
      "Epoch 7886\n",
      "-------------------------------\n",
      "tensor(124.2554)\n",
      "tensor(40.9294)\n",
      "tensor(86.3835)\n",
      "tensor(1.9971)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.702435\n",
      "Epoch 7887\n",
      "-------------------------------\n",
      "tensor(186.3416)\n",
      "tensor(62.9871)\n",
      "tensor(133.5590)\n",
      "tensor(3.3396)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.143486\n",
      "Epoch 7888\n",
      "-------------------------------\n",
      "tensor(231.5651)\n",
      "tensor(72.6847)\n",
      "tensor(170.7243)\n",
      "tensor(3.7994)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 36.573315\n",
      "Epoch 7889\n",
      "-------------------------------\n",
      "tensor(53.2424)\n",
      "tensor(17.6085)\n",
      "tensor(37.9248)\n",
      "tensor(1.2572)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.497997\n",
      "Epoch 7890\n",
      "-------------------------------\n",
      "tensor(170.4263)\n",
      "tensor(45.5976)\n",
      "tensor(118.3783)\n",
      "tensor(2.4604)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.027782\n",
      "Epoch 7891\n",
      "-------------------------------\n",
      "tensor(333.1165)\n",
      "tensor(119.9905)\n",
      "tensor(258.9051)\n",
      "tensor(6.8428)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 37.972935\n",
      "Epoch 7892\n",
      "-------------------------------\n",
      "tensor(509.6076)\n",
      "tensor(165.5189)\n",
      "tensor(383.6638)\n",
      "tensor(9.8551)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.055763\n",
      "Epoch 7893\n",
      "-------------------------------\n",
      "tensor(674.8116)\n",
      "tensor(208.0516)\n",
      "tensor(497.6378)\n",
      "tensor(11.5680)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.188179\n",
      "Epoch 7894\n",
      "-------------------------------\n",
      "tensor(687.2282)\n",
      "tensor(235.1236)\n",
      "tensor(524.4917)\n",
      "tensor(12.9233)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.980457\n",
      "Epoch 7895\n",
      "-------------------------------\n",
      "tensor(382.4324)\n",
      "tensor(127.7721)\n",
      "tensor(291.1097)\n",
      "tensor(7.5859)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.676357\n",
      "Epoch 7896\n",
      "-------------------------------\n",
      "tensor(187.9805)\n",
      "tensor(60.8849)\n",
      "tensor(140.0466)\n",
      "tensor(3.4651)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.188625\n",
      "Epoch 7897\n",
      "-------------------------------\n",
      "tensor(348.3346)\n",
      "tensor(119.3268)\n",
      "tensor(264.1370)\n",
      "tensor(6.8883)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.079254\n",
      "Epoch 7898\n",
      "-------------------------------\n",
      "tensor(157.9869)\n",
      "tensor(54.1321)\n",
      "tensor(118.8193)\n",
      "tensor(3.3007)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.936047\n",
      "Epoch 7899\n",
      "-------------------------------\n",
      "tensor(138.5442)\n",
      "tensor(47.5583)\n",
      "tensor(103.0195)\n",
      "tensor(2.3752)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.838242\n",
      "Epoch 7900\n",
      "-------------------------------\n",
      "tensor(208.4654)\n",
      "tensor(73.8286)\n",
      "tensor(159.4355)\n",
      "tensor(3.9755)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.383316\n",
      "Epoch 7901\n",
      "-------------------------------\n",
      "tensor(152.8038)\n",
      "tensor(56.9188)\n",
      "tensor(118.4302)\n",
      "tensor(3.0920)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.937569\n",
      "Epoch 7902\n",
      "-------------------------------\n",
      "tensor(56.9313)\n",
      "tensor(25.3100)\n",
      "tensor(43.1787)\n",
      "tensor(1.3100)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.475777\n",
      "Epoch 7903\n",
      "-------------------------------\n",
      "tensor(95.0767)\n",
      "tensor(25.1728)\n",
      "tensor(59.0729)\n",
      "tensor(1.1667)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.561592\n",
      "Epoch 7904\n",
      "-------------------------------\n",
      "tensor(213.9071)\n",
      "tensor(66.1643)\n",
      "tensor(155.2596)\n",
      "tensor(3.5920)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.341652\n",
      "Epoch 7905\n",
      "-------------------------------\n",
      "tensor(153.0272)\n",
      "tensor(51.0254)\n",
      "tensor(116.4780)\n",
      "tensor(2.8246)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 35.902451\n",
      "Epoch 7906\n",
      "-------------------------------\n",
      "tensor(162.3740)\n",
      "tensor(47.5166)\n",
      "tensor(116.3909)\n",
      "tensor(2.6841)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.957657\n",
      "Epoch 7907\n",
      "-------------------------------\n",
      "tensor(208.1747)\n",
      "tensor(58.9381)\n",
      "tensor(148.8214)\n",
      "tensor(3.2780)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.368713\n",
      "Epoch 7908\n",
      "-------------------------------\n",
      "tensor(267.7442)\n",
      "tensor(101.2628)\n",
      "tensor(208.5423)\n",
      "tensor(5.6861)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.060070\n",
      "Epoch 7909\n",
      "-------------------------------\n",
      "tensor(83.2187)\n",
      "tensor(27.0003)\n",
      "tensor(58.5662)\n",
      "tensor(1.3583)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.574738\n",
      "Epoch 7910\n",
      "-------------------------------\n",
      "tensor(145.6724)\n",
      "tensor(60.9360)\n",
      "tensor(118.6642)\n",
      "tensor(3.5139)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 35.988876\n",
      "Epoch 7911\n",
      "-------------------------------\n",
      "tensor(351.6582)\n",
      "tensor(107.4233)\n",
      "tensor(260.1408)\n",
      "tensor(6.1556)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.193771\n",
      "Epoch 7912\n",
      "-------------------------------\n",
      "tensor(523.3165)\n",
      "tensor(173.6239)\n",
      "tensor(396.4296)\n",
      "tensor(9.4810)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.562759\n",
      "Epoch 7913\n",
      "-------------------------------\n",
      "tensor(671.2154)\n",
      "tensor(218.7182)\n",
      "tensor(509.4482)\n",
      "tensor(13.0273)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 45.290333\n",
      "Epoch 7914\n",
      "-------------------------------\n",
      "tensor(678.8837)\n",
      "tensor(226.6750)\n",
      "tensor(516.8999)\n",
      "tensor(13.0917)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 45.657974\n",
      "Epoch 7915\n",
      "-------------------------------\n",
      "tensor(361.9023)\n",
      "tensor(123.3470)\n",
      "tensor(275.7142)\n",
      "tensor(6.6369)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.472454\n",
      "Epoch 7916\n",
      "-------------------------------\n",
      "tensor(199.1535)\n",
      "tensor(74.2813)\n",
      "tensor(155.2571)\n",
      "tensor(4.0233)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.433510\n",
      "Epoch 7917\n",
      "-------------------------------\n",
      "tensor(344.9771)\n",
      "tensor(105.8999)\n",
      "tensor(253.3614)\n",
      "tensor(5.9605)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 37.973221\n",
      "Epoch 7918\n",
      "-------------------------------\n",
      "tensor(172.7482)\n",
      "tensor(52.4010)\n",
      "tensor(125.0303)\n",
      "tensor(2.9160)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.097767\n",
      "Epoch 7919\n",
      "-------------------------------\n",
      "tensor(126.6123)\n",
      "tensor(43.3760)\n",
      "tensor(92.9454)\n",
      "tensor(2.3416)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.806335\n",
      "Epoch 7920\n",
      "-------------------------------\n",
      "tensor(207.9976)\n",
      "tensor(68.3441)\n",
      "tensor(154.7346)\n",
      "tensor(3.7800)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.376030\n",
      "Epoch 7921\n",
      "-------------------------------\n",
      "tensor(162.8721)\n",
      "tensor(52.1290)\n",
      "tensor(118.9076)\n",
      "tensor(2.8472)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.972168\n",
      "Epoch 7922\n",
      "-------------------------------\n",
      "tensor(62.5491)\n",
      "tensor(18.7676)\n",
      "tensor(46.5215)\n",
      "tensor(1.0352)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.498032\n",
      "Epoch 7923\n",
      "-------------------------------\n",
      "tensor(73.5208)\n",
      "tensor(27.2697)\n",
      "tensor(54.8060)\n",
      "tensor(1.4919)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.532333\n",
      "Epoch 7924\n",
      "-------------------------------\n",
      "tensor(202.5146)\n",
      "tensor(70.3425)\n",
      "tensor(154.2090)\n",
      "tensor(3.9636)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.317390\n",
      "Epoch 7925\n",
      "-------------------------------\n",
      "tensor(162.0880)\n",
      "tensor(56.1583)\n",
      "tensor(123.0623)\n",
      "tensor(3.1580)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.035389\n",
      "Epoch 7926\n",
      "-------------------------------\n",
      "tensor(148.9819)\n",
      "tensor(50.0710)\n",
      "tensor(108.2262)\n",
      "tensor(2.7790)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.927094\n",
      "Epoch 7927\n",
      "-------------------------------\n",
      "tensor(206.5423)\n",
      "tensor(72.6306)\n",
      "tensor(157.5861)\n",
      "tensor(4.1921)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.369881\n",
      "Epoch 7928\n",
      "-------------------------------\n",
      "tensor(269.7245)\n",
      "tensor(88.4073)\n",
      "tensor(202.7702)\n",
      "tensor(4.8881)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.028625\n",
      "Epoch 7929\n",
      "-------------------------------\n",
      "tensor(66.2523)\n",
      "tensor(21.2148)\n",
      "tensor(46.3146)\n",
      "tensor(1.0359)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.534222\n",
      "Epoch 7930\n",
      "-------------------------------\n",
      "tensor(182.7360)\n",
      "tensor(58.0592)\n",
      "tensor(136.3297)\n",
      "tensor(3.1743)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.147259\n",
      "Epoch 7931\n",
      "-------------------------------\n",
      "tensor(381.5755)\n",
      "tensor(126.1793)\n",
      "tensor(288.5119)\n",
      "tensor(7.1238)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 38.613441\n",
      "Epoch 7932\n",
      "-------------------------------\n",
      "tensor(572.4051)\n",
      "tensor(186.1346)\n",
      "tensor(430.3825)\n",
      "tensor(10.9740)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.438610\n",
      "Epoch 7933\n",
      "-------------------------------\n",
      "tensor(743.3434)\n",
      "tensor(239.6265)\n",
      "tensor(558.7936)\n",
      "tensor(13.4235)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 47.431084\n",
      "Epoch 7934\n",
      "-------------------------------\n",
      "tensor(756.6162)\n",
      "tensor(245.8925)\n",
      "tensor(571.1133)\n",
      "tensor(14.2263)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 48.043682\n",
      "Epoch 7935\n",
      "-------------------------------\n",
      "tensor(396.6770)\n",
      "tensor(133.1716)\n",
      "tensor(306.0384)\n",
      "tensor(7.7906)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.053204\n",
      "Epoch 7936\n",
      "-------------------------------\n",
      "tensor(235.9242)\n",
      "tensor(74.4039)\n",
      "tensor(175.0934)\n",
      "tensor(4.0627)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.605160\n",
      "Epoch 7937\n",
      "-------------------------------\n",
      "tensor(371.5694)\n",
      "tensor(128.5681)\n",
      "tensor(286.4714)\n",
      "tensor(7.5423)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 38.495205\n",
      "Epoch 7938\n",
      "-------------------------------\n",
      "tensor(194.4470)\n",
      "tensor(64.8871)\n",
      "tensor(147.4082)\n",
      "tensor(3.8090)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.236687\n",
      "Epoch 7939\n",
      "-------------------------------\n",
      "tensor(132.1612)\n",
      "tensor(47.7629)\n",
      "tensor(101.1460)\n",
      "tensor(2.6565)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.826836\n",
      "Epoch 7940\n",
      "-------------------------------\n",
      "tensor(225.7753)\n",
      "tensor(80.7735)\n",
      "tensor(174.7658)\n",
      "tensor(4.6313)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.588387\n",
      "Epoch 7941\n",
      "-------------------------------\n",
      "tensor(175.8902)\n",
      "tensor(64.2123)\n",
      "tensor(136.7281)\n",
      "tensor(3.7160)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.138428\n",
      "Epoch 7942\n",
      "-------------------------------\n",
      "tensor(72.6004)\n",
      "tensor(29.0353)\n",
      "tensor(56.4782)\n",
      "tensor(1.6909)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.552879\n",
      "Epoch 7943\n",
      "-------------------------------\n",
      "tensor(78.6805)\n",
      "tensor(23.9876)\n",
      "tensor(58.3551)\n",
      "tensor(1.2407)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.555611\n",
      "Epoch 7944\n",
      "-------------------------------\n",
      "tensor(228.9531)\n",
      "tensor(76.8792)\n",
      "tensor(174.2151)\n",
      "tensor(4.2831)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.549969\n",
      "Epoch 7945\n",
      "-------------------------------\n",
      "tensor(188.9204)\n",
      "tensor(67.9267)\n",
      "tensor(144.4584)\n",
      "tensor(3.7714)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.155148\n",
      "Epoch 7946\n",
      "-------------------------------\n",
      "tensor(170.7668)\n",
      "tensor(47.7693)\n",
      "tensor(120.6152)\n",
      "tensor(2.6199)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.972038\n",
      "Epoch 7947\n",
      "-------------------------------\n",
      "tensor(251.5930)\n",
      "tensor(74.6624)\n",
      "tensor(183.4932)\n",
      "tensor(4.2143)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 36.772114\n",
      "Epoch 7948\n",
      "-------------------------------\n",
      "tensor(311.4850)\n",
      "tensor(107.0034)\n",
      "tensor(234.6117)\n",
      "tensor(5.8828)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 37.546444\n",
      "Epoch 7949\n",
      "-------------------------------\n",
      "tensor(75.3672)\n",
      "tensor(27.3526)\n",
      "tensor(54.4144)\n",
      "tensor(1.4425)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.500072\n",
      "Epoch 7950\n",
      "-------------------------------\n",
      "tensor(205.2215)\n",
      "tensor(74.7612)\n",
      "tensor(158.8436)\n",
      "tensor(4.0290)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.353397\n",
      "Epoch 7951\n",
      "-------------------------------\n",
      "tensor(450.1041)\n",
      "tensor(133.9738)\n",
      "tensor(329.8380)\n",
      "tensor(8.1993)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 39.598896\n",
      "Epoch 7952\n",
      "-------------------------------\n",
      "tensor(664.9064)\n",
      "tensor(220.5634)\n",
      "tensor(503.9317)\n",
      "tensor(12.2731)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 45.135178\n",
      "Epoch 7953\n",
      "-------------------------------\n",
      "tensor(862.1439)\n",
      "tensor(282.4957)\n",
      "tensor(652.7828)\n",
      "tensor(16.2742)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 51.631119\n",
      "Epoch 7954\n",
      "-------------------------------\n",
      "tensor(875.6982)\n",
      "tensor(286.8250)\n",
      "tensor(663.3433)\n",
      "tensor(16.4867)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 52.234627\n",
      "Epoch 7955\n",
      "-------------------------------\n",
      "tensor(462.0356)\n",
      "tensor(161.8795)\n",
      "tensor(355.2588)\n",
      "tensor(9.0144)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.216553\n",
      "Epoch 7956\n",
      "-------------------------------\n",
      "tensor(260.4835)\n",
      "tensor(92.2964)\n",
      "tensor(197.9701)\n",
      "tensor(5.0810)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.859978\n",
      "Epoch 7957\n",
      "-------------------------------\n",
      "tensor(441.4828)\n",
      "tensor(145.2189)\n",
      "tensor(333.0368)\n",
      "tensor(8.2372)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 39.684429\n",
      "Epoch 7958\n",
      "-------------------------------\n",
      "tensor(216.0840)\n",
      "tensor(73.6989)\n",
      "tensor(163.5921)\n",
      "tensor(4.1702)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.454823\n",
      "Epoch 7959\n",
      "-------------------------------\n",
      "tensor(161.8057)\n",
      "tensor(50.7070)\n",
      "tensor(122.0257)\n",
      "tensor(2.8460)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.993462\n",
      "Epoch 7960\n",
      "-------------------------------\n",
      "tensor(271.1602)\n",
      "tensor(86.3619)\n",
      "tensor(202.8136)\n",
      "tensor(4.8169)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 36.989517\n",
      "Epoch 7961\n",
      "-------------------------------\n",
      "tensor(205.1257)\n",
      "tensor(64.9071)\n",
      "tensor(155.4306)\n",
      "tensor(3.6434)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.333153\n",
      "Epoch 7962\n",
      "-------------------------------\n",
      "tensor(82.0290)\n",
      "tensor(24.5855)\n",
      "tensor(61.0931)\n",
      "tensor(1.3107)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.551888\n",
      "Epoch 7963\n",
      "-------------------------------\n",
      "tensor(94.5824)\n",
      "tensor(33.8850)\n",
      "tensor(70.9321)\n",
      "tensor(1.9383)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.594406\n",
      "Epoch 7964\n",
      "-------------------------------\n",
      "tensor(265.0034)\n",
      "tensor(88.7256)\n",
      "tensor(200.7060)\n",
      "tensor(5.0996)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 36.929901\n",
      "Epoch 7965\n",
      "-------------------------------\n",
      "tensor(215.1887)\n",
      "tensor(68.8339)\n",
      "tensor(160.2317)\n",
      "tensor(3.9580)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.390541\n",
      "Epoch 7966\n",
      "-------------------------------\n",
      "tensor(189.8603)\n",
      "tensor(69.6849)\n",
      "tensor(143.7908)\n",
      "tensor(3.7895)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.271297\n",
      "Epoch 7967\n",
      "-------------------------------\n",
      "tensor(275.7487)\n",
      "tensor(100.1732)\n",
      "tensor(208.3415)\n",
      "tensor(5.5077)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.195641\n",
      "Epoch 7968\n",
      "-------------------------------\n",
      "tensor(355.4252)\n",
      "tensor(113.6582)\n",
      "tensor(267.6837)\n",
      "tensor(6.3549)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.187828\n",
      "Epoch 7969\n",
      "-------------------------------\n",
      "tensor(76.3636)\n",
      "tensor(24.6161)\n",
      "tensor(54.8756)\n",
      "tensor(1.4787)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.537979\n",
      "Epoch 7970\n",
      "-------------------------------\n",
      "tensor(258.2860)\n",
      "tensor(74.2136)\n",
      "tensor(188.2915)\n",
      "tensor(4.2530)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 36.865257\n",
      "Epoch 7971\n",
      "-------------------------------\n",
      "tensor(510.6224)\n",
      "tensor(181.5087)\n",
      "tensor(397.2285)\n",
      "tensor(10.3096)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 41.388775\n",
      "Epoch 7972\n",
      "-------------------------------\n",
      "tensor(759.6019)\n",
      "tensor(250.8729)\n",
      "tensor(579.0480)\n",
      "tensor(14.8477)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 48.111507\n",
      "Epoch 7973\n",
      "-------------------------------\n",
      "tensor(973.6506)\n",
      "tensor(314.8125)\n",
      "tensor(734.1072)\n",
      "tensor(17.8103)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 56.205200\n",
      "Epoch 7974\n",
      "-------------------------------\n",
      "tensor(969.7780)\n",
      "tensor(325.1077)\n",
      "tensor(741.9061)\n",
      "tensor(18.6061)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 56.396675\n",
      "Epoch 7975\n",
      "-------------------------------\n",
      "tensor(502.9174)\n",
      "tensor(166.1985)\n",
      "tensor(384.3211)\n",
      "tensor(9.9015)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.115299\n",
      "Epoch 7976\n",
      "-------------------------------\n",
      "tensor(304.2944)\n",
      "tensor(95.5591)\n",
      "tensor(227.0589)\n",
      "tensor(5.5291)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.430088\n",
      "Epoch 7977\n",
      "-------------------------------\n",
      "tensor(470.7345)\n",
      "tensor(165.0330)\n",
      "tensor(364.2488)\n",
      "tensor(9.5336)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.513069\n",
      "Epoch 7978\n",
      "-------------------------------\n",
      "tensor(252.8180)\n",
      "tensor(88.1266)\n",
      "tensor(192.2132)\n",
      "tensor(5.1401)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.916939\n",
      "Epoch 7979\n",
      "-------------------------------\n",
      "tensor(166.5011)\n",
      "tensor(56.4364)\n",
      "tensor(125.3253)\n",
      "tensor(2.9958)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.060169\n",
      "Epoch 7980\n",
      "-------------------------------\n",
      "tensor(291.5948)\n",
      "tensor(100.9833)\n",
      "tensor(223.9365)\n",
      "tensor(5.6033)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.326469\n",
      "Epoch 7981\n",
      "-------------------------------\n",
      "tensor(231.5876)\n",
      "tensor(82.2365)\n",
      "tensor(178.3048)\n",
      "tensor(4.5519)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.624825\n",
      "Epoch 7982\n",
      "-------------------------------\n",
      "tensor(98.4541)\n",
      "tensor(38.4676)\n",
      "tensor(76.5144)\n",
      "tensor(2.0729)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.644024\n",
      "Epoch 7983\n",
      "-------------------------------\n",
      "tensor(97.8017)\n",
      "tensor(27.1295)\n",
      "tensor(70.9652)\n",
      "tensor(1.5526)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.618904\n",
      "Epoch 7984\n",
      "-------------------------------\n",
      "tensor(294.2235)\n",
      "tensor(93.2877)\n",
      "tensor(222.3302)\n",
      "tensor(5.2963)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.299801\n",
      "Epoch 7985\n",
      "-------------------------------\n",
      "tensor(251.4267)\n",
      "tensor(81.3634)\n",
      "tensor(188.0977)\n",
      "tensor(4.4702)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.759914\n",
      "Epoch 7986\n",
      "-------------------------------\n",
      "tensor(204.8861)\n",
      "tensor(66.0414)\n",
      "tensor(152.8853)\n",
      "tensor(3.8252)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.295509\n",
      "Epoch 7987\n",
      "-------------------------------\n",
      "tensor(324.5697)\n",
      "tensor(98.6648)\n",
      "tensor(239.6406)\n",
      "tensor(5.6700)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 37.666504\n",
      "Epoch 7988\n",
      "-------------------------------\n",
      "tensor(384.7535)\n",
      "tensor(140.8261)\n",
      "tensor(298.6833)\n",
      "tensor(7.8971)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.833569\n",
      "Epoch 7989\n",
      "-------------------------------\n",
      "tensor(87.2393)\n",
      "tensor(23.8480)\n",
      "tensor(52.5673)\n",
      "tensor(1.0974)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.529484\n",
      "Epoch 7990\n",
      "-------------------------------\n",
      "tensor(299.6071)\n",
      "tensor(102.6601)\n",
      "tensor(229.0535)\n",
      "tensor(5.7033)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.372730\n",
      "Epoch 7991\n",
      "-------------------------------\n",
      "tensor(616.4925)\n",
      "tensor(187.0298)\n",
      "tensor(455.9911)\n",
      "tensor(11.1357)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 43.439968\n",
      "Epoch 7992\n",
      "-------------------------------\n",
      "tensor(888.0085)\n",
      "tensor(301.5161)\n",
      "tensor(679.7348)\n",
      "tensor(16.8659)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 52.993988\n",
      "Epoch 7993\n",
      "-------------------------------\n",
      "tensor(1131.0903)\n",
      "tensor(373.7787)\n",
      "tensor(863.9968)\n",
      "tensor(21.7202)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.780018\n",
      "Epoch 7994\n",
      "-------------------------------\n",
      "tensor(1123.1808)\n",
      "tensor(376.4358)\n",
      "tensor(861.4059)\n",
      "tensor(21.8908)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.574951\n",
      "Epoch 7995\n",
      "-------------------------------\n",
      "tensor(565.9778)\n",
      "tensor(199.2090)\n",
      "tensor(439.3198)\n",
      "tensor(11.1180)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.646236\n",
      "Epoch 7996\n",
      "-------------------------------\n",
      "tensor(348.3552)\n",
      "tensor(130.2364)\n",
      "tensor(277.4912)\n",
      "tensor(7.3639)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.262791\n",
      "Epoch 7997\n",
      "-------------------------------\n",
      "tensor(553.2558)\n",
      "tensor(176.5493)\n",
      "tensor(414.6057)\n",
      "tensor(10.1461)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.101059\n",
      "Epoch 7998\n",
      "-------------------------------\n",
      "tensor(295.3069)\n",
      "tensor(96.5514)\n",
      "tensor(222.3918)\n",
      "tensor(5.6433)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.336983\n",
      "Epoch 7999\n",
      "-------------------------------\n",
      "tensor(186.9454)\n",
      "tensor(61.5130)\n",
      "tensor(143.0358)\n",
      "tensor(3.3315)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.186279\n",
      "Epoch 8000\n",
      "-------------------------------\n",
      "tensor(339.2385)\n",
      "tensor(110.8429)\n",
      "tensor(256.0237)\n",
      "tensor(6.1225)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.893017\n",
      "Epoch 8001\n",
      "-------------------------------\n",
      "tensor(270.3692)\n",
      "tensor(87.7968)\n",
      "tensor(203.0050)\n",
      "tensor(4.8347)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.959206\n",
      "Epoch 8002\n",
      "-------------------------------\n",
      "tensor(113.1044)\n",
      "tensor(36.1754)\n",
      "tensor(86.0005)\n",
      "tensor(1.9775)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.663551\n",
      "Epoch 8003\n",
      "-------------------------------\n",
      "tensor(110.4417)\n",
      "tensor(37.8364)\n",
      "tensor(82.4339)\n",
      "tensor(2.1394)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.638859\n",
      "Epoch 8004\n",
      "-------------------------------\n",
      "tensor(334.1350)\n",
      "tensor(111.2662)\n",
      "tensor(254.0497)\n",
      "tensor(6.3055)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.837326\n",
      "Epoch 8005\n",
      "-------------------------------\n",
      "tensor(283.2042)\n",
      "tensor(92.3988)\n",
      "tensor(214.2129)\n",
      "tensor(5.1565)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.156418\n",
      "Epoch 8006\n",
      "-------------------------------\n",
      "tensor(230.9957)\n",
      "tensor(80.9514)\n",
      "tensor(171.0092)\n",
      "tensor(4.5899)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.581440\n",
      "Epoch 8007\n",
      "-------------------------------\n",
      "tensor(358.2839)\n",
      "tensor(122.0204)\n",
      "tensor(271.9136)\n",
      "tensor(7.0853)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.261189\n",
      "Epoch 8008\n",
      "-------------------------------\n",
      "tensor(435.1343)\n",
      "tensor(150.1486)\n",
      "tensor(334.7742)\n",
      "tensor(8.4197)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.677765\n",
      "Epoch 8009\n",
      "-------------------------------\n",
      "tensor(92.7970)\n",
      "tensor(22.4362)\n",
      "tensor(60.1282)\n",
      "tensor(1.0913)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.592979\n",
      "Epoch 8010\n",
      "-------------------------------\n",
      "tensor(318.8234)\n",
      "tensor(103.1793)\n",
      "tensor(240.6609)\n",
      "tensor(5.8987)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.764729\n",
      "Epoch 8011\n",
      "-------------------------------\n",
      "tensor(648.5565)\n",
      "tensor(211.8893)\n",
      "tensor(490.6378)\n",
      "tensor(11.8909)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 44.846504\n",
      "Epoch 8012\n",
      "-------------------------------\n",
      "tensor(945.7251)\n",
      "tensor(316.7181)\n",
      "tensor(724.0670)\n",
      "tensor(18.4260)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.323471\n",
      "Epoch 8013\n",
      "-------------------------------\n",
      "tensor(1206.6650)\n",
      "tensor(404.3380)\n",
      "tensor(922.2362)\n",
      "tensor(23.2260)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.754166\n",
      "Epoch 8014\n",
      "-------------------------------\n",
      "tensor(1210.4950)\n",
      "tensor(403.2931)\n",
      "tensor(926.4359)\n",
      "tensor(23.1614)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.087807\n",
      "Epoch 8015\n",
      "-------------------------------\n",
      "tensor(624.0729)\n",
      "tensor(208.3511)\n",
      "tensor(477.7505)\n",
      "tensor(12.0614)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.204960\n",
      "Epoch 8016\n",
      "-------------------------------\n",
      "tensor(381.4999)\n",
      "tensor(123.4996)\n",
      "tensor(287.4946)\n",
      "tensor(6.9906)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.646236\n",
      "Epoch 8017\n",
      "-------------------------------\n",
      "tensor(587.4568)\n",
      "tensor(201.1306)\n",
      "tensor(454.5566)\n",
      "tensor(11.6251)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.198647\n",
      "Epoch 8018\n",
      "-------------------------------\n",
      "tensor(317.0490)\n",
      "tensor(106.4629)\n",
      "tensor(240.8887)\n",
      "tensor(6.0525)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.674107\n",
      "Epoch 8019\n",
      "-------------------------------\n",
      "tensor(203.9870)\n",
      "tensor(70.5366)\n",
      "tensor(155.7416)\n",
      "tensor(4.0302)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.369217\n",
      "Epoch 8020\n",
      "-------------------------------\n",
      "tensor(362.9087)\n",
      "tensor(124.6173)\n",
      "tensor(277.8964)\n",
      "tensor(7.1438)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.350014\n",
      "Epoch 8021\n",
      "-------------------------------\n",
      "tensor(285.4016)\n",
      "tensor(98.8746)\n",
      "tensor(220.1947)\n",
      "tensor(5.7126)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.268887\n",
      "Epoch 8022\n",
      "-------------------------------\n",
      "tensor(124.6737)\n",
      "tensor(44.1536)\n",
      "tensor(93.9793)\n",
      "tensor(2.5209)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.781567\n",
      "Epoch 8023\n",
      "-------------------------------\n",
      "tensor(125.3622)\n",
      "tensor(40.2986)\n",
      "tensor(88.1853)\n",
      "tensor(2.1141)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.737740\n",
      "Epoch 8024\n",
      "-------------------------------\n",
      "tensor(358.5701)\n",
      "tensor(121.2253)\n",
      "tensor(274.4516)\n",
      "tensor(6.9255)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.248245\n",
      "Epoch 8025\n",
      "-------------------------------\n",
      "tensor(297.6424)\n",
      "tensor(103.5311)\n",
      "tensor(231.4616)\n",
      "tensor(6.0107)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.433411\n",
      "Epoch 8026\n",
      "-------------------------------\n",
      "tensor(249.9409)\n",
      "tensor(80.3084)\n",
      "tensor(186.0014)\n",
      "tensor(4.4438)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.802490\n",
      "Epoch 8027\n",
      "-------------------------------\n",
      "tensor(388.9391)\n",
      "tensor(129.3695)\n",
      "tensor(294.8669)\n",
      "tensor(7.4136)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.757565\n",
      "Epoch 8028\n",
      "-------------------------------\n",
      "tensor(480.9467)\n",
      "tensor(158.4898)\n",
      "tensor(363.7857)\n",
      "tensor(8.7654)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.485493\n",
      "Epoch 8029\n",
      "-------------------------------\n",
      "tensor(102.2306)\n",
      "tensor(32.7699)\n",
      "tensor(72.5494)\n",
      "tensor(1.8647)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.683155\n",
      "Epoch 8030\n",
      "-------------------------------\n",
      "tensor(353.7208)\n",
      "tensor(110.6524)\n",
      "tensor(262.0309)\n",
      "tensor(6.0310)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.181915\n",
      "Epoch 8031\n",
      "-------------------------------\n",
      "tensor(716.1616)\n",
      "tensor(232.6774)\n",
      "tensor(543.9926)\n",
      "tensor(13.7307)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.711452\n",
      "Epoch 8032\n",
      "-------------------------------\n",
      "tensor(1040.8041)\n",
      "tensor(353.3946)\n",
      "tensor(801.1912)\n",
      "tensor(20.4106)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.644726\n",
      "Epoch 8033\n",
      "-------------------------------\n",
      "tensor(1320.9331)\n",
      "tensor(444.3971)\n",
      "tensor(1010.9491)\n",
      "tensor(25.5407)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.027466\n",
      "Epoch 8034\n",
      "-------------------------------\n",
      "tensor(1323.0391)\n",
      "tensor(436.1035)\n",
      "tensor(1004.3788)\n",
      "tensor(24.5904)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 73.885674\n",
      "Epoch 8035\n",
      "-------------------------------\n",
      "tensor(679.7151)\n",
      "tensor(226.7241)\n",
      "tensor(517.1605)\n",
      "tensor(12.9617)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.696587\n",
      "Epoch 8036\n",
      "-------------------------------\n",
      "tensor(418.0602)\n",
      "tensor(133.0818)\n",
      "tensor(309.2455)\n",
      "tensor(7.4064)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.256073\n",
      "Epoch 8037\n",
      "-------------------------------\n",
      "tensor(642.2681)\n",
      "tensor(220.2819)\n",
      "tensor(496.6424)\n",
      "tensor(12.5682)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.700638\n",
      "Epoch 8038\n",
      "-------------------------------\n",
      "tensor(346.3201)\n",
      "tensor(116.7796)\n",
      "tensor(263.2051)\n",
      "tensor(6.4505)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.020290\n",
      "Epoch 8039\n",
      "-------------------------------\n",
      "tensor(220.2057)\n",
      "tensor(74.2727)\n",
      "tensor(170.2770)\n",
      "tensor(4.4423)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.464962\n",
      "Epoch 8040\n",
      "-------------------------------\n",
      "tensor(399.2973)\n",
      "tensor(133.7484)\n",
      "tensor(303.6299)\n",
      "tensor(7.7043)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.850914\n",
      "Epoch 8041\n",
      "-------------------------------\n",
      "tensor(314.7883)\n",
      "tensor(105.1419)\n",
      "tensor(240.3850)\n",
      "tensor(6.0533)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.543606\n",
      "Epoch 8042\n",
      "-------------------------------\n",
      "tensor(133.6344)\n",
      "tensor(44.0376)\n",
      "tensor(101.6805)\n",
      "tensor(2.5152)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.751427\n",
      "Epoch 8043\n",
      "-------------------------------\n",
      "tensor(130.0512)\n",
      "tensor(44.8388)\n",
      "tensor(97.9756)\n",
      "tensor(2.5649)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.727036\n",
      "Epoch 8044\n",
      "-------------------------------\n",
      "tensor(394.2437)\n",
      "tensor(133.8256)\n",
      "tensor(301.5480)\n",
      "tensor(7.7650)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.801102\n",
      "Epoch 8045\n",
      "-------------------------------\n",
      "tensor(329.7109)\n",
      "tensor(111.7595)\n",
      "tensor(252.6097)\n",
      "tensor(6.5989)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.787975\n",
      "Epoch 8046\n",
      "-------------------------------\n",
      "tensor(278.3790)\n",
      "tensor(95.5994)\n",
      "tensor(207.7466)\n",
      "tensor(5.0739)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.070976\n",
      "Epoch 8047\n",
      "-------------------------------\n",
      "tensor(419.8581)\n",
      "tensor(150.3019)\n",
      "tensor(323.5504)\n",
      "tensor(8.4896)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.292740\n",
      "Epoch 8048\n",
      "-------------------------------\n",
      "tensor(537.6384)\n",
      "tensor(168.1226)\n",
      "tensor(398.5840)\n",
      "tensor(9.2556)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.579563\n",
      "Epoch 8049\n",
      "-------------------------------\n",
      "tensor(117.2381)\n",
      "tensor(36.2194)\n",
      "tensor(86.4823)\n",
      "tensor(1.9988)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.726913\n",
      "Epoch 8050\n",
      "-------------------------------\n",
      "tensor(375.7308)\n",
      "tensor(111.4251)\n",
      "tensor(274.2770)\n",
      "tensor(5.9886)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.430809\n",
      "Epoch 8051\n",
      "-------------------------------\n",
      "tensor(762.3031)\n",
      "tensor(257.8474)\n",
      "tensor(583.4427)\n",
      "tensor(14.5771)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.333176\n",
      "Epoch 8052\n",
      "-------------------------------\n",
      "tensor(1114.9169)\n",
      "tensor(379.6551)\n",
      "tensor(861.9535)\n",
      "tensor(22.4071)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.260918\n",
      "Epoch 8053\n",
      "-------------------------------\n",
      "tensor(1417.2035)\n",
      "tensor(476.5050)\n",
      "tensor(1086.0581)\n",
      "tensor(27.4891)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.956367\n",
      "Epoch 8054\n",
      "-------------------------------\n",
      "tensor(1409.1980)\n",
      "tensor(470.8637)\n",
      "tensor(1074.5820)\n",
      "tensor(26.6045)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.567520\n",
      "Epoch 8055\n",
      "-------------------------------\n",
      "tensor(703.7883)\n",
      "tensor(229.4197)\n",
      "tensor(537.4144)\n",
      "tensor(13.0787)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 46.450478\n",
      "Epoch 8056\n",
      "-------------------------------\n",
      "tensor(480.4536)\n",
      "tensor(150.6933)\n",
      "tensor(355.8954)\n",
      "tensor(7.9701)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.289200\n",
      "Epoch 8057\n",
      "-------------------------------\n",
      "tensor(689.6990)\n",
      "tensor(229.5923)\n",
      "tensor(525.0167)\n",
      "tensor(13.3914)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.983410\n",
      "Epoch 8058\n",
      "-------------------------------\n",
      "tensor(396.5002)\n",
      "tensor(121.3624)\n",
      "tensor(291.5533)\n",
      "tensor(6.8999)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.780369\n",
      "Epoch 8059\n",
      "-------------------------------\n",
      "tensor(218.7027)\n",
      "tensor(87.5718)\n",
      "tensor(177.6175)\n",
      "tensor(5.0440)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.562286\n",
      "Epoch 8060\n",
      "-------------------------------\n",
      "tensor(416.7004)\n",
      "tensor(152.2616)\n",
      "tensor(329.0667)\n",
      "tensor(8.7635)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.389542\n",
      "Epoch 8061\n",
      "-------------------------------\n",
      "tensor(341.1552)\n",
      "tensor(123.7154)\n",
      "tensor(266.8759)\n",
      "tensor(7.0487)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.015614\n",
      "Epoch 8062\n",
      "-------------------------------\n",
      "tensor(153.7714)\n",
      "tensor(58.0185)\n",
      "tensor(119.7385)\n",
      "tensor(3.1995)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.904789\n",
      "Epoch 8063\n",
      "-------------------------------\n",
      "tensor(128.9875)\n",
      "tensor(41.7814)\n",
      "tensor(97.3991)\n",
      "tensor(2.4399)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.726280\n",
      "Epoch 8064\n",
      "-------------------------------\n",
      "tensor(419.6089)\n",
      "tensor(144.4357)\n",
      "tensor(326.0795)\n",
      "tensor(8.4103)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.313652\n",
      "Epoch 8065\n",
      "-------------------------------\n",
      "tensor(365.4384)\n",
      "tensor(131.3161)\n",
      "tensor(287.4696)\n",
      "tensor(7.6029)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.386230\n",
      "Epoch 8066\n",
      "-------------------------------\n",
      "tensor(288.8058)\n",
      "tensor(86.1578)\n",
      "tensor(210.7047)\n",
      "tensor(4.9840)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.122746\n",
      "Epoch 8067\n",
      "-------------------------------\n",
      "tensor(474.6205)\n",
      "tensor(151.9472)\n",
      "tensor(359.2664)\n",
      "tensor(9.1382)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.352009\n",
      "Epoch 8068\n",
      "-------------------------------\n",
      "tensor(550.4550)\n",
      "tensor(190.6432)\n",
      "tensor(421.5546)\n",
      "tensor(10.3625)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.269638\n",
      "Epoch 8069\n",
      "-------------------------------\n",
      "tensor(90.4296)\n",
      "tensor(21.6133)\n",
      "tensor(55.3742)\n",
      "tensor(0.9603)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.583767\n",
      "Epoch 8070\n",
      "-------------------------------\n",
      "tensor(452.2500)\n",
      "tensor(147.5171)\n",
      "tensor(339.4723)\n",
      "tensor(7.9957)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.837845\n",
      "Epoch 8071\n",
      "-------------------------------\n",
      "tensor(884.2476)\n",
      "tensor(276.1820)\n",
      "tensor(661.5518)\n",
      "tensor(16.1114)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.386562\n",
      "Epoch 8072\n",
      "-------------------------------\n",
      "tensor(1245.9281)\n",
      "tensor(434.2417)\n",
      "tensor(966.7271)\n",
      "tensor(24.9216)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 70.373657\n",
      "Epoch 8073\n",
      "-------------------------------\n",
      "tensor(1555.4156)\n",
      "tensor(527.6342)\n",
      "tensor(1201.0017)\n",
      "tensor(30.6725)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 90.035736\n",
      "Epoch 8074\n",
      "-------------------------------\n",
      "tensor(1512.7247)\n",
      "tensor(502.8965)\n",
      "tensor(1159.3694)\n",
      "tensor(29.0570)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 86.467926\n",
      "Epoch 8075\n",
      "-------------------------------\n",
      "tensor(715.1045)\n",
      "tensor(248.7632)\n",
      "tensor(552.2146)\n",
      "tensor(13.9752)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.047016\n",
      "Epoch 8076\n",
      "-------------------------------\n",
      "tensor(523.8687)\n",
      "tensor(185.5191)\n",
      "tensor(407.6378)\n",
      "tensor(10.5352)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.658478\n",
      "Epoch 8077\n",
      "-------------------------------\n",
      "tensor(722.6345)\n",
      "tensor(236.7896)\n",
      "tensor(548.9588)\n",
      "tensor(13.5091)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.883053\n",
      "Epoch 8078\n",
      "-------------------------------\n",
      "tensor(431.9269)\n",
      "tensor(144.1839)\n",
      "tensor(328.7090)\n",
      "tensor(8.1839)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.468155\n",
      "Epoch 8079\n",
      "-------------------------------\n",
      "tensor(231.6021)\n",
      "tensor(74.1569)\n",
      "tensor(168.6268)\n",
      "tensor(4.1087)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.458572\n",
      "Epoch 8080\n",
      "-------------------------------\n",
      "tensor(449.7749)\n",
      "tensor(146.5805)\n",
      "tensor(337.9980)\n",
      "tensor(8.3352)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.758678\n",
      "Epoch 8081\n",
      "-------------------------------\n",
      "tensor(374.6386)\n",
      "tensor(121.4216)\n",
      "tensor(279.9660)\n",
      "tensor(6.9177)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.353714\n",
      "Epoch 8082\n",
      "-------------------------------\n",
      "tensor(169.8274)\n",
      "tensor(53.7849)\n",
      "tensor(128.9152)\n",
      "tensor(3.1910)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.992592\n",
      "Epoch 8083\n",
      "-------------------------------\n",
      "tensor(128.5365)\n",
      "tensor(45.3154)\n",
      "tensor(97.4312)\n",
      "tensor(2.4266)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.731747\n",
      "Epoch 8084\n",
      "-------------------------------\n",
      "tensor(442.9272)\n",
      "tensor(149.8703)\n",
      "tensor(338.3172)\n",
      "tensor(8.4241)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.720108\n",
      "Epoch 8085\n",
      "-------------------------------\n",
      "tensor(394.7485)\n",
      "tensor(132.7352)\n",
      "tensor(301.3016)\n",
      "tensor(7.5119)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.825157\n",
      "Epoch 8086\n",
      "-------------------------------\n",
      "tensor(287.5413)\n",
      "tensor(97.2583)\n",
      "tensor(220.5968)\n",
      "tensor(5.4986)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.193413\n",
      "Epoch 8087\n",
      "-------------------------------\n",
      "tensor(508.8617)\n",
      "tensor(173.1576)\n",
      "tensor(386.8133)\n",
      "tensor(9.6704)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.985195\n",
      "Epoch 8088\n",
      "-------------------------------\n",
      "tensor(594.9955)\n",
      "tensor(192.1053)\n",
      "tensor(447.8423)\n",
      "tensor(10.8757)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.032112\n",
      "Epoch 8089\n",
      "-------------------------------\n",
      "tensor(93.1098)\n",
      "tensor(34.7085)\n",
      "tensor(66.2833)\n",
      "tensor(1.8065)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.581024\n",
      "Epoch 8090\n",
      "-------------------------------\n",
      "tensor(472.9264)\n",
      "tensor(154.6773)\n",
      "tensor(353.8875)\n",
      "tensor(8.6708)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.270096\n",
      "Epoch 8091\n",
      "-------------------------------\n",
      "tensor(913.5412)\n",
      "tensor(309.8501)\n",
      "tensor(701.6390)\n",
      "tensor(17.5195)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.020710\n",
      "Epoch 8092\n",
      "-------------------------------\n",
      "tensor(1306.2395)\n",
      "tensor(433.7779)\n",
      "tensor(1001.5244)\n",
      "tensor(25.3653)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 73.559570\n",
      "Epoch 8093\n",
      "-------------------------------\n",
      "tensor(1608.6925)\n",
      "tensor(551.6291)\n",
      "tensor(1241.9082)\n",
      "tensor(31.8694)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 93.356018\n",
      "Epoch 8094\n",
      "-------------------------------\n",
      "tensor(1557.5564)\n",
      "tensor(524.4702)\n",
      "tensor(1198.7817)\n",
      "tensor(30.0294)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 90.196053\n",
      "Epoch 8095\n",
      "-------------------------------\n",
      "tensor(737.1528)\n",
      "tensor(239.8569)\n",
      "tensor(561.2751)\n",
      "tensor(14.0228)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.619419\n",
      "Epoch 8096\n",
      "-------------------------------\n",
      "tensor(562.4866)\n",
      "tensor(175.9969)\n",
      "tensor(424.1031)\n",
      "tensor(10.1574)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.532219\n",
      "Epoch 8097\n",
      "-------------------------------\n",
      "tensor(724.5661)\n",
      "tensor(254.9757)\n",
      "tensor(563.7646)\n",
      "tensor(14.7185)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.390186\n",
      "Epoch 8098\n",
      "-------------------------------\n",
      "tensor(450.7109)\n",
      "tensor(155.2886)\n",
      "tensor(348.6211)\n",
      "tensor(9.0011)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.948616\n",
      "Epoch 8099\n",
      "-------------------------------\n",
      "tensor(218.5149)\n",
      "tensor(76.0014)\n",
      "tensor(166.9115)\n",
      "tensor(4.2830)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.498703\n",
      "Epoch 8100\n",
      "-------------------------------\n",
      "tensor(452.6657)\n",
      "tensor(158.5085)\n",
      "tensor(350.2653)\n",
      "tensor(9.0979)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.061424\n",
      "Epoch 8101\n",
      "-------------------------------\n",
      "tensor(379.5695)\n",
      "tensor(135.3158)\n",
      "tensor(295.3896)\n",
      "tensor(7.8039)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.715702\n",
      "Epoch 8102\n",
      "-------------------------------\n",
      "tensor(180.3945)\n",
      "tensor(68.4832)\n",
      "tensor(142.3458)\n",
      "tensor(3.9722)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.199501\n",
      "Epoch 8103\n",
      "-------------------------------\n",
      "tensor(127.8716)\n",
      "tensor(36.7629)\n",
      "tensor(91.8517)\n",
      "tensor(1.9544)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.790783\n",
      "Epoch 8104\n",
      "-------------------------------\n",
      "tensor(454.3068)\n",
      "tensor(149.5379)\n",
      "tensor(346.8389)\n",
      "tensor(8.5285)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.013210\n",
      "Epoch 8105\n",
      "-------------------------------\n",
      "tensor(416.1960)\n",
      "tensor(142.4602)\n",
      "tensor(321.1270)\n",
      "tensor(8.1820)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.286991\n",
      "Epoch 8106\n",
      "-------------------------------\n",
      "tensor(292.4066)\n",
      "tensor(89.6800)\n",
      "tensor(215.0259)\n",
      "tensor(4.9802)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.215714\n",
      "Epoch 8107\n",
      "-------------------------------\n",
      "tensor(538.8519)\n",
      "tensor(171.4317)\n",
      "tensor(403.7766)\n",
      "tensor(9.7999)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.660233\n",
      "Epoch 8108\n",
      "-------------------------------\n",
      "tensor(591.8249)\n",
      "tensor(201.8460)\n",
      "tensor(453.0187)\n",
      "tensor(11.1758)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.248795\n",
      "Epoch 8109\n",
      "-------------------------------\n",
      "tensor(76.9441)\n",
      "tensor(23.2385)\n",
      "tensor(41.8847)\n",
      "tensor(1.2035)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.508553\n",
      "Epoch 8110\n",
      "-------------------------------\n",
      "tensor(542.4686)\n",
      "tensor(175.0806)\n",
      "tensor(408.0272)\n",
      "tensor(9.5206)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.752796\n",
      "Epoch 8111\n",
      "-------------------------------\n",
      "tensor(1007.9622)\n",
      "tensor(330.7548)\n",
      "tensor(766.0416)\n",
      "tensor(19.6093)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 57.836258\n",
      "Epoch 8112\n",
      "-------------------------------\n",
      "tensor(1408.3619)\n",
      "tensor(492.3443)\n",
      "tensor(1094.1428)\n",
      "tensor(28.1339)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 80.139877\n",
      "Epoch 8113\n",
      "-------------------------------\n",
      "tensor(1750.6007)\n",
      "tensor(581.3879)\n",
      "tensor(1338.2809)\n",
      "tensor(33.4818)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 103.868484\n",
      "Epoch 8114\n",
      "-------------------------------\n",
      "tensor(1673.6307)\n",
      "tensor(564.4423)\n",
      "tensor(1285.0002)\n",
      "tensor(32.1742)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 97.851395\n",
      "Epoch 8115\n",
      "-------------------------------\n",
      "tensor(775.2640)\n",
      "tensor(264.9141)\n",
      "tensor(597.0147)\n",
      "tensor(15.2441)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.187813\n",
      "Epoch 8116\n",
      "-------------------------------\n",
      "tensor(602.5759)\n",
      "tensor(207.2464)\n",
      "tensor(463.2915)\n",
      "tensor(11.7527)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.889950\n",
      "Epoch 8117\n",
      "-------------------------------\n",
      "tensor(786.0425)\n",
      "tensor(260.7335)\n",
      "tensor(602.0412)\n",
      "tensor(15.1071)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.413307\n",
      "Epoch 8118\n",
      "-------------------------------\n",
      "tensor(492.6667)\n",
      "tensor(163.1047)\n",
      "tensor(376.4436)\n",
      "tensor(9.4696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.949829\n",
      "Epoch 8119\n",
      "-------------------------------\n",
      "tensor(234.4296)\n",
      "tensor(79.9589)\n",
      "tensor(173.6812)\n",
      "tensor(4.3567)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.657452\n",
      "Epoch 8120\n",
      "-------------------------------\n",
      "tensor(487.6611)\n",
      "tensor(165.2133)\n",
      "tensor(372.1747)\n",
      "tensor(9.3862)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.682503\n",
      "Epoch 8121\n",
      "-------------------------------\n",
      "tensor(410.3827)\n",
      "tensor(139.3177)\n",
      "tensor(316.2603)\n",
      "tensor(8.0097)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.151253\n",
      "Epoch 8122\n",
      "-------------------------------\n",
      "tensor(198.9412)\n",
      "tensor(67.7802)\n",
      "tensor(153.9604)\n",
      "tensor(3.9266)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.276951\n",
      "Epoch 8123\n",
      "-------------------------------\n",
      "tensor(130.5171)\n",
      "tensor(43.6335)\n",
      "tensor(96.0563)\n",
      "tensor(2.4002)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.780312\n",
      "Epoch 8124\n",
      "-------------------------------\n",
      "tensor(483.4518)\n",
      "tensor(164.3168)\n",
      "tensor(369.6069)\n",
      "tensor(9.4038)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.626347\n",
      "Epoch 8125\n",
      "-------------------------------\n",
      "tensor(444.1914)\n",
      "tensor(154.1579)\n",
      "tensor(343.5692)\n",
      "tensor(8.9611)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.900257\n",
      "Epoch 8126\n",
      "-------------------------------\n",
      "tensor(302.2776)\n",
      "tensor(97.0757)\n",
      "tensor(227.7830)\n",
      "tensor(5.3115)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.482334\n",
      "Epoch 8127\n",
      "-------------------------------\n",
      "tensor(569.9955)\n",
      "tensor(191.8736)\n",
      "tensor(436.8964)\n",
      "tensor(10.8573)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.728592\n",
      "Epoch 8128\n",
      "-------------------------------\n",
      "tensor(631.8137)\n",
      "tensor(204.5396)\n",
      "tensor(475.4322)\n",
      "tensor(11.5383)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.190205\n",
      "Epoch 8129\n",
      "-------------------------------\n",
      "tensor(63.8379)\n",
      "tensor(24.9368)\n",
      "tensor(41.1388)\n",
      "tensor(1.1992)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.626423\n",
      "Epoch 8130\n",
      "-------------------------------\n",
      "tensor(561.4169)\n",
      "tensor(178.6636)\n",
      "tensor(421.9785)\n",
      "tensor(10.4365)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.335312\n",
      "Epoch 8131\n",
      "-------------------------------\n",
      "tensor(1043.0540)\n",
      "tensor(346.9437)\n",
      "tensor(796.9666)\n",
      "tensor(19.8583)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 59.689919\n",
      "Epoch 8132\n",
      "-------------------------------\n",
      "tensor(1461.2617)\n",
      "tensor(497.3953)\n",
      "tensor(1129.2542)\n",
      "tensor(28.8660)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 83.564072\n",
      "Epoch 8133\n",
      "-------------------------------\n",
      "tensor(1796.3392)\n",
      "tensor(610.4238)\n",
      "tensor(1384.2822)\n",
      "tensor(35.2980)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.700073\n",
      "Epoch 8134\n",
      "-------------------------------\n",
      "tensor(1722.4509)\n",
      "tensor(581.8605)\n",
      "tensor(1326.9952)\n",
      "tensor(33.4470)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 102.488663\n",
      "Epoch 8135\n",
      "-------------------------------\n",
      "tensor(791.2297)\n",
      "tensor(267.3719)\n",
      "tensor(612.2549)\n",
      "tensor(15.4885)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.725933\n",
      "Epoch 8136\n",
      "-------------------------------\n",
      "tensor(641.1904)\n",
      "tensor(210.5979)\n",
      "tensor(488.9314)\n",
      "tensor(11.8409)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.713364\n",
      "Epoch 8137\n",
      "-------------------------------\n",
      "tensor(812.2297)\n",
      "tensor(270.6772)\n",
      "tensor(622.1698)\n",
      "tensor(16.1682)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 50.274796\n",
      "Epoch 8138\n",
      "-------------------------------\n",
      "tensor(516.9321)\n",
      "tensor(174.5699)\n",
      "tensor(394.5540)\n",
      "tensor(10.1346)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.435509\n",
      "Epoch 8139\n",
      "-------------------------------\n",
      "tensor(231.8423)\n",
      "tensor(79.6939)\n",
      "tensor(177.9365)\n",
      "tensor(4.4646)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.590008\n",
      "Epoch 8140\n",
      "-------------------------------\n",
      "tensor(505.5061)\n",
      "tensor(173.0282)\n",
      "tensor(389.1544)\n",
      "tensor(9.8987)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.104679\n",
      "Epoch 8141\n",
      "-------------------------------\n",
      "tensor(430.4723)\n",
      "tensor(148.3736)\n",
      "tensor(332.7594)\n",
      "tensor(8.5418)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.559944\n",
      "Epoch 8142\n",
      "-------------------------------\n",
      "tensor(212.1824)\n",
      "tensor(74.7512)\n",
      "tensor(164.5813)\n",
      "tensor(4.3274)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.411282\n",
      "Epoch 8143\n",
      "-------------------------------\n",
      "tensor(131.1799)\n",
      "tensor(41.7923)\n",
      "tensor(96.3261)\n",
      "tensor(2.2651)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.756100\n",
      "Epoch 8144\n",
      "-------------------------------\n",
      "tensor(504.3445)\n",
      "tensor(169.5472)\n",
      "tensor(385.1407)\n",
      "tensor(9.6423)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.020130\n",
      "Epoch 8145\n",
      "-------------------------------\n",
      "tensor(473.6997)\n",
      "tensor(163.4229)\n",
      "tensor(364.3606)\n",
      "tensor(9.3564)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.392738\n",
      "Epoch 8146\n",
      "-------------------------------\n",
      "tensor(314.6602)\n",
      "tensor(97.2529)\n",
      "tensor(233.4197)\n",
      "tensor(5.4106)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.540180\n",
      "Epoch 8147\n",
      "-------------------------------\n",
      "tensor(610.5500)\n",
      "tensor(197.8854)\n",
      "tensor(460.7955)\n",
      "tensor(11.2402)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.555336\n",
      "Epoch 8148\n",
      "-------------------------------\n",
      "tensor(663.8599)\n",
      "tensor(213.0280)\n",
      "tensor(501.0361)\n",
      "tensor(12.3148)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.938713\n",
      "Epoch 8149\n",
      "-------------------------------\n",
      "tensor(61.0544)\n",
      "tensor(20.1947)\n",
      "tensor(40.2322)\n",
      "tensor(1.2449)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.462040\n",
      "Epoch 8150\n",
      "-------------------------------\n",
      "tensor(592.5568)\n",
      "tensor(191.7976)\n",
      "tensor(447.7732)\n",
      "tensor(10.9082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 43.154884\n",
      "Epoch 8151\n",
      "-------------------------------\n",
      "tensor(1084.6244)\n",
      "tensor(365.4934)\n",
      "tensor(834.9489)\n",
      "tensor(21.3931)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 61.852077\n",
      "Epoch 8152\n",
      "-------------------------------\n",
      "tensor(1513.1345)\n",
      "tensor(514.0447)\n",
      "tensor(1165.0704)\n",
      "tensor(29.6770)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 86.621941\n",
      "Epoch 8153\n",
      "-------------------------------\n",
      "tensor(1842.5676)\n",
      "tensor(618.4202)\n",
      "tensor(1413.5924)\n",
      "tensor(35.3682)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 111.710701\n",
      "Epoch 8154\n",
      "-------------------------------\n",
      "tensor(1731.3313)\n",
      "tensor(584.0861)\n",
      "tensor(1335.2977)\n",
      "tensor(33.4222)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 103.060005\n",
      "Epoch 8155\n",
      "-------------------------------\n",
      "tensor(761.1765)\n",
      "tensor(257.0266)\n",
      "tensor(586.4070)\n",
      "tensor(15.2334)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.709213\n",
      "Epoch 8156\n",
      "-------------------------------\n",
      "tensor(673.1799)\n",
      "tensor(219.6061)\n",
      "tensor(511.7780)\n",
      "tensor(12.7996)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 45.613747\n",
      "Epoch 8157\n",
      "-------------------------------\n",
      "tensor(792.1465)\n",
      "tensor(278.3610)\n",
      "tensor(618.9247)\n",
      "tensor(16.1211)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.870564\n",
      "Epoch 8158\n",
      "-------------------------------\n",
      "tensor(532.2020)\n",
      "tensor(188.3446)\n",
      "tensor(417.2831)\n",
      "tensor(11.0358)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.988136\n",
      "Epoch 8159\n",
      "-------------------------------\n",
      "tensor(208.8170)\n",
      "tensor(67.0637)\n",
      "tensor(158.4790)\n",
      "tensor(3.7111)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.422573\n",
      "Epoch 8160\n",
      "-------------------------------\n",
      "tensor(496.2011)\n",
      "tensor(167.7625)\n",
      "tensor(383.6601)\n",
      "tensor(9.5736)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 41.002605\n",
      "Epoch 8161\n",
      "-------------------------------\n",
      "tensor(435.7339)\n",
      "tensor(149.6900)\n",
      "tensor(338.9958)\n",
      "tensor(8.5524)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.758995\n",
      "Epoch 8162\n",
      "-------------------------------\n",
      "tensor(224.6971)\n",
      "tensor(79.9809)\n",
      "tensor(177.6082)\n",
      "tensor(4.5394)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.623554\n",
      "Epoch 8163\n",
      "-------------------------------\n",
      "tensor(115.9513)\n",
      "tensor(33.9200)\n",
      "tensor(81.2403)\n",
      "tensor(1.9659)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.737602\n",
      "Epoch 8164\n",
      "-------------------------------\n",
      "tensor(498.8962)\n",
      "tensor(162.7236)\n",
      "tensor(376.6624)\n",
      "tensor(9.4712)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.923946\n",
      "Epoch 8165\n",
      "-------------------------------\n",
      "tensor(488.9062)\n",
      "tensor(160.9087)\n",
      "tensor(371.7666)\n",
      "tensor(9.5290)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.745495\n",
      "Epoch 8166\n",
      "-------------------------------\n",
      "tensor(284.7349)\n",
      "tensor(98.1720)\n",
      "tensor(219.2801)\n",
      "tensor(5.1921)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 37.287262\n",
      "Epoch 8167\n",
      "-------------------------------\n",
      "tensor(614.7088)\n",
      "tensor(211.2816)\n",
      "tensor(474.9642)\n",
      "tensor(11.8157)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.940823\n",
      "Epoch 8168\n",
      "-------------------------------\n",
      "tensor(634.3788)\n",
      "tensor(204.7631)\n",
      "tensor(478.8622)\n",
      "tensor(11.7006)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 44.223522\n",
      "Epoch 8169\n",
      "-------------------------------\n",
      "tensor(45.9835)\n",
      "tensor(16.7983)\n",
      "tensor(3.8174)\n",
      "tensor(0.2082)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.487190\n",
      "Epoch 8170\n",
      "-------------------------------\n",
      "tensor(641.0181)\n",
      "tensor(203.2414)\n",
      "tensor(480.6378)\n",
      "tensor(12.1373)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.526279\n",
      "Epoch 8171\n",
      "-------------------------------\n",
      "tensor(1113.8373)\n",
      "tensor(382.3725)\n",
      "tensor(860.1605)\n",
      "tensor(21.7534)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 63.216774\n",
      "Epoch 8172\n",
      "-------------------------------\n",
      "tensor(1519.6038)\n",
      "tensor(510.3796)\n",
      "tensor(1170.1005)\n",
      "tensor(29.6790)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 87.897423\n",
      "Epoch 8173\n",
      "-------------------------------\n",
      "tensor(1790.2736)\n",
      "tensor(611.1968)\n",
      "tensor(1390.0537)\n",
      "tensor(35.5794)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 108.048111\n",
      "Epoch 8174\n",
      "-------------------------------\n",
      "tensor(1627.5548)\n",
      "tensor(562.3286)\n",
      "tensor(1266.7319)\n",
      "tensor(32.8238)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 96.244904\n",
      "Epoch 8175\n",
      "-------------------------------\n",
      "tensor(664.4160)\n",
      "tensor(219.4879)\n",
      "tensor(509.9685)\n",
      "tensor(12.6744)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.448078\n",
      "Epoch 8176\n",
      "-------------------------------\n",
      "tensor(685.3267)\n",
      "tensor(230.7143)\n",
      "tensor(528.9322)\n",
      "tensor(13.3537)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 46.055321\n",
      "Epoch 8177\n",
      "-------------------------------\n",
      "tensor(736.8908)\n",
      "tensor(241.1467)\n",
      "tensor(559.2978)\n",
      "tensor(14.2079)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.554646\n",
      "Epoch 8178\n",
      "-------------------------------\n",
      "tensor(545.5870)\n",
      "tensor(175.8590)\n",
      "tensor(412.6594)\n",
      "tensor(10.5982)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 42.074951\n",
      "Epoch 8179\n",
      "-------------------------------\n",
      "tensor(175.1791)\n",
      "tensor(55.8655)\n",
      "tensor(130.2665)\n",
      "tensor(2.9845)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.245789\n",
      "Epoch 8180\n",
      "-------------------------------\n",
      "tensor(466.5217)\n",
      "tensor(153.8967)\n",
      "tensor(356.3150)\n",
      "tensor(8.6849)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.399021\n",
      "Epoch 8181\n",
      "-------------------------------\n",
      "tensor(426.4585)\n",
      "tensor(140.7303)\n",
      "tensor(324.5683)\n",
      "tensor(7.9356)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 39.544003\n",
      "Epoch 8182\n",
      "-------------------------------\n",
      "tensor(234.7975)\n",
      "tensor(77.1216)\n",
      "tensor(177.4717)\n",
      "tensor(4.3101)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.712952\n",
      "Epoch 8183\n",
      "-------------------------------\n",
      "tensor(89.8793)\n",
      "tensor(30.5388)\n",
      "tensor(66.8588)\n",
      "tensor(1.7225)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.649555\n",
      "Epoch 8184\n",
      "-------------------------------\n",
      "tensor(464.1303)\n",
      "tensor(154.8079)\n",
      "tensor(354.4625)\n",
      "tensor(8.8005)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.237984\n",
      "Epoch 8185\n",
      "-------------------------------\n",
      "tensor(478.7325)\n",
      "tensor(158.8462)\n",
      "tensor(365.9988)\n",
      "tensor(8.9748)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.558296\n",
      "Epoch 8186\n",
      "-------------------------------\n",
      "tensor(250.9629)\n",
      "tensor(85.0858)\n",
      "tensor(192.1197)\n",
      "tensor(5.0005)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.862667\n",
      "Epoch 8187\n",
      "-------------------------------\n",
      "tensor(604.3447)\n",
      "tensor(199.5173)\n",
      "tensor(459.9435)\n",
      "tensor(11.4994)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 43.517250\n",
      "Epoch 8188\n",
      "-------------------------------\n",
      "tensor(586.8647)\n",
      "tensor(194.0970)\n",
      "tensor(448.1778)\n",
      "tensor(11.4722)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.100113\n",
      "Epoch 8189\n",
      "-------------------------------\n",
      "tensor(47.7585)\n",
      "tensor(21.8438)\n",
      "tensor(22.0989)\n",
      "tensor(0.9099)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.694717\n",
      "Epoch 8190\n",
      "-------------------------------\n",
      "tensor(621.0368)\n",
      "tensor(204.0867)\n",
      "tensor(478.0602)\n",
      "tensor(11.8963)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 44.338448\n",
      "Epoch 8191\n",
      "-------------------------------\n",
      "tensor(1051.8521)\n",
      "tensor(349.7724)\n",
      "tensor(807.0344)\n",
      "tensor(20.5922)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 60.322201\n",
      "Epoch 8192\n",
      "-------------------------------\n",
      "tensor(1405.9877)\n",
      "tensor(481.8093)\n",
      "tensor(1084.8787)\n",
      "tensor(27.6308)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 79.905334\n",
      "Epoch 8193\n",
      "-------------------------------\n",
      "tensor(1673.7953)\n",
      "tensor(554.5557)\n",
      "tensor(1282.8263)\n",
      "tensor(31.8693)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 98.440811\n",
      "Epoch 8194\n",
      "-------------------------------\n",
      "tensor(1541.2438)\n",
      "tensor(523.2951)\n",
      "tensor(1188.0267)\n",
      "tensor(30.3522)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 88.934288\n",
      "Epoch 8195\n",
      "-------------------------------\n",
      "tensor(655.5123)\n",
      "tensor(228.5508)\n",
      "tensor(509.0815)\n",
      "tensor(12.9932)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.529961\n",
      "Epoch 8196\n",
      "-------------------------------\n",
      "tensor(602.1837)\n",
      "tensor(215.5086)\n",
      "tensor(472.3506)\n",
      "tensor(12.3620)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.087444\n",
      "Epoch 8197\n",
      "-------------------------------\n",
      "tensor(706.5546)\n",
      "tensor(229.4980)\n",
      "tensor(535.3278)\n",
      "tensor(13.2461)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.698082\n",
      "Epoch 8198\n",
      "-------------------------------\n",
      "tensor(487.6253)\n",
      "tensor(162.8271)\n",
      "tensor(372.9342)\n",
      "tensor(9.5638)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.935757\n",
      "Epoch 8199\n",
      "-------------------------------\n",
      "tensor(184.3016)\n",
      "tensor(59.1411)\n",
      "tensor(134.5455)\n",
      "tensor(3.0059)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.326424\n",
      "Epoch 8200\n",
      "-------------------------------\n",
      "tensor(445.8228)\n",
      "tensor(146.6142)\n",
      "tensor(336.5059)\n",
      "tensor(8.1153)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.913078\n",
      "Epoch 8201\n",
      "-------------------------------\n",
      "tensor(394.8409)\n",
      "tensor(130.3923)\n",
      "tensor(298.9409)\n",
      "tensor(7.2820)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.935234\n",
      "Epoch 8202\n",
      "-------------------------------\n",
      "tensor(205.1661)\n",
      "tensor(68.1222)\n",
      "tensor(156.9644)\n",
      "tensor(3.8424)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.418781\n",
      "Epoch 8203\n",
      "-------------------------------\n",
      "tensor(100.6006)\n",
      "tensor(32.9433)\n",
      "tensor(72.2784)\n",
      "tensor(1.7790)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.713455\n",
      "Epoch 8204\n",
      "-------------------------------\n",
      "tensor(441.6692)\n",
      "tensor(146.5255)\n",
      "tensor(334.3400)\n",
      "tensor(8.2533)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.804585\n",
      "Epoch 8205\n",
      "-------------------------------\n",
      "tensor(436.6223)\n",
      "tensor(145.3181)\n",
      "tensor(330.9961)\n",
      "tensor(8.2038)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.695164\n",
      "Epoch 8206\n",
      "-------------------------------\n",
      "tensor(253.1178)\n",
      "tensor(83.2784)\n",
      "tensor(193.8073)\n",
      "tensor(4.7000)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.883430\n",
      "Epoch 8207\n",
      "-------------------------------\n",
      "tensor(556.6082)\n",
      "tensor(182.4437)\n",
      "tensor(420.6958)\n",
      "tensor(10.1197)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 42.227997\n",
      "Epoch 8208\n",
      "-------------------------------\n",
      "tensor(560.3275)\n",
      "tensor(187.0693)\n",
      "tensor(431.1908)\n",
      "tensor(10.8493)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.509113\n",
      "Epoch 8209\n",
      "-------------------------------\n",
      "tensor(55.6389)\n",
      "tensor(21.5991)\n",
      "tensor(12.2390)\n",
      "tensor(0.4676)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.454964\n",
      "Epoch 8210\n",
      "-------------------------------\n",
      "tensor(577.2075)\n",
      "tensor(189.0247)\n",
      "tensor(441.3982)\n",
      "tensor(11.6500)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.763832\n",
      "Epoch 8211\n",
      "-------------------------------\n",
      "tensor(1002.0507)\n",
      "tensor(348.5968)\n",
      "tensor(779.4219)\n",
      "tensor(20.2288)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.289742\n",
      "Epoch 8212\n",
      "-------------------------------\n",
      "tensor(1379.4133)\n",
      "tensor(457.8982)\n",
      "tensor(1057.3127)\n",
      "tensor(26.3208)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 78.157654\n",
      "Epoch 8213\n",
      "-------------------------------\n",
      "tensor(1651.9415)\n",
      "tensor(551.9994)\n",
      "tensor(1265.3361)\n",
      "tensor(31.8084)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 96.325203\n",
      "Epoch 8214\n",
      "-------------------------------\n",
      "tensor(1535.3038)\n",
      "tensor(529.5930)\n",
      "tensor(1193.6299)\n",
      "tensor(30.6750)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 89.096077\n",
      "Epoch 8215\n",
      "-------------------------------\n",
      "tensor(682.2639)\n",
      "tensor(226.4740)\n",
      "tensor(522.6379)\n",
      "tensor(13.3711)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.904182\n",
      "Epoch 8216\n",
      "-------------------------------\n",
      "tensor(591.3141)\n",
      "tensor(197.9547)\n",
      "tensor(454.2056)\n",
      "tensor(11.6804)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.259460\n",
      "Epoch 8217\n",
      "-------------------------------\n",
      "tensor(716.5502)\n",
      "tensor(236.2203)\n",
      "tensor(546.7688)\n",
      "tensor(13.8351)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.671501\n",
      "Epoch 8218\n",
      "-------------------------------\n",
      "tensor(466.1009)\n",
      "tensor(163.2776)\n",
      "tensor(362.3151)\n",
      "tensor(9.4741)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.260967\n",
      "Epoch 8219\n",
      "-------------------------------\n",
      "tensor(199.8275)\n",
      "tensor(60.8388)\n",
      "tensor(145.8271)\n",
      "tensor(3.2628)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.204559\n",
      "Epoch 8220\n",
      "-------------------------------\n",
      "tensor(447.6608)\n",
      "tensor(146.7051)\n",
      "tensor(338.8049)\n",
      "tensor(8.2613)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.761208\n",
      "Epoch 8221\n",
      "-------------------------------\n",
      "tensor(387.2785)\n",
      "tensor(128.7936)\n",
      "tensor(294.5977)\n",
      "tensor(7.3117)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.655556\n",
      "Epoch 8222\n",
      "-------------------------------\n",
      "tensor(195.8526)\n",
      "tensor(66.5650)\n",
      "tensor(149.4090)\n",
      "tensor(3.8112)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.177631\n",
      "Epoch 8223\n",
      "-------------------------------\n",
      "tensor(119.0990)\n",
      "tensor(36.6151)\n",
      "tensor(78.9063)\n",
      "tensor(1.7690)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.617512\n",
      "Epoch 8224\n",
      "-------------------------------\n",
      "tensor(447.1798)\n",
      "tensor(145.7679)\n",
      "tensor(333.8432)\n",
      "tensor(8.0845)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.730968\n",
      "Epoch 8225\n",
      "-------------------------------\n",
      "tensor(429.5553)\n",
      "tensor(141.3947)\n",
      "tensor(321.8244)\n",
      "tensor(7.9211)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.365097\n",
      "Epoch 8226\n",
      "-------------------------------\n",
      "tensor(268.4779)\n",
      "tensor(87.3480)\n",
      "tensor(201.2680)\n",
      "tensor(4.7811)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.877571\n",
      "Epoch 8227\n",
      "-------------------------------\n",
      "tensor(539.4307)\n",
      "tensor(177.0354)\n",
      "tensor(409.4376)\n",
      "tensor(9.8746)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.812664\n",
      "Epoch 8228\n",
      "-------------------------------\n",
      "tensor(572.6911)\n",
      "tensor(182.3093)\n",
      "tensor(431.6094)\n",
      "tensor(10.8609)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.510654\n",
      "Epoch 8229\n",
      "-------------------------------\n",
      "tensor(37.6220)\n",
      "tensor(14.3136)\n",
      "tensor(21.4181)\n",
      "tensor(0.8269)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.411987\n",
      "Epoch 8230\n",
      "-------------------------------\n",
      "tensor(533.3721)\n",
      "tensor(175.0920)\n",
      "tensor(405.9445)\n",
      "tensor(10.2197)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.633595\n",
      "Epoch 8231\n",
      "-------------------------------\n",
      "tensor(953.2322)\n",
      "tensor(325.0674)\n",
      "tensor(737.0167)\n",
      "tensor(18.9730)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 55.925140\n",
      "Epoch 8232\n",
      "-------------------------------\n",
      "tensor(1320.5529)\n",
      "tensor(437.4902)\n",
      "tensor(1008.4264)\n",
      "tensor(25.2416)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 74.064537\n",
      "Epoch 8233\n",
      "-------------------------------\n",
      "tensor(1578.7694)\n",
      "tensor(531.1522)\n",
      "tensor(1212.6260)\n",
      "tensor(30.3781)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 91.643997\n",
      "Epoch 8234\n",
      "-------------------------------\n",
      "tensor(1456.6578)\n",
      "tensor(494.3042)\n",
      "tensor(1126.7449)\n",
      "tensor(28.4510)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 83.457108\n",
      "Epoch 8235\n",
      "-------------------------------\n",
      "tensor(613.8228)\n",
      "tensor(204.5784)\n",
      "tensor(469.4648)\n",
      "tensor(12.1310)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.070728\n",
      "Epoch 8236\n",
      "-------------------------------\n",
      "tensor(593.6941)\n",
      "tensor(194.0575)\n",
      "tensor(452.7817)\n",
      "tensor(11.5126)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.439835\n",
      "Epoch 8237\n",
      "-------------------------------\n",
      "tensor(651.6788)\n",
      "tensor(230.1097)\n",
      "tensor(510.2086)\n",
      "tensor(13.1684)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.305485\n",
      "Epoch 8238\n",
      "-------------------------------\n",
      "tensor(467.3590)\n",
      "tensor(167.8398)\n",
      "tensor(367.4261)\n",
      "tensor(9.7212)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.511715\n",
      "Epoch 8239\n",
      "-------------------------------\n",
      "tensor(165.8663)\n",
      "tensor(49.3503)\n",
      "tensor(117.0706)\n",
      "tensor(2.5655)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.999397\n",
      "Epoch 8240\n",
      "-------------------------------\n",
      "tensor(416.2114)\n",
      "tensor(137.2662)\n",
      "tensor(316.8689)\n",
      "tensor(7.7705)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.221893\n",
      "Epoch 8241\n",
      "-------------------------------\n",
      "tensor(371.1449)\n",
      "tensor(125.0510)\n",
      "tensor(287.1229)\n",
      "tensor(7.1573)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.515507\n",
      "Epoch 8242\n",
      "-------------------------------\n",
      "tensor(199.6058)\n",
      "tensor(69.2401)\n",
      "tensor(156.3083)\n",
      "tensor(3.9534)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.325325\n",
      "Epoch 8243\n",
      "-------------------------------\n",
      "tensor(86.9599)\n",
      "tensor(25.4586)\n",
      "tensor(59.1810)\n",
      "tensor(1.4053)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.565517\n",
      "Epoch 8244\n",
      "-------------------------------\n",
      "tensor(414.1253)\n",
      "tensor(134.0288)\n",
      "tensor(311.3477)\n",
      "tensor(7.7399)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.155041\n",
      "Epoch 8245\n",
      "-------------------------------\n",
      "tensor(422.7411)\n",
      "tensor(136.9279)\n",
      "tensor(318.5867)\n",
      "tensor(8.0423)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.313839\n",
      "Epoch 8246\n",
      "-------------------------------\n",
      "tensor(225.9051)\n",
      "tensor(79.7420)\n",
      "tensor(173.9446)\n",
      "tensor(4.1192)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.615669\n",
      "Epoch 8247\n",
      "-------------------------------\n",
      "tensor(528.2167)\n",
      "tensor(182.6208)\n",
      "tensor(406.4991)\n",
      "tensor(10.0316)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.744476\n",
      "Epoch 8248\n",
      "-------------------------------\n",
      "tensor(522.1633)\n",
      "tensor(165.7141)\n",
      "tensor(393.1663)\n",
      "tensor(9.5432)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.427917\n",
      "Epoch 8249\n",
      "-------------------------------\n",
      "tensor(54.4855)\n",
      "tensor(15.0637)\n",
      "tensor(22.3429)\n",
      "tensor(0.0954)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.496510\n",
      "Epoch 8250\n",
      "-------------------------------\n",
      "tensor(571.6915)\n",
      "tensor(178.7879)\n",
      "tensor(428.7889)\n",
      "tensor(10.8190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.632313\n",
      "Epoch 8251\n",
      "-------------------------------\n",
      "tensor(961.4511)\n",
      "tensor(336.9792)\n",
      "tensor(748.6506)\n",
      "tensor(19.3536)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 56.431416\n",
      "Epoch 8252\n",
      "-------------------------------\n",
      "tensor(1311.7065)\n",
      "tensor(438.0500)\n",
      "tensor(1008.5064)\n",
      "tensor(25.5345)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 74.086349\n",
      "Epoch 8253\n",
      "-------------------------------\n",
      "tensor(1564.8468)\n",
      "tensor(526.1179)\n",
      "tensor(1202.0779)\n",
      "tensor(30.3605)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 90.165428\n",
      "Epoch 8254\n",
      "-------------------------------\n",
      "tensor(1458.8722)\n",
      "tensor(500.2941)\n",
      "tensor(1127.1963)\n",
      "tensor(28.7434)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 83.375641\n",
      "Epoch 8255\n",
      "-------------------------------\n",
      "tensor(652.9963)\n",
      "tensor(210.9585)\n",
      "tensor(493.6645)\n",
      "tensor(12.1041)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.712288\n",
      "Epoch 8256\n",
      "-------------------------------\n",
      "tensor(554.8540)\n",
      "tensor(183.7166)\n",
      "tensor(425.7596)\n",
      "tensor(10.6029)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.320057\n",
      "Epoch 8257\n",
      "-------------------------------\n",
      "tensor(676.8848)\n",
      "tensor(218.6386)\n",
      "tensor(509.8819)\n",
      "tensor(12.8660)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.491932\n",
      "Epoch 8258\n",
      "-------------------------------\n",
      "tensor(447.9496)\n",
      "tensor(149.2200)\n",
      "tensor(337.6408)\n",
      "tensor(8.5710)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.821243\n",
      "Epoch 8259\n",
      "-------------------------------\n",
      "tensor(182.7056)\n",
      "tensor(61.9280)\n",
      "tensor(138.5408)\n",
      "tensor(3.2870)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.049953\n",
      "Epoch 8260\n",
      "-------------------------------\n",
      "tensor(423.0765)\n",
      "tensor(142.5440)\n",
      "tensor(322.5518)\n",
      "tensor(7.9077)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.281990\n",
      "Epoch 8261\n",
      "-------------------------------\n",
      "tensor(368.2203)\n",
      "tensor(125.0330)\n",
      "tensor(281.2536)\n",
      "tensor(6.9491)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.327316\n",
      "Epoch 8262\n",
      "-------------------------------\n",
      "tensor(187.9550)\n",
      "tensor(65.5053)\n",
      "tensor(143.5485)\n",
      "tensor(3.5975)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.093060\n",
      "Epoch 8263\n",
      "-------------------------------\n",
      "tensor(107.7324)\n",
      "tensor(32.2008)\n",
      "tensor(74.4601)\n",
      "tensor(1.7282)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.519428\n",
      "Epoch 8264\n",
      "-------------------------------\n",
      "tensor(426.3662)\n",
      "tensor(138.1920)\n",
      "tensor(318.7143)\n",
      "tensor(7.7069)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.252110\n",
      "Epoch 8265\n",
      "-------------------------------\n",
      "tensor(405.6389)\n",
      "tensor(133.4076)\n",
      "tensor(308.0030)\n",
      "tensor(7.4449)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.969379\n",
      "Epoch 8266\n",
      "-------------------------------\n",
      "tensor(249.7255)\n",
      "tensor(81.3060)\n",
      "tensor(188.4497)\n",
      "tensor(4.7648)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.691723\n",
      "Epoch 8267\n",
      "-------------------------------\n",
      "tensor(520.2281)\n",
      "tensor(165.4131)\n",
      "tensor(390.2192)\n",
      "tensor(9.4959)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.228004\n",
      "Epoch 8268\n",
      "-------------------------------\n",
      "tensor(539.5217)\n",
      "tensor(180.5885)\n",
      "tensor(414.5798)\n",
      "tensor(10.6272)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.735867\n",
      "Epoch 8269\n",
      "-------------------------------\n",
      "tensor(48.7565)\n",
      "tensor(12.6908)\n",
      "tensor(14.0074)\n",
      "tensor(0.2291)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.361988\n",
      "Epoch 8270\n",
      "-------------------------------\n",
      "tensor(518.9664)\n",
      "tensor(168.7995)\n",
      "tensor(393.5895)\n",
      "tensor(9.8332)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.265030\n",
      "Epoch 8271\n",
      "-------------------------------\n",
      "tensor(928.7783)\n",
      "tensor(306.2870)\n",
      "tensor(709.3898)\n",
      "tensor(17.8532)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.613266\n",
      "Epoch 8272\n",
      "-------------------------------\n",
      "tensor(1267.2571)\n",
      "tensor(429.3789)\n",
      "tensor(977.0123)\n",
      "tensor(24.6377)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 71.507599\n",
      "Epoch 8273\n",
      "-------------------------------\n",
      "tensor(1507.5817)\n",
      "tensor(505.2003)\n",
      "tensor(1162.2527)\n",
      "tensor(29.4319)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.924423\n",
      "Epoch 8274\n",
      "-------------------------------\n",
      "tensor(1374.3641)\n",
      "tensor(472.1999)\n",
      "tensor(1068.6542)\n",
      "tensor(27.5215)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.406509\n",
      "Epoch 8275\n",
      "-------------------------------\n",
      "tensor(565.8010)\n",
      "tensor(191.0739)\n",
      "tensor(433.8300)\n",
      "tensor(10.8634)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.696812\n",
      "Epoch 8276\n",
      "-------------------------------\n",
      "tensor(565.7472)\n",
      "tensor(199.9192)\n",
      "tensor(442.3057)\n",
      "tensor(11.6507)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.776417\n",
      "Epoch 8277\n",
      "-------------------------------\n",
      "tensor(623.3093)\n",
      "tensor(202.1528)\n",
      "tensor(472.7907)\n",
      "tensor(11.4889)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.071835\n",
      "Epoch 8278\n",
      "-------------------------------\n",
      "tensor(455.6230)\n",
      "tensor(151.6965)\n",
      "tensor(347.9301)\n",
      "tensor(8.7629)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 40.031178\n",
      "Epoch 8279\n",
      "-------------------------------\n",
      "tensor(148.4762)\n",
      "tensor(47.1952)\n",
      "tensor(108.3094)\n",
      "tensor(2.4207)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.859234\n",
      "Epoch 8280\n",
      "-------------------------------\n",
      "tensor(395.3219)\n",
      "tensor(129.1413)\n",
      "tensor(298.4156)\n",
      "tensor(7.1384)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.823776\n",
      "Epoch 8281\n",
      "-------------------------------\n",
      "tensor(358.6981)\n",
      "tensor(117.5689)\n",
      "tensor(271.9663)\n",
      "tensor(6.5283)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.226963\n",
      "Epoch 8282\n",
      "-------------------------------\n",
      "tensor(193.5534)\n",
      "tensor(63.5957)\n",
      "tensor(148.6980)\n",
      "tensor(3.5268)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.245941\n",
      "Epoch 8283\n",
      "-------------------------------\n",
      "tensor(76.7268)\n",
      "tensor(25.6356)\n",
      "tensor(55.4457)\n",
      "tensor(1.4704)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.535549\n",
      "Epoch 8284\n",
      "-------------------------------\n",
      "tensor(389.9273)\n",
      "tensor(128.3171)\n",
      "tensor(295.1961)\n",
      "tensor(7.3247)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.763683\n",
      "Epoch 8285\n",
      "-------------------------------\n",
      "tensor(403.7220)\n",
      "tensor(130.7775)\n",
      "tensor(303.9247)\n",
      "tensor(7.4185)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.966919\n",
      "Epoch 8286\n",
      "-------------------------------\n",
      "tensor(206.7956)\n",
      "tensor(74.1078)\n",
      "tensor(163.5058)\n",
      "tensor(4.2331)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.341225\n",
      "Epoch 8287\n",
      "-------------------------------\n",
      "tensor(513.9668)\n",
      "tensor(163.7551)\n",
      "tensor(388.2368)\n",
      "tensor(9.5967)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.060909\n",
      "Epoch 8288\n",
      "-------------------------------\n",
      "tensor(497.0014)\n",
      "tensor(163.6492)\n",
      "tensor(379.9124)\n",
      "tensor(9.6491)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.805553\n",
      "Epoch 8289\n",
      "-------------------------------\n",
      "tensor(58.8189)\n",
      "tensor(15.6005)\n",
      "tensor(21.2833)\n",
      "tensor(0.2994)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.482422\n",
      "Epoch 8290\n",
      "-------------------------------\n",
      "tensor(541.1530)\n",
      "tensor(181.4213)\n",
      "tensor(412.8062)\n",
      "tensor(10.9471)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 42.103931\n",
      "Epoch 8291\n",
      "-------------------------------\n",
      "tensor(917.5560)\n",
      "tensor(306.8471)\n",
      "tensor(706.4366)\n",
      "tensor(17.5213)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.378475\n",
      "Epoch 8292\n",
      "-------------------------------\n",
      "tensor(1227.3352)\n",
      "tensor(404.1462)\n",
      "tensor(936.0032)\n",
      "tensor(23.5044)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.121048\n",
      "Epoch 8293\n",
      "-------------------------------\n",
      "tensor(1422.3873)\n",
      "tensor(489.6651)\n",
      "tensor(1106.3228)\n",
      "tensor(28.1034)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 81.528122\n",
      "Epoch 8294\n",
      "-------------------------------\n",
      "tensor(1294.7301)\n",
      "tensor(430.8595)\n",
      "tensor(996.4070)\n",
      "tensor(25.4658)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 73.257736\n",
      "Epoch 8295\n",
      "-------------------------------\n",
      "tensor(520.1031)\n",
      "tensor(180.1957)\n",
      "tensor(402.7589)\n",
      "tensor(10.3582)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.762676\n",
      "Epoch 8296\n",
      "-------------------------------\n",
      "tensor(537.5784)\n",
      "tensor(184.0622)\n",
      "tensor(415.6445)\n",
      "tensor(10.8729)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.172760\n",
      "Epoch 8297\n",
      "-------------------------------\n",
      "tensor(573.2026)\n",
      "tensor(196.5054)\n",
      "tensor(443.3802)\n",
      "tensor(10.9736)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.063408\n",
      "Epoch 8298\n",
      "-------------------------------\n",
      "tensor(422.1518)\n",
      "tensor(151.1760)\n",
      "tensor(331.5042)\n",
      "tensor(8.4749)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.697559\n",
      "Epoch 8299\n",
      "-------------------------------\n",
      "tensor(139.4512)\n",
      "tensor(36.1701)\n",
      "tensor(95.2325)\n",
      "tensor(2.1212)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.991352\n",
      "Epoch 8300\n",
      "-------------------------------\n",
      "tensor(372.7269)\n",
      "tensor(115.0291)\n",
      "tensor(275.8872)\n",
      "tensor(6.6872)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.587112\n",
      "Epoch 8301\n",
      "-------------------------------\n",
      "tensor(340.7345)\n",
      "tensor(106.0417)\n",
      "tensor(252.7665)\n",
      "tensor(6.1966)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.093025\n",
      "Epoch 8302\n",
      "-------------------------------\n",
      "tensor(189.8170)\n",
      "tensor(57.4524)\n",
      "tensor(138.2082)\n",
      "tensor(3.4299)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 36.336716\n",
      "Epoch 8303\n",
      "-------------------------------\n",
      "tensor(66.9749)\n",
      "tensor(26.1985)\n",
      "tensor(53.3336)\n",
      "tensor(1.2277)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.657368\n",
      "Epoch 8304\n",
      "-------------------------------\n",
      "tensor(364.3823)\n",
      "tensor(122.2005)\n",
      "tensor(279.7411)\n",
      "tensor(6.7212)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.485294\n",
      "Epoch 8305\n",
      "-------------------------------\n",
      "tensor(379.5523)\n",
      "tensor(123.9358)\n",
      "tensor(290.1356)\n",
      "tensor(6.8501)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.690269\n",
      "Epoch 8306\n",
      "-------------------------------\n",
      "tensor(191.4845)\n",
      "tensor(69.2608)\n",
      "tensor(149.1196)\n",
      "tensor(4.0945)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.265800\n",
      "Epoch 8307\n",
      "-------------------------------\n",
      "tensor(479.2842)\n",
      "tensor(162.4997)\n",
      "tensor(367.9944)\n",
      "tensor(9.2123)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.530655\n",
      "Epoch 8308\n",
      "-------------------------------\n",
      "tensor(460.0069)\n",
      "tensor(145.7918)\n",
      "tensor(347.1536)\n",
      "tensor(9.0394)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.965015\n",
      "Epoch 8309\n",
      "-------------------------------\n",
      "tensor(61.3107)\n",
      "tensor(22.1020)\n",
      "tensor(23.1796)\n",
      "tensor(0.7342)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.537407\n",
      "Epoch 8310\n",
      "-------------------------------\n",
      "tensor(504.3454)\n",
      "tensor(168.7959)\n",
      "tensor(390.6919)\n",
      "tensor(9.8105)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.295383\n",
      "Epoch 8311\n",
      "-------------------------------\n",
      "tensor(844.9058)\n",
      "tensor(283.1706)\n",
      "tensor(648.9698)\n",
      "tensor(16.7893)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.575882\n",
      "Epoch 8312\n",
      "-------------------------------\n",
      "tensor(1121.2374)\n",
      "tensor(378.7297)\n",
      "tensor(863.7556)\n",
      "tensor(21.7621)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.649231\n",
      "Epoch 8313\n",
      "-------------------------------\n",
      "tensor(1320.5201)\n",
      "tensor(440.3553)\n",
      "tensor(1011.4506)\n",
      "tensor(25.4359)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.489204\n",
      "Epoch 8314\n",
      "-------------------------------\n",
      "tensor(1207.3391)\n",
      "tensor(408.8101)\n",
      "tensor(928.8198)\n",
      "tensor(23.3646)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.097694\n",
      "Epoch 8315\n",
      "-------------------------------\n",
      "tensor(517.9289)\n",
      "tensor(162.0246)\n",
      "tensor(390.0005)\n",
      "tensor(9.7849)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.304276\n",
      "Epoch 8316\n",
      "-------------------------------\n",
      "tensor(472.2161)\n",
      "tensor(161.4620)\n",
      "tensor(365.7967)\n",
      "tensor(9.2730)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.562618\n",
      "Epoch 8317\n",
      "-------------------------------\n",
      "tensor(541.1161)\n",
      "tensor(177.6747)\n",
      "tensor(412.9521)\n",
      "tensor(10.3502)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.979717\n",
      "Epoch 8318\n",
      "-------------------------------\n",
      "tensor(385.6364)\n",
      "tensor(124.7027)\n",
      "tensor(291.1132)\n",
      "tensor(7.3301)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.678696\n",
      "Epoch 8319\n",
      "-------------------------------\n",
      "tensor(129.1632)\n",
      "tensor(48.6539)\n",
      "tensor(103.5378)\n",
      "tensor(2.5362)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.787941\n",
      "Epoch 8320\n",
      "-------------------------------\n",
      "tensor(342.1441)\n",
      "tensor(118.9447)\n",
      "tensor(262.8938)\n",
      "tensor(6.5177)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.005642\n",
      "Epoch 8321\n",
      "-------------------------------\n",
      "tensor(308.4435)\n",
      "tensor(107.4182)\n",
      "tensor(235.4709)\n",
      "tensor(5.8527)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.488934\n",
      "Epoch 8322\n",
      "-------------------------------\n",
      "tensor(165.6917)\n",
      "tensor(59.7773)\n",
      "tensor(126.0508)\n",
      "tensor(3.1524)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.969669\n",
      "Epoch 8323\n",
      "-------------------------------\n",
      "tensor(77.5976)\n",
      "tensor(21.8661)\n",
      "tensor(52.8135)\n",
      "tensor(1.2756)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.503189\n",
      "Epoch 8324\n",
      "-------------------------------\n",
      "tensor(344.0733)\n",
      "tensor(111.9596)\n",
      "tensor(259.1644)\n",
      "tensor(6.4234)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.981880\n",
      "Epoch 8325\n",
      "-------------------------------\n",
      "tensor(343.4859)\n",
      "tensor(114.1999)\n",
      "tensor(260.5479)\n",
      "tensor(6.5215)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.979919\n",
      "Epoch 8326\n",
      "-------------------------------\n",
      "tensor(195.7912)\n",
      "tensor(62.3996)\n",
      "tensor(146.8050)\n",
      "tensor(3.4959)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.200485\n",
      "Epoch 8327\n",
      "-------------------------------\n",
      "tensor(437.3845)\n",
      "tensor(140.4178)\n",
      "tensor(329.7170)\n",
      "tensor(7.9444)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.512424\n",
      "Epoch 8328\n",
      "-------------------------------\n",
      "tensor(431.9126)\n",
      "tensor(148.7259)\n",
      "tensor(332.5272)\n",
      "tensor(8.4680)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.460861\n",
      "Epoch 8329\n",
      "-------------------------------\n",
      "tensor(50.8321)\n",
      "tensor(15.9365)\n",
      "tensor(9.6484)\n",
      "tensor(0.2092)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.451168\n",
      "Epoch 8330\n",
      "-------------------------------\n",
      "tensor(453.1603)\n",
      "tensor(147.9679)\n",
      "tensor(341.9120)\n",
      "tensor(8.9782)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.021381\n",
      "Epoch 8331\n",
      "-------------------------------\n",
      "tensor(786.0151)\n",
      "tensor(256.8576)\n",
      "tensor(599.5408)\n",
      "tensor(14.7207)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.229568\n",
      "Epoch 8332\n",
      "-------------------------------\n",
      "tensor(1056.5563)\n",
      "tensor(354.5016)\n",
      "tensor(813.6019)\n",
      "tensor(20.5722)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.653465\n",
      "Epoch 8333\n",
      "-------------------------------\n",
      "tensor(1248.2523)\n",
      "tensor(424.5889)\n",
      "tensor(964.3015)\n",
      "tensor(24.7791)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.391663\n",
      "Epoch 8334\n",
      "-------------------------------\n",
      "tensor(1151.2937)\n",
      "tensor(388.0461)\n",
      "tensor(888.1254)\n",
      "tensor(22.5341)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.388054\n",
      "Epoch 8335\n",
      "-------------------------------\n",
      "tensor(488.9753)\n",
      "tensor(158.6902)\n",
      "tensor(371.6317)\n",
      "tensor(9.1042)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.738068\n",
      "Epoch 8336\n",
      "-------------------------------\n",
      "tensor(468.7426)\n",
      "tensor(153.1387)\n",
      "tensor(355.7717)\n",
      "tensor(8.7345)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.221588\n",
      "Epoch 8337\n",
      "-------------------------------\n",
      "tensor(525.1725)\n",
      "tensor(176.2998)\n",
      "tensor(400.3929)\n",
      "tensor(9.9999)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.534790\n",
      "Epoch 8338\n",
      "-------------------------------\n",
      "tensor(371.2346)\n",
      "tensor(123.1899)\n",
      "tensor(282.5080)\n",
      "tensor(6.9925)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.454288\n",
      "Epoch 8339\n",
      "-------------------------------\n",
      "tensor(130.0372)\n",
      "tensor(45.1935)\n",
      "tensor(98.4618)\n",
      "tensor(2.5367)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.740921\n",
      "Epoch 8340\n",
      "-------------------------------\n",
      "tensor(330.6185)\n",
      "tensor(112.1762)\n",
      "tensor(253.0995)\n",
      "tensor(6.3804)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.806396\n",
      "Epoch 8341\n",
      "-------------------------------\n",
      "tensor(296.6069)\n",
      "tensor(100.5344)\n",
      "tensor(226.7165)\n",
      "tensor(5.7109)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.324844\n",
      "Epoch 8342\n",
      "-------------------------------\n",
      "tensor(159.3945)\n",
      "tensor(54.2840)\n",
      "tensor(120.7580)\n",
      "tensor(3.0594)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.919254\n",
      "Epoch 8343\n",
      "-------------------------------\n",
      "tensor(68.2768)\n",
      "tensor(22.3036)\n",
      "tensor(52.0416)\n",
      "tensor(1.2550)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.448895\n",
      "Epoch 8344\n",
      "-------------------------------\n",
      "tensor(330.8091)\n",
      "tensor(110.6843)\n",
      "tensor(252.4049)\n",
      "tensor(6.2443)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.762890\n",
      "Epoch 8345\n",
      "-------------------------------\n",
      "tensor(333.0837)\n",
      "tensor(112.4736)\n",
      "tensor(255.1570)\n",
      "tensor(6.3156)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.801781\n",
      "Epoch 8346\n",
      "-------------------------------\n",
      "tensor(186.3107)\n",
      "tensor(59.4038)\n",
      "tensor(139.1920)\n",
      "tensor(3.4862)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.033688\n",
      "Epoch 8347\n",
      "-------------------------------\n",
      "tensor(426.2278)\n",
      "tensor(137.2823)\n",
      "tensor(322.1223)\n",
      "tensor(7.9435)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.263912\n",
      "Epoch 8348\n",
      "-------------------------------\n",
      "tensor(426.8773)\n",
      "tensor(138.1141)\n",
      "tensor(318.0660)\n",
      "tensor(8.1197)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.283455\n",
      "Epoch 8349\n",
      "-------------------------------\n",
      "tensor(52.9175)\n",
      "tensor(20.3079)\n",
      "tensor(8.6762)\n",
      "tensor(0.6289)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.484653\n",
      "Epoch 8350\n",
      "-------------------------------\n",
      "tensor(427.2211)\n",
      "tensor(140.6762)\n",
      "tensor(326.7852)\n",
      "tensor(7.8687)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.407646\n",
      "Epoch 8351\n",
      "-------------------------------\n",
      "tensor(740.9274)\n",
      "tensor(232.9096)\n",
      "tensor(559.9576)\n",
      "tensor(13.8545)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.415138\n",
      "Epoch 8352\n",
      "-------------------------------\n",
      "tensor(984.0842)\n",
      "tensor(344.5391)\n",
      "tensor(766.6920)\n",
      "tensor(19.9816)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.417515\n",
      "Epoch 8353\n",
      "-------------------------------\n",
      "tensor(1183.8784)\n",
      "tensor(395.8424)\n",
      "tensor(909.4598)\n",
      "tensor(22.9539)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.755119\n",
      "Epoch 8354\n",
      "-------------------------------\n",
      "tensor(1111.8411)\n",
      "tensor(360.9198)\n",
      "tensor(843.2311)\n",
      "tensor(20.5705)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.842789\n",
      "Epoch 8355\n",
      "-------------------------------\n",
      "tensor(478.7108)\n",
      "tensor(167.7266)\n",
      "tensor(373.1653)\n",
      "tensor(9.4369)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.715488\n",
      "Epoch 8356\n",
      "-------------------------------\n",
      "tensor(429.4739)\n",
      "tensor(143.5244)\n",
      "tensor(327.0926)\n",
      "tensor(7.7618)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.602276\n",
      "Epoch 8357\n",
      "-------------------------------\n",
      "tensor(508.1538)\n",
      "tensor(171.2867)\n",
      "tensor(389.4102)\n",
      "tensor(10.1115)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.232048\n",
      "Epoch 8358\n",
      "-------------------------------\n",
      "tensor(344.3390)\n",
      "tensor(113.9290)\n",
      "tensor(260.7767)\n",
      "tensor(6.6533)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.070473\n",
      "Epoch 8359\n",
      "-------------------------------\n",
      "tensor(138.4008)\n",
      "tensor(48.8413)\n",
      "tensor(104.4725)\n",
      "tensor(2.6309)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.882500\n",
      "Epoch 8360\n",
      "-------------------------------\n",
      "tensor(317.1243)\n",
      "tensor(109.4437)\n",
      "tensor(245.2057)\n",
      "tensor(6.2116)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.711735\n",
      "Epoch 8361\n",
      "-------------------------------\n",
      "tensor(278.9579)\n",
      "tensor(96.2729)\n",
      "tensor(215.2076)\n",
      "tensor(5.4582)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.160156\n",
      "Epoch 8362\n",
      "-------------------------------\n",
      "tensor(139.9978)\n",
      "tensor(49.3925)\n",
      "tensor(111.4481)\n",
      "tensor(2.8312)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.867874\n",
      "Epoch 8363\n",
      "-------------------------------\n",
      "tensor(76.7086)\n",
      "tensor(24.6008)\n",
      "tensor(53.6982)\n",
      "tensor(1.3852)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.529636\n",
      "Epoch 8364\n",
      "-------------------------------\n",
      "tensor(315.6009)\n",
      "tensor(106.9017)\n",
      "tensor(240.8635)\n",
      "tensor(6.2290)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.611198\n",
      "Epoch 8365\n",
      "-------------------------------\n",
      "tensor(306.2835)\n",
      "tensor(106.2327)\n",
      "tensor(236.7263)\n",
      "tensor(6.3132)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.494850\n",
      "Epoch 8366\n",
      "-------------------------------\n",
      "tensor(184.7540)\n",
      "tensor(59.2342)\n",
      "tensor(139.3534)\n",
      "tensor(3.0880)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.079060\n",
      "Epoch 8367\n",
      "-------------------------------\n",
      "tensor(390.4702)\n",
      "tensor(133.0146)\n",
      "tensor(299.7555)\n",
      "tensor(7.4269)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.786434\n",
      "Epoch 8368\n",
      "-------------------------------\n",
      "tensor(402.1945)\n",
      "tensor(127.3619)\n",
      "tensor(303.5058)\n",
      "tensor(7.1873)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.993015\n",
      "Epoch 8369\n",
      "-------------------------------\n",
      "tensor(51.9047)\n",
      "tensor(19.2720)\n",
      "tensor(11.5658)\n",
      "tensor(0.3285)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.484905\n",
      "Epoch 8370\n",
      "-------------------------------\n",
      "tensor(432.0170)\n",
      "tensor(131.4062)\n",
      "tensor(315.9877)\n",
      "tensor(7.9775)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.344379\n",
      "Epoch 8371\n",
      "-------------------------------\n",
      "tensor(733.3032)\n",
      "tensor(255.9486)\n",
      "tensor(572.0008)\n",
      "tensor(14.6406)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.602264\n",
      "Epoch 8372\n",
      "-------------------------------\n",
      "tensor(1005.3460)\n",
      "tensor(333.4493)\n",
      "tensor(772.6149)\n",
      "tensor(19.7128)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.173737\n",
      "Epoch 8373\n",
      "-------------------------------\n",
      "tensor(1195.1162)\n",
      "tensor(404.5742)\n",
      "tensor(921.6567)\n",
      "tensor(23.4044)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.577789\n",
      "Epoch 8374\n",
      "-------------------------------\n",
      "tensor(1115.3843)\n",
      "tensor(377.9011)\n",
      "tensor(859.0082)\n",
      "tensor(21.7579)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.456909\n",
      "Epoch 8375\n",
      "-------------------------------\n",
      "tensor(498.5013)\n",
      "tensor(157.1484)\n",
      "tensor(374.3546)\n",
      "tensor(9.0138)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.002529\n",
      "Epoch 8376\n",
      "-------------------------------\n",
      "tensor(433.1609)\n",
      "tensor(137.6508)\n",
      "tensor(324.5388)\n",
      "tensor(7.8429)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.639519\n",
      "Epoch 8377\n",
      "-------------------------------\n",
      "tensor(513.9791)\n",
      "tensor(173.4984)\n",
      "tensor(397.1825)\n",
      "tensor(9.8566)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.466721\n",
      "Epoch 8378\n",
      "-------------------------------\n",
      "tensor(345.7963)\n",
      "tensor(112.9543)\n",
      "tensor(263.3110)\n",
      "tensor(6.2800)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.155739\n",
      "Epoch 8379\n",
      "-------------------------------\n",
      "tensor(137.6063)\n",
      "tensor(49.8737)\n",
      "tensor(106.3763)\n",
      "tensor(2.9664)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.915154\n",
      "Epoch 8380\n",
      "-------------------------------\n",
      "tensor(321.9663)\n",
      "tensor(110.4455)\n",
      "tensor(247.4431)\n",
      "tensor(6.4139)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.791710\n",
      "Epoch 8381\n",
      "-------------------------------\n",
      "tensor(281.4889)\n",
      "tensor(95.6218)\n",
      "tensor(215.5914)\n",
      "tensor(5.5324)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.231377\n",
      "Epoch 8382\n",
      "-------------------------------\n",
      "tensor(146.6675)\n",
      "tensor(49.0554)\n",
      "tensor(109.9092)\n",
      "tensor(2.8066)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.916737\n",
      "Epoch 8383\n",
      "-------------------------------\n",
      "tensor(75.7112)\n",
      "tensor(26.7358)\n",
      "tensor(57.5913)\n",
      "tensor(1.4759)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.547531\n",
      "Epoch 8384\n",
      "-------------------------------\n",
      "tensor(318.8457)\n",
      "tensor(109.9989)\n",
      "tensor(246.9814)\n",
      "tensor(6.3067)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.690277\n",
      "Epoch 8385\n",
      "-------------------------------\n",
      "tensor(312.4120)\n",
      "tensor(108.9958)\n",
      "tensor(241.0878)\n",
      "tensor(6.2045)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.577663\n",
      "Epoch 8386\n",
      "-------------------------------\n",
      "tensor(191.4533)\n",
      "tensor(60.4535)\n",
      "tensor(142.0103)\n",
      "tensor(3.4305)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.122459\n",
      "Epoch 8387\n",
      "-------------------------------\n",
      "tensor(399.2213)\n",
      "tensor(133.0758)\n",
      "tensor(305.9700)\n",
      "tensor(7.6998)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.875397\n",
      "Epoch 8388\n",
      "-------------------------------\n",
      "tensor(417.7315)\n",
      "tensor(129.2350)\n",
      "tensor(308.6501)\n",
      "tensor(7.6565)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.002636\n",
      "Epoch 8389\n",
      "-------------------------------\n",
      "tensor(50.0158)\n",
      "tensor(15.6521)\n",
      "tensor(9.4831)\n",
      "tensor(0.0851)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.483551\n",
      "Epoch 8390\n",
      "-------------------------------\n",
      "tensor(396.6722)\n",
      "tensor(128.9169)\n",
      "tensor(301.4082)\n",
      "tensor(7.1935)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.942860\n",
      "Epoch 8391\n",
      "-------------------------------\n",
      "tensor(704.6654)\n",
      "tensor(225.8114)\n",
      "tensor(533.9760)\n",
      "tensor(13.3696)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.237358\n",
      "Epoch 8392\n",
      "-------------------------------\n",
      "tensor(959.5164)\n",
      "tensor(328.8639)\n",
      "tensor(741.7776)\n",
      "tensor(19.0515)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.139652\n",
      "Epoch 8393\n",
      "-------------------------------\n",
      "tensor(1161.3912)\n",
      "tensor(388.4201)\n",
      "tensor(894.2721)\n",
      "tensor(22.4578)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 65.714706\n",
      "Epoch 8394\n",
      "-------------------------------\n",
      "tensor(1097.2419)\n",
      "tensor(360.7029)\n",
      "tensor(836.1016)\n",
      "tensor(20.8730)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.158897\n",
      "Epoch 8395\n",
      "-------------------------------\n",
      "tensor(480.1678)\n",
      "tensor(169.7349)\n",
      "tensor(374.8533)\n",
      "tensor(9.4476)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.874264\n",
      "Epoch 8396\n",
      "-------------------------------\n",
      "tensor(413.8728)\n",
      "tensor(145.8117)\n",
      "tensor(322.2614)\n",
      "tensor(8.1028)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.510174\n",
      "Epoch 8397\n",
      "-------------------------------\n",
      "tensor(502.5796)\n",
      "tensor(164.5878)\n",
      "tensor(384.0022)\n",
      "tensor(9.6057)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.143787\n",
      "Epoch 8398\n",
      "-------------------------------\n",
      "tensor(343.2051)\n",
      "tensor(112.5966)\n",
      "tensor(259.2849)\n",
      "tensor(6.4382)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.992622\n",
      "Epoch 8399\n",
      "-------------------------------\n",
      "tensor(130.3960)\n",
      "tensor(44.0173)\n",
      "tensor(101.8442)\n",
      "tensor(2.5407)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.772774\n",
      "Epoch 8400\n",
      "-------------------------------\n",
      "tensor(314.2440)\n",
      "tensor(104.0934)\n",
      "tensor(242.3882)\n",
      "tensor(5.9749)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.589817\n",
      "Epoch 8401\n",
      "-------------------------------\n",
      "tensor(281.6070)\n",
      "tensor(91.8690)\n",
      "tensor(213.1331)\n",
      "tensor(5.1781)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.095695\n",
      "Epoch 8402\n",
      "-------------------------------\n",
      "tensor(148.1229)\n",
      "tensor(46.8484)\n",
      "tensor(110.3297)\n",
      "tensor(2.5728)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.829731\n",
      "Epoch 8403\n",
      "-------------------------------\n",
      "tensor(71.9034)\n",
      "tensor(26.9517)\n",
      "tensor(54.0078)\n",
      "tensor(1.5516)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.464005\n",
      "Epoch 8404\n",
      "-------------------------------\n",
      "tensor(313.5575)\n",
      "tensor(107.5915)\n",
      "tensor(240.7743)\n",
      "tensor(6.2201)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.535362\n",
      "Epoch 8405\n",
      "-------------------------------\n",
      "tensor(308.6540)\n",
      "tensor(104.2214)\n",
      "tensor(236.8107)\n",
      "tensor(6.0618)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.452507\n",
      "Epoch 8406\n",
      "-------------------------------\n",
      "tensor(182.9899)\n",
      "tensor(64.4847)\n",
      "tensor(137.4311)\n",
      "tensor(3.4853)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.027676\n",
      "Epoch 8407\n",
      "-------------------------------\n",
      "tensor(388.6867)\n",
      "tensor(137.1641)\n",
      "tensor(300.5121)\n",
      "tensor(7.8057)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.652100\n",
      "Epoch 8408\n",
      "-------------------------------\n",
      "tensor(401.5956)\n",
      "tensor(128.5032)\n",
      "tensor(302.8508)\n",
      "tensor(7.2286)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.787956\n",
      "Epoch 8409\n",
      "-------------------------------\n",
      "tensor(48.9186)\n",
      "tensor(15.5124)\n",
      "tensor(7.5880)\n",
      "tensor(0.2695)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.435356\n",
      "Epoch 8410\n",
      "-------------------------------\n",
      "tensor(423.4006)\n",
      "tensor(126.7121)\n",
      "tensor(310.3147)\n",
      "tensor(7.6720)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.190830\n",
      "Epoch 8411\n",
      "-------------------------------\n",
      "tensor(729.9307)\n",
      "tensor(245.5599)\n",
      "tensor(559.6891)\n",
      "tensor(13.8128)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.322891\n",
      "Epoch 8412\n",
      "-------------------------------\n",
      "tensor(1003.2355)\n",
      "tensor(333.5250)\n",
      "tensor(769.7950)\n",
      "tensor(19.5456)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.839035\n",
      "Epoch 8413\n",
      "-------------------------------\n",
      "tensor(1205.7142)\n",
      "tensor(408.6179)\n",
      "tensor(929.2484)\n",
      "tensor(23.6781)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.004227\n",
      "Epoch 8414\n",
      "-------------------------------\n",
      "tensor(1136.1239)\n",
      "tensor(380.6816)\n",
      "tensor(875.0267)\n",
      "tensor(21.9293)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.431213\n",
      "Epoch 8415\n",
      "-------------------------------\n",
      "tensor(509.2441)\n",
      "tensor(164.0868)\n",
      "tensor(385.6112)\n",
      "tensor(9.6220)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.104622\n",
      "Epoch 8416\n",
      "-------------------------------\n",
      "tensor(439.1683)\n",
      "tensor(138.7048)\n",
      "tensor(330.3469)\n",
      "tensor(8.0889)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.652699\n",
      "Epoch 8417\n",
      "-------------------------------\n",
      "tensor(519.0287)\n",
      "tensor(180.9890)\n",
      "tensor(403.8326)\n",
      "tensor(10.2678)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.548077\n",
      "Epoch 8418\n",
      "-------------------------------\n",
      "tensor(350.0281)\n",
      "tensor(120.3079)\n",
      "tensor(270.1920)\n",
      "tensor(6.7783)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.124054\n",
      "Epoch 8419\n",
      "-------------------------------\n",
      "tensor(142.2873)\n",
      "tensor(47.8905)\n",
      "tensor(106.4648)\n",
      "tensor(2.7704)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.820431\n",
      "Epoch 8420\n",
      "-------------------------------\n",
      "tensor(327.5674)\n",
      "tensor(111.7463)\n",
      "tensor(251.3407)\n",
      "tensor(6.4677)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.767227\n",
      "Epoch 8421\n",
      "-------------------------------\n",
      "tensor(286.2816)\n",
      "tensor(98.5231)\n",
      "tensor(219.9955)\n",
      "tensor(5.7077)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.187012\n",
      "Epoch 8422\n",
      "-------------------------------\n",
      "tensor(148.2678)\n",
      "tensor(52.3242)\n",
      "tensor(113.1271)\n",
      "tensor(3.0330)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.824913\n",
      "Epoch 8423\n",
      "-------------------------------\n",
      "tensor(75.7117)\n",
      "tensor(23.5837)\n",
      "tensor(57.1560)\n",
      "tensor(1.2399)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.446556\n",
      "Epoch 8424\n",
      "-------------------------------\n",
      "tensor(325.1579)\n",
      "tensor(107.8780)\n",
      "tensor(250.7411)\n",
      "tensor(6.1193)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.668152\n",
      "Epoch 8425\n",
      "-------------------------------\n",
      "tensor(326.4514)\n",
      "tensor(109.4651)\n",
      "tensor(246.4185)\n",
      "tensor(6.0676)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.585392\n",
      "Epoch 8426\n",
      "-------------------------------\n",
      "tensor(195.6892)\n",
      "tensor(62.1602)\n",
      "tensor(145.9942)\n",
      "tensor(3.5595)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.090973\n",
      "Epoch 8427\n",
      "-------------------------------\n",
      "tensor(421.0445)\n",
      "tensor(134.3105)\n",
      "tensor(315.7395)\n",
      "tensor(7.5793)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.107170\n",
      "Epoch 8428\n",
      "-------------------------------\n",
      "tensor(438.6139)\n",
      "tensor(139.1713)\n",
      "tensor(324.7212)\n",
      "tensor(8.1810)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.391117\n",
      "Epoch 8429\n",
      "-------------------------------\n",
      "tensor(48.5473)\n",
      "tensor(16.2355)\n",
      "tensor(12.5447)\n",
      "tensor(0.1277)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.389771\n",
      "Epoch 8430\n",
      "-------------------------------\n",
      "tensor(411.1787)\n",
      "tensor(135.1696)\n",
      "tensor(312.2170)\n",
      "tensor(7.6422)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.071430\n",
      "Epoch 8431\n",
      "-------------------------------\n",
      "tensor(741.7991)\n",
      "tensor(239.3512)\n",
      "tensor(564.6978)\n",
      "tensor(14.0702)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.476906\n",
      "Epoch 8432\n",
      "-------------------------------\n",
      "tensor(1018.0166)\n",
      "tensor(346.1092)\n",
      "tensor(785.6979)\n",
      "tensor(20.0545)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.625095\n",
      "Epoch 8433\n",
      "-------------------------------\n",
      "tensor(1231.8163)\n",
      "tensor(417.6634)\n",
      "tensor(952.0804)\n",
      "tensor(24.2270)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.496796\n",
      "Epoch 8434\n",
      "-------------------------------\n",
      "tensor(1170.0927)\n",
      "tensor(387.2277)\n",
      "tensor(894.4589)\n",
      "tensor(22.3162)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.767921\n",
      "Epoch 8435\n",
      "-------------------------------\n",
      "tensor(526.4255)\n",
      "tensor(179.6092)\n",
      "tensor(404.4455)\n",
      "tensor(10.0506)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.520317\n",
      "Epoch 8436\n",
      "-------------------------------\n",
      "tensor(438.2970)\n",
      "tensor(149.6393)\n",
      "tensor(333.2737)\n",
      "tensor(8.3798)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.558926\n",
      "Epoch 8437\n",
      "-------------------------------\n",
      "tensor(548.4478)\n",
      "tensor(180.2371)\n",
      "tensor(417.0494)\n",
      "tensor(10.1884)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.931107\n",
      "Epoch 8438\n",
      "-------------------------------\n",
      "tensor(353.3123)\n",
      "tensor(117.3474)\n",
      "tensor(268.7430)\n",
      "tensor(6.5671)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.036926\n",
      "Epoch 8439\n",
      "-------------------------------\n",
      "tensor(158.5609)\n",
      "tensor(51.0038)\n",
      "tensor(115.9871)\n",
      "tensor(2.8806)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.819618\n",
      "Epoch 8440\n",
      "-------------------------------\n",
      "tensor(342.4474)\n",
      "tensor(111.3935)\n",
      "tensor(258.6650)\n",
      "tensor(6.3521)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.854202\n",
      "Epoch 8441\n",
      "-------------------------------\n",
      "tensor(296.5715)\n",
      "tensor(95.5531)\n",
      "tensor(222.5714)\n",
      "tensor(5.4124)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.164722\n",
      "Epoch 8442\n",
      "-------------------------------\n",
      "tensor(145.9933)\n",
      "tensor(45.6433)\n",
      "tensor(110.6514)\n",
      "tensor(2.6110)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.722797\n",
      "Epoch 8443\n",
      "-------------------------------\n",
      "tensor(84.1104)\n",
      "tensor(31.0951)\n",
      "tensor(64.0309)\n",
      "tensor(1.7498)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.411369\n",
      "Epoch 8444\n",
      "-------------------------------\n",
      "tensor(336.2729)\n",
      "tensor(114.3279)\n",
      "tensor(257.9376)\n",
      "tensor(6.5729)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.774883\n",
      "Epoch 8445\n",
      "-------------------------------\n",
      "tensor(320.2115)\n",
      "tensor(106.6115)\n",
      "tensor(244.9780)\n",
      "tensor(6.1870)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.535416\n",
      "Epoch 8446\n",
      "-------------------------------\n",
      "tensor(203.2038)\n",
      "tensor(72.5294)\n",
      "tensor(152.6339)\n",
      "tensor(3.9072)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.266457\n",
      "Epoch 8447\n",
      "-------------------------------\n",
      "tensor(401.5913)\n",
      "tensor(143.5291)\n",
      "tensor(313.1041)\n",
      "tensor(8.1566)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.036823\n",
      "Epoch 8448\n",
      "-------------------------------\n",
      "tensor(426.5279)\n",
      "tensor(136.2171)\n",
      "tensor(322.0295)\n",
      "tensor(7.7082)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.284325\n",
      "Epoch 8449\n",
      "-------------------------------\n",
      "tensor(25.7151)\n",
      "tensor(8.1794)\n",
      "tensor(1.0930)\n",
      "tensor(0.0397)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.271912\n",
      "Epoch 8450\n",
      "-------------------------------\n",
      "tensor(442.1160)\n",
      "tensor(130.0717)\n",
      "tensor(324.6535)\n",
      "tensor(7.7266)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.545368\n",
      "Epoch 8451\n",
      "-------------------------------\n",
      "tensor(762.9996)\n",
      "tensor(258.1318)\n",
      "tensor(587.3547)\n",
      "tensor(14.7879)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.389122\n",
      "Epoch 8452\n",
      "-------------------------------\n",
      "tensor(1043.9543)\n",
      "tensor(354.6279)\n",
      "tensor(806.1371)\n",
      "tensor(20.7600)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.937290\n",
      "Epoch 8453\n",
      "-------------------------------\n",
      "tensor(1260.6622)\n",
      "tensor(419.2451)\n",
      "tensor(964.6040)\n",
      "tensor(24.1030)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.709496\n",
      "Epoch 8454\n",
      "-------------------------------\n",
      "tensor(1177.6119)\n",
      "tensor(390.8096)\n",
      "tensor(903.3019)\n",
      "tensor(22.3566)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.747406\n",
      "Epoch 8455\n",
      "-------------------------------\n",
      "tensor(504.9386)\n",
      "tensor(170.6356)\n",
      "tensor(391.6921)\n",
      "tensor(10.1912)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.324329\n",
      "Epoch 8456\n",
      "-------------------------------\n",
      "tensor(471.0155)\n",
      "tensor(150.9341)\n",
      "tensor(357.6363)\n",
      "tensor(8.6205)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.396702\n",
      "Epoch 8457\n",
      "-------------------------------\n",
      "tensor(525.4845)\n",
      "tensor(185.6450)\n",
      "tensor(412.5255)\n",
      "tensor(10.9399)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.740116\n",
      "Epoch 8458\n",
      "-------------------------------\n",
      "tensor(372.0997)\n",
      "tensor(127.5402)\n",
      "tensor(289.6003)\n",
      "tensor(7.4896)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.508389\n",
      "Epoch 8459\n",
      "-------------------------------\n",
      "tensor(132.4900)\n",
      "tensor(48.2813)\n",
      "tensor(101.3977)\n",
      "tensor(2.7471)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.790634\n",
      "Epoch 8460\n",
      "-------------------------------\n",
      "tensor(331.6294)\n",
      "tensor(118.5569)\n",
      "tensor(258.6343)\n",
      "tensor(6.9376)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.905640\n",
      "Epoch 8461\n",
      "-------------------------------\n",
      "tensor(295.3755)\n",
      "tensor(107.1818)\n",
      "tensor(231.5833)\n",
      "tensor(6.3394)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.395782\n",
      "Epoch 8462\n",
      "-------------------------------\n",
      "tensor(156.2315)\n",
      "tensor(59.9533)\n",
      "tensor(123.7692)\n",
      "tensor(3.6218)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.955730\n",
      "Epoch 8463\n",
      "-------------------------------\n",
      "tensor(73.4704)\n",
      "tensor(21.8505)\n",
      "tensor(52.5957)\n",
      "tensor(0.8850)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.487209\n",
      "Epoch 8464\n",
      "-------------------------------\n",
      "tensor(331.6633)\n",
      "tensor(111.4109)\n",
      "tensor(256.5048)\n",
      "tensor(6.2458)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.826382\n",
      "Epoch 8465\n",
      "-------------------------------\n",
      "tensor(333.9651)\n",
      "tensor(118.8729)\n",
      "tensor(261.5081)\n",
      "tensor(6.7420)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.832222\n",
      "Epoch 8466\n",
      "-------------------------------\n",
      "tensor(197.4252)\n",
      "tensor(53.3919)\n",
      "tensor(138.5979)\n",
      "tensor(2.9629)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.080547\n",
      "Epoch 8467\n",
      "-------------------------------\n",
      "tensor(428.6001)\n",
      "tensor(133.0970)\n",
      "tensor(322.1790)\n",
      "tensor(7.7738)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.342480\n",
      "Epoch 8468\n",
      "-------------------------------\n",
      "tensor(415.6841)\n",
      "tensor(142.9195)\n",
      "tensor(316.5886)\n",
      "tensor(7.8244)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.135883\n",
      "Epoch 8469\n",
      "-------------------------------\n",
      "tensor(52.5811)\n",
      "tensor(16.0233)\n",
      "tensor(14.5163)\n",
      "tensor(0.2804)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.296852\n",
      "Epoch 8470\n",
      "-------------------------------\n",
      "tensor(455.8066)\n",
      "tensor(154.3835)\n",
      "tensor(349.0826)\n",
      "tensor(8.4754)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.931084\n",
      "Epoch 8471\n",
      "-------------------------------\n",
      "tensor(785.9153)\n",
      "tensor(252.2072)\n",
      "tensor(597.7356)\n",
      "tensor(15.1720)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.188625\n",
      "Epoch 8472\n",
      "-------------------------------\n",
      "tensor(1043.0446)\n",
      "tensor(367.1605)\n",
      "tensor(815.8514)\n",
      "tensor(21.3288)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.053684\n",
      "Epoch 8473\n",
      "-------------------------------\n",
      "tensor(1251.6875)\n",
      "tensor(423.6491)\n",
      "tensor(964.1360)\n",
      "tensor(24.5201)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.621445\n",
      "Epoch 8474\n",
      "-------------------------------\n",
      "tensor(1169.4918)\n",
      "tensor(381.4467)\n",
      "tensor(891.8680)\n",
      "tensor(21.6851)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.737801\n",
      "Epoch 8475\n",
      "-------------------------------\n",
      "tensor(508.3948)\n",
      "tensor(169.2102)\n",
      "tensor(387.9688)\n",
      "tensor(10.2238)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.905647\n",
      "Epoch 8476\n",
      "-------------------------------\n",
      "tensor(454.9869)\n",
      "tensor(142.1829)\n",
      "tensor(338.7569)\n",
      "tensor(8.3766)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.634991\n",
      "Epoch 8477\n",
      "-------------------------------\n",
      "tensor(538.7886)\n",
      "tensor(181.4076)\n",
      "tensor(413.4379)\n",
      "tensor(10.5369)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.761314\n",
      "Epoch 8478\n",
      "-------------------------------\n",
      "tensor(355.6537)\n",
      "tensor(118.4706)\n",
      "tensor(271.1404)\n",
      "tensor(6.9144)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.098606\n",
      "Epoch 8479\n",
      "-------------------------------\n",
      "tensor(150.4228)\n",
      "tensor(52.3929)\n",
      "tensor(113.7850)\n",
      "tensor(2.7936)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.797825\n",
      "Epoch 8480\n",
      "-------------------------------\n",
      "tensor(335.2819)\n",
      "tensor(115.6200)\n",
      "tensor(259.1861)\n",
      "tensor(6.5005)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.830799\n",
      "Epoch 8481\n",
      "-------------------------------\n",
      "tensor(292.8305)\n",
      "tensor(101.6108)\n",
      "tensor(225.3854)\n",
      "tensor(5.6897)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.190582\n",
      "Epoch 8482\n",
      "-------------------------------\n",
      "tensor(143.4761)\n",
      "tensor(52.0663)\n",
      "tensor(114.7517)\n",
      "tensor(2.9416)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.773060\n",
      "Epoch 8483\n",
      "-------------------------------\n",
      "tensor(84.8707)\n",
      "tensor(24.9618)\n",
      "tensor(59.2481)\n",
      "tensor(1.4262)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.440533\n",
      "Epoch 8484\n",
      "-------------------------------\n",
      "tensor(335.9140)\n",
      "tensor(110.3474)\n",
      "tensor(254.4111)\n",
      "tensor(6.3805)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.775875\n",
      "Epoch 8485\n",
      "-------------------------------\n",
      "tensor(319.5864)\n",
      "tensor(107.2286)\n",
      "tensor(245.7187)\n",
      "tensor(6.2955)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.577034\n",
      "Epoch 8486\n",
      "-------------------------------\n",
      "tensor(200.1151)\n",
      "tensor(65.8067)\n",
      "tensor(146.6869)\n",
      "tensor(3.5034)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.209225\n",
      "Epoch 8487\n",
      "-------------------------------\n",
      "tensor(403.5635)\n",
      "tensor(136.3504)\n",
      "tensor(310.7579)\n",
      "tensor(7.8296)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.001434\n",
      "Epoch 8488\n",
      "-------------------------------\n",
      "tensor(416.6169)\n",
      "tensor(139.0323)\n",
      "tensor(317.7008)\n",
      "tensor(7.7924)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.130894\n",
      "Epoch 8489\n",
      "-------------------------------\n",
      "tensor(31.9161)\n",
      "tensor(12.7028)\n",
      "tensor(2.7676)\n",
      "tensor(0.1631)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.262157\n",
      "Epoch 8490\n",
      "-------------------------------\n",
      "tensor(433.4155)\n",
      "tensor(130.5341)\n",
      "tensor(319.2875)\n",
      "tensor(7.7116)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.327904\n",
      "Epoch 8491\n",
      "-------------------------------\n",
      "tensor(754.1447)\n",
      "tensor(246.2892)\n",
      "tensor(573.9775)\n",
      "tensor(14.0768)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.929222\n",
      "Epoch 8492\n",
      "-------------------------------\n",
      "tensor(1021.0817)\n",
      "tensor(348.4552)\n",
      "tensor(790.7977)\n",
      "tensor(20.2840)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.003975\n",
      "Epoch 8493\n",
      "-------------------------------\n",
      "tensor(1226.0206)\n",
      "tensor(410.0165)\n",
      "tensor(941.6994)\n",
      "tensor(23.8964)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.845978\n",
      "Epoch 8494\n",
      "-------------------------------\n",
      "tensor(1141.2034)\n",
      "tensor(381.7577)\n",
      "tensor(877.1215)\n",
      "tensor(21.8539)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.734108\n",
      "Epoch 8495\n",
      "-------------------------------\n",
      "tensor(493.3882)\n",
      "tensor(161.0696)\n",
      "tensor(376.3814)\n",
      "tensor(9.3320)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.736526\n",
      "Epoch 8496\n",
      "-------------------------------\n",
      "tensor(456.1320)\n",
      "tensor(147.4028)\n",
      "tensor(345.7109)\n",
      "tensor(8.3012)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.904171\n",
      "Epoch 8497\n",
      "-------------------------------\n",
      "tensor(518.6512)\n",
      "tensor(175.5788)\n",
      "tensor(399.3095)\n",
      "tensor(10.1847)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.360561\n",
      "Epoch 8498\n",
      "-------------------------------\n",
      "tensor(362.4813)\n",
      "tensor(118.3451)\n",
      "tensor(276.9628)\n",
      "tensor(6.8376)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.203522\n",
      "Epoch 8499\n",
      "-------------------------------\n",
      "tensor(129.0685)\n",
      "tensor(49.2977)\n",
      "tensor(101.3456)\n",
      "tensor(2.8205)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.651741\n",
      "Epoch 8500\n",
      "-------------------------------\n",
      "tensor(323.9932)\n",
      "tensor(114.9563)\n",
      "tensor(252.3470)\n",
      "tensor(6.6426)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.644688\n",
      "Epoch 8501\n",
      "-------------------------------\n",
      "tensor(288.7769)\n",
      "tensor(102.3215)\n",
      "tensor(224.6021)\n",
      "tensor(5.9258)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.146030\n",
      "Epoch 8502\n",
      "-------------------------------\n",
      "tensor(152.9610)\n",
      "tensor(55.4556)\n",
      "tensor(118.7299)\n",
      "tensor(3.2231)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.776367\n",
      "Epoch 8503\n",
      "-------------------------------\n",
      "tensor(70.2813)\n",
      "tensor(21.9663)\n",
      "tensor(52.8497)\n",
      "tensor(1.1600)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.339710\n",
      "Epoch 8504\n",
      "-------------------------------\n",
      "tensor(324.4420)\n",
      "tensor(110.3195)\n",
      "tensor(250.7868)\n",
      "tensor(6.2613)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.587471\n",
      "Epoch 8505\n",
      "-------------------------------\n",
      "tensor(326.8058)\n",
      "tensor(115.2694)\n",
      "tensor(252.8245)\n",
      "tensor(6.4971)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.586273\n",
      "Epoch 8506\n",
      "-------------------------------\n",
      "tensor(193.2647)\n",
      "tensor(55.1190)\n",
      "tensor(139.0052)\n",
      "tensor(3.1452)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.968395\n",
      "Epoch 8507\n",
      "-------------------------------\n",
      "tensor(417.6382)\n",
      "tensor(131.1890)\n",
      "tensor(316.3161)\n",
      "tensor(7.6657)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.067333\n",
      "Epoch 8508\n",
      "-------------------------------\n",
      "tensor(417.8888)\n",
      "tensor(134.8609)\n",
      "tensor(309.8395)\n",
      "tensor(7.9075)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.937843\n",
      "Epoch 8509\n",
      "-------------------------------\n",
      "tensor(36.9076)\n",
      "tensor(15.5429)\n",
      "tensor(8.7985)\n",
      "tensor(0.5399)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.299133\n",
      "Epoch 8510\n",
      "-------------------------------\n",
      "tensor(406.8078)\n",
      "tensor(142.1060)\n",
      "tensor(314.8999)\n",
      "tensor(7.9552)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.033089\n",
      "Epoch 8511\n",
      "-------------------------------\n",
      "tensor(732.4496)\n",
      "tensor(225.0241)\n",
      "tensor(547.3639)\n",
      "tensor(13.1215)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.978615\n",
      "Epoch 8512\n",
      "-------------------------------\n",
      "tensor(989.0354)\n",
      "tensor(335.5901)\n",
      "tensor(761.7924)\n",
      "tensor(19.1889)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.319462\n",
      "Epoch 8513\n",
      "-------------------------------\n",
      "tensor(1188.6284)\n",
      "tensor(404.6005)\n",
      "tensor(922.4450)\n",
      "tensor(23.6270)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.302917\n",
      "Epoch 8514\n",
      "-------------------------------\n",
      "tensor(1125.0891)\n",
      "tensor(372.4689)\n",
      "tensor(861.8203)\n",
      "tensor(21.7775)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.611412\n",
      "Epoch 8515\n",
      "-------------------------------\n",
      "tensor(496.3982)\n",
      "tensor(172.2291)\n",
      "tensor(385.4805)\n",
      "tensor(9.3577)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.941582\n",
      "Epoch 8516\n",
      "-------------------------------\n",
      "tensor(422.4739)\n",
      "tensor(150.6102)\n",
      "tensor(330.3517)\n",
      "tensor(8.3321)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.419464\n",
      "Epoch 8517\n",
      "-------------------------------\n",
      "tensor(526.1694)\n",
      "tensor(167.3120)\n",
      "tensor(394.7874)\n",
      "tensor(9.5904)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.328644\n",
      "Epoch 8518\n",
      "-------------------------------\n",
      "tensor(343.4301)\n",
      "tensor(110.8194)\n",
      "tensor(259.8294)\n",
      "tensor(6.3656)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.888607\n",
      "Epoch 8519\n",
      "-------------------------------\n",
      "tensor(148.7008)\n",
      "tensor(49.5940)\n",
      "tensor(105.8934)\n",
      "tensor(2.6173)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.779434\n",
      "Epoch 8520\n",
      "-------------------------------\n",
      "tensor(325.1992)\n",
      "tensor(106.9861)\n",
      "tensor(244.4233)\n",
      "tensor(6.0114)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.631435\n",
      "Epoch 8521\n",
      "-------------------------------\n",
      "tensor(285.3826)\n",
      "tensor(92.6127)\n",
      "tensor(212.5042)\n",
      "tensor(5.1936)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.037601\n",
      "Epoch 8522\n",
      "-------------------------------\n",
      "tensor(149.4013)\n",
      "tensor(46.7323)\n",
      "tensor(107.3345)\n",
      "tensor(2.5696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.721645\n",
      "Epoch 8523\n",
      "-------------------------------\n",
      "tensor(80.0912)\n",
      "tensor(29.6263)\n",
      "tensor(59.4668)\n",
      "tensor(1.5802)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.415043\n",
      "Epoch 8524\n",
      "-------------------------------\n",
      "tensor(320.6209)\n",
      "tensor(110.5499)\n",
      "tensor(246.0344)\n",
      "tensor(6.2541)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.585899\n",
      "Epoch 8525\n",
      "-------------------------------\n",
      "tensor(307.0710)\n",
      "tensor(106.0196)\n",
      "tensor(235.8964)\n",
      "tensor(6.0524)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.395874\n",
      "Epoch 8526\n",
      "-------------------------------\n",
      "tensor(195.3080)\n",
      "tensor(63.4215)\n",
      "tensor(146.2865)\n",
      "tensor(3.4329)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.158157\n",
      "Epoch 8527\n",
      "-------------------------------\n",
      "tensor(389.8038)\n",
      "tensor(132.2194)\n",
      "tensor(300.0094)\n",
      "tensor(7.3769)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.759480\n",
      "Epoch 8528\n",
      "-------------------------------\n",
      "tensor(413.0625)\n",
      "tensor(132.0429)\n",
      "tensor(311.5134)\n",
      "tensor(7.6414)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.093216\n",
      "Epoch 8529\n",
      "-------------------------------\n",
      "tensor(33.5227)\n",
      "tensor(14.2589)\n",
      "tensor(9.7851)\n",
      "tensor(0.5059)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.350121\n",
      "Epoch 8530\n",
      "-------------------------------\n",
      "tensor(406.4112)\n",
      "tensor(130.4148)\n",
      "tensor(308.2733)\n",
      "tensor(7.8284)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.996861\n",
      "Epoch 8531\n",
      "-------------------------------\n",
      "tensor(722.2018)\n",
      "tensor(242.5070)\n",
      "tensor(556.2380)\n",
      "tensor(14.0608)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.058807\n",
      "Epoch 8532\n",
      "-------------------------------\n",
      "tensor(994.0580)\n",
      "tensor(334.0876)\n",
      "tensor(764.3566)\n",
      "tensor(19.5258)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.630569\n",
      "Epoch 8533\n",
      "-------------------------------\n",
      "tensor(1187.4039)\n",
      "tensor(400.4270)\n",
      "tensor(917.9625)\n",
      "tensor(22.8845)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.236206\n",
      "Epoch 8534\n",
      "-------------------------------\n",
      "tensor(1097.0690)\n",
      "tensor(364.0573)\n",
      "tensor(842.5210)\n",
      "tensor(21.4670)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.519455\n",
      "Epoch 8535\n",
      "-------------------------------\n",
      "tensor(452.5276)\n",
      "tensor(162.2502)\n",
      "tensor(357.7169)\n",
      "tensor(9.4012)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.236893\n",
      "Epoch 8536\n",
      "-------------------------------\n",
      "tensor(442.9896)\n",
      "tensor(154.1810)\n",
      "tensor(345.2365)\n",
      "tensor(8.8735)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.932663\n",
      "Epoch 8537\n",
      "-------------------------------\n",
      "tensor(494.5431)\n",
      "tensor(168.0183)\n",
      "tensor(381.6880)\n",
      "tensor(9.7157)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.895107\n",
      "Epoch 8538\n",
      "-------------------------------\n",
      "tensor(348.0613)\n",
      "tensor(122.1166)\n",
      "tensor(274.4347)\n",
      "tensor(7.1411)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.148808\n",
      "Epoch 8539\n",
      "-------------------------------\n",
      "tensor(124.8044)\n",
      "tensor(37.3336)\n",
      "tensor(89.2145)\n",
      "tensor(2.1003)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.708710\n",
      "Epoch 8540\n",
      "-------------------------------\n",
      "tensor(314.3755)\n",
      "tensor(102.5921)\n",
      "tensor(237.4391)\n",
      "tensor(5.9716)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.556896\n",
      "Epoch 8541\n",
      "-------------------------------\n",
      "tensor(282.0763)\n",
      "tensor(92.8471)\n",
      "tensor(213.6878)\n",
      "tensor(5.4806)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.133904\n",
      "Epoch 8542\n",
      "-------------------------------\n",
      "tensor(152.2294)\n",
      "tensor(49.9866)\n",
      "tensor(114.1181)\n",
      "tensor(3.0492)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.877674\n",
      "Epoch 8543\n",
      "-------------------------------\n",
      "tensor(64.3977)\n",
      "tensor(23.9742)\n",
      "tensor(49.4701)\n",
      "tensor(1.0012)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.451336\n",
      "Epoch 8544\n",
      "-------------------------------\n",
      "tensor(308.7523)\n",
      "tensor(105.2828)\n",
      "tensor(239.4300)\n",
      "tensor(5.7779)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.486828\n",
      "Epoch 8545\n",
      "-------------------------------\n",
      "tensor(315.1359)\n",
      "tensor(108.6604)\n",
      "tensor(243.6137)\n",
      "tensor(6.0012)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.519043\n",
      "Epoch 8546\n",
      "-------------------------------\n",
      "tensor(178.1367)\n",
      "tensor(54.4760)\n",
      "tensor(130.3483)\n",
      "tensor(3.1233)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.929817\n",
      "Epoch 8547\n",
      "-------------------------------\n",
      "tensor(403.3322)\n",
      "tensor(129.9869)\n",
      "tensor(303.7117)\n",
      "tensor(7.3642)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.759327\n",
      "Epoch 8548\n",
      "-------------------------------\n",
      "tensor(409.5964)\n",
      "tensor(126.2657)\n",
      "tensor(299.7134)\n",
      "tensor(7.4367)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.841087\n",
      "Epoch 8549\n",
      "-------------------------------\n",
      "tensor(53.2966)\n",
      "tensor(17.0564)\n",
      "tensor(4.5547)\n",
      "tensor(0.0950)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.495125\n",
      "Epoch 8550\n",
      "-------------------------------\n",
      "tensor(397.4649)\n",
      "tensor(130.9623)\n",
      "tensor(304.3318)\n",
      "tensor(7.2511)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.910702\n",
      "Epoch 8551\n",
      "-------------------------------\n",
      "tensor(696.6442)\n",
      "tensor(223.6338)\n",
      "tensor(527.2535)\n",
      "tensor(13.3627)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.003639\n",
      "Epoch 8552\n",
      "-------------------------------\n",
      "tensor(935.9955)\n",
      "tensor(322.8654)\n",
      "tensor(727.8629)\n",
      "tensor(18.6526)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.211185\n",
      "Epoch 8553\n",
      "-------------------------------\n",
      "tensor(1122.8466)\n",
      "tensor(374.8296)\n",
      "tensor(864.6533)\n",
      "tensor(21.8205)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.632626\n",
      "Epoch 8554\n",
      "-------------------------------\n",
      "tensor(1052.0486)\n",
      "tensor(348.4922)\n",
      "tensor(805.1558)\n",
      "tensor(20.1177)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 60.139126\n",
      "Epoch 8555\n",
      "-------------------------------\n",
      "tensor(461.2626)\n",
      "tensor(161.4371)\n",
      "tensor(360.0187)\n",
      "tensor(9.0134)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.345665\n",
      "Epoch 8556\n",
      "-------------------------------\n",
      "tensor(392.9838)\n",
      "tensor(138.4528)\n",
      "tensor(307.9447)\n",
      "tensor(7.7538)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.064682\n",
      "Epoch 8557\n",
      "-------------------------------\n",
      "tensor(487.3212)\n",
      "tensor(159.5340)\n",
      "tensor(369.1795)\n",
      "tensor(9.1999)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.782871\n",
      "Epoch 8558\n",
      "-------------------------------\n",
      "tensor(319.8645)\n",
      "tensor(106.0084)\n",
      "tensor(242.3095)\n",
      "tensor(6.1310)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.776123\n",
      "Epoch 8559\n",
      "-------------------------------\n",
      "tensor(134.8970)\n",
      "tensor(44.4279)\n",
      "tensor(102.4232)\n",
      "tensor(2.3956)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.895782\n",
      "Epoch 8560\n",
      "-------------------------------\n",
      "tensor(303.3869)\n",
      "tensor(100.1624)\n",
      "tensor(232.3696)\n",
      "tensor(5.6107)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.528374\n",
      "Epoch 8561\n",
      "-------------------------------\n",
      "tensor(263.5058)\n",
      "tensor(86.6818)\n",
      "tensor(201.4268)\n",
      "tensor(4.8369)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.005367\n",
      "Epoch 8562\n",
      "-------------------------------\n",
      "tensor(135.8844)\n",
      "tensor(44.0779)\n",
      "tensor(102.1487)\n",
      "tensor(2.3711)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.844334\n",
      "Epoch 8563\n",
      "-------------------------------\n",
      "tensor(75.2962)\n",
      "tensor(26.0901)\n",
      "tensor(54.3976)\n",
      "tensor(1.5049)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.552029\n",
      "Epoch 8564\n",
      "-------------------------------\n",
      "tensor(302.3860)\n",
      "tensor(101.1191)\n",
      "tensor(229.6488)\n",
      "tensor(5.8364)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.427448\n",
      "Epoch 8565\n",
      "-------------------------------\n",
      "tensor(291.7354)\n",
      "tensor(96.0632)\n",
      "tensor(220.3714)\n",
      "tensor(5.5550)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.226978\n",
      "Epoch 8566\n",
      "-------------------------------\n",
      "tensor(177.7391)\n",
      "tensor(62.6144)\n",
      "tensor(138.8117)\n",
      "tensor(3.4300)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.059555\n",
      "Epoch 8567\n",
      "-------------------------------\n",
      "tensor(365.8119)\n",
      "tensor(126.6368)\n",
      "tensor(284.0748)\n",
      "tensor(7.0485)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.338665\n",
      "Epoch 8568\n",
      "-------------------------------\n",
      "tensor(382.6678)\n",
      "tensor(124.4549)\n",
      "tensor(293.0692)\n",
      "tensor(7.3358)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.555546\n",
      "Epoch 8569\n",
      "-------------------------------\n",
      "tensor(62.0704)\n",
      "tensor(20.5340)\n",
      "tensor(9.3343)\n",
      "tensor(0.1825)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.534641\n",
      "Epoch 8570\n",
      "-------------------------------\n",
      "tensor(399.3319)\n",
      "tensor(128.0444)\n",
      "tensor(300.0031)\n",
      "tensor(8.1510)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.022026\n",
      "Epoch 8571\n",
      "-------------------------------\n",
      "tensor(682.4955)\n",
      "tensor(239.3573)\n",
      "tensor(534.4493)\n",
      "tensor(13.5953)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.188839\n",
      "Epoch 8572\n",
      "-------------------------------\n",
      "tensor(946.4946)\n",
      "tensor(302.7344)\n",
      "tensor(715.3781)\n",
      "tensor(17.6636)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.240494\n",
      "Epoch 8573\n",
      "-------------------------------\n",
      "tensor(1122.6859)\n",
      "tensor(380.6785)\n",
      "tensor(865.8781)\n",
      "tensor(21.6832)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.709583\n",
      "Epoch 8574\n",
      "-------------------------------\n",
      "tensor(1052.8463)\n",
      "tensor(354.7567)\n",
      "tensor(814.5375)\n",
      "tensor(20.8052)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 60.350410\n",
      "Epoch 8575\n",
      "-------------------------------\n",
      "tensor(474.8934)\n",
      "tensor(156.5387)\n",
      "tensor(361.6820)\n",
      "tensor(9.2521)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.432228\n",
      "Epoch 8576\n",
      "-------------------------------\n",
      "tensor(397.6812)\n",
      "tensor(129.5411)\n",
      "tensor(301.9632)\n",
      "tensor(7.8723)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.941708\n",
      "Epoch 8577\n",
      "-------------------------------\n",
      "tensor(483.4219)\n",
      "tensor(170.4544)\n",
      "tensor(379.4338)\n",
      "tensor(9.4363)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.774612\n",
      "Epoch 8578\n",
      "-------------------------------\n",
      "tensor(311.5812)\n",
      "tensor(114.6964)\n",
      "tensor(249.2315)\n",
      "tensor(6.3942)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.671108\n",
      "Epoch 8579\n",
      "-------------------------------\n",
      "tensor(141.5397)\n",
      "tensor(38.5386)\n",
      "tensor(98.9311)\n",
      "tensor(2.2860)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.791466\n",
      "Epoch 8580\n",
      "-------------------------------\n",
      "tensor(309.4571)\n",
      "tensor(96.8660)\n",
      "tensor(230.2352)\n",
      "tensor(5.6406)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.433296\n",
      "Epoch 8581\n",
      "-------------------------------\n",
      "tensor(267.0200)\n",
      "tensor(84.6078)\n",
      "tensor(199.3331)\n",
      "tensor(4.9513)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.895714\n",
      "Epoch 8582\n",
      "-------------------------------\n",
      "tensor(136.3658)\n",
      "tensor(42.5751)\n",
      "tensor(99.6709)\n",
      "tensor(2.5376)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.726562\n",
      "Epoch 8583\n",
      "-------------------------------\n",
      "tensor(76.4236)\n",
      "tensor(26.7415)\n",
      "tensor(57.0982)\n",
      "tensor(1.2885)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.433750\n",
      "Epoch 8584\n",
      "-------------------------------\n",
      "tensor(303.0750)\n",
      "tensor(100.4032)\n",
      "tensor(232.3078)\n",
      "tensor(5.5721)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.322708\n",
      "Epoch 8585\n",
      "-------------------------------\n",
      "tensor(295.7022)\n",
      "tensor(95.4795)\n",
      "tensor(222.2216)\n",
      "tensor(5.2697)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.141151\n",
      "Epoch 8586\n",
      "-------------------------------\n",
      "tensor(181.4446)\n",
      "tensor(64.2937)\n",
      "tensor(140.2988)\n",
      "tensor(3.6557)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.952995\n",
      "Epoch 8587\n",
      "-------------------------------\n",
      "tensor(374.1852)\n",
      "tensor(126.5089)\n",
      "tensor(287.7798)\n",
      "tensor(7.0285)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.337387\n",
      "Epoch 8588\n",
      "-------------------------------\n",
      "tensor(398.2941)\n",
      "tensor(122.9901)\n",
      "tensor(292.5929)\n",
      "tensor(7.5923)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.682819\n",
      "Epoch 8589\n",
      "-------------------------------\n",
      "tensor(51.2582)\n",
      "tensor(18.4197)\n",
      "tensor(14.0293)\n",
      "tensor(0.0385)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.440323\n",
      "Epoch 8590\n",
      "-------------------------------\n",
      "tensor(368.6573)\n",
      "tensor(125.5870)\n",
      "tensor(285.0800)\n",
      "tensor(7.2057)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.484360\n",
      "Epoch 8591\n",
      "-------------------------------\n",
      "tensor(670.4219)\n",
      "tensor(216.2304)\n",
      "tensor(508.9061)\n",
      "tensor(12.6293)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.205742\n",
      "Epoch 8592\n",
      "-------------------------------\n",
      "tensor(928.0193)\n",
      "tensor(305.9126)\n",
      "tensor(706.9363)\n",
      "tensor(17.5939)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 54.438408\n",
      "Epoch 8593\n",
      "-------------------------------\n",
      "tensor(1124.0947)\n",
      "tensor(382.4151)\n",
      "tensor(867.8414)\n",
      "tensor(22.0047)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 63.674637\n",
      "Epoch 8594\n",
      "-------------------------------\n",
      "tensor(1084.6527)\n",
      "tensor(358.0685)\n",
      "tensor(827.3896)\n",
      "tensor(20.5405)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 61.468380\n",
      "Epoch 8595\n",
      "-------------------------------\n",
      "tensor(508.5956)\n",
      "tensor(169.6969)\n",
      "tensor(389.8854)\n",
      "tensor(9.5800)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.218330\n",
      "Epoch 8596\n",
      "-------------------------------\n",
      "tensor(387.7721)\n",
      "tensor(126.2337)\n",
      "tensor(294.1475)\n",
      "tensor(7.1069)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.744225\n",
      "Epoch 8597\n",
      "-------------------------------\n",
      "tensor(510.6647)\n",
      "tensor(174.0339)\n",
      "tensor(392.1560)\n",
      "tensor(9.9289)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.183155\n",
      "Epoch 8598\n",
      "-------------------------------\n",
      "tensor(313.3347)\n",
      "tensor(106.8106)\n",
      "tensor(239.3138)\n",
      "tensor(6.0732)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.515587\n",
      "Epoch 8599\n",
      "-------------------------------\n",
      "tensor(154.2143)\n",
      "tensor(50.4746)\n",
      "tensor(117.6601)\n",
      "tensor(2.8227)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.825523\n",
      "Epoch 8600\n",
      "-------------------------------\n",
      "tensor(320.4796)\n",
      "tensor(106.5731)\n",
      "tensor(243.5772)\n",
      "tensor(5.9473)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.545021\n",
      "Epoch 8601\n",
      "-------------------------------\n",
      "tensor(270.2817)\n",
      "tensor(90.2687)\n",
      "tensor(204.7160)\n",
      "tensor(4.9852)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.865520\n",
      "Epoch 8602\n",
      "-------------------------------\n",
      "tensor(125.9720)\n",
      "tensor(43.2967)\n",
      "tensor(97.8356)\n",
      "tensor(2.3412)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.630539\n",
      "Epoch 8603\n",
      "-------------------------------\n",
      "tensor(89.7271)\n",
      "tensor(29.0035)\n",
      "tensor(64.6186)\n",
      "tensor(1.6942)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.434307\n",
      "Epoch 8604\n",
      "-------------------------------\n",
      "tensor(317.6904)\n",
      "tensor(104.6346)\n",
      "tensor(240.4293)\n",
      "tensor(6.0748)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.471703\n",
      "Epoch 8605\n",
      "-------------------------------\n",
      "tensor(290.0576)\n",
      "tensor(94.7782)\n",
      "tensor(219.6854)\n",
      "tensor(5.5670)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.081417\n",
      "Epoch 8606\n",
      "-------------------------------\n",
      "tensor(200.0496)\n",
      "tensor(70.1547)\n",
      "tensor(151.9035)\n",
      "tensor(3.7368)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.118237\n",
      "Epoch 8607\n",
      "-------------------------------\n",
      "tensor(368.2042)\n",
      "tensor(128.1709)\n",
      "tensor(282.2583)\n",
      "tensor(7.1334)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.200855\n",
      "Epoch 8608\n",
      "-------------------------------\n",
      "tensor(411.7741)\n",
      "tensor(132.8342)\n",
      "tensor(312.9018)\n",
      "tensor(7.5430)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.984608\n",
      "Epoch 8609\n",
      "-------------------------------\n",
      "tensor(46.6525)\n",
      "tensor(15.3738)\n",
      "tensor(21.8860)\n",
      "tensor(0.7253)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.307930\n",
      "Epoch 8610\n",
      "-------------------------------\n",
      "tensor(388.0702)\n",
      "tensor(112.6903)\n",
      "tensor(284.2249)\n",
      "tensor(6.8959)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.406937\n",
      "Epoch 8611\n",
      "-------------------------------\n",
      "tensor(698.6130)\n",
      "tensor(240.5293)\n",
      "tensor(540.6280)\n",
      "tensor(13.7835)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.198448\n",
      "Epoch 8612\n",
      "-------------------------------\n",
      "tensor(981.7261)\n",
      "tensor(329.8996)\n",
      "tensor(756.9503)\n",
      "tensor(19.2137)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.016907\n",
      "Epoch 8613\n",
      "-------------------------------\n",
      "tensor(1193.9559)\n",
      "tensor(394.7033)\n",
      "tensor(917.6511)\n",
      "tensor(22.9073)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.270012\n",
      "Epoch 8614\n",
      "-------------------------------\n",
      "tensor(1120.3796)\n",
      "tensor(383.8823)\n",
      "tensor(866.5394)\n",
      "tensor(22.1947)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.500893\n",
      "Epoch 8615\n",
      "-------------------------------\n",
      "tensor(515.2425)\n",
      "tensor(166.8809)\n",
      "tensor(389.8360)\n",
      "tensor(9.4280)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.144356\n",
      "Epoch 8616\n",
      "-------------------------------\n",
      "tensor(409.2672)\n",
      "tensor(141.2602)\n",
      "tensor(317.1104)\n",
      "tensor(8.0942)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.091354\n",
      "Epoch 8617\n",
      "-------------------------------\n",
      "tensor(524.6934)\n",
      "tensor(167.7870)\n",
      "tensor(396.0164)\n",
      "tensor(9.5125)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.409344\n",
      "Epoch 8618\n",
      "-------------------------------\n",
      "tensor(336.9997)\n",
      "tensor(106.2589)\n",
      "tensor(252.8540)\n",
      "tensor(6.0380)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.844173\n",
      "Epoch 8619\n",
      "-------------------------------\n",
      "tensor(147.6430)\n",
      "tensor(53.2237)\n",
      "tensor(113.4039)\n",
      "tensor(2.9456)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.838604\n",
      "Epoch 8620\n",
      "-------------------------------\n",
      "tensor(323.4493)\n",
      "tensor(110.1424)\n",
      "tensor(248.2755)\n",
      "tensor(6.2116)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.661278\n",
      "Epoch 8621\n",
      "-------------------------------\n",
      "tensor(280.0105)\n",
      "tensor(94.0914)\n",
      "tensor(212.6609)\n",
      "tensor(5.2769)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.021347\n",
      "Epoch 8622\n",
      "-------------------------------\n",
      "tensor(141.3981)\n",
      "tensor(46.8589)\n",
      "tensor(105.1059)\n",
      "tensor(2.5899)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.712688\n",
      "Epoch 8623\n",
      "-------------------------------\n",
      "tensor(83.0249)\n",
      "tensor(28.4230)\n",
      "tensor(62.4977)\n",
      "tensor(1.5575)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.411980\n",
      "Epoch 8624\n",
      "-------------------------------\n",
      "tensor(321.4754)\n",
      "tensor(108.9231)\n",
      "tensor(248.5303)\n",
      "tensor(6.1239)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.565769\n",
      "Epoch 8625\n",
      "-------------------------------\n",
      "tensor(308.8387)\n",
      "tensor(105.0134)\n",
      "tensor(236.2397)\n",
      "tensor(5.7155)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.330208\n",
      "Epoch 8626\n",
      "-------------------------------\n",
      "tensor(199.8018)\n",
      "tensor(62.8630)\n",
      "tensor(149.2936)\n",
      "tensor(3.8679)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.079624\n",
      "Epoch 8627\n",
      "-------------------------------\n",
      "tensor(397.0577)\n",
      "tensor(124.9009)\n",
      "tensor(298.7381)\n",
      "tensor(7.3320)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.668179\n",
      "Epoch 8628\n",
      "-------------------------------\n",
      "tensor(411.6705)\n",
      "tensor(148.6253)\n",
      "tensor(320.1840)\n",
      "tensor(8.3996)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.048866\n",
      "Epoch 8629\n",
      "-------------------------------\n",
      "tensor(44.0453)\n",
      "tensor(12.8114)\n",
      "tensor(12.9075)\n",
      "tensor(0.0215)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.233002\n",
      "Epoch 8630\n",
      "-------------------------------\n",
      "tensor(390.9145)\n",
      "tensor(140.5757)\n",
      "tensor(305.5772)\n",
      "tensor(7.9626)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.653492\n",
      "Epoch 8631\n",
      "-------------------------------\n",
      "tensor(726.4471)\n",
      "tensor(224.4959)\n",
      "tensor(543.6507)\n",
      "tensor(13.1520)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.863106\n",
      "Epoch 8632\n",
      "-------------------------------\n",
      "tensor(990.8381)\n",
      "tensor(336.6415)\n",
      "tensor(762.7765)\n",
      "tensor(19.2118)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.158764\n",
      "Epoch 8633\n",
      "-------------------------------\n",
      "tensor(1194.0875)\n",
      "tensor(409.0932)\n",
      "tensor(928.0499)\n",
      "tensor(23.8524)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.770271\n",
      "Epoch 8634\n",
      "-------------------------------\n",
      "tensor(1128.4542)\n",
      "tensor(375.5131)\n",
      "tensor(869.1754)\n",
      "tensor(22.0040)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.823822\n",
      "Epoch 8635\n",
      "-------------------------------\n",
      "tensor(508.8885)\n",
      "tensor(167.3938)\n",
      "tensor(384.7152)\n",
      "tensor(9.6155)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.741405\n",
      "Epoch 8636\n",
      "-------------------------------\n",
      "tensor(407.7890)\n",
      "tensor(150.4298)\n",
      "tensor(322.9245)\n",
      "tensor(8.6864)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.989956\n",
      "Epoch 8637\n",
      "-------------------------------\n",
      "tensor(525.0875)\n",
      "tensor(166.5719)\n",
      "tensor(393.5174)\n",
      "tensor(9.3210)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.212452\n",
      "Epoch 8638\n",
      "-------------------------------\n",
      "tensor(332.5972)\n",
      "tensor(111.4972)\n",
      "tensor(253.6850)\n",
      "tensor(6.3730)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.671436\n",
      "Epoch 8639\n",
      "-------------------------------\n",
      "tensor(156.9136)\n",
      "tensor(46.3289)\n",
      "tensor(110.6782)\n",
      "tensor(2.3532)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.740582\n",
      "Epoch 8640\n",
      "-------------------------------\n",
      "tensor(329.8747)\n",
      "tensor(102.9893)\n",
      "tensor(244.5241)\n",
      "tensor(5.6386)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.574524\n",
      "Epoch 8641\n",
      "-------------------------------\n",
      "tensor(282.6645)\n",
      "tensor(87.9835)\n",
      "tensor(208.8423)\n",
      "tensor(4.8281)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.923218\n",
      "Epoch 8642\n",
      "-------------------------------\n",
      "tensor(139.1299)\n",
      "tensor(42.0128)\n",
      "tensor(102.0332)\n",
      "tensor(2.2985)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.630039\n",
      "Epoch 8643\n",
      "-------------------------------\n",
      "tensor(84.8336)\n",
      "tensor(30.2678)\n",
      "tensor(63.1821)\n",
      "tensor(1.6364)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.379375\n",
      "Epoch 8644\n",
      "-------------------------------\n",
      "tensor(323.6539)\n",
      "tensor(106.7064)\n",
      "tensor(244.6436)\n",
      "tensor(5.9333)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.522038\n",
      "Epoch 8645\n",
      "-------------------------------\n",
      "tensor(303.0781)\n",
      "tensor(96.8982)\n",
      "tensor(227.8274)\n",
      "tensor(5.3551)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.241215\n",
      "Epoch 8646\n",
      "-------------------------------\n",
      "tensor(195.7210)\n",
      "tensor(70.6855)\n",
      "tensor(150.1357)\n",
      "tensor(4.0175)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.127586\n",
      "Epoch 8647\n",
      "-------------------------------\n",
      "tensor(381.2714)\n",
      "tensor(129.9725)\n",
      "tensor(292.0266)\n",
      "tensor(7.2563)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.480907\n",
      "Epoch 8648\n",
      "-------------------------------\n",
      "tensor(408.1924)\n",
      "tensor(137.7968)\n",
      "tensor(314.7312)\n",
      "tensor(8.1165)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.934593\n",
      "Epoch 8649\n",
      "-------------------------------\n",
      "tensor(32.9950)\n",
      "tensor(13.4930)\n",
      "tensor(13.5477)\n",
      "tensor(0.3035)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.251106\n",
      "Epoch 8650\n",
      "-------------------------------\n",
      "tensor(397.3421)\n",
      "tensor(126.0678)\n",
      "tensor(298.7952)\n",
      "tensor(7.6702)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.763836\n",
      "Epoch 8651\n",
      "-------------------------------\n",
      "tensor(716.4673)\n",
      "tensor(234.9533)\n",
      "tensor(546.8713)\n",
      "tensor(13.3614)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.751198\n",
      "Epoch 8652\n",
      "-------------------------------\n",
      "tensor(990.6859)\n",
      "tensor(328.4467)\n",
      "tensor(758.5534)\n",
      "tensor(18.9926)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.205894\n",
      "Epoch 8653\n",
      "-------------------------------\n",
      "tensor(1199.6840)\n",
      "tensor(405.1128)\n",
      "tensor(923.1206)\n",
      "tensor(23.2734)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.467545\n",
      "Epoch 8654\n",
      "-------------------------------\n",
      "tensor(1142.3844)\n",
      "tensor(379.0305)\n",
      "tensor(877.3939)\n",
      "tensor(21.9127)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.643219\n",
      "Epoch 8655\n",
      "-------------------------------\n",
      "tensor(519.8698)\n",
      "tensor(174.0053)\n",
      "tensor(400.1154)\n",
      "tensor(10.1082)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.533226\n",
      "Epoch 8656\n",
      "-------------------------------\n",
      "tensor(426.6904)\n",
      "tensor(139.3666)\n",
      "tensor(324.3000)\n",
      "tensor(8.0108)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.396374\n",
      "Epoch 8657\n",
      "-------------------------------\n",
      "tensor(530.2556)\n",
      "tensor(181.8587)\n",
      "tensor(410.1795)\n",
      "tensor(10.3524)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.624794\n",
      "Epoch 8658\n",
      "-------------------------------\n",
      "tensor(340.4322)\n",
      "tensor(116.3948)\n",
      "tensor(263.0190)\n",
      "tensor(6.5678)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.890629\n",
      "Epoch 8659\n",
      "-------------------------------\n",
      "tensor(153.1041)\n",
      "tensor(51.0040)\n",
      "tensor(114.3394)\n",
      "tensor(2.9685)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.782471\n",
      "Epoch 8660\n",
      "-------------------------------\n",
      "tensor(332.2218)\n",
      "tensor(112.2018)\n",
      "tensor(253.7182)\n",
      "tensor(6.5301)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.722755\n",
      "Epoch 8661\n",
      "-------------------------------\n",
      "tensor(284.4672)\n",
      "tensor(96.6403)\n",
      "tensor(217.3846)\n",
      "tensor(5.6591)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.063164\n",
      "Epoch 8662\n",
      "-------------------------------\n",
      "tensor(141.9365)\n",
      "tensor(48.8607)\n",
      "tensor(107.3120)\n",
      "tensor(2.9193)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.702362\n",
      "Epoch 8663\n",
      "-------------------------------\n",
      "tensor(90.2067)\n",
      "tensor(29.4007)\n",
      "tensor(63.2692)\n",
      "tensor(1.3741)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.412117\n",
      "Epoch 8664\n",
      "-------------------------------\n",
      "tensor(329.6173)\n",
      "tensor(111.4575)\n",
      "tensor(252.1535)\n",
      "tensor(6.1986)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.629196\n",
      "Epoch 8665\n",
      "-------------------------------\n",
      "tensor(307.5511)\n",
      "tensor(106.6992)\n",
      "tensor(238.1663)\n",
      "tensor(6.0324)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.383766\n",
      "Epoch 8666\n",
      "-------------------------------\n",
      "tensor(205.2580)\n",
      "tensor(63.7508)\n",
      "tensor(151.1181)\n",
      "tensor(3.5528)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.159439\n",
      "Epoch 8667\n",
      "-------------------------------\n",
      "tensor(394.5356)\n",
      "tensor(129.7022)\n",
      "tensor(300.7142)\n",
      "tensor(7.4138)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.683758\n",
      "Epoch 8668\n",
      "-------------------------------\n",
      "tensor(419.1686)\n",
      "tensor(138.5171)\n",
      "tensor(319.1047)\n",
      "tensor(7.7971)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.103790\n",
      "Epoch 8669\n",
      "-------------------------------\n",
      "tensor(47.8593)\n",
      "tensor(16.2724)\n",
      "tensor(15.1674)\n",
      "tensor(0.5774)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.304482\n",
      "Epoch 8670\n",
      "-------------------------------\n",
      "tensor(410.1750)\n",
      "tensor(135.8145)\n",
      "tensor(308.7980)\n",
      "tensor(7.4611)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.951859\n",
      "Epoch 8671\n",
      "-------------------------------\n",
      "tensor(733.8842)\n",
      "tensor(242.3621)\n",
      "tensor(565.2859)\n",
      "tensor(14.5750)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.385410\n",
      "Epoch 8672\n",
      "-------------------------------\n",
      "tensor(1014.6646)\n",
      "tensor(350.2416)\n",
      "tensor(788.2278)\n",
      "tensor(20.4124)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.585560\n",
      "Epoch 8673\n",
      "-------------------------------\n",
      "tensor(1247.3588)\n",
      "tensor(420.3320)\n",
      "tensor(960.6510)\n",
      "tensor(24.0846)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.195335\n",
      "Epoch 8674\n",
      "-------------------------------\n",
      "tensor(1208.1118)\n",
      "tensor(396.3673)\n",
      "tensor(920.1715)\n",
      "tensor(22.5779)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 67.618454\n",
      "Epoch 8675\n",
      "-------------------------------\n",
      "tensor(577.3523)\n",
      "tensor(191.1159)\n",
      "tensor(442.2904)\n",
      "tensor(11.3732)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.638077\n",
      "Epoch 8676\n",
      "-------------------------------\n",
      "tensor(410.2621)\n",
      "tensor(137.6090)\n",
      "tensor(314.2551)\n",
      "tensor(7.7187)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.028969\n",
      "Epoch 8677\n",
      "-------------------------------\n",
      "tensor(565.7675)\n",
      "tensor(192.1318)\n",
      "tensor(435.7438)\n",
      "tensor(11.1580)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.401695\n",
      "Epoch 8678\n",
      "-------------------------------\n",
      "tensor(335.0199)\n",
      "tensor(114.0909)\n",
      "tensor(257.0666)\n",
      "tensor(6.6503)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.712624\n",
      "Epoch 8679\n",
      "-------------------------------\n",
      "tensor(184.4396)\n",
      "tensor(60.9582)\n",
      "tensor(134.7514)\n",
      "tensor(3.2249)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.931576\n",
      "Epoch 8680\n",
      "-------------------------------\n",
      "tensor(352.2514)\n",
      "tensor(119.0012)\n",
      "tensor(267.7105)\n",
      "tensor(6.6665)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.952908\n",
      "Epoch 8681\n",
      "-------------------------------\n",
      "tensor(292.4404)\n",
      "tensor(99.8729)\n",
      "tensor(221.8963)\n",
      "tensor(5.6079)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.057076\n",
      "Epoch 8682\n",
      "-------------------------------\n",
      "tensor(133.1253)\n",
      "tensor(47.3440)\n",
      "tensor(103.0559)\n",
      "tensor(2.6976)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.587067\n",
      "Epoch 8683\n",
      "-------------------------------\n",
      "tensor(106.1616)\n",
      "tensor(32.3374)\n",
      "tensor(75.3988)\n",
      "tensor(1.7399)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.425682\n",
      "Epoch 8684\n",
      "-------------------------------\n",
      "tensor(349.4135)\n",
      "tensor(115.3777)\n",
      "tensor(265.3849)\n",
      "tensor(6.5641)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.899803\n",
      "Epoch 8685\n",
      "-------------------------------\n",
      "tensor(310.9360)\n",
      "tensor(105.1004)\n",
      "tensor(238.2091)\n",
      "tensor(6.0759)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.356186\n",
      "Epoch 8686\n",
      "-------------------------------\n",
      "tensor(227.9635)\n",
      "tensor(72.9892)\n",
      "tensor(170.4272)\n",
      "tensor(3.9187)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.324398\n",
      "Epoch 8687\n",
      "-------------------------------\n",
      "tensor(397.6093)\n",
      "tensor(131.3477)\n",
      "tensor(303.6543)\n",
      "tensor(7.3151)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.724964\n",
      "Epoch 8688\n",
      "-------------------------------\n",
      "tensor(450.0651)\n",
      "tensor(147.4414)\n",
      "tensor(343.4545)\n",
      "tensor(8.4629)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.733364\n",
      "Epoch 8689\n",
      "-------------------------------\n",
      "tensor(49.6685)\n",
      "tensor(23.7535)\n",
      "tensor(34.4353)\n",
      "tensor(1.2507)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.276604\n",
      "Epoch 8690\n",
      "-------------------------------\n",
      "tensor(406.6140)\n",
      "tensor(126.6507)\n",
      "tensor(301.7091)\n",
      "tensor(7.5469)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.699146\n",
      "Epoch 8691\n",
      "-------------------------------\n",
      "tensor(756.6049)\n",
      "tensor(253.1669)\n",
      "tensor(579.7919)\n",
      "tensor(14.4975)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.914909\n",
      "Epoch 8692\n",
      "-------------------------------\n",
      "tensor(1067.7753)\n",
      "tensor(356.1211)\n",
      "tensor(820.5294)\n",
      "tensor(20.5436)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.955143\n",
      "Epoch 8693\n",
      "-------------------------------\n",
      "tensor(1300.7020)\n",
      "tensor(435.7908)\n",
      "tensor(1002.5894)\n",
      "tensor(25.2160)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 73.227554\n",
      "Epoch 8694\n",
      "-------------------------------\n",
      "tensor(1231.2842)\n",
      "tensor(418.8530)\n",
      "tensor(950.8359)\n",
      "tensor(24.2501)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.450317\n",
      "Epoch 8695\n",
      "-------------------------------\n",
      "tensor(563.4154)\n",
      "tensor(184.6459)\n",
      "tensor(429.6591)\n",
      "tensor(10.3716)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.343052\n",
      "Epoch 8696\n",
      "-------------------------------\n",
      "tensor(459.2464)\n",
      "tensor(157.3905)\n",
      "tensor(353.4048)\n",
      "tensor(8.8820)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.929371\n",
      "Epoch 8697\n",
      "-------------------------------\n",
      "tensor(586.2155)\n",
      "tensor(187.3285)\n",
      "tensor(441.3398)\n",
      "tensor(10.5575)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.732891\n",
      "Epoch 8698\n",
      "-------------------------------\n",
      "tensor(373.6377)\n",
      "tensor(117.7868)\n",
      "tensor(280.3152)\n",
      "tensor(6.6046)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.261711\n",
      "Epoch 8699\n",
      "-------------------------------\n",
      "tensor(165.6641)\n",
      "tensor(59.0611)\n",
      "tensor(127.6215)\n",
      "tensor(3.3630)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.836674\n",
      "Epoch 8700\n",
      "-------------------------------\n",
      "tensor(360.2708)\n",
      "tensor(121.5046)\n",
      "tensor(276.4243)\n",
      "tensor(6.9302)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.113052\n",
      "Epoch 8701\n",
      "-------------------------------\n",
      "tensor(310.4797)\n",
      "tensor(102.9303)\n",
      "tensor(235.8986)\n",
      "tensor(5.8375)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.329815\n",
      "Epoch 8702\n",
      "-------------------------------\n",
      "tensor(156.7316)\n",
      "tensor(50.2767)\n",
      "tensor(115.8450)\n",
      "tensor(2.8066)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.717506\n",
      "Epoch 8703\n",
      "-------------------------------\n",
      "tensor(90.6281)\n",
      "tensor(32.9907)\n",
      "tensor(70.4837)\n",
      "tensor(1.8419)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.353825\n",
      "Epoch 8704\n",
      "-------------------------------\n",
      "tensor(357.5559)\n",
      "tensor(122.7847)\n",
      "tensor(276.3244)\n",
      "tensor(6.9222)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.042465\n",
      "Epoch 8705\n",
      "-------------------------------\n",
      "tensor(340.6265)\n",
      "tensor(116.7702)\n",
      "tensor(261.0099)\n",
      "tensor(6.4398)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.751995\n",
      "Epoch 8706\n",
      "-------------------------------\n",
      "tensor(221.6781)\n",
      "tensor(71.9192)\n",
      "tensor(167.1348)\n",
      "tensor(4.2724)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.234684\n",
      "Epoch 8707\n",
      "-------------------------------\n",
      "tensor(438.0723)\n",
      "tensor(142.7233)\n",
      "tensor(331.8254)\n",
      "tensor(8.2741)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.350574\n",
      "Epoch 8708\n",
      "-------------------------------\n",
      "tensor(463.3996)\n",
      "tensor(158.6862)\n",
      "tensor(356.0169)\n",
      "tensor(8.9111)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.222660\n",
      "Epoch 8709\n",
      "-------------------------------\n",
      "tensor(48.8771)\n",
      "tensor(14.7646)\n",
      "tensor(14.5873)\n",
      "tensor(0.2244)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.519806\n",
      "Epoch 8710\n",
      "-------------------------------\n",
      "tensor(439.3463)\n",
      "tensor(149.0910)\n",
      "tensor(339.8510)\n",
      "tensor(8.3682)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.732689\n",
      "Epoch 8711\n",
      "-------------------------------\n",
      "tensor(789.8248)\n",
      "tensor(255.3414)\n",
      "tensor(602.1430)\n",
      "tensor(15.2503)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.180679\n",
      "Epoch 8712\n",
      "-------------------------------\n",
      "tensor(1077.4331)\n",
      "tensor(373.9033)\n",
      "tensor(837.5304)\n",
      "tensor(21.5496)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.491901\n",
      "Epoch 8713\n",
      "-------------------------------\n",
      "tensor(1309.0834)\n",
      "tensor(439.5291)\n",
      "tensor(1009.8842)\n",
      "tensor(25.3121)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.106659\n",
      "Epoch 8714\n",
      "-------------------------------\n",
      "tensor(1234.9822)\n",
      "tensor(409.5906)\n",
      "tensor(949.4182)\n",
      "tensor(23.7681)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.574852\n",
      "Epoch 8715\n",
      "-------------------------------\n",
      "tensor(550.8255)\n",
      "tensor(188.6665)\n",
      "tensor(425.9728)\n",
      "tensor(11.1266)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.199074\n",
      "Epoch 8716\n",
      "-------------------------------\n",
      "tensor(447.9255)\n",
      "tensor(163.8154)\n",
      "tensor(356.0284)\n",
      "tensor(9.4225)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.064873\n",
      "Epoch 8717\n",
      "-------------------------------\n",
      "tensor(569.5813)\n",
      "tensor(185.9845)\n",
      "tensor(433.2226)\n",
      "tensor(10.7512)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.567448\n",
      "Epoch 8718\n",
      "-------------------------------\n",
      "tensor(367.1702)\n",
      "tensor(124.8612)\n",
      "tensor(282.3938)\n",
      "tensor(7.3476)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.392521\n",
      "Epoch 8719\n",
      "-------------------------------\n",
      "tensor(162.9721)\n",
      "tensor(50.8764)\n",
      "tensor(120.6324)\n",
      "tensor(2.6170)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.930042\n",
      "Epoch 8720\n",
      "-------------------------------\n",
      "tensor(357.2935)\n",
      "tensor(116.2896)\n",
      "tensor(270.7836)\n",
      "tensor(6.4449)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.154327\n",
      "Epoch 8721\n",
      "-------------------------------\n",
      "tensor(305.2846)\n",
      "tensor(100.2603)\n",
      "tensor(233.2279)\n",
      "tensor(5.6231)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.423275\n",
      "Epoch 8722\n",
      "-------------------------------\n",
      "tensor(153.4734)\n",
      "tensor(50.6633)\n",
      "tensor(116.7502)\n",
      "tensor(2.8283)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.867004\n",
      "Epoch 8723\n",
      "-------------------------------\n",
      "tensor(90.9067)\n",
      "tensor(29.2705)\n",
      "tensor(65.3482)\n",
      "tensor(1.5874)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.514095\n",
      "Epoch 8724\n",
      "-------------------------------\n",
      "tensor(354.7708)\n",
      "tensor(115.9494)\n",
      "tensor(267.8267)\n",
      "tensor(6.5168)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.084740\n",
      "Epoch 8725\n",
      "-------------------------------\n",
      "tensor(337.5553)\n",
      "tensor(109.3045)\n",
      "tensor(253.9461)\n",
      "tensor(6.1443)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.775463\n",
      "Epoch 8726\n",
      "-------------------------------\n",
      "tensor(211.0234)\n",
      "tensor(72.9228)\n",
      "tensor(165.4069)\n",
      "tensor(4.1294)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.229225\n",
      "Epoch 8727\n",
      "-------------------------------\n",
      "tensor(428.3811)\n",
      "tensor(142.6005)\n",
      "tensor(326.4179)\n",
      "tensor(7.8362)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.214725\n",
      "Epoch 8728\n",
      "-------------------------------\n",
      "tensor(452.7819)\n",
      "tensor(151.0734)\n",
      "tensor(350.5720)\n",
      "tensor(8.9325)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.785168\n",
      "Epoch 8729\n",
      "-------------------------------\n",
      "tensor(54.6734)\n",
      "tensor(23.1278)\n",
      "tensor(10.7943)\n",
      "tensor(0.7171)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.403130\n",
      "Epoch 8730\n",
      "-------------------------------\n",
      "tensor(451.4734)\n",
      "tensor(148.2810)\n",
      "tensor(342.3688)\n",
      "tensor(9.2736)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.890945\n",
      "Epoch 8731\n",
      "-------------------------------\n",
      "tensor(795.5194)\n",
      "tensor(276.2033)\n",
      "tensor(620.4818)\n",
      "tensor(15.7956)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.722809\n",
      "Epoch 8732\n",
      "-------------------------------\n",
      "tensor(1113.5543)\n",
      "tensor(359.8058)\n",
      "tensor(844.8428)\n",
      "tensor(20.7093)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.642162\n",
      "Epoch 8733\n",
      "-------------------------------\n",
      "tensor(1343.4391)\n",
      "tensor(452.3104)\n",
      "tensor(1032.1881)\n",
      "tensor(25.7316)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.526787\n",
      "Epoch 8734\n",
      "-------------------------------\n",
      "tensor(1278.0543)\n",
      "tensor(432.1057)\n",
      "tensor(989.6068)\n",
      "tensor(25.1048)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.093742\n",
      "Epoch 8735\n",
      "-------------------------------\n",
      "tensor(594.6262)\n",
      "tensor(198.9508)\n",
      "tensor(456.2300)\n",
      "tensor(11.7584)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.165234\n",
      "Epoch 8736\n",
      "-------------------------------\n",
      "tensor(463.6812)\n",
      "tensor(153.1031)\n",
      "tensor(354.8568)\n",
      "tensor(9.2407)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.076019\n",
      "Epoch 8737\n",
      "-------------------------------\n",
      "tensor(595.8334)\n",
      "tensor(208.0060)\n",
      "tensor(464.0780)\n",
      "tensor(11.6156)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.368946\n",
      "Epoch 8738\n",
      "-------------------------------\n",
      "tensor(372.2438)\n",
      "tensor(135.1641)\n",
      "tensor(292.4445)\n",
      "tensor(7.6075)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.426651\n",
      "Epoch 8739\n",
      "-------------------------------\n",
      "tensor(183.5446)\n",
      "tensor(52.5302)\n",
      "tensor(130.9567)\n",
      "tensor(2.9329)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.946423\n",
      "Epoch 8740\n",
      "-------------------------------\n",
      "tensor(380.7101)\n",
      "tensor(121.2636)\n",
      "tensor(284.8465)\n",
      "tensor(6.8954)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.379818\n",
      "Epoch 8741\n",
      "-------------------------------\n",
      "tensor(321.6985)\n",
      "tensor(104.3630)\n",
      "tensor(241.9047)\n",
      "tensor(5.9768)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.483948\n",
      "Epoch 8742\n",
      "-------------------------------\n",
      "tensor(156.7340)\n",
      "tensor(51.7784)\n",
      "tensor(117.1153)\n",
      "tensor(3.0133)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.749256\n",
      "Epoch 8743\n",
      "-------------------------------\n",
      "tensor(102.2263)\n",
      "tensor(32.3702)\n",
      "tensor(74.6702)\n",
      "tensor(1.6068)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.426861\n",
      "Epoch 8744\n",
      "-------------------------------\n",
      "tensor(379.4843)\n",
      "tensor(121.9864)\n",
      "tensor(284.3896)\n",
      "tensor(6.6749)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.317738\n",
      "Epoch 8745\n",
      "-------------------------------\n",
      "tensor(349.3403)\n",
      "tensor(111.2475)\n",
      "tensor(262.8299)\n",
      "tensor(6.1347)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.875713\n",
      "Epoch 8746\n",
      "-------------------------------\n",
      "tensor(230.7621)\n",
      "tensor(80.3681)\n",
      "tensor(178.1879)\n",
      "tensor(4.5957)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.367134\n",
      "Epoch 8747\n",
      "-------------------------------\n",
      "tensor(448.3836)\n",
      "tensor(147.8400)\n",
      "tensor(339.4543)\n",
      "tensor(8.1899)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.578735\n",
      "Epoch 8748\n",
      "-------------------------------\n",
      "tensor(492.9494)\n",
      "tensor(157.5372)\n",
      "tensor(372.2315)\n",
      "tensor(9.4643)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.551559\n",
      "Epoch 8749\n",
      "-------------------------------\n",
      "tensor(64.3705)\n",
      "tensor(20.6049)\n",
      "tensor(33.5448)\n",
      "tensor(0.8643)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.432236\n",
      "Epoch 8750\n",
      "-------------------------------\n",
      "tensor(433.5630)\n",
      "tensor(141.2784)\n",
      "tensor(331.4649)\n",
      "tensor(8.1906)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.526638\n",
      "Epoch 8751\n",
      "-------------------------------\n",
      "tensor(796.7003)\n",
      "tensor(267.4878)\n",
      "tensor(614.9113)\n",
      "tensor(15.7858)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.461704\n",
      "Epoch 8752\n",
      "-------------------------------\n",
      "tensor(1117.1053)\n",
      "tensor(375.8867)\n",
      "tensor(859.3310)\n",
      "tensor(21.6261)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.094208\n",
      "Epoch 8753\n",
      "-------------------------------\n",
      "tensor(1371.5543)\n",
      "tensor(458.3223)\n",
      "tensor(1052.5968)\n",
      "tensor(26.2083)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.203354\n",
      "Epoch 8754\n",
      "-------------------------------\n",
      "tensor(1318.6455)\n",
      "tensor(442.6420)\n",
      "tensor(1012.9357)\n",
      "tensor(25.2536)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.028481\n",
      "Epoch 8755\n",
      "-------------------------------\n",
      "tensor(630.0616)\n",
      "tensor(203.3443)\n",
      "tensor(478.6086)\n",
      "tensor(12.0842)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.065804\n",
      "Epoch 8756\n",
      "-------------------------------\n",
      "tensor(458.0526)\n",
      "tensor(155.7685)\n",
      "tensor(350.9422)\n",
      "tensor(8.8965)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.111763\n",
      "Epoch 8757\n",
      "-------------------------------\n",
      "tensor(617.5693)\n",
      "tensor(205.3768)\n",
      "tensor(475.7281)\n",
      "tensor(11.8078)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.899834\n",
      "Epoch 8758\n",
      "-------------------------------\n",
      "tensor(382.2387)\n",
      "tensor(126.0699)\n",
      "tensor(290.0049)\n",
      "tensor(7.1924)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.422596\n",
      "Epoch 8759\n",
      "-------------------------------\n",
      "tensor(192.1519)\n",
      "tensor(64.7161)\n",
      "tensor(142.4701)\n",
      "tensor(3.4997)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.005337\n",
      "Epoch 8760\n",
      "-------------------------------\n",
      "tensor(382.5144)\n",
      "tensor(128.8006)\n",
      "tensor(294.2879)\n",
      "tensor(7.2659)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.506554\n",
      "Epoch 8761\n",
      "-------------------------------\n",
      "tensor(322.1419)\n",
      "tensor(108.4343)\n",
      "tensor(247.4438)\n",
      "tensor(6.0909)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.538033\n",
      "Epoch 8762\n",
      "-------------------------------\n",
      "tensor(154.8805)\n",
      "tensor(52.4370)\n",
      "tensor(118.8651)\n",
      "tensor(2.8854)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.740612\n",
      "Epoch 8763\n",
      "-------------------------------\n",
      "tensor(105.7709)\n",
      "tensor(34.7608)\n",
      "tensor(77.7650)\n",
      "tensor(2.0163)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.455669\n",
      "Epoch 8764\n",
      "-------------------------------\n",
      "tensor(382.1953)\n",
      "tensor(127.3327)\n",
      "tensor(291.2408)\n",
      "tensor(7.3498)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.447735\n",
      "Epoch 8765\n",
      "-------------------------------\n",
      "tensor(351.2014)\n",
      "tensor(116.8041)\n",
      "tensor(267.4000)\n",
      "tensor(6.7771)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.923897\n",
      "Epoch 8766\n",
      "-------------------------------\n",
      "tensor(241.5408)\n",
      "tensor(82.0603)\n",
      "tensor(184.0400)\n",
      "tensor(4.4619)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.517368\n",
      "Epoch 8767\n",
      "-------------------------------\n",
      "tensor(443.2336)\n",
      "tensor(151.8999)\n",
      "tensor(342.6504)\n",
      "tensor(8.5474)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.645298\n",
      "Epoch 8768\n",
      "-------------------------------\n",
      "tensor(491.5786)\n",
      "tensor(160.6142)\n",
      "tensor(375.7439)\n",
      "tensor(9.2106)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.605728\n",
      "Epoch 8769\n",
      "-------------------------------\n",
      "tensor(56.9956)\n",
      "tensor(21.9019)\n",
      "tensor(23.7699)\n",
      "tensor(0.8278)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.236698\n",
      "Epoch 8770\n",
      "-------------------------------\n",
      "tensor(455.9952)\n",
      "tensor(141.5096)\n",
      "tensor(340.0670)\n",
      "tensor(8.6244)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.662956\n",
      "Epoch 8771\n",
      "-------------------------------\n",
      "tensor(826.1917)\n",
      "tensor(283.5414)\n",
      "tensor(639.7532)\n",
      "tensor(16.1704)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.600990\n",
      "Epoch 8772\n",
      "-------------------------------\n",
      "tensor(1161.1938)\n",
      "tensor(385.4536)\n",
      "tensor(893.4193)\n",
      "tensor(22.3405)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 65.456268\n",
      "Epoch 8773\n",
      "-------------------------------\n",
      "tensor(1412.4165)\n",
      "tensor(476.3981)\n",
      "tensor(1089.6238)\n",
      "tensor(27.6485)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.090172\n",
      "Epoch 8774\n",
      "-------------------------------\n",
      "tensor(1337.1792)\n",
      "tensor(458.0566)\n",
      "tensor(1039.0370)\n",
      "tensor(26.2881)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.934006\n",
      "Epoch 8775\n",
      "-------------------------------\n",
      "tensor(618.5423)\n",
      "tensor(197.5230)\n",
      "tensor(467.0074)\n",
      "tensor(11.5386)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.762043\n",
      "Epoch 8776\n",
      "-------------------------------\n",
      "tensor(502.0179)\n",
      "tensor(159.6556)\n",
      "tensor(378.7365)\n",
      "tensor(9.2297)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.813988\n",
      "Epoch 8777\n",
      "-------------------------------\n",
      "tensor(624.6767)\n",
      "tensor(213.5388)\n",
      "tensor(483.4828)\n",
      "tensor(12.1818)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.015820\n",
      "Epoch 8778\n",
      "-------------------------------\n",
      "tensor(401.8185)\n",
      "tensor(134.0179)\n",
      "tensor(307.2121)\n",
      "tensor(7.6327)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.810139\n",
      "Epoch 8779\n",
      "-------------------------------\n",
      "tensor(179.7537)\n",
      "tensor(63.2753)\n",
      "tensor(137.9796)\n",
      "tensor(3.5556)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.960232\n",
      "Epoch 8780\n",
      "-------------------------------\n",
      "tensor(389.8400)\n",
      "tensor(134.9274)\n",
      "tensor(301.0754)\n",
      "tensor(7.6771)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.648106\n",
      "Epoch 8781\n",
      "-------------------------------\n",
      "tensor(332.6151)\n",
      "tensor(116.0235)\n",
      "tensor(257.2602)\n",
      "tensor(6.6064)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.714375\n",
      "Epoch 8782\n",
      "-------------------------------\n",
      "tensor(163.1849)\n",
      "tensor(59.1311)\n",
      "tensor(126.8653)\n",
      "tensor(3.3586)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.821526\n",
      "Epoch 8783\n",
      "-------------------------------\n",
      "tensor(108.8445)\n",
      "tensor(32.7255)\n",
      "tensor(74.2609)\n",
      "tensor(1.6926)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.443729\n",
      "Epoch 8784\n",
      "-------------------------------\n",
      "tensor(388.5587)\n",
      "tensor(128.8407)\n",
      "tensor(296.3984)\n",
      "tensor(7.3394)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.527061\n",
      "Epoch 8785\n",
      "-------------------------------\n",
      "tensor(361.3788)\n",
      "tensor(123.5460)\n",
      "tensor(280.2429)\n",
      "tensor(7.1015)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.099243\n",
      "Epoch 8786\n",
      "-------------------------------\n",
      "tensor(241.1986)\n",
      "tensor(76.1915)\n",
      "tensor(179.2089)\n",
      "tensor(4.2728)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.417850\n",
      "Epoch 8787\n",
      "-------------------------------\n",
      "tensor(463.6280)\n",
      "tensor(150.9437)\n",
      "tensor(353.4403)\n",
      "tensor(8.7815)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.001003\n",
      "Epoch 8788\n",
      "-------------------------------\n",
      "tensor(503.4355)\n",
      "tensor(162.5940)\n",
      "tensor(378.9124)\n",
      "tensor(9.3775)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.814079\n",
      "Epoch 8789\n",
      "-------------------------------\n",
      "tensor(61.3161)\n",
      "tensor(16.0385)\n",
      "tensor(24.9718)\n",
      "tensor(0.2191)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.378571\n",
      "Epoch 8790\n",
      "-------------------------------\n",
      "tensor(456.7767)\n",
      "tensor(146.6360)\n",
      "tensor(345.3780)\n",
      "tensor(8.1761)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.854355\n",
      "Epoch 8791\n",
      "-------------------------------\n",
      "tensor(833.1575)\n",
      "tensor(267.0144)\n",
      "tensor(632.6682)\n",
      "tensor(15.5998)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.596424\n",
      "Epoch 8792\n",
      "-------------------------------\n",
      "tensor(1141.6013)\n",
      "tensor(395.9279)\n",
      "tensor(888.8809)\n",
      "tensor(22.9089)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.746689\n",
      "Epoch 8793\n",
      "-------------------------------\n",
      "tensor(1391.1107)\n",
      "tensor(468.1823)\n",
      "tensor(1075.3344)\n",
      "tensor(27.1256)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 78.784973\n",
      "Epoch 8794\n",
      "-------------------------------\n",
      "tensor(1326.5797)\n",
      "tensor(440.1907)\n",
      "tensor(1015.2625)\n",
      "tensor(25.2946)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.321449\n",
      "Epoch 8795\n",
      "-------------------------------\n",
      "tensor(605.1564)\n",
      "tensor(207.0853)\n",
      "tensor(467.3381)\n",
      "tensor(11.4578)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.568485\n",
      "Epoch 8796\n",
      "-------------------------------\n",
      "tensor(480.8485)\n",
      "tensor(167.4955)\n",
      "tensor(372.9681)\n",
      "tensor(9.2554)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.559841\n",
      "Epoch 8797\n",
      "-------------------------------\n",
      "tensor(620.5073)\n",
      "tensor(200.5738)\n",
      "tensor(472.1484)\n",
      "tensor(11.6044)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.788296\n",
      "Epoch 8798\n",
      "-------------------------------\n",
      "tensor(396.5024)\n",
      "tensor(126.7463)\n",
      "tensor(299.5029)\n",
      "tensor(7.2349)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.623901\n",
      "Epoch 8799\n",
      "-------------------------------\n",
      "tensor(175.1942)\n",
      "tensor(61.8329)\n",
      "tensor(134.1091)\n",
      "tensor(3.4946)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.853645\n",
      "Epoch 8800\n",
      "-------------------------------\n",
      "tensor(382.4407)\n",
      "tensor(128.8895)\n",
      "tensor(293.1088)\n",
      "tensor(7.3499)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.448158\n",
      "Epoch 8801\n",
      "-------------------------------\n",
      "tensor(330.1031)\n",
      "tensor(109.5397)\n",
      "tensor(250.8783)\n",
      "tensor(6.2141)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.584892\n",
      "Epoch 8802\n",
      "-------------------------------\n",
      "tensor(164.4864)\n",
      "tensor(53.3963)\n",
      "tensor(123.8627)\n",
      "tensor(2.9872)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.776199\n",
      "Epoch 8803\n",
      "-------------------------------\n",
      "tensor(97.5627)\n",
      "tensor(35.7825)\n",
      "tensor(73.4678)\n",
      "tensor(1.9983)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.377846\n",
      "Epoch 8804\n",
      "-------------------------------\n",
      "tensor(379.2523)\n",
      "tensor(131.3304)\n",
      "tensor(292.8913)\n",
      "tensor(7.5355)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.403336\n",
      "Epoch 8805\n",
      "-------------------------------\n",
      "tensor(357.7116)\n",
      "tensor(124.6378)\n",
      "tensor(277.0690)\n",
      "tensor(7.1859)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.053215\n",
      "Epoch 8806\n",
      "-------------------------------\n",
      "tensor(234.3693)\n",
      "tensor(76.3628)\n",
      "tensor(176.6574)\n",
      "tensor(4.1946)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.384655\n",
      "Epoch 8807\n",
      "-------------------------------\n",
      "tensor(460.9465)\n",
      "tensor(156.9941)\n",
      "tensor(352.4240)\n",
      "tensor(8.8449)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.859543\n",
      "Epoch 8808\n",
      "-------------------------------\n",
      "tensor(501.9510)\n",
      "tensor(160.0101)\n",
      "tensor(378.4157)\n",
      "tensor(8.9692)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.678764\n",
      "Epoch 8809\n",
      "-------------------------------\n",
      "tensor(35.1165)\n",
      "tensor(14.0735)\n",
      "tensor(22.9199)\n",
      "tensor(0.7379)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.224060\n",
      "Epoch 8810\n",
      "-------------------------------\n",
      "tensor(480.4704)\n",
      "tensor(146.3938)\n",
      "tensor(354.0479)\n",
      "tensor(8.4145)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.189144\n",
      "Epoch 8811\n",
      "-------------------------------\n",
      "tensor(865.9944)\n",
      "tensor(288.9119)\n",
      "tensor(664.2902)\n",
      "tensor(16.4856)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.904392\n",
      "Epoch 8812\n",
      "-------------------------------\n",
      "tensor(1196.6713)\n",
      "tensor(408.1310)\n",
      "tensor(930.1696)\n",
      "tensor(23.9223)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.900467\n",
      "Epoch 8813\n",
      "-------------------------------\n",
      "tensor(1440.7932)\n",
      "tensor(491.7547)\n",
      "tensor(1121.1602)\n",
      "tensor(28.6736)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 82.413132\n",
      "Epoch 8814\n",
      "-------------------------------\n",
      "tensor(1345.8881)\n",
      "tensor(452.6882)\n",
      "tensor(1039.5265)\n",
      "tensor(26.0666)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 76.238220\n",
      "Epoch 8815\n",
      "-------------------------------\n",
      "tensor(587.4749)\n",
      "tensor(194.3192)\n",
      "tensor(447.0279)\n",
      "tensor(11.1185)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.864151\n",
      "Epoch 8816\n",
      "-------------------------------\n",
      "tensor(528.2892)\n",
      "tensor(172.8708)\n",
      "tensor(400.7560)\n",
      "tensor(9.8119)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.369133\n",
      "Epoch 8817\n",
      "-------------------------------\n",
      "tensor(617.4890)\n",
      "tensor(208.4921)\n",
      "tensor(474.5401)\n",
      "tensor(11.8388)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.885342\n",
      "Epoch 8818\n",
      "-------------------------------\n",
      "tensor(419.6831)\n",
      "tensor(140.4997)\n",
      "tensor(321.1079)\n",
      "tensor(7.9154)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.278126\n",
      "Epoch 8819\n",
      "-------------------------------\n",
      "tensor(163.1151)\n",
      "tensor(56.0735)\n",
      "tensor(124.0612)\n",
      "tensor(3.1954)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.902100\n",
      "Epoch 8820\n",
      "-------------------------------\n",
      "tensor(387.7870)\n",
      "tensor(130.5951)\n",
      "tensor(298.0559)\n",
      "tensor(7.4876)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.628559\n",
      "Epoch 8821\n",
      "-------------------------------\n",
      "tensor(344.2674)\n",
      "tensor(115.4166)\n",
      "tensor(262.4649)\n",
      "tensor(6.5644)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.850452\n",
      "Epoch 8822\n",
      "-------------------------------\n",
      "tensor(180.5516)\n",
      "tensor(60.3404)\n",
      "tensor(135.9428)\n",
      "tensor(3.4014)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.934612\n",
      "Epoch 8823\n",
      "-------------------------------\n",
      "tensor(90.4511)\n",
      "tensor(30.1135)\n",
      "tensor(66.8656)\n",
      "tensor(1.6398)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.379040\n",
      "Epoch 8824\n",
      "-------------------------------\n",
      "tensor(386.4115)\n",
      "tensor(129.5323)\n",
      "tensor(297.7951)\n",
      "tensor(7.3637)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.526875\n",
      "Epoch 8825\n",
      "-------------------------------\n",
      "tensor(382.7069)\n",
      "tensor(128.0662)\n",
      "tensor(292.9928)\n",
      "tensor(7.1903)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.435047\n",
      "Epoch 8826\n",
      "-------------------------------\n",
      "tensor(226.7324)\n",
      "tensor(75.0211)\n",
      "tensor(172.0513)\n",
      "tensor(4.3899)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.314640\n",
      "Epoch 8827\n",
      "-------------------------------\n",
      "tensor(486.6172)\n",
      "tensor(159.8437)\n",
      "tensor(370.0622)\n",
      "tensor(9.2265)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.365776\n",
      "Epoch 8828\n",
      "-------------------------------\n",
      "tensor(504.6015)\n",
      "tensor(164.2283)\n",
      "tensor(381.6555)\n",
      "tensor(9.5833)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.786827\n",
      "Epoch 8829\n",
      "-------------------------------\n",
      "tensor(48.9594)\n",
      "tensor(17.0972)\n",
      "tensor(9.6744)\n",
      "tensor(0.1215)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.273293\n",
      "Epoch 8830\n",
      "-------------------------------\n",
      "tensor(486.8426)\n",
      "tensor(157.5886)\n",
      "tensor(370.7858)\n",
      "tensor(8.9618)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.462894\n",
      "Epoch 8831\n",
      "-------------------------------\n",
      "tensor(861.8747)\n",
      "tensor(281.3158)\n",
      "tensor(658.7969)\n",
      "tensor(16.4856)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.799065\n",
      "Epoch 8832\n",
      "-------------------------------\n",
      "tensor(1165.6757)\n",
      "tensor(402.2327)\n",
      "tensor(909.6164)\n",
      "tensor(23.2831)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.167435\n",
      "Epoch 8833\n",
      "-------------------------------\n",
      "tensor(1403.4244)\n",
      "tensor(473.4888)\n",
      "tensor(1083.3602)\n",
      "tensor(27.5639)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.463570\n",
      "Epoch 8834\n",
      "-------------------------------\n",
      "tensor(1325.4717)\n",
      "tensor(444.3358)\n",
      "tensor(1016.8568)\n",
      "tensor(25.1500)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.416542\n",
      "Epoch 8835\n",
      "-------------------------------\n",
      "tensor(604.0381)\n",
      "tensor(195.4310)\n",
      "tensor(459.1781)\n",
      "tensor(10.9952)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.426922\n",
      "Epoch 8836\n",
      "-------------------------------\n",
      "tensor(493.9433)\n",
      "tensor(160.5052)\n",
      "tensor(374.3215)\n",
      "tensor(8.8414)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.766594\n",
      "Epoch 8837\n",
      "-------------------------------\n",
      "tensor(618.0674)\n",
      "tensor(204.1737)\n",
      "tensor(472.5722)\n",
      "tensor(11.8164)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.858101\n",
      "Epoch 8838\n",
      "-------------------------------\n",
      "tensor(400.4878)\n",
      "tensor(126.2072)\n",
      "tensor(300.6577)\n",
      "tensor(7.2244)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.832424\n",
      "Epoch 8839\n",
      "-------------------------------\n",
      "tensor(171.5098)\n",
      "tensor(66.3033)\n",
      "tensor(135.6040)\n",
      "tensor(3.6895)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.964649\n",
      "Epoch 8840\n",
      "-------------------------------\n",
      "tensor(379.3360)\n",
      "tensor(134.3793)\n",
      "tensor(296.6914)\n",
      "tensor(7.6190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.514633\n",
      "Epoch 8841\n",
      "-------------------------------\n",
      "tensor(327.1485)\n",
      "tensor(114.9352)\n",
      "tensor(254.6820)\n",
      "tensor(6.4620)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.639996\n",
      "Epoch 8842\n",
      "-------------------------------\n",
      "tensor(160.9885)\n",
      "tensor(57.3573)\n",
      "tensor(127.1657)\n",
      "tensor(3.1743)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.813225\n",
      "Epoch 8843\n",
      "-------------------------------\n",
      "tensor(98.1376)\n",
      "tensor(32.1435)\n",
      "tensor(71.5897)\n",
      "tensor(1.9245)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.429466\n",
      "Epoch 8844\n",
      "-------------------------------\n",
      "tensor(379.5634)\n",
      "tensor(129.7347)\n",
      "tensor(292.1202)\n",
      "tensor(7.6155)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.445168\n",
      "Epoch 8845\n",
      "-------------------------------\n",
      "tensor(357.7472)\n",
      "tensor(125.3002)\n",
      "tensor(277.8773)\n",
      "tensor(7.4111)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.094109\n",
      "Epoch 8846\n",
      "-------------------------------\n",
      "tensor(233.9479)\n",
      "tensor(74.0977)\n",
      "tensor(176.3068)\n",
      "tensor(3.9938)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.372337\n",
      "Epoch 8847\n",
      "-------------------------------\n",
      "tensor(458.3332)\n",
      "tensor(154.6763)\n",
      "tensor(351.1259)\n",
      "tensor(8.7817)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.933346\n",
      "Epoch 8848\n",
      "-------------------------------\n",
      "tensor(491.3564)\n",
      "tensor(158.0141)\n",
      "tensor(373.3292)\n",
      "tensor(8.8055)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.642124\n",
      "Epoch 8849\n",
      "-------------------------------\n",
      "tensor(51.4892)\n",
      "tensor(19.9751)\n",
      "tensor(12.2206)\n",
      "tensor(0.5999)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.345100\n",
      "Epoch 8850\n",
      "-------------------------------\n",
      "tensor(480.0934)\n",
      "tensor(146.8677)\n",
      "tensor(357.7322)\n",
      "tensor(8.8076)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.237179\n",
      "Epoch 8851\n",
      "-------------------------------\n",
      "tensor(847.9882)\n",
      "tensor(292.9200)\n",
      "tensor(658.2547)\n",
      "tensor(16.8993)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.523624\n",
      "Epoch 8852\n",
      "-------------------------------\n",
      "tensor(1174.6390)\n",
      "tensor(398.3688)\n",
      "tensor(911.1814)\n",
      "tensor(23.2405)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.661766\n",
      "Epoch 8853\n",
      "-------------------------------\n",
      "tensor(1421.0684)\n",
      "tensor(475.0058)\n",
      "tensor(1094.7198)\n",
      "tensor(27.4953)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.622894\n",
      "Epoch 8854\n",
      "-------------------------------\n",
      "tensor(1332.2725)\n",
      "tensor(453.6361)\n",
      "tensor(1029.8441)\n",
      "tensor(25.9483)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.490944\n",
      "Epoch 8855\n",
      "-------------------------------\n",
      "tensor(596.9821)\n",
      "tensor(195.8235)\n",
      "tensor(457.5696)\n",
      "tensor(11.2691)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.420300\n",
      "Epoch 8856\n",
      "-------------------------------\n",
      "tensor(510.1231)\n",
      "tensor(168.3511)\n",
      "tensor(389.2294)\n",
      "tensor(9.4223)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.174774\n",
      "Epoch 8857\n",
      "-------------------------------\n",
      "tensor(626.7763)\n",
      "tensor(199.5489)\n",
      "tensor(475.2246)\n",
      "tensor(11.9409)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.929756\n",
      "Epoch 8858\n",
      "-------------------------------\n",
      "tensor(405.2288)\n",
      "tensor(130.5232)\n",
      "tensor(308.5276)\n",
      "tensor(7.5244)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.946621\n",
      "Epoch 8859\n",
      "-------------------------------\n",
      "tensor(170.0394)\n",
      "tensor(62.9283)\n",
      "tensor(132.6555)\n",
      "tensor(3.5907)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.972996\n",
      "Epoch 8860\n",
      "-------------------------------\n",
      "tensor(384.0265)\n",
      "tensor(133.8410)\n",
      "tensor(297.5029)\n",
      "tensor(7.7213)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.632946\n",
      "Epoch 8861\n",
      "-------------------------------\n",
      "tensor(331.6871)\n",
      "tensor(115.0388)\n",
      "tensor(256.4623)\n",
      "tensor(6.6691)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.775658\n",
      "Epoch 8862\n",
      "-------------------------------\n",
      "tensor(167.7207)\n",
      "tensor(58.5018)\n",
      "tensor(128.4138)\n",
      "tensor(3.4217)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.915787\n",
      "Epoch 8863\n",
      "-------------------------------\n",
      "tensor(94.1030)\n",
      "tensor(31.7518)\n",
      "tensor(72.3080)\n",
      "tensor(1.6712)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.466537\n",
      "Epoch 8864\n",
      "-------------------------------\n",
      "tensor(380.5025)\n",
      "tensor(131.1526)\n",
      "tensor(297.1264)\n",
      "tensor(7.4173)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.523140\n",
      "Epoch 8865\n",
      "-------------------------------\n",
      "tensor(367.8476)\n",
      "tensor(130.6063)\n",
      "tensor(286.1743)\n",
      "tensor(7.2937)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.258495\n",
      "Epoch 8866\n",
      "-------------------------------\n",
      "tensor(238.1027)\n",
      "tensor(70.0960)\n",
      "tensor(173.8205)\n",
      "tensor(4.0450)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.395641\n",
      "Epoch 8867\n",
      "-------------------------------\n",
      "tensor(480.1224)\n",
      "tensor(151.2092)\n",
      "tensor(359.9363)\n",
      "tensor(8.7319)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.139191\n",
      "Epoch 8868\n",
      "-------------------------------\n",
      "tensor(510.6828)\n",
      "tensor(163.4267)\n",
      "tensor(382.1877)\n",
      "tensor(9.3611)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.797924\n",
      "Epoch 8869\n",
      "-------------------------------\n",
      "tensor(58.4658)\n",
      "tensor(16.9679)\n",
      "tensor(23.7873)\n",
      "tensor(0.4834)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.311703\n",
      "Epoch 8870\n",
      "-------------------------------\n",
      "tensor(466.2434)\n",
      "tensor(152.9811)\n",
      "tensor(355.6820)\n",
      "tensor(8.4463)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.093922\n",
      "Epoch 8871\n",
      "-------------------------------\n",
      "tensor(847.5750)\n",
      "tensor(273.2025)\n",
      "tensor(644.5694)\n",
      "tensor(16.1781)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.120304\n",
      "Epoch 8872\n",
      "-------------------------------\n",
      "tensor(1159.1140)\n",
      "tensor(403.7241)\n",
      "tensor(903.1852)\n",
      "tensor(23.2865)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 65.597191\n",
      "Epoch 8873\n",
      "-------------------------------\n",
      "tensor(1409.5090)\n",
      "tensor(471.3054)\n",
      "tensor(1086.6053)\n",
      "tensor(27.0963)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.140091\n",
      "Epoch 8874\n",
      "-------------------------------\n",
      "tensor(1325.8914)\n",
      "tensor(438.1104)\n",
      "tensor(1014.9852)\n",
      "tensor(25.2568)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.296410\n",
      "Epoch 8875\n",
      "-------------------------------\n",
      "tensor(576.1825)\n",
      "tensor(204.0729)\n",
      "tensor(448.5958)\n",
      "tensor(11.3464)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.129059\n",
      "Epoch 8876\n",
      "-------------------------------\n",
      "tensor(505.3900)\n",
      "tensor(180.3807)\n",
      "tensor(394.1856)\n",
      "tensor(10.0923)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.397163\n",
      "Epoch 8877\n",
      "-------------------------------\n",
      "tensor(605.0922)\n",
      "tensor(196.3466)\n",
      "tensor(464.1408)\n",
      "tensor(11.3483)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.598007\n",
      "Epoch 8878\n",
      "-------------------------------\n",
      "tensor(415.6409)\n",
      "tensor(136.7525)\n",
      "tensor(318.5414)\n",
      "tensor(7.7439)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.101673\n",
      "Epoch 8879\n",
      "-------------------------------\n",
      "tensor(150.1188)\n",
      "tensor(49.5453)\n",
      "tensor(116.3828)\n",
      "tensor(3.0124)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.723030\n",
      "Epoch 8880\n",
      "-------------------------------\n",
      "tensor(376.4711)\n",
      "tensor(122.5859)\n",
      "tensor(289.0635)\n",
      "tensor(7.1729)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.393547\n",
      "Epoch 8881\n",
      "-------------------------------\n",
      "tensor(336.7404)\n",
      "tensor(107.4163)\n",
      "tensor(256.6925)\n",
      "tensor(6.2467)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.750305\n",
      "Epoch 8882\n",
      "-------------------------------\n",
      "tensor(180.6866)\n",
      "tensor(54.0546)\n",
      "tensor(134.8769)\n",
      "tensor(3.1174)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.950504\n",
      "Epoch 8883\n",
      "-------------------------------\n",
      "tensor(78.1116)\n",
      "tensor(33.4124)\n",
      "tensor(62.1287)\n",
      "tensor(1.8721)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.387589\n",
      "Epoch 8884\n",
      "-------------------------------\n",
      "tensor(370.3899)\n",
      "tensor(130.8401)\n",
      "tensor(288.5142)\n",
      "tensor(7.5701)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.346497\n",
      "Epoch 8885\n",
      "-------------------------------\n",
      "tensor(372.6048)\n",
      "tensor(128.1959)\n",
      "tensor(287.8502)\n",
      "tensor(7.4902)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.328362\n",
      "Epoch 8886\n",
      "-------------------------------\n",
      "tensor(209.7677)\n",
      "tensor(74.5422)\n",
      "tensor(164.4073)\n",
      "tensor(4.0398)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.202721\n",
      "Epoch 8887\n",
      "-------------------------------\n",
      "tensor(468.9947)\n",
      "tensor(169.6131)\n",
      "tensor(368.4386)\n",
      "tensor(9.4905)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.198742\n",
      "Epoch 8888\n",
      "-------------------------------\n",
      "tensor(483.8920)\n",
      "tensor(147.2855)\n",
      "tensor(361.9318)\n",
      "tensor(8.4637)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.293182\n",
      "Epoch 8889\n",
      "-------------------------------\n",
      "tensor(51.2355)\n",
      "tensor(14.9962)\n",
      "tensor(12.7468)\n",
      "tensor(0.0007)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.206745\n",
      "Epoch 8890\n",
      "-------------------------------\n",
      "tensor(518.4378)\n",
      "tensor(159.5453)\n",
      "tensor(384.4797)\n",
      "tensor(9.4785)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.925095\n",
      "Epoch 8891\n",
      "-------------------------------\n",
      "tensor(886.1807)\n",
      "tensor(308.7448)\n",
      "tensor(687.4869)\n",
      "tensor(17.4755)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.871437\n",
      "Epoch 8892\n",
      "-------------------------------\n",
      "tensor(1220.1119)\n",
      "tensor(402.7405)\n",
      "tensor(938.4714)\n",
      "tensor(23.5622)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 68.876656\n",
      "Epoch 8893\n",
      "-------------------------------\n",
      "tensor(1446.2059)\n",
      "tensor(494.3812)\n",
      "tensor(1124.8912)\n",
      "tensor(28.7191)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 82.674385\n",
      "Epoch 8894\n",
      "-------------------------------\n",
      "tensor(1340.2003)\n",
      "tensor(458.1300)\n",
      "tensor(1038.7711)\n",
      "tensor(26.3478)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.994156\n",
      "Epoch 8895\n",
      "-------------------------------\n",
      "tensor(586.2565)\n",
      "tensor(183.5016)\n",
      "tensor(440.7505)\n",
      "tensor(10.4522)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.979847\n",
      "Epoch 8896\n",
      "-------------------------------\n",
      "tensor(530.6163)\n",
      "tensor(171.3698)\n",
      "tensor(403.2596)\n",
      "tensor(9.8532)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.623257\n",
      "Epoch 8897\n",
      "-------------------------------\n",
      "tensor(611.5643)\n",
      "tensor(203.7963)\n",
      "tensor(470.3395)\n",
      "tensor(11.5142)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.660114\n",
      "Epoch 8898\n",
      "-------------------------------\n",
      "tensor(421.9247)\n",
      "tensor(136.5390)\n",
      "tensor(322.6055)\n",
      "tensor(7.6521)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.193123\n",
      "Epoch 8899\n",
      "-------------------------------\n",
      "tensor(155.6931)\n",
      "tensor(57.8090)\n",
      "tensor(121.4768)\n",
      "tensor(3.4021)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.800339\n",
      "Epoch 8900\n",
      "-------------------------------\n",
      "tensor(382.5755)\n",
      "tensor(132.4122)\n",
      "tensor(295.4553)\n",
      "tensor(7.6561)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.532097\n",
      "Epoch 8901\n",
      "-------------------------------\n",
      "tensor(339.0998)\n",
      "tensor(116.3514)\n",
      "tensor(260.6431)\n",
      "tensor(6.7070)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.803513\n",
      "Epoch 8902\n",
      "-------------------------------\n",
      "tensor(178.3821)\n",
      "tensor(61.0503)\n",
      "tensor(135.3153)\n",
      "tensor(3.5119)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.921551\n",
      "Epoch 8903\n",
      "-------------------------------\n",
      "tensor(83.7274)\n",
      "tensor(28.6143)\n",
      "tensor(65.7866)\n",
      "tensor(1.5762)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.357986\n",
      "Epoch 8904\n",
      "-------------------------------\n",
      "tensor(381.1964)\n",
      "tensor(130.2554)\n",
      "tensor(295.7590)\n",
      "tensor(7.3836)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.443584\n",
      "Epoch 8905\n",
      "-------------------------------\n",
      "tensor(379.4572)\n",
      "tensor(131.5034)\n",
      "tensor(292.1521)\n",
      "tensor(7.3492)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.407780\n",
      "Epoch 8906\n",
      "-------------------------------\n",
      "tensor(226.1147)\n",
      "tensor(70.0804)\n",
      "tensor(168.2192)\n",
      "tensor(4.0593)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.284657\n",
      "Epoch 8907\n",
      "-------------------------------\n",
      "tensor(491.8792)\n",
      "tensor(158.0754)\n",
      "tensor(371.0087)\n",
      "tensor(9.0428)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.384438\n",
      "Epoch 8908\n",
      "-------------------------------\n",
      "tensor(504.8223)\n",
      "tensor(161.9045)\n",
      "tensor(378.6888)\n",
      "tensor(9.4121)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.677574\n",
      "Epoch 8909\n",
      "-------------------------------\n",
      "tensor(47.6715)\n",
      "tensor(14.5633)\n",
      "tensor(7.4580)\n",
      "tensor(0.1008)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.185341\n",
      "Epoch 8910\n",
      "-------------------------------\n",
      "tensor(496.4271)\n",
      "tensor(164.1010)\n",
      "tensor(375.7568)\n",
      "tensor(9.1047)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.539726\n",
      "Epoch 8911\n",
      "-------------------------------\n",
      "tensor(879.1823)\n",
      "tensor(286.6690)\n",
      "tensor(673.0289)\n",
      "tensor(16.9019)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.408882\n",
      "Epoch 8912\n",
      "-------------------------------\n",
      "tensor(1196.0999)\n",
      "tensor(413.3834)\n",
      "tensor(931.5167)\n",
      "tensor(23.9640)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.586487\n",
      "Epoch 8913\n",
      "-------------------------------\n",
      "tensor(1438.4147)\n",
      "tensor(486.4766)\n",
      "tensor(1114.3989)\n",
      "tensor(28.0870)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 82.206688\n",
      "Epoch 8914\n",
      "-------------------------------\n",
      "tensor(1346.2983)\n",
      "tensor(447.0428)\n",
      "tensor(1032.2200)\n",
      "tensor(25.7814)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.506889\n",
      "Epoch 8915\n",
      "-------------------------------\n",
      "tensor(582.0244)\n",
      "tensor(200.8425)\n",
      "tensor(449.5459)\n",
      "tensor(11.1393)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.116535\n",
      "Epoch 8916\n",
      "-------------------------------\n",
      "tensor(519.0857)\n",
      "tensor(181.9758)\n",
      "tensor(403.1390)\n",
      "tensor(10.2333)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.593899\n",
      "Epoch 8917\n",
      "-------------------------------\n",
      "tensor(611.6631)\n",
      "tensor(198.8972)\n",
      "tensor(468.9623)\n",
      "tensor(11.4037)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.675812\n",
      "Epoch 8918\n",
      "-------------------------------\n",
      "tensor(430.3593)\n",
      "tensor(140.4596)\n",
      "tensor(325.3659)\n",
      "tensor(7.8427)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.200558\n",
      "Epoch 8919\n",
      "-------------------------------\n",
      "tensor(163.1540)\n",
      "tensor(53.0665)\n",
      "tensor(119.1304)\n",
      "tensor(2.9969)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.699154\n",
      "Epoch 8920\n",
      "-------------------------------\n",
      "tensor(385.7160)\n",
      "tensor(125.4678)\n",
      "tensor(294.0443)\n",
      "tensor(7.2042)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.458149\n",
      "Epoch 8921\n",
      "-------------------------------\n",
      "tensor(342.9810)\n",
      "tensor(109.8556)\n",
      "tensor(260.4534)\n",
      "tensor(6.2803)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.764614\n",
      "Epoch 8922\n",
      "-------------------------------\n",
      "tensor(183.3812)\n",
      "tensor(55.6883)\n",
      "tensor(136.2315)\n",
      "tensor(3.1361)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.889805\n",
      "Epoch 8923\n",
      "-------------------------------\n",
      "tensor(81.7905)\n",
      "tensor(33.1493)\n",
      "tensor(64.0447)\n",
      "tensor(1.8727)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.310345\n",
      "Epoch 8924\n",
      "-------------------------------\n",
      "tensor(379.3840)\n",
      "tensor(131.6743)\n",
      "tensor(293.6573)\n",
      "tensor(7.5794)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.395058\n",
      "Epoch 8925\n",
      "-------------------------------\n",
      "tensor(379.6063)\n",
      "tensor(128.4944)\n",
      "tensor(291.4024)\n",
      "tensor(7.4288)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.349823\n",
      "Epoch 8926\n",
      "-------------------------------\n",
      "tensor(218.6457)\n",
      "tensor(77.5103)\n",
      "tensor(170.1596)\n",
      "tensor(4.2565)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.187279\n",
      "Epoch 8927\n",
      "-------------------------------\n",
      "tensor(486.3450)\n",
      "tensor(162.8718)\n",
      "tensor(370.5882)\n",
      "tensor(9.5106)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.259075\n",
      "Epoch 8928\n",
      "-------------------------------\n",
      "tensor(510.3884)\n",
      "tensor(156.1395)\n",
      "tensor(381.4521)\n",
      "tensor(8.9098)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.793545\n",
      "Epoch 8929\n",
      "-------------------------------\n",
      "tensor(55.1108)\n",
      "tensor(20.1052)\n",
      "tensor(9.8715)\n",
      "tensor(0.4796)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.164314\n",
      "Epoch 8930\n",
      "-------------------------------\n",
      "tensor(494.6379)\n",
      "tensor(153.2126)\n",
      "tensor(369.0732)\n",
      "tensor(9.0707)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.341911\n",
      "Epoch 8931\n",
      "-------------------------------\n",
      "tensor(875.1791)\n",
      "tensor(299.2002)\n",
      "tensor(675.9542)\n",
      "tensor(16.9972)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.371971\n",
      "Epoch 8932\n",
      "-------------------------------\n",
      "tensor(1219.5344)\n",
      "tensor(402.6024)\n",
      "tensor(934.7133)\n",
      "tensor(23.3209)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 68.408157\n",
      "Epoch 8933\n",
      "-------------------------------\n",
      "tensor(1469.6735)\n",
      "tensor(495.9977)\n",
      "tensor(1134.3101)\n",
      "tensor(28.4155)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 83.835403\n",
      "Epoch 8934\n",
      "-------------------------------\n",
      "tensor(1382.5599)\n",
      "tensor(464.8766)\n",
      "tensor(1069.2649)\n",
      "tensor(26.8382)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.444550\n",
      "Epoch 8935\n",
      "-------------------------------\n",
      "tensor(613.7600)\n",
      "tensor(205.5833)\n",
      "tensor(472.9765)\n",
      "tensor(12.1258)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.750668\n",
      "Epoch 8936\n",
      "-------------------------------\n",
      "tensor(532.7944)\n",
      "tensor(174.2735)\n",
      "tensor(406.2484)\n",
      "tensor(10.2771)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.530178\n",
      "Epoch 8937\n",
      "-------------------------------\n",
      "tensor(636.4392)\n",
      "tensor(221.3887)\n",
      "tensor(495.2842)\n",
      "tensor(12.4614)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.388908\n",
      "Epoch 8938\n",
      "-------------------------------\n",
      "tensor(420.5691)\n",
      "tensor(149.2915)\n",
      "tensor(329.8637)\n",
      "tensor(8.4357)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.243916\n",
      "Epoch 8939\n",
      "-------------------------------\n",
      "tensor(178.3318)\n",
      "tensor(54.4906)\n",
      "tensor(129.6100)\n",
      "tensor(3.0876)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.850586\n",
      "Epoch 8940\n",
      "-------------------------------\n",
      "tensor(403.6418)\n",
      "tensor(131.9797)\n",
      "tensor(305.7069)\n",
      "tensor(7.6103)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.757759\n",
      "Epoch 8941\n",
      "-------------------------------\n",
      "tensor(350.5282)\n",
      "tensor(116.2704)\n",
      "tensor(266.5914)\n",
      "tensor(6.7479)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.878845\n",
      "Epoch 8942\n",
      "-------------------------------\n",
      "tensor(178.8991)\n",
      "tensor(60.3900)\n",
      "tensor(135.5155)\n",
      "tensor(3.5699)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.855362\n",
      "Epoch 8943\n",
      "-------------------------------\n",
      "tensor(102.2732)\n",
      "tensor(32.3207)\n",
      "tensor(72.0633)\n",
      "tensor(1.5142)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.344231\n",
      "Epoch 8944\n",
      "-------------------------------\n",
      "tensor(404.4297)\n",
      "tensor(132.3981)\n",
      "tensor(304.6332)\n",
      "tensor(7.2766)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.691120\n",
      "Epoch 8945\n",
      "-------------------------------\n",
      "tensor(385.8147)\n",
      "tensor(127.5503)\n",
      "tensor(293.2856)\n",
      "tensor(7.1339)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.413364\n",
      "Epoch 8946\n",
      "-------------------------------\n",
      "tensor(240.7902)\n",
      "tensor(78.9768)\n",
      "tensor(181.4306)\n",
      "tensor(4.3949)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.375961\n",
      "Epoch 8947\n",
      "-------------------------------\n",
      "tensor(488.3554)\n",
      "tensor(161.1965)\n",
      "tensor(373.3898)\n",
      "tensor(9.0594)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.453011\n",
      "Epoch 8948\n",
      "-------------------------------\n",
      "tensor(513.2350)\n",
      "tensor(162.8239)\n",
      "tensor(387.2028)\n",
      "tensor(9.7336)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.944469\n",
      "Epoch 8949\n",
      "-------------------------------\n",
      "tensor(39.4539)\n",
      "tensor(14.9963)\n",
      "tensor(12.3832)\n",
      "tensor(0.5009)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.173912\n",
      "Epoch 8950\n",
      "-------------------------------\n",
      "tensor(492.9467)\n",
      "tensor(163.3333)\n",
      "tensor(377.6392)\n",
      "tensor(9.4366)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.524506\n",
      "Epoch 8951\n",
      "-------------------------------\n",
      "tensor(873.0618)\n",
      "tensor(295.7144)\n",
      "tensor(676.6637)\n",
      "tensor(17.3255)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.501190\n",
      "Epoch 8952\n",
      "-------------------------------\n",
      "tensor(1200.4951)\n",
      "tensor(402.1342)\n",
      "tensor(924.1806)\n",
      "tensor(23.2803)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.370949\n",
      "Epoch 8953\n",
      "-------------------------------\n",
      "tensor(1432.3165)\n",
      "tensor(485.6765)\n",
      "tensor(1110.0238)\n",
      "tensor(27.8948)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 81.959770\n",
      "Epoch 8954\n",
      "-------------------------------\n",
      "tensor(1326.8978)\n",
      "tensor(448.8217)\n",
      "tensor(1029.6782)\n",
      "tensor(26.0784)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.105446\n",
      "Epoch 8955\n",
      "-------------------------------\n",
      "tensor(570.2996)\n",
      "tensor(188.1959)\n",
      "tensor(436.5143)\n",
      "tensor(11.3286)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.519337\n",
      "Epoch 8956\n",
      "-------------------------------\n",
      "tensor(525.2231)\n",
      "tensor(174.0383)\n",
      "tensor(404.6999)\n",
      "tensor(10.6811)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.331158\n",
      "Epoch 8957\n",
      "-------------------------------\n",
      "tensor(610.0724)\n",
      "tensor(202.2636)\n",
      "tensor(467.8936)\n",
      "tensor(11.5154)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.488022\n",
      "Epoch 8958\n",
      "-------------------------------\n",
      "tensor(417.7676)\n",
      "tensor(141.7965)\n",
      "tensor(322.2336)\n",
      "tensor(8.2539)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.035618\n",
      "Epoch 8959\n",
      "-------------------------------\n",
      "tensor(164.9285)\n",
      "tensor(52.5531)\n",
      "tensor(119.0573)\n",
      "tensor(2.6100)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.720375\n",
      "Epoch 8960\n",
      "-------------------------------\n",
      "tensor(385.6209)\n",
      "tensor(127.6646)\n",
      "tensor(292.2191)\n",
      "tensor(7.0082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.436317\n",
      "Epoch 8961\n",
      "-------------------------------\n",
      "tensor(340.9544)\n",
      "tensor(114.4003)\n",
      "tensor(258.6183)\n",
      "tensor(6.3016)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.700653\n",
      "Epoch 8962\n",
      "-------------------------------\n",
      "tensor(179.2300)\n",
      "tensor(62.1684)\n",
      "tensor(135.2986)\n",
      "tensor(3.3696)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.818062\n",
      "Epoch 8963\n",
      "-------------------------------\n",
      "tensor(90.6359)\n",
      "tensor(26.0696)\n",
      "tensor(63.5809)\n",
      "tensor(1.4169)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.286884\n",
      "Epoch 8964\n",
      "-------------------------------\n",
      "tensor(385.9760)\n",
      "tensor(123.3668)\n",
      "tensor(290.2408)\n",
      "tensor(6.9167)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.377834\n",
      "Epoch 8965\n",
      "-------------------------------\n",
      "tensor(377.3104)\n",
      "tensor(121.2986)\n",
      "tensor(285.5536)\n",
      "tensor(6.8072)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.277496\n",
      "Epoch 8966\n",
      "-------------------------------\n",
      "tensor(222.7205)\n",
      "tensor(75.4680)\n",
      "tensor(167.6099)\n",
      "tensor(4.2367)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.230835\n",
      "Epoch 8967\n",
      "-------------------------------\n",
      "tensor(474.4796)\n",
      "tensor(154.9370)\n",
      "tensor(360.9208)\n",
      "tensor(8.7404)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.106037\n",
      "Epoch 8968\n",
      "-------------------------------\n",
      "tensor(478.4029)\n",
      "tensor(165.7752)\n",
      "tensor(371.3143)\n",
      "tensor(9.5649)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.279076\n",
      "Epoch 8969\n",
      "-------------------------------\n",
      "tensor(35.7366)\n",
      "tensor(11.2196)\n",
      "tensor(2.4310)\n",
      "tensor(0.1268)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.252735\n",
      "Epoch 8970\n",
      "-------------------------------\n",
      "tensor(491.9773)\n",
      "tensor(160.8339)\n",
      "tensor(377.1386)\n",
      "tensor(9.7082)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.640846\n",
      "Epoch 8971\n",
      "-------------------------------\n",
      "tensor(865.2592)\n",
      "tensor(284.3762)\n",
      "tensor(663.5172)\n",
      "tensor(16.4675)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.896843\n",
      "Epoch 8972\n",
      "-------------------------------\n",
      "tensor(1173.0222)\n",
      "tensor(397.3877)\n",
      "tensor(906.2488)\n",
      "tensor(22.8047)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.133575\n",
      "Epoch 8973\n",
      "-------------------------------\n",
      "tensor(1412.2039)\n",
      "tensor(472.5714)\n",
      "tensor(1085.4377)\n",
      "tensor(27.1570)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.662483\n",
      "Epoch 8974\n",
      "-------------------------------\n",
      "tensor(1319.1418)\n",
      "tensor(440.1589)\n",
      "tensor(1017.9329)\n",
      "tensor(25.4278)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 74.824203\n",
      "Epoch 8975\n",
      "-------------------------------\n",
      "tensor(562.1106)\n",
      "tensor(196.9229)\n",
      "tensor(443.0781)\n",
      "tensor(11.6006)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.780060\n",
      "Epoch 8976\n",
      "-------------------------------\n",
      "tensor(525.8536)\n",
      "tensor(177.6802)\n",
      "tensor(407.5475)\n",
      "tensor(10.1314)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.741543\n",
      "Epoch 8977\n",
      "-------------------------------\n",
      "tensor(598.4921)\n",
      "tensor(205.2483)\n",
      "tensor(463.9398)\n",
      "tensor(12.0153)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.495781\n",
      "Epoch 8978\n",
      "-------------------------------\n",
      "tensor(421.7258)\n",
      "tensor(143.7540)\n",
      "tensor(325.9941)\n",
      "tensor(8.4343)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.291252\n",
      "Epoch 8979\n",
      "-------------------------------\n",
      "tensor(153.6451)\n",
      "tensor(53.7986)\n",
      "tensor(115.5498)\n",
      "tensor(2.8950)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.796326\n",
      "Epoch 8980\n",
      "-------------------------------\n",
      "tensor(378.0742)\n",
      "tensor(131.6380)\n",
      "tensor(292.8215)\n",
      "tensor(7.5732)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.505466\n",
      "Epoch 8981\n",
      "-------------------------------\n",
      "tensor(335.5999)\n",
      "tensor(118.6845)\n",
      "tensor(261.6409)\n",
      "tensor(6.9250)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.835243\n",
      "Epoch 8982\n",
      "-------------------------------\n",
      "tensor(177.7934)\n",
      "tensor(65.8025)\n",
      "tensor(139.3636)\n",
      "tensor(3.9208)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.986465\n",
      "Epoch 8983\n",
      "-------------------------------\n",
      "tensor(82.6336)\n",
      "tensor(23.8444)\n",
      "tensor(59.7886)\n",
      "tensor(1.0538)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.396442\n",
      "Epoch 8984\n",
      "-------------------------------\n",
      "tensor(376.1564)\n",
      "tensor(124.6687)\n",
      "tensor(290.1077)\n",
      "tensor(6.9270)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.410191\n",
      "Epoch 8985\n",
      "-------------------------------\n",
      "tensor(376.7872)\n",
      "tensor(130.5156)\n",
      "tensor(294.4647)\n",
      "tensor(7.2996)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.439671\n",
      "Epoch 8986\n",
      "-------------------------------\n",
      "tensor(219.4732)\n",
      "tensor(62.7426)\n",
      "tensor(157.6826)\n",
      "tensor(3.6605)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.221390\n",
      "Epoch 8987\n",
      "-------------------------------\n",
      "tensor(485.5657)\n",
      "tensor(150.1737)\n",
      "tensor(363.3802)\n",
      "tensor(8.7734)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.354599\n",
      "Epoch 8988\n",
      "-------------------------------\n",
      "tensor(470.0949)\n",
      "tensor(165.8204)\n",
      "tensor(367.3840)\n",
      "tensor(9.1562)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.204479\n",
      "Epoch 8989\n",
      "-------------------------------\n",
      "tensor(40.2221)\n",
      "tensor(13.2798)\n",
      "tensor(13.2946)\n",
      "tensor(0.1211)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.214970\n",
      "Epoch 8990\n",
      "-------------------------------\n",
      "tensor(499.0134)\n",
      "tensor(168.5875)\n",
      "tensor(388.8921)\n",
      "tensor(9.6216)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.987957\n",
      "Epoch 8991\n",
      "-------------------------------\n",
      "tensor(849.8914)\n",
      "tensor(283.3270)\n",
      "tensor(656.1609)\n",
      "tensor(17.2860)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 51.708149\n",
      "Epoch 8992\n",
      "-------------------------------\n",
      "tensor(1126.8363)\n",
      "tensor(398.6114)\n",
      "tensor(883.8212)\n",
      "tensor(22.8314)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.159805\n",
      "Epoch 8993\n",
      "-------------------------------\n",
      "tensor(1356.2771)\n",
      "tensor(444.4616)\n",
      "tensor(1035.4198)\n",
      "tensor(25.4894)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 76.253174\n",
      "Epoch 8994\n",
      "-------------------------------\n",
      "tensor(1255.4121)\n",
      "tensor(420.6093)\n",
      "tensor(965.8936)\n",
      "tensor(23.9669)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.422249\n",
      "Epoch 8995\n",
      "-------------------------------\n",
      "tensor(548.4352)\n",
      "tensor(187.9684)\n",
      "tensor(426.5357)\n",
      "tensor(11.3528)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.061390\n",
      "Epoch 8996\n",
      "-------------------------------\n",
      "tensor(480.7309)\n",
      "tensor(161.6849)\n",
      "tensor(369.6267)\n",
      "tensor(9.4908)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.348118\n",
      "Epoch 8997\n",
      "-------------------------------\n",
      "tensor(582.9037)\n",
      "tensor(188.3530)\n",
      "tensor(444.4459)\n",
      "tensor(11.1319)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.662468\n",
      "Epoch 8998\n",
      "-------------------------------\n",
      "tensor(389.2251)\n",
      "tensor(124.2492)\n",
      "tensor(294.5386)\n",
      "tensor(7.4440)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.427834\n",
      "Epoch 8999\n",
      "-------------------------------\n",
      "tensor(161.8574)\n",
      "tensor(59.4307)\n",
      "tensor(121.3785)\n",
      "tensor(2.9760)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.699696\n",
      "Epoch 9000\n",
      "-------------------------------\n",
      "tensor(364.5605)\n",
      "tensor(127.8266)\n",
      "tensor(279.1982)\n",
      "tensor(7.0205)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.086365\n",
      "Epoch 9001\n",
      "-------------------------------\n",
      "tensor(318.0604)\n",
      "tensor(112.2785)\n",
      "tensor(243.2308)\n",
      "tensor(6.2177)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.336666\n",
      "Epoch 9002\n",
      "-------------------------------\n",
      "tensor(157.2184)\n",
      "tensor(58.6045)\n",
      "tensor(124.2522)\n",
      "tensor(3.3226)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.647705\n",
      "Epoch 9003\n",
      "-------------------------------\n",
      "tensor(93.8701)\n",
      "tensor(26.1334)\n",
      "tensor(64.6650)\n",
      "tensor(1.3583)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.278824\n",
      "Epoch 9004\n",
      "-------------------------------\n",
      "tensor(363.8994)\n",
      "tensor(119.9277)\n",
      "tensor(276.0640)\n",
      "tensor(6.7256)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.036209\n",
      "Epoch 9005\n",
      "-------------------------------\n",
      "tensor(344.9677)\n",
      "tensor(119.6636)\n",
      "tensor(266.4832)\n",
      "tensor(6.7711)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.795444\n",
      "Epoch 9006\n",
      "-------------------------------\n",
      "tensor(227.4465)\n",
      "tensor(65.4016)\n",
      "tensor(162.9436)\n",
      "tensor(3.5396)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.181526\n",
      "Epoch 9007\n",
      "-------------------------------\n",
      "tensor(455.6958)\n",
      "tensor(131.8302)\n",
      "tensor(331.5421)\n",
      "tensor(7.7613)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.397343\n",
      "Epoch 9008\n",
      "-------------------------------\n",
      "tensor(480.0432)\n",
      "tensor(158.8148)\n",
      "tensor(365.4851)\n",
      "tensor(8.8148)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.261158\n",
      "Epoch 9009\n",
      "-------------------------------\n",
      "tensor(48.4912)\n",
      "tensor(22.2970)\n",
      "tensor(29.6828)\n",
      "tensor(1.1766)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.200584\n",
      "Epoch 9010\n",
      "-------------------------------\n",
      "tensor(429.0597)\n",
      "tensor(147.2098)\n",
      "tensor(332.1860)\n",
      "tensor(8.0930)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.263382\n",
      "Epoch 9011\n",
      "-------------------------------\n",
      "tensor(791.3465)\n",
      "tensor(264.0770)\n",
      "tensor(608.3629)\n",
      "tensor(15.5782)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.121922\n",
      "Epoch 9012\n",
      "-------------------------------\n",
      "tensor(1092.9202)\n",
      "tensor(372.0816)\n",
      "tensor(848.8680)\n",
      "tensor(21.4919)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.283455\n",
      "Epoch 9013\n",
      "-------------------------------\n",
      "tensor(1325.9598)\n",
      "tensor(439.5077)\n",
      "tensor(1019.2172)\n",
      "tensor(25.3002)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.570381\n",
      "Epoch 9014\n",
      "-------------------------------\n",
      "tensor(1234.2820)\n",
      "tensor(414.0253)\n",
      "tensor(953.8721)\n",
      "tensor(23.9833)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.767197\n",
      "Epoch 9015\n",
      "-------------------------------\n",
      "tensor(527.9062)\n",
      "tensor(185.8750)\n",
      "tensor(414.4792)\n",
      "tensor(10.8992)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.580116\n",
      "Epoch 9016\n",
      "-------------------------------\n",
      "tensor(482.1770)\n",
      "tensor(166.6239)\n",
      "tensor(377.5945)\n",
      "tensor(9.6462)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.535297\n",
      "Epoch 9017\n",
      "-------------------------------\n",
      "tensor(565.9954)\n",
      "tensor(184.0740)\n",
      "tensor(430.2497)\n",
      "tensor(11.1043)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.226124\n",
      "Epoch 9018\n",
      "-------------------------------\n",
      "tensor(380.8345)\n",
      "tensor(134.2201)\n",
      "tensor(297.1569)\n",
      "tensor(7.9315)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.469254\n",
      "Epoch 9019\n",
      "-------------------------------\n",
      "tensor(151.0473)\n",
      "tensor(46.1086)\n",
      "tensor(109.0770)\n",
      "tensor(2.3728)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.650105\n",
      "Epoch 9020\n",
      "-------------------------------\n",
      "tensor(352.5048)\n",
      "tensor(116.9116)\n",
      "tensor(269.1099)\n",
      "tensor(6.6291)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.916679\n",
      "Epoch 9021\n",
      "-------------------------------\n",
      "tensor(311.6732)\n",
      "tensor(105.3053)\n",
      "tensor(238.6836)\n",
      "tensor(6.0575)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.304932\n",
      "Epoch 9022\n",
      "-------------------------------\n",
      "tensor(162.5589)\n",
      "tensor(56.9344)\n",
      "tensor(125.1492)\n",
      "tensor(3.3644)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.739758\n",
      "Epoch 9023\n",
      "-------------------------------\n",
      "tensor(82.2830)\n",
      "tensor(24.6892)\n",
      "tensor(58.0120)\n",
      "tensor(1.0855)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.289467\n",
      "Epoch 9024\n",
      "-------------------------------\n",
      "tensor(352.0467)\n",
      "tensor(115.0221)\n",
      "tensor(267.5349)\n",
      "tensor(6.2753)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.883621\n",
      "Epoch 9025\n",
      "-------------------------------\n",
      "tensor(345.5345)\n",
      "tensor(115.8725)\n",
      "tensor(266.5265)\n",
      "tensor(6.4048)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.807201\n",
      "Epoch 9026\n",
      "-------------------------------\n",
      "tensor(204.4881)\n",
      "tensor(62.7642)\n",
      "tensor(150.7498)\n",
      "tensor(3.6400)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.010769\n",
      "Epoch 9027\n",
      "-------------------------------\n",
      "tensor(442.7308)\n",
      "tensor(138.7600)\n",
      "tensor(332.2214)\n",
      "tensor(7.9196)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.390694\n",
      "Epoch 9028\n",
      "-------------------------------\n",
      "tensor(446.6685)\n",
      "tensor(146.2444)\n",
      "tensor(339.9691)\n",
      "tensor(8.6464)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.521496\n",
      "Epoch 9029\n",
      "-------------------------------\n",
      "tensor(49.0830)\n",
      "tensor(16.7944)\n",
      "tensor(3.2989)\n",
      "tensor(0.0466)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.173199\n",
      "Epoch 9030\n",
      "-------------------------------\n",
      "tensor(448.2649)\n",
      "tensor(148.0470)\n",
      "tensor(344.0176)\n",
      "tensor(8.4775)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.638008\n",
      "Epoch 9031\n",
      "-------------------------------\n",
      "tensor(790.9069)\n",
      "tensor(258.8112)\n",
      "tensor(603.7169)\n",
      "tensor(15.2959)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.042553\n",
      "Epoch 9032\n",
      "-------------------------------\n",
      "tensor(1078.3259)\n",
      "tensor(366.9796)\n",
      "tensor(832.1209)\n",
      "tensor(20.8000)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.196774\n",
      "Epoch 9033\n",
      "-------------------------------\n",
      "tensor(1301.1453)\n",
      "tensor(427.4849)\n",
      "tensor(997.9247)\n",
      "tensor(24.6133)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 73.244789\n",
      "Epoch 9034\n",
      "-------------------------------\n",
      "tensor(1200.9823)\n",
      "tensor(409.5036)\n",
      "tensor(935.3556)\n",
      "tensor(23.9092)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.027420\n",
      "Epoch 9035\n",
      "-------------------------------\n",
      "tensor(511.6754)\n",
      "tensor(182.0664)\n",
      "tensor(400.7384)\n",
      "tensor(10.4139)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.162056\n",
      "Epoch 9036\n",
      "-------------------------------\n",
      "tensor(466.7684)\n",
      "tensor(171.0929)\n",
      "tensor(372.4028)\n",
      "tensor(9.9661)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.184402\n",
      "Epoch 9037\n",
      "-------------------------------\n",
      "tensor(555.7621)\n",
      "tensor(181.1418)\n",
      "tensor(421.7760)\n",
      "tensor(10.4291)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.984272\n",
      "Epoch 9038\n",
      "-------------------------------\n",
      "tensor(376.1830)\n",
      "tensor(129.7341)\n",
      "tensor(291.3384)\n",
      "tensor(7.7471)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.362343\n",
      "Epoch 9039\n",
      "-------------------------------\n",
      "tensor(149.7065)\n",
      "tensor(44.7904)\n",
      "tensor(107.7203)\n",
      "tensor(2.0328)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.687656\n",
      "Epoch 9040\n",
      "-------------------------------\n",
      "tensor(349.0568)\n",
      "tensor(112.7894)\n",
      "tensor(264.6105)\n",
      "tensor(6.0785)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.901672\n",
      "Epoch 9041\n",
      "-------------------------------\n",
      "tensor(307.4521)\n",
      "tensor(101.0812)\n",
      "tensor(234.5672)\n",
      "tensor(5.5124)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.295849\n",
      "Epoch 9042\n",
      "-------------------------------\n",
      "tensor(161.9593)\n",
      "tensor(54.5495)\n",
      "tensor(123.3272)\n",
      "tensor(2.9258)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.754948\n",
      "Epoch 9043\n",
      "-------------------------------\n",
      "tensor(81.1368)\n",
      "tensor(23.8105)\n",
      "tensor(55.7718)\n",
      "tensor(1.3147)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.273857\n",
      "Epoch 9044\n",
      "-------------------------------\n",
      "tensor(349.5739)\n",
      "tensor(110.7660)\n",
      "tensor(261.0421)\n",
      "tensor(6.2031)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.785877\n",
      "Epoch 9045\n",
      "-------------------------------\n",
      "tensor(345.6915)\n",
      "tensor(108.6791)\n",
      "tensor(257.9302)\n",
      "tensor(6.0594)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.722931\n",
      "Epoch 9046\n",
      "-------------------------------\n",
      "tensor(199.1097)\n",
      "tensor(69.9148)\n",
      "tensor(152.5226)\n",
      "tensor(3.8746)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.046627\n",
      "Epoch 9047\n",
      "-------------------------------\n",
      "tensor(443.6802)\n",
      "tensor(135.7402)\n",
      "tensor(327.9991)\n",
      "tensor(7.8321)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.332413\n",
      "Epoch 9048\n",
      "-------------------------------\n",
      "tensor(444.4897)\n",
      "tensor(149.3665)\n",
      "tensor(344.1443)\n",
      "tensor(8.6947)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.550980\n",
      "Epoch 9049\n",
      "-------------------------------\n",
      "tensor(46.7184)\n",
      "tensor(16.0407)\n",
      "tensor(8.1616)\n",
      "tensor(0.4163)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.250504\n",
      "Epoch 9050\n",
      "-------------------------------\n",
      "tensor(461.6719)\n",
      "tensor(149.9095)\n",
      "tensor(351.3132)\n",
      "tensor(9.1897)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.946003\n",
      "Epoch 9051\n",
      "-------------------------------\n",
      "tensor(804.2376)\n",
      "tensor(274.7465)\n",
      "tensor(623.5519)\n",
      "tensor(15.7759)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.820259\n",
      "Epoch 9052\n",
      "-------------------------------\n",
      "tensor(1105.9431)\n",
      "tensor(364.4849)\n",
      "tensor(846.8995)\n",
      "tensor(21.0567)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.442894\n",
      "Epoch 9053\n",
      "-------------------------------\n",
      "tensor(1323.4143)\n",
      "tensor(444.4703)\n",
      "tensor(1021.0814)\n",
      "tensor(25.3692)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.653145\n",
      "Epoch 9054\n",
      "-------------------------------\n",
      "tensor(1237.3812)\n",
      "tensor(415.4631)\n",
      "tensor(958.3154)\n",
      "tensor(24.2267)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.798477\n",
      "Epoch 9055\n",
      "-------------------------------\n",
      "tensor(545.9461)\n",
      "tensor(187.7627)\n",
      "tensor(422.8348)\n",
      "tensor(11.0499)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.015293\n",
      "Epoch 9056\n",
      "-------------------------------\n",
      "tensor(470.2117)\n",
      "tensor(159.9205)\n",
      "tensor(363.6157)\n",
      "tensor(9.5920)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.294731\n",
      "Epoch 9057\n",
      "-------------------------------\n",
      "tensor(565.6291)\n",
      "tensor(195.9976)\n",
      "tensor(441.5974)\n",
      "tensor(10.9623)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.579880\n",
      "Epoch 9058\n",
      "-------------------------------\n",
      "tensor(372.6595)\n",
      "tensor(135.7760)\n",
      "tensor(296.5123)\n",
      "tensor(7.6683)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.490479\n",
      "Epoch 9059\n",
      "-------------------------------\n",
      "tensor(161.7857)\n",
      "tensor(43.7574)\n",
      "tensor(112.8068)\n",
      "tensor(2.5107)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.778008\n",
      "Epoch 9060\n",
      "-------------------------------\n",
      "tensor(362.2330)\n",
      "tensor(113.3937)\n",
      "tensor(270.0083)\n",
      "tensor(6.5673)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.072033\n",
      "Epoch 9061\n",
      "-------------------------------\n",
      "tensor(313.7775)\n",
      "tensor(99.7924)\n",
      "tensor(235.7086)\n",
      "tensor(5.8589)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.371220\n",
      "Epoch 9062\n",
      "-------------------------------\n",
      "tensor(162.4556)\n",
      "tensor(51.2627)\n",
      "tensor(119.9558)\n",
      "tensor(3.0965)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.775391\n",
      "Epoch 9063\n",
      "-------------------------------\n",
      "tensor(88.6606)\n",
      "tensor(30.3671)\n",
      "tensor(63.7860)\n",
      "tensor(1.3524)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.355728\n",
      "Epoch 9064\n",
      "-------------------------------\n",
      "tensor(357.2435)\n",
      "tensor(117.8478)\n",
      "tensor(270.4135)\n",
      "tensor(6.4005)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.981140\n",
      "Epoch 9065\n",
      "-------------------------------\n",
      "tensor(344.5959)\n",
      "tensor(112.6216)\n",
      "tensor(261.3644)\n",
      "tensor(6.2232)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.795414\n",
      "Epoch 9066\n",
      "-------------------------------\n",
      "tensor(209.0729)\n",
      "tensor(72.2763)\n",
      "tensor(160.1289)\n",
      "tensor(4.0256)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.131645\n",
      "Epoch 9067\n",
      "-------------------------------\n",
      "tensor(437.2834)\n",
      "tensor(147.3642)\n",
      "tensor(335.3333)\n",
      "tensor(8.1290)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.410065\n",
      "Epoch 9068\n",
      "-------------------------------\n",
      "tensor(457.9699)\n",
      "tensor(141.2238)\n",
      "tensor(343.4762)\n",
      "tensor(8.6772)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.728706\n",
      "Epoch 9069\n",
      "-------------------------------\n",
      "tensor(48.2970)\n",
      "tensor(16.6714)\n",
      "tensor(11.2127)\n",
      "tensor(0.5604)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.241623\n",
      "Epoch 9070\n",
      "-------------------------------\n",
      "tensor(446.6617)\n",
      "tensor(148.6155)\n",
      "tensor(343.4810)\n",
      "tensor(8.5689)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.709579\n",
      "Epoch 9071\n",
      "-------------------------------\n",
      "tensor(790.4396)\n",
      "tensor(269.3398)\n",
      "tensor(613.3676)\n",
      "tensor(15.8478)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.326271\n",
      "Epoch 9072\n",
      "-------------------------------\n",
      "tensor(1094.2738)\n",
      "tensor(362.5174)\n",
      "tensor(839.1235)\n",
      "tensor(20.9738)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.906944\n",
      "Epoch 9073\n",
      "-------------------------------\n",
      "tensor(1318.0127)\n",
      "tensor(444.9423)\n",
      "tensor(1016.6307)\n",
      "tensor(25.3595)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.202873\n",
      "Epoch 9074\n",
      "-------------------------------\n",
      "tensor(1246.5400)\n",
      "tensor(417.1127)\n",
      "tensor(958.9588)\n",
      "tensor(23.8583)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.021225\n",
      "Epoch 9075\n",
      "-------------------------------\n",
      "tensor(569.9592)\n",
      "tensor(182.3576)\n",
      "tensor(433.1343)\n",
      "tensor(10.9668)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.362686\n",
      "Epoch 9076\n",
      "-------------------------------\n",
      "tensor(457.6585)\n",
      "tensor(151.5576)\n",
      "tensor(350.5995)\n",
      "tensor(8.7455)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.895470\n",
      "Epoch 9077\n",
      "-------------------------------\n",
      "tensor(573.9672)\n",
      "tensor(195.1362)\n",
      "tensor(443.5741)\n",
      "tensor(11.1792)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.563984\n",
      "Epoch 9078\n",
      "-------------------------------\n",
      "tensor(370.1106)\n",
      "tensor(126.0721)\n",
      "tensor(284.9368)\n",
      "tensor(7.2843)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.188499\n",
      "Epoch 9079\n",
      "-------------------------------\n",
      "tensor(165.0687)\n",
      "tensor(54.4914)\n",
      "tensor(124.1836)\n",
      "tensor(2.9094)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.746567\n",
      "Epoch 9080\n",
      "-------------------------------\n",
      "tensor(361.9841)\n",
      "tensor(121.8729)\n",
      "tensor(275.8502)\n",
      "tensor(6.7450)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.051315\n",
      "Epoch 9081\n",
      "-------------------------------\n",
      "tensor(311.3261)\n",
      "tensor(106.1522)\n",
      "tensor(237.4016)\n",
      "tensor(5.8631)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.273251\n",
      "Epoch 9082\n",
      "-------------------------------\n",
      "tensor(153.9259)\n",
      "tensor(54.8961)\n",
      "tensor(118.5365)\n",
      "tensor(2.9789)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.655476\n",
      "Epoch 9083\n",
      "-------------------------------\n",
      "tensor(96.1216)\n",
      "tensor(28.1860)\n",
      "tensor(66.8608)\n",
      "tensor(1.5645)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.293587\n",
      "Epoch 9084\n",
      "-------------------------------\n",
      "tensor(362.1299)\n",
      "tensor(116.7555)\n",
      "tensor(273.0489)\n",
      "tensor(6.6541)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.960117\n",
      "Epoch 9085\n",
      "-------------------------------\n",
      "tensor(341.3475)\n",
      "tensor(111.1632)\n",
      "tensor(259.1670)\n",
      "tensor(6.3814)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.686249\n",
      "Epoch 9086\n",
      "-------------------------------\n",
      "tensor(218.3630)\n",
      "tensor(73.3175)\n",
      "tensor(165.2239)\n",
      "tensor(4.0058)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.198296\n",
      "Epoch 9087\n",
      "-------------------------------\n",
      "tensor(433.1944)\n",
      "tensor(143.2740)\n",
      "tensor(329.7970)\n",
      "tensor(8.0063)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.333210\n",
      "Epoch 9088\n",
      "-------------------------------\n",
      "tensor(455.4892)\n",
      "tensor(153.1884)\n",
      "tensor(350.9606)\n",
      "tensor(8.7573)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.771313\n",
      "Epoch 9089\n",
      "-------------------------------\n",
      "tensor(45.4769)\n",
      "tensor(14.7288)\n",
      "tensor(9.1077)\n",
      "tensor(0.3623)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.225979\n",
      "Epoch 9090\n",
      "-------------------------------\n",
      "tensor(450.1841)\n",
      "tensor(141.9399)\n",
      "tensor(339.0798)\n",
      "tensor(8.6161)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.647377\n",
      "Epoch 9091\n",
      "-------------------------------\n",
      "tensor(799.2494)\n",
      "tensor(269.6603)\n",
      "tensor(616.5159)\n",
      "tensor(15.5196)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.531605\n",
      "Epoch 9092\n",
      "-------------------------------\n",
      "tensor(1103.1476)\n",
      "tensor(369.9678)\n",
      "tensor(853.3258)\n",
      "tensor(21.3218)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.655201\n",
      "Epoch 9093\n",
      "-------------------------------\n",
      "tensor(1332.8651)\n",
      "tensor(445.6223)\n",
      "tensor(1027.0505)\n",
      "tensor(25.9282)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.008347\n",
      "Epoch 9094\n",
      "-------------------------------\n",
      "tensor(1247.4734)\n",
      "tensor(426.8394)\n",
      "tensor(971.4518)\n",
      "tensor(24.5651)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.532257\n",
      "Epoch 9095\n",
      "-------------------------------\n",
      "tensor(563.1935)\n",
      "tensor(182.6948)\n",
      "tensor(428.9745)\n",
      "tensor(10.6872)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.216892\n",
      "Epoch 9096\n",
      "-------------------------------\n",
      "tensor(474.9751)\n",
      "tensor(155.9910)\n",
      "tensor(362.9729)\n",
      "tensor(9.0864)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.144382\n",
      "Epoch 9097\n",
      "-------------------------------\n",
      "tensor(591.2843)\n",
      "tensor(190.3573)\n",
      "tensor(445.8769)\n",
      "tensor(11.0805)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.674232\n",
      "Epoch 9098\n",
      "-------------------------------\n",
      "tensor(372.3180)\n",
      "tensor(127.6994)\n",
      "tensor(286.8436)\n",
      "tensor(7.3347)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.237484\n",
      "Epoch 9099\n",
      "-------------------------------\n",
      "tensor(168.0323)\n",
      "tensor(53.4850)\n",
      "tensor(124.5388)\n",
      "tensor(2.8612)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.721466\n",
      "Epoch 9100\n",
      "-------------------------------\n",
      "tensor(363.8080)\n",
      "tensor(120.3030)\n",
      "tensor(277.6411)\n",
      "tensor(6.7368)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.015678\n",
      "Epoch 9101\n",
      "-------------------------------\n",
      "tensor(313.9644)\n",
      "tensor(104.7618)\n",
      "tensor(239.2125)\n",
      "tensor(5.8713)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.238480\n",
      "Epoch 9102\n",
      "-------------------------------\n",
      "tensor(157.3979)\n",
      "tensor(53.6859)\n",
      "tensor(119.4989)\n",
      "tensor(2.9977)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.600437\n",
      "Epoch 9103\n",
      "-------------------------------\n",
      "tensor(98.7777)\n",
      "tensor(30.2041)\n",
      "tensor(67.4033)\n",
      "tensor(1.5185)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.254257\n",
      "Epoch 9104\n",
      "-------------------------------\n",
      "tensor(367.3740)\n",
      "tensor(118.7313)\n",
      "tensor(274.5729)\n",
      "tensor(6.5443)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.005905\n",
      "Epoch 9105\n",
      "-------------------------------\n",
      "tensor(350.4630)\n",
      "tensor(113.7810)\n",
      "tensor(260.8124)\n",
      "tensor(6.2226)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.678600\n",
      "Epoch 9106\n",
      "-------------------------------\n",
      "tensor(225.3857)\n",
      "tensor(74.1001)\n",
      "tensor(170.3282)\n",
      "tensor(4.1775)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.181335\n",
      "Epoch 9107\n",
      "-------------------------------\n",
      "tensor(441.2886)\n",
      "tensor(141.7465)\n",
      "tensor(332.4421)\n",
      "tensor(7.9234)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.348175\n",
      "Epoch 9108\n",
      "-------------------------------\n",
      "tensor(477.0657)\n",
      "tensor(153.6826)\n",
      "tensor(363.7640)\n",
      "tensor(9.1506)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.043144\n",
      "Epoch 9109\n",
      "-------------------------------\n",
      "tensor(45.9218)\n",
      "tensor(13.9541)\n",
      "tensor(28.7122)\n",
      "tensor(0.9021)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.161770\n",
      "Epoch 9110\n",
      "-------------------------------\n",
      "tensor(419.8896)\n",
      "tensor(137.1339)\n",
      "tensor(320.8265)\n",
      "tensor(8.1455)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.051125\n",
      "Epoch 9111\n",
      "-------------------------------\n",
      "tensor(772.0380)\n",
      "tensor(260.9753)\n",
      "tensor(595.6596)\n",
      "tensor(15.2077)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.514694\n",
      "Epoch 9112\n",
      "-------------------------------\n",
      "tensor(1086.5238)\n",
      "tensor(356.5399)\n",
      "tensor(828.5366)\n",
      "tensor(20.1859)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.201717\n",
      "Epoch 9113\n",
      "-------------------------------\n",
      "tensor(1312.2278)\n",
      "tensor(432.9673)\n",
      "tensor(1008.2439)\n",
      "tensor(24.8317)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.083603\n",
      "Epoch 9114\n",
      "-------------------------------\n",
      "tensor(1210.3771)\n",
      "tensor(420.8384)\n",
      "tensor(950.2554)\n",
      "tensor(24.6637)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 68.768440\n",
      "Epoch 9115\n",
      "-------------------------------\n",
      "tensor(519.9947)\n",
      "tensor(178.1410)\n",
      "tensor(400.6597)\n",
      "tensor(10.2872)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.423775\n",
      "Epoch 9116\n",
      "-------------------------------\n",
      "tensor(478.7186)\n",
      "tensor(172.6561)\n",
      "tensor(378.6222)\n",
      "tensor(10.3326)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.604431\n",
      "Epoch 9117\n",
      "-------------------------------\n",
      "tensor(554.9709)\n",
      "tensor(183.4108)\n",
      "tensor(425.9370)\n",
      "tensor(10.3178)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.160004\n",
      "Epoch 9118\n",
      "-------------------------------\n",
      "tensor(386.6453)\n",
      "tensor(135.9735)\n",
      "tensor(302.8325)\n",
      "tensor(7.9187)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.582687\n",
      "Epoch 9119\n",
      "-------------------------------\n",
      "tensor(139.8507)\n",
      "tensor(38.6998)\n",
      "tensor(101.1645)\n",
      "tensor(1.8844)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.541702\n",
      "Epoch 9120\n",
      "-------------------------------\n",
      "tensor(353.4146)\n",
      "tensor(110.3262)\n",
      "tensor(264.9926)\n",
      "tensor(5.9898)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.852577\n",
      "Epoch 9121\n",
      "-------------------------------\n",
      "tensor(316.3356)\n",
      "tensor(100.1293)\n",
      "tensor(238.1910)\n",
      "tensor(5.4533)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.319866\n",
      "Epoch 9122\n",
      "-------------------------------\n",
      "tensor(167.1575)\n",
      "tensor(53.1866)\n",
      "tensor(127.3666)\n",
      "tensor(2.8630)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.766697\n",
      "Epoch 9123\n",
      "-------------------------------\n",
      "tensor(75.1989)\n",
      "tensor(24.5909)\n",
      "tensor(53.3813)\n",
      "tensor(1.4128)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.267677\n",
      "Epoch 9124\n",
      "-------------------------------\n",
      "tensor(349.9782)\n",
      "tensor(111.6101)\n",
      "tensor(262.2615)\n",
      "tensor(6.3328)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.817410\n",
      "Epoch 9125\n",
      "-------------------------------\n",
      "tensor(354.8257)\n",
      "tensor(109.3706)\n",
      "tensor(262.6003)\n",
      "tensor(6.1361)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.846119\n",
      "Epoch 9126\n",
      "-------------------------------\n",
      "tensor(189.9908)\n",
      "tensor(73.6845)\n",
      "tensor(153.9968)\n",
      "tensor(4.0967)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.961250\n",
      "Epoch 9127\n",
      "-------------------------------\n",
      "tensor(438.6010)\n",
      "tensor(150.7583)\n",
      "tensor(339.2538)\n",
      "tensor(8.2405)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.399109\n",
      "Epoch 9128\n",
      "-------------------------------\n",
      "tensor(438.9384)\n",
      "tensor(147.1944)\n",
      "tensor(338.9259)\n",
      "tensor(8.8159)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.358120\n",
      "Epoch 9129\n",
      "-------------------------------\n",
      "tensor(45.7008)\n",
      "tensor(13.5443)\n",
      "tensor(9.7174)\n",
      "tensor(0.1332)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.113308\n",
      "Epoch 9130\n",
      "-------------------------------\n",
      "tensor(463.2867)\n",
      "tensor(153.2557)\n",
      "tensor(353.7011)\n",
      "tensor(9.5130)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.020699\n",
      "Epoch 9131\n",
      "-------------------------------\n",
      "tensor(797.0426)\n",
      "tensor(272.2582)\n",
      "tensor(617.6943)\n",
      "tensor(15.4114)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.490303\n",
      "Epoch 9132\n",
      "-------------------------------\n",
      "tensor(1085.2556)\n",
      "tensor(353.5316)\n",
      "tensor(827.2051)\n",
      "tensor(20.4560)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.439396\n",
      "Epoch 9133\n",
      "-------------------------------\n",
      "tensor(1272.6395)\n",
      "tensor(435.5974)\n",
      "tensor(991.8818)\n",
      "tensor(24.9816)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 72.091423\n",
      "Epoch 9134\n",
      "-------------------------------\n",
      "tensor(1171.8203)\n",
      "tensor(396.9573)\n",
      "tensor(910.8392)\n",
      "tensor(23.4476)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.410805\n",
      "Epoch 9135\n",
      "-------------------------------\n",
      "tensor(493.3412)\n",
      "tensor(168.1680)\n",
      "tensor(381.7899)\n",
      "tensor(9.7423)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.686913\n",
      "Epoch 9136\n",
      "-------------------------------\n",
      "tensor(471.3792)\n",
      "tensor(161.9979)\n",
      "tensor(366.3495)\n",
      "tensor(9.6434)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.260098\n",
      "Epoch 9137\n",
      "-------------------------------\n",
      "tensor(532.1774)\n",
      "tensor(181.0983)\n",
      "tensor(411.4105)\n",
      "tensor(10.0566)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.627457\n",
      "Epoch 9138\n",
      "-------------------------------\n",
      "tensor(369.5702)\n",
      "tensor(132.2142)\n",
      "tensor(292.6410)\n",
      "tensor(7.4836)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.415581\n",
      "Epoch 9139\n",
      "-------------------------------\n",
      "tensor(139.4267)\n",
      "tensor(37.5594)\n",
      "tensor(96.6275)\n",
      "tensor(2.1012)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.645775\n",
      "Epoch 9140\n",
      "-------------------------------\n",
      "tensor(341.5617)\n",
      "tensor(106.4157)\n",
      "tensor(254.1719)\n",
      "tensor(6.0877)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.755089\n",
      "Epoch 9141\n",
      "-------------------------------\n",
      "tensor(303.6888)\n",
      "tensor(95.8438)\n",
      "tensor(227.6799)\n",
      "tensor(5.5426)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.249241\n",
      "Epoch 9142\n",
      "-------------------------------\n",
      "tensor(164.3387)\n",
      "tensor(51.0389)\n",
      "tensor(120.7877)\n",
      "tensor(2.9943)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.810871\n",
      "Epoch 9143\n",
      "-------------------------------\n",
      "tensor(73.0455)\n",
      "tensor(25.9592)\n",
      "tensor(53.5821)\n",
      "tensor(1.2106)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.330643\n",
      "Epoch 9144\n",
      "-------------------------------\n",
      "tensor(334.5514)\n",
      "tensor(110.7800)\n",
      "tensor(255.6164)\n",
      "tensor(6.0835)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.654556\n",
      "Epoch 9145\n",
      "-------------------------------\n",
      "tensor(335.2481)\n",
      "tensor(108.8956)\n",
      "tensor(256.7684)\n",
      "tensor(6.0271)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.659538\n",
      "Epoch 9146\n",
      "-------------------------------\n",
      "tensor(185.3423)\n",
      "tensor(66.0070)\n",
      "tensor(143.3612)\n",
      "tensor(3.8445)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.906845\n",
      "Epoch 9147\n",
      "-------------------------------\n",
      "tensor(419.9112)\n",
      "tensor(141.9547)\n",
      "tensor(324.0260)\n",
      "tensor(8.0919)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.077122\n",
      "Epoch 9148\n",
      "-------------------------------\n",
      "tensor(425.1230)\n",
      "tensor(135.5426)\n",
      "tensor(318.7692)\n",
      "tensor(8.2320)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.990627\n",
      "Epoch 9149\n",
      "-------------------------------\n",
      "tensor(47.9314)\n",
      "tensor(18.5805)\n",
      "tensor(6.2998)\n",
      "tensor(0.3856)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.242008\n",
      "Epoch 9150\n",
      "-------------------------------\n",
      "tensor(418.8421)\n",
      "tensor(140.3094)\n",
      "tensor(324.8092)\n",
      "tensor(8.0902)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.133404\n",
      "Epoch 9151\n",
      "-------------------------------\n",
      "tensor(728.3450)\n",
      "tensor(237.9025)\n",
      "tensor(559.0059)\n",
      "tensor(14.0483)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.980476\n",
      "Epoch 9152\n",
      "-------------------------------\n",
      "tensor(984.1340)\n",
      "tensor(334.1841)\n",
      "tensor(761.6767)\n",
      "tensor(19.2843)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.019241\n",
      "Epoch 9153\n",
      "-------------------------------\n",
      "tensor(1184.0702)\n",
      "tensor(398.3011)\n",
      "tensor(914.2225)\n",
      "tensor(23.0089)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.662811\n",
      "Epoch 9154\n",
      "-------------------------------\n",
      "tensor(1124.1005)\n",
      "tensor(373.5075)\n",
      "tensor(861.7936)\n",
      "tensor(21.3093)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.375847\n",
      "Epoch 9155\n",
      "-------------------------------\n",
      "tensor(512.7823)\n",
      "tensor(171.3694)\n",
      "tensor(395.4788)\n",
      "tensor(9.6891)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.149647\n",
      "Epoch 9156\n",
      "-------------------------------\n",
      "tensor(412.5483)\n",
      "tensor(136.4181)\n",
      "tensor(316.4216)\n",
      "tensor(7.5905)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.030773\n",
      "Epoch 9157\n",
      "-------------------------------\n",
      "tensor(524.9252)\n",
      "tensor(175.6304)\n",
      "tensor(402.3617)\n",
      "tensor(10.1707)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.316822\n",
      "Epoch 9158\n",
      "-------------------------------\n",
      "tensor(333.5689)\n",
      "tensor(109.5857)\n",
      "tensor(253.9329)\n",
      "tensor(6.3686)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.642609\n",
      "Epoch 9159\n",
      "-------------------------------\n",
      "tensor(151.2105)\n",
      "tensor(53.6133)\n",
      "tensor(115.4819)\n",
      "tensor(2.8872)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.692425\n",
      "Epoch 9160\n",
      "-------------------------------\n",
      "tensor(323.3368)\n",
      "tensor(111.8351)\n",
      "tensor(250.9993)\n",
      "tensor(6.2833)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.521931\n",
      "Epoch 9161\n",
      "-------------------------------\n",
      "tensor(277.5538)\n",
      "tensor(96.2334)\n",
      "tensor(215.1318)\n",
      "tensor(5.3864)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.860958\n",
      "Epoch 9162\n",
      "-------------------------------\n",
      "tensor(137.9765)\n",
      "tensor(49.0279)\n",
      "tensor(107.0575)\n",
      "tensor(2.6795)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.536861\n",
      "Epoch 9163\n",
      "-------------------------------\n",
      "tensor(86.7267)\n",
      "tensor(26.8118)\n",
      "tensor(61.1835)\n",
      "tensor(1.5462)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.259426\n",
      "Epoch 9164\n",
      "-------------------------------\n",
      "tensor(324.8183)\n",
      "tensor(108.0551)\n",
      "tensor(247.4522)\n",
      "tensor(6.2689)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.430225\n",
      "Epoch 9165\n",
      "-------------------------------\n",
      "tensor(304.0361)\n",
      "tensor(103.2243)\n",
      "tensor(233.9117)\n",
      "tensor(6.0609)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.144470\n",
      "Epoch 9166\n",
      "-------------------------------\n",
      "tensor(204.6387)\n",
      "tensor(65.9181)\n",
      "tensor(149.1929)\n",
      "tensor(3.4252)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.011536\n",
      "Epoch 9167\n",
      "-------------------------------\n",
      "tensor(383.8425)\n",
      "tensor(129.7856)\n",
      "tensor(293.7312)\n",
      "tensor(7.3357)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.507351\n",
      "Epoch 9168\n",
      "-------------------------------\n",
      "tensor(409.3347)\n",
      "tensor(133.0386)\n",
      "tensor(313.3059)\n",
      "tensor(7.4076)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.921143\n",
      "Epoch 9169\n",
      "-------------------------------\n",
      "tensor(42.1844)\n",
      "tensor(16.2271)\n",
      "tensor(8.7170)\n",
      "tensor(0.4283)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.169960\n",
      "Epoch 9170\n",
      "-------------------------------\n",
      "tensor(412.6404)\n",
      "tensor(121.5860)\n",
      "tensor(306.7121)\n",
      "tensor(7.3985)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.938049\n",
      "Epoch 9171\n",
      "-------------------------------\n",
      "tensor(713.4673)\n",
      "tensor(249.1604)\n",
      "tensor(558.0544)\n",
      "tensor(14.5328)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.862663\n",
      "Epoch 9172\n",
      "-------------------------------\n",
      "tensor(983.4648)\n",
      "tensor(335.6685)\n",
      "tensor(765.6442)\n",
      "tensor(19.5819)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.334949\n",
      "Epoch 9173\n",
      "-------------------------------\n",
      "tensor(1193.2142)\n",
      "tensor(389.4931)\n",
      "tensor(912.5453)\n",
      "tensor(22.4725)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.026756\n",
      "Epoch 9174\n",
      "-------------------------------\n",
      "tensor(1113.4098)\n",
      "tensor(378.6904)\n",
      "tensor(861.2032)\n",
      "tensor(21.6243)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.197468\n",
      "Epoch 9175\n",
      "-------------------------------\n",
      "tensor(503.2539)\n",
      "tensor(165.9514)\n",
      "tensor(384.8024)\n",
      "tensor(9.5908)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.861500\n",
      "Epoch 9176\n",
      "-------------------------------\n",
      "tensor(416.1552)\n",
      "tensor(140.3319)\n",
      "tensor(320.1289)\n",
      "tensor(7.9712)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.123871\n",
      "Epoch 9177\n",
      "-------------------------------\n",
      "tensor(519.8210)\n",
      "tensor(170.4650)\n",
      "tensor(396.2106)\n",
      "tensor(9.7971)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.204128\n",
      "Epoch 9178\n",
      "-------------------------------\n",
      "tensor(339.0781)\n",
      "tensor(108.7945)\n",
      "tensor(255.4229)\n",
      "tensor(6.2703)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.706375\n",
      "Epoch 9179\n",
      "-------------------------------\n",
      "tensor(146.7120)\n",
      "tensor(53.3499)\n",
      "tensor(112.6472)\n",
      "tensor(2.8897)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.663555\n",
      "Epoch 9180\n",
      "-------------------------------\n",
      "tensor(322.6607)\n",
      "tensor(112.2616)\n",
      "tensor(249.0409)\n",
      "tensor(6.3197)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.508659\n",
      "Epoch 9181\n",
      "-------------------------------\n",
      "tensor(276.9261)\n",
      "tensor(96.6714)\n",
      "tensor(213.7068)\n",
      "tensor(5.4759)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.882179\n",
      "Epoch 9182\n",
      "-------------------------------\n",
      "tensor(138.1370)\n",
      "tensor(49.7342)\n",
      "tensor(106.0660)\n",
      "tensor(2.8255)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.574017\n",
      "Epoch 9183\n",
      "-------------------------------\n",
      "tensor(91.5983)\n",
      "tensor(27.8216)\n",
      "tensor(61.7520)\n",
      "tensor(1.3248)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.277046\n",
      "Epoch 9184\n",
      "-------------------------------\n",
      "tensor(323.1712)\n",
      "tensor(107.6696)\n",
      "tensor(246.9970)\n",
      "tensor(5.9682)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.414574\n",
      "Epoch 9185\n",
      "-------------------------------\n",
      "tensor(301.9980)\n",
      "tensor(104.7376)\n",
      "tensor(234.5627)\n",
      "tensor(5.7941)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.121677\n",
      "Epoch 9186\n",
      "-------------------------------\n",
      "tensor(203.0370)\n",
      "tensor(59.2328)\n",
      "tensor(147.5426)\n",
      "tensor(3.5032)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.935966\n",
      "Epoch 9187\n",
      "-------------------------------\n",
      "tensor(390.0444)\n",
      "tensor(119.6552)\n",
      "tensor(293.0849)\n",
      "tensor(7.0399)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.428020\n",
      "Epoch 9188\n",
      "-------------------------------\n",
      "tensor(411.6900)\n",
      "tensor(137.2893)\n",
      "tensor(314.1704)\n",
      "tensor(8.0786)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.736977\n",
      "Epoch 9189\n",
      "-------------------------------\n",
      "tensor(48.5172)\n",
      "tensor(14.7763)\n",
      "tensor(19.0282)\n",
      "tensor(0.1900)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.141663\n",
      "Epoch 9190\n",
      "-------------------------------\n",
      "tensor(369.1838)\n",
      "tensor(126.7450)\n",
      "tensor(285.9494)\n",
      "tensor(7.2575)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.262348\n",
      "Epoch 9191\n",
      "-------------------------------\n",
      "tensor(686.4326)\n",
      "tensor(215.6655)\n",
      "tensor(518.1819)\n",
      "tensor(12.7039)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.588158\n",
      "Epoch 9192\n",
      "-------------------------------\n",
      "tensor(943.7748)\n",
      "tensor(321.8118)\n",
      "tensor(728.4707)\n",
      "tensor(18.2696)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.049973\n",
      "Epoch 9193\n",
      "-------------------------------\n",
      "tensor(1148.2356)\n",
      "tensor(383.3223)\n",
      "tensor(886.0928)\n",
      "tensor(22.1989)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 64.901360\n",
      "Epoch 9194\n",
      "-------------------------------\n",
      "tensor(1083.7229)\n",
      "tensor(362.4890)\n",
      "tensor(838.0422)\n",
      "tensor(21.1328)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 61.612911\n",
      "Epoch 9195\n",
      "-------------------------------\n",
      "tensor(480.1329)\n",
      "tensor(169.8366)\n",
      "tensor(374.9099)\n",
      "tensor(9.5011)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.362980\n",
      "Epoch 9196\n",
      "-------------------------------\n",
      "tensor(401.4936)\n",
      "tensor(145.8273)\n",
      "tensor(318.7895)\n",
      "tensor(8.3376)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.849693\n",
      "Epoch 9197\n",
      "-------------------------------\n",
      "tensor(505.6127)\n",
      "tensor(163.9608)\n",
      "tensor(383.5321)\n",
      "tensor(9.4077)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.749920\n",
      "Epoch 9198\n",
      "-------------------------------\n",
      "tensor(327.7503)\n",
      "tensor(110.8040)\n",
      "tensor(251.5692)\n",
      "tensor(6.4781)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.504375\n",
      "Epoch 9199\n",
      "-------------------------------\n",
      "tensor(143.6158)\n",
      "tensor(44.4116)\n",
      "tensor(105.6572)\n",
      "tensor(2.2458)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.569733\n",
      "Epoch 9200\n",
      "-------------------------------\n",
      "tensor(315.7263)\n",
      "tensor(101.9462)\n",
      "tensor(239.8914)\n",
      "tensor(5.6097)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.326721\n",
      "Epoch 9201\n",
      "-------------------------------\n",
      "tensor(272.9585)\n",
      "tensor(88.5450)\n",
      "tensor(207.6475)\n",
      "tensor(4.8887)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.759251\n",
      "Epoch 9202\n",
      "-------------------------------\n",
      "tensor(142.6731)\n",
      "tensor(45.8832)\n",
      "tensor(104.7794)\n",
      "tensor(2.4285)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.520790\n",
      "Epoch 9203\n",
      "-------------------------------\n",
      "tensor(80.8716)\n",
      "tensor(26.6497)\n",
      "tensor(57.1066)\n",
      "tensor(1.4552)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.199593\n",
      "Epoch 9204\n",
      "-------------------------------\n",
      "tensor(316.7898)\n",
      "tensor(103.0893)\n",
      "tensor(239.0058)\n",
      "tensor(5.7975)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.259750\n",
      "Epoch 9205\n",
      "-------------------------------\n",
      "tensor(301.5710)\n",
      "tensor(96.7968)\n",
      "tensor(228.6953)\n",
      "tensor(5.4435)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.114468\n",
      "Epoch 9206\n",
      "-------------------------------\n",
      "tensor(186.7716)\n",
      "tensor(66.4885)\n",
      "tensor(140.6200)\n",
      "tensor(3.7215)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.908150\n",
      "Epoch 9207\n",
      "-------------------------------\n",
      "tensor(380.9003)\n",
      "tensor(128.9168)\n",
      "tensor(290.8916)\n",
      "tensor(7.2847)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.320061\n",
      "Epoch 9208\n",
      "-------------------------------\n",
      "tensor(397.2668)\n",
      "tensor(135.9556)\n",
      "tensor(307.1425)\n",
      "tensor(7.7289)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.641743\n",
      "Epoch 9209\n",
      "-------------------------------\n",
      "tensor(40.7725)\n",
      "tensor(13.8783)\n",
      "tensor(7.1760)\n",
      "tensor(0.0314)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.136108\n",
      "Epoch 9210\n",
      "-------------------------------\n",
      "tensor(392.5882)\n",
      "tensor(120.3591)\n",
      "tensor(293.4003)\n",
      "tensor(7.3802)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.547688\n",
      "Epoch 9211\n",
      "-------------------------------\n",
      "tensor(702.8333)\n",
      "tensor(232.6206)\n",
      "tensor(536.6512)\n",
      "tensor(13.1723)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.096985\n",
      "Epoch 9212\n",
      "-------------------------------\n",
      "tensor(978.0275)\n",
      "tensor(326.7560)\n",
      "tensor(752.9841)\n",
      "tensor(18.7317)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.612656\n",
      "Epoch 9213\n",
      "-------------------------------\n",
      "tensor(1196.3441)\n",
      "tensor(398.1273)\n",
      "tensor(918.9570)\n",
      "tensor(23.0367)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 67.035278\n",
      "Epoch 9214\n",
      "-------------------------------\n",
      "tensor(1138.2946)\n",
      "tensor(385.7354)\n",
      "tensor(883.2250)\n",
      "tensor(22.2180)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.515175\n",
      "Epoch 9215\n",
      "-------------------------------\n",
      "tensor(528.9933)\n",
      "tensor(174.0998)\n",
      "tensor(404.4524)\n",
      "tensor(10.1488)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.332016\n",
      "Epoch 9216\n",
      "-------------------------------\n",
      "tensor(415.1255)\n",
      "tensor(138.3036)\n",
      "tensor(320.2631)\n",
      "tensor(8.1071)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.968506\n",
      "Epoch 9217\n",
      "-------------------------------\n",
      "tensor(529.1188)\n",
      "tensor(178.7579)\n",
      "tensor(406.5461)\n",
      "tensor(10.1412)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.399746\n",
      "Epoch 9218\n",
      "-------------------------------\n",
      "tensor(336.1552)\n",
      "tensor(113.9460)\n",
      "tensor(258.7628)\n",
      "tensor(6.5582)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.686913\n",
      "Epoch 9219\n",
      "-------------------------------\n",
      "tensor(153.5878)\n",
      "tensor(51.6207)\n",
      "tensor(114.9239)\n",
      "tensor(2.7822)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.633186\n",
      "Epoch 9220\n",
      "-------------------------------\n",
      "tensor(330.1033)\n",
      "tensor(112.0249)\n",
      "tensor(253.5876)\n",
      "tensor(6.3000)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.508385\n",
      "Epoch 9221\n",
      "-------------------------------\n",
      "tensor(284.9883)\n",
      "tensor(97.6235)\n",
      "tensor(218.1686)\n",
      "tensor(5.4893)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.864120\n",
      "Epoch 9222\n",
      "-------------------------------\n",
      "tensor(142.6167)\n",
      "tensor(50.7621)\n",
      "tensor(108.8329)\n",
      "tensor(2.8482)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.522614\n",
      "Epoch 9223\n",
      "-------------------------------\n",
      "tensor(88.6268)\n",
      "tensor(26.1585)\n",
      "tensor(61.9222)\n",
      "tensor(1.3091)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.227791\n",
      "Epoch 9224\n",
      "-------------------------------\n",
      "tensor(332.7769)\n",
      "tensor(107.6468)\n",
      "tensor(251.7441)\n",
      "tensor(5.9522)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.479839\n",
      "Epoch 9225\n",
      "-------------------------------\n",
      "tensor(313.3110)\n",
      "tensor(103.2040)\n",
      "tensor(239.1403)\n",
      "tensor(5.6683)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.226616\n",
      "Epoch 9226\n",
      "-------------------------------\n",
      "tensor(202.8714)\n",
      "tensor(65.4485)\n",
      "tensor(152.0617)\n",
      "tensor(3.8440)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.967579\n",
      "Epoch 9227\n",
      "-------------------------------\n",
      "tensor(402.3367)\n",
      "tensor(126.3571)\n",
      "tensor(301.6954)\n",
      "tensor(7.2673)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.581886\n",
      "Epoch 9228\n",
      "-------------------------------\n",
      "tensor(417.5876)\n",
      "tensor(149.3683)\n",
      "tensor(327.2451)\n",
      "tensor(8.3795)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.142208\n",
      "Epoch 9229\n",
      "-------------------------------\n",
      "tensor(46.3890)\n",
      "tensor(13.2808)\n",
      "tensor(12.2927)\n",
      "tensor(0.2886)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.194450\n",
      "Epoch 9230\n",
      "-------------------------------\n",
      "tensor(401.3430)\n",
      "tensor(136.6520)\n",
      "tensor(312.0227)\n",
      "tensor(7.8259)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.733524\n",
      "Epoch 9231\n",
      "-------------------------------\n",
      "tensor(717.6960)\n",
      "tensor(233.8927)\n",
      "tensor(550.9268)\n",
      "tensor(14.1117)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.710716\n",
      "Epoch 9232\n",
      "-------------------------------\n",
      "tensor(976.0613)\n",
      "tensor(339.2406)\n",
      "tensor(759.9281)\n",
      "tensor(19.4120)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.812920\n",
      "Epoch 9233\n",
      "-------------------------------\n",
      "tensor(1181.3666)\n",
      "tensor(392.4978)\n",
      "tensor(910.8349)\n",
      "tensor(22.6499)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.769882\n",
      "Epoch 9234\n",
      "-------------------------------\n",
      "tensor(1103.1758)\n",
      "tensor(369.9163)\n",
      "tensor(853.0975)\n",
      "tensor(21.5023)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 62.725170\n",
      "Epoch 9235\n",
      "-------------------------------\n",
      "tensor(485.0988)\n",
      "tensor(167.0679)\n",
      "tensor(376.7709)\n",
      "tensor(9.9197)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.531693\n",
      "Epoch 9236\n",
      "-------------------------------\n",
      "tensor(415.8875)\n",
      "tensor(145.5911)\n",
      "tensor(326.2219)\n",
      "tensor(8.7783)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.078640\n",
      "Epoch 9237\n",
      "-------------------------------\n",
      "tensor(521.4590)\n",
      "tensor(163.1465)\n",
      "tensor(391.2918)\n",
      "tensor(9.4119)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.111141\n",
      "Epoch 9238\n",
      "-------------------------------\n",
      "tensor(334.3412)\n",
      "tensor(108.3704)\n",
      "tensor(254.8624)\n",
      "tensor(6.4791)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.708542\n",
      "Epoch 9239\n",
      "-------------------------------\n",
      "tensor(145.9501)\n",
      "tensor(49.7738)\n",
      "tensor(109.0726)\n",
      "tensor(2.4158)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.682735\n",
      "Epoch 9240\n",
      "-------------------------------\n",
      "tensor(318.8785)\n",
      "tensor(106.9211)\n",
      "tensor(245.3295)\n",
      "tensor(5.8498)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.480373\n",
      "Epoch 9241\n",
      "-------------------------------\n",
      "tensor(276.7877)\n",
      "tensor(92.9670)\n",
      "tensor(212.1622)\n",
      "tensor(5.1099)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.888531\n",
      "Epoch 9242\n",
      "-------------------------------\n",
      "tensor(141.5899)\n",
      "tensor(48.1448)\n",
      "tensor(107.0857)\n",
      "tensor(2.5930)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.602531\n",
      "Epoch 9243\n",
      "-------------------------------\n",
      "tensor(81.9165)\n",
      "tensor(25.4456)\n",
      "tensor(58.0617)\n",
      "tensor(1.3915)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.287235\n",
      "Epoch 9244\n",
      "-------------------------------\n",
      "tensor(320.8654)\n",
      "tensor(104.8558)\n",
      "tensor(242.6021)\n",
      "tensor(5.8641)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.397972\n",
      "Epoch 9245\n",
      "-------------------------------\n",
      "tensor(305.8263)\n",
      "tensor(100.6491)\n",
      "tensor(231.6779)\n",
      "tensor(5.5855)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.169109\n",
      "Epoch 9246\n",
      "-------------------------------\n",
      "tensor(196.5608)\n",
      "tensor(63.6425)\n",
      "tensor(145.7513)\n",
      "tensor(3.5758)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.992054\n",
      "Epoch 9247\n",
      "-------------------------------\n",
      "tensor(387.5412)\n",
      "tensor(124.6382)\n",
      "tensor(292.4664)\n",
      "tensor(7.0043)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.448452\n",
      "Epoch 9248\n",
      "-------------------------------\n",
      "tensor(403.0024)\n",
      "tensor(138.7591)\n",
      "tensor(313.7357)\n",
      "tensor(7.9826)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.750832\n",
      "Epoch 9249\n",
      "-------------------------------\n",
      "tensor(54.3518)\n",
      "tensor(19.6439)\n",
      "tensor(6.1570)\n",
      "tensor(0.3001)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.293541\n",
      "Epoch 9250\n",
      "-------------------------------\n",
      "tensor(397.5432)\n",
      "tensor(129.5622)\n",
      "tensor(300.9597)\n",
      "tensor(7.9666)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.847218\n",
      "Epoch 9251\n",
      "-------------------------------\n",
      "tensor(704.8666)\n",
      "tensor(233.5632)\n",
      "tensor(543.5642)\n",
      "tensor(13.3058)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.538986\n",
      "Epoch 9252\n",
      "-------------------------------\n",
      "tensor(971.7656)\n",
      "tensor(318.5761)\n",
      "tensor(743.5531)\n",
      "tensor(18.4135)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.140892\n",
      "Epoch 9253\n",
      "-------------------------------\n",
      "tensor(1173.4979)\n",
      "tensor(397.7210)\n",
      "tensor(905.8141)\n",
      "tensor(22.7981)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.081390\n",
      "Epoch 9254\n",
      "-------------------------------\n",
      "tensor(1123.0042)\n",
      "tensor(371.8179)\n",
      "tensor(865.8428)\n",
      "tensor(21.4417)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.475849\n",
      "Epoch 9255\n",
      "-------------------------------\n",
      "tensor(523.2501)\n",
      "tensor(173.6823)\n",
      "tensor(401.4056)\n",
      "tensor(10.1975)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.439129\n",
      "Epoch 9256\n",
      "-------------------------------\n",
      "tensor(410.0637)\n",
      "tensor(131.0596)\n",
      "tensor(309.1361)\n",
      "tensor(7.7064)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.985901\n",
      "Epoch 9257\n",
      "-------------------------------\n",
      "tensor(522.7455)\n",
      "tensor(183.1225)\n",
      "tensor(410.5061)\n",
      "tensor(10.2536)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.543228\n",
      "Epoch 9258\n",
      "-------------------------------\n",
      "tensor(324.9128)\n",
      "tensor(115.3225)\n",
      "tensor(256.4197)\n",
      "tensor(6.3580)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.715012\n",
      "Epoch 9259\n",
      "-------------------------------\n",
      "tensor(158.6992)\n",
      "tensor(49.2451)\n",
      "tensor(115.9235)\n",
      "tensor(2.9856)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.806122\n",
      "Epoch 9260\n",
      "-------------------------------\n",
      "tensor(330.8735)\n",
      "tensor(108.3329)\n",
      "tensor(250.3420)\n",
      "tensor(6.3839)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.657497\n",
      "Epoch 9261\n",
      "-------------------------------\n",
      "tensor(280.1534)\n",
      "tensor(92.2688)\n",
      "tensor(212.1161)\n",
      "tensor(5.4645)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.968117\n",
      "Epoch 9262\n",
      "-------------------------------\n",
      "tensor(138.0702)\n",
      "tensor(45.2809)\n",
      "tensor(102.2775)\n",
      "tensor(2.7461)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.637451\n",
      "Epoch 9263\n",
      "-------------------------------\n",
      "tensor(89.8372)\n",
      "tensor(30.7720)\n",
      "tensor(66.9453)\n",
      "tensor(1.4573)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.376030\n",
      "Epoch 9264\n",
      "-------------------------------\n",
      "tensor(324.8797)\n",
      "tensor(109.5913)\n",
      "tensor(251.9398)\n",
      "tensor(6.0940)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.559055\n",
      "Epoch 9265\n",
      "-------------------------------\n",
      "tensor(305.2596)\n",
      "tensor(103.1159)\n",
      "tensor(234.9364)\n",
      "tensor(5.7432)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.211269\n",
      "Epoch 9266\n",
      "-------------------------------\n",
      "tensor(206.0794)\n",
      "tensor(67.7675)\n",
      "tensor(154.4799)\n",
      "tensor(3.8303)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.006310\n",
      "Epoch 9267\n",
      "-------------------------------\n",
      "tensor(385.2768)\n",
      "tensor(129.7919)\n",
      "tensor(296.9265)\n",
      "tensor(7.4407)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.414799\n",
      "Epoch 9268\n",
      "-------------------------------\n",
      "tensor(429.0812)\n",
      "tensor(132.1844)\n",
      "tensor(318.3352)\n",
      "tensor(7.8076)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.050865\n",
      "Epoch 9269\n",
      "-------------------------------\n",
      "tensor(67.3788)\n",
      "tensor(20.3355)\n",
      "tensor(28.0545)\n",
      "tensor(0.4109)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.251945\n",
      "Epoch 9270\n",
      "-------------------------------\n",
      "tensor(383.1954)\n",
      "tensor(121.7519)\n",
      "tensor(290.1710)\n",
      "tensor(6.6688)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.369595\n",
      "Epoch 9271\n",
      "-------------------------------\n",
      "tensor(699.3893)\n",
      "tensor(226.5165)\n",
      "tensor(532.7382)\n",
      "tensor(13.3642)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.015068\n",
      "Epoch 9272\n",
      "-------------------------------\n",
      "tensor(960.6992)\n",
      "tensor(332.1789)\n",
      "tensor(748.6567)\n",
      "tensor(19.2142)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 56.079243\n",
      "Epoch 9273\n",
      "-------------------------------\n",
      "tensor(1173.7181)\n",
      "tensor(394.0933)\n",
      "tensor(905.4843)\n",
      "tensor(22.7806)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.074745\n",
      "Epoch 9274\n",
      "-------------------------------\n",
      "tensor(1120.4546)\n",
      "tensor(369.2766)\n",
      "tensor(858.2684)\n",
      "tensor(20.9061)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 63.223511\n",
      "Epoch 9275\n",
      "-------------------------------\n",
      "tensor(510.4186)\n",
      "tensor(172.9596)\n",
      "tensor(396.1048)\n",
      "tensor(9.9302)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.064964\n",
      "Epoch 9276\n",
      "-------------------------------\n",
      "tensor(409.2948)\n",
      "tensor(133.5997)\n",
      "tensor(313.6546)\n",
      "tensor(7.4392)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.914665\n",
      "Epoch 9277\n",
      "-------------------------------\n",
      "tensor(520.6115)\n",
      "tensor(177.8120)\n",
      "tensor(403.1169)\n",
      "tensor(10.3889)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.271404\n",
      "Epoch 9278\n",
      "-------------------------------\n",
      "tensor(330.1104)\n",
      "tensor(110.3684)\n",
      "tensor(252.7553)\n",
      "tensor(6.4172)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.592575\n",
      "Epoch 9279\n",
      "-------------------------------\n",
      "tensor(148.6223)\n",
      "tensor(53.9482)\n",
      "tensor(117.3415)\n",
      "tensor(2.9820)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.654465\n",
      "Epoch 9280\n",
      "-------------------------------\n",
      "tensor(323.6338)\n",
      "tensor(113.2954)\n",
      "tensor(252.2854)\n",
      "tensor(6.4115)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.482227\n",
      "Epoch 9281\n",
      "-------------------------------\n",
      "tensor(275.6985)\n",
      "tensor(97.0429)\n",
      "tensor(215.2125)\n",
      "tensor(5.4997)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.816399\n",
      "Epoch 9282\n",
      "-------------------------------\n",
      "tensor(137.6853)\n",
      "tensor(50.0394)\n",
      "tensor(106.3205)\n",
      "tensor(2.7686)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.501736\n",
      "Epoch 9283\n",
      "-------------------------------\n",
      "tensor(89.4083)\n",
      "tensor(26.5838)\n",
      "tensor(62.6820)\n",
      "tensor(1.4904)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.255772\n",
      "Epoch 9284\n",
      "-------------------------------\n",
      "tensor(326.0686)\n",
      "tensor(108.2526)\n",
      "tensor(248.3629)\n",
      "tensor(6.2477)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.430748\n",
      "Epoch 9285\n",
      "-------------------------------\n",
      "tensor(299.8052)\n",
      "tensor(102.9852)\n",
      "tensor(232.8611)\n",
      "tensor(6.0745)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.110588\n",
      "Epoch 9286\n",
      "-------------------------------\n",
      "tensor(205.4685)\n",
      "tensor(64.9926)\n",
      "tensor(150.0883)\n",
      "tensor(3.4049)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.947311\n",
      "Epoch 9287\n",
      "-------------------------------\n",
      "tensor(390.8393)\n",
      "tensor(121.2157)\n",
      "tensor(293.5977)\n",
      "tensor(7.3240)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.237823\n",
      "Epoch 9288\n",
      "-------------------------------\n",
      "tensor(430.5829)\n",
      "tensor(136.6954)\n",
      "tensor(324.5794)\n",
      "tensor(7.5730)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.124310\n",
      "Epoch 9289\n",
      "-------------------------------\n",
      "tensor(57.0467)\n",
      "tensor(21.0950)\n",
      "tensor(27.4532)\n",
      "tensor(0.8589)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.237789\n",
      "Epoch 9290\n",
      "-------------------------------\n",
      "tensor(390.2303)\n",
      "tensor(117.5076)\n",
      "tensor(287.1534)\n",
      "tensor(6.8070)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.337757\n",
      "Epoch 9291\n",
      "-------------------------------\n",
      "tensor(716.1559)\n",
      "tensor(235.6473)\n",
      "tensor(547.8984)\n",
      "tensor(13.4630)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.443947\n",
      "Epoch 9292\n",
      "-------------------------------\n",
      "tensor(999.7853)\n",
      "tensor(336.8347)\n",
      "tensor(775.3116)\n",
      "tensor(19.5857)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.574936\n",
      "Epoch 9293\n",
      "-------------------------------\n",
      "tensor(1226.3414)\n",
      "tensor(411.5667)\n",
      "tensor(946.8521)\n",
      "tensor(23.8758)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.841583\n",
      "Epoch 9294\n",
      "-------------------------------\n",
      "tensor(1173.8602)\n",
      "tensor(392.0323)\n",
      "tensor(904.7370)\n",
      "tensor(22.3568)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.058990\n",
      "Epoch 9295\n",
      "-------------------------------\n",
      "tensor(543.3290)\n",
      "tensor(175.7883)\n",
      "tensor(416.2520)\n",
      "tensor(10.1587)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.733543\n",
      "Epoch 9296\n",
      "-------------------------------\n",
      "tensor(444.7089)\n",
      "tensor(140.7310)\n",
      "tensor(333.2100)\n",
      "tensor(7.8433)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.423412\n",
      "Epoch 9297\n",
      "-------------------------------\n",
      "tensor(558.8428)\n",
      "tensor(188.1952)\n",
      "tensor(431.9109)\n",
      "tensor(10.7453)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.096897\n",
      "Epoch 9298\n",
      "-------------------------------\n",
      "tensor(353.0908)\n",
      "tensor(113.9713)\n",
      "tensor(270.0392)\n",
      "tensor(6.3625)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.881134\n",
      "Epoch 9299\n",
      "-------------------------------\n",
      "tensor(158.9579)\n",
      "tensor(59.5421)\n",
      "tensor(125.4572)\n",
      "tensor(3.5692)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.694237\n",
      "Epoch 9300\n",
      "-------------------------------\n",
      "tensor(344.8588)\n",
      "tensor(120.5286)\n",
      "tensor(268.2566)\n",
      "tensor(7.0653)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.788643\n",
      "Epoch 9301\n",
      "-------------------------------\n",
      "tensor(294.4154)\n",
      "tensor(101.8262)\n",
      "tensor(227.7200)\n",
      "tensor(5.9600)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.034939\n",
      "Epoch 9302\n",
      "-------------------------------\n",
      "tensor(145.5604)\n",
      "tensor(50.0309)\n",
      "tensor(110.7853)\n",
      "tensor(2.9461)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.545273\n",
      "Epoch 9303\n",
      "-------------------------------\n",
      "tensor(90.1289)\n",
      "tensor(31.4752)\n",
      "tensor(69.4134)\n",
      "tensor(1.6755)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.241707\n",
      "Epoch 9304\n",
      "-------------------------------\n",
      "tensor(343.6298)\n",
      "tensor(119.6831)\n",
      "tensor(267.8084)\n",
      "tensor(6.7907)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.708752\n",
      "Epoch 9305\n",
      "-------------------------------\n",
      "tensor(322.3738)\n",
      "tensor(115.6951)\n",
      "tensor(251.3009)\n",
      "tensor(6.5451)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.380142\n",
      "Epoch 9306\n",
      "-------------------------------\n",
      "tensor(220.9356)\n",
      "tensor(65.9696)\n",
      "tensor(162.1643)\n",
      "tensor(3.7053)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.047787\n",
      "Epoch 9307\n",
      "-------------------------------\n",
      "tensor(416.3986)\n",
      "tensor(136.3348)\n",
      "tensor(318.5791)\n",
      "tensor(7.8690)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.855511\n",
      "Epoch 9308\n",
      "-------------------------------\n",
      "tensor(462.2578)\n",
      "tensor(142.3958)\n",
      "tensor(339.8579)\n",
      "tensor(8.1958)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.539101\n",
      "Epoch 9309\n",
      "-------------------------------\n",
      "tensor(57.2182)\n",
      "tensor(16.3917)\n",
      "tensor(31.9834)\n",
      "tensor(0.4392)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.126991\n",
      "Epoch 9310\n",
      "-------------------------------\n",
      "tensor(387.0945)\n",
      "tensor(130.3668)\n",
      "tensor(297.0049)\n",
      "tensor(7.2030)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.464058\n",
      "Epoch 9311\n",
      "-------------------------------\n",
      "tensor(732.3234)\n",
      "tensor(230.9873)\n",
      "tensor(553.6405)\n",
      "tensor(13.4111)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.782555\n",
      "Epoch 9312\n",
      "-------------------------------\n",
      "tensor(1022.0569)\n",
      "tensor(346.3457)\n",
      "tensor(789.5699)\n",
      "tensor(19.9145)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.425831\n",
      "Epoch 9313\n",
      "-------------------------------\n",
      "tensor(1251.9238)\n",
      "tensor(425.8371)\n",
      "tensor(973.5691)\n",
      "tensor(24.5897)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.692543\n",
      "Epoch 9314\n",
      "-------------------------------\n",
      "tensor(1203.5337)\n",
      "tensor(398.7655)\n",
      "tensor(924.8436)\n",
      "tensor(23.1508)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 67.301125\n",
      "Epoch 9315\n",
      "-------------------------------\n",
      "tensor(553.8356)\n",
      "tensor(190.1486)\n",
      "tensor(425.6706)\n",
      "tensor(10.3908)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.156979\n",
      "Epoch 9316\n",
      "-------------------------------\n",
      "tensor(435.7141)\n",
      "tensor(154.1931)\n",
      "tensor(338.3618)\n",
      "tensor(8.6842)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.552040\n",
      "Epoch 9317\n",
      "-------------------------------\n",
      "tensor(567.6830)\n",
      "tensor(183.5948)\n",
      "tensor(430.9652)\n",
      "tensor(10.2836)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.203106\n",
      "Epoch 9318\n",
      "-------------------------------\n",
      "tensor(356.1901)\n",
      "tensor(119.1759)\n",
      "tensor(272.5009)\n",
      "tensor(6.6460)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.859215\n",
      "Epoch 9319\n",
      "-------------------------------\n",
      "tensor(165.6244)\n",
      "tensor(50.3471)\n",
      "tensor(122.5990)\n",
      "tensor(2.8522)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.612366\n",
      "Epoch 9320\n",
      "-------------------------------\n",
      "tensor(357.9245)\n",
      "tensor(112.0181)\n",
      "tensor(266.2462)\n",
      "tensor(6.2536)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.764446\n",
      "Epoch 9321\n",
      "-------------------------------\n",
      "tensor(300.1909)\n",
      "tensor(92.9585)\n",
      "tensor(226.1052)\n",
      "tensor(5.2322)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.996281\n",
      "Epoch 9322\n",
      "-------------------------------\n",
      "tensor(147.4199)\n",
      "tensor(42.7432)\n",
      "tensor(109.6315)\n",
      "tensor(2.3629)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.507923\n",
      "Epoch 9323\n",
      "-------------------------------\n",
      "tensor(89.4791)\n",
      "tensor(36.2255)\n",
      "tensor(69.7584)\n",
      "tensor(2.0212)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.224670\n",
      "Epoch 9324\n",
      "-------------------------------\n",
      "tensor(345.8849)\n",
      "tensor(118.3464)\n",
      "tensor(266.0808)\n",
      "tensor(6.7623)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.710278\n",
      "Epoch 9325\n",
      "-------------------------------\n",
      "tensor(325.5061)\n",
      "tensor(106.1814)\n",
      "tensor(247.0486)\n",
      "tensor(6.0941)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.381882\n",
      "Epoch 9326\n",
      "-------------------------------\n",
      "tensor(212.0014)\n",
      "tensor(79.2321)\n",
      "tensor(165.8976)\n",
      "tensor(4.3374)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.108803\n",
      "Epoch 9327\n",
      "-------------------------------\n",
      "tensor(408.4121)\n",
      "tensor(147.4214)\n",
      "tensor(318.8019)\n",
      "tensor(8.1917)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.832008\n",
      "Epoch 9328\n",
      "-------------------------------\n",
      "tensor(455.9911)\n",
      "tensor(143.6635)\n",
      "tensor(342.6725)\n",
      "tensor(8.3174)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.514809\n",
      "Epoch 9329\n",
      "-------------------------------\n",
      "tensor(46.2768)\n",
      "tensor(15.8251)\n",
      "tensor(27.8369)\n",
      "tensor(0.4477)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.089119\n",
      "Epoch 9330\n",
      "-------------------------------\n",
      "tensor(416.3464)\n",
      "tensor(128.1272)\n",
      "tensor(308.2558)\n",
      "tensor(7.5360)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.744740\n",
      "Epoch 9331\n",
      "-------------------------------\n",
      "tensor(767.6368)\n",
      "tensor(253.0422)\n",
      "tensor(587.4431)\n",
      "tensor(14.1312)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.158878\n",
      "Epoch 9332\n",
      "-------------------------------\n",
      "tensor(1074.7200)\n",
      "tensor(358.5197)\n",
      "tensor(829.9434)\n",
      "tensor(20.9934)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.041878\n",
      "Epoch 9333\n",
      "-------------------------------\n",
      "tensor(1304.8674)\n",
      "tensor(451.5538)\n",
      "tensor(1019.0337)\n",
      "tensor(26.2762)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 73.695946\n",
      "Epoch 9334\n",
      "-------------------------------\n",
      "tensor(1254.7321)\n",
      "tensor(420.8593)\n",
      "tensor(965.9164)\n",
      "tensor(24.0371)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.464630\n",
      "Epoch 9335\n",
      "-------------------------------\n",
      "tensor(583.8226)\n",
      "tensor(183.5195)\n",
      "tensor(442.0882)\n",
      "tensor(10.2050)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.626320\n",
      "Epoch 9336\n",
      "-------------------------------\n",
      "tensor(472.7903)\n",
      "tensor(148.1129)\n",
      "tensor(352.8541)\n",
      "tensor(8.0329)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.835331\n",
      "Epoch 9337\n",
      "-------------------------------\n",
      "tensor(600.4429)\n",
      "tensor(196.8776)\n",
      "tensor(455.0719)\n",
      "tensor(11.0819)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.029316\n",
      "Epoch 9338\n",
      "-------------------------------\n",
      "tensor(380.6340)\n",
      "tensor(117.4117)\n",
      "tensor(282.0034)\n",
      "tensor(6.3911)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.252388\n",
      "Epoch 9339\n",
      "-------------------------------\n",
      "tensor(169.8152)\n",
      "tensor(66.0211)\n",
      "tensor(136.6503)\n",
      "tensor(3.9043)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.795395\n",
      "Epoch 9340\n",
      "-------------------------------\n",
      "tensor(366.0846)\n",
      "tensor(128.5732)\n",
      "tensor(286.8492)\n",
      "tensor(7.4071)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.138870\n",
      "Epoch 9341\n",
      "-------------------------------\n",
      "tensor(316.3531)\n",
      "tensor(108.5597)\n",
      "tensor(243.3330)\n",
      "tensor(6.1287)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.282173\n",
      "Epoch 9342\n",
      "-------------------------------\n",
      "tensor(157.7746)\n",
      "tensor(53.0396)\n",
      "tensor(118.4207)\n",
      "tensor(2.8932)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.589478\n",
      "Epoch 9343\n",
      "-------------------------------\n",
      "tensor(93.2096)\n",
      "tensor(32.6746)\n",
      "tensor(74.5746)\n",
      "tensor(1.9949)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.241325\n",
      "Epoch 9344\n",
      "-------------------------------\n",
      "tensor(367.7362)\n",
      "tensor(126.6463)\n",
      "tensor(287.1014)\n",
      "tensor(7.3043)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.086452\n",
      "Epoch 9345\n",
      "-------------------------------\n",
      "tensor(345.3824)\n",
      "tensor(119.6341)\n",
      "tensor(269.6658)\n",
      "tensor(6.7713)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.710934\n",
      "Epoch 9346\n",
      "-------------------------------\n",
      "tensor(230.0755)\n",
      "tensor(74.3050)\n",
      "tensor(173.5326)\n",
      "tensor(4.4586)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.157059\n",
      "Epoch 9347\n",
      "-------------------------------\n",
      "tensor(438.3307)\n",
      "tensor(144.6555)\n",
      "tensor(339.1808)\n",
      "tensor(8.7314)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.359219\n",
      "Epoch 9348\n",
      "-------------------------------\n",
      "tensor(462.6781)\n",
      "tensor(163.5701)\n",
      "tensor(360.4283)\n",
      "tensor(9.1546)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.869835\n",
      "Epoch 9349\n",
      "-------------------------------\n",
      "tensor(41.4651)\n",
      "tensor(10.9178)\n",
      "tensor(14.2317)\n",
      "tensor(0.3685)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.024265\n",
      "Epoch 9350\n",
      "-------------------------------\n",
      "tensor(445.2665)\n",
      "tensor(153.0245)\n",
      "tensor(339.9417)\n",
      "tensor(8.6096)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.323994\n",
      "Epoch 9351\n",
      "-------------------------------\n",
      "tensor(819.6614)\n",
      "tensor(255.7069)\n",
      "tensor(617.2310)\n",
      "tensor(14.6719)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.935345\n",
      "Epoch 9352\n",
      "-------------------------------\n",
      "tensor(1120.4709)\n",
      "tensor(382.3815)\n",
      "tensor(869.7203)\n",
      "tensor(21.9286)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.315781\n",
      "Epoch 9353\n",
      "-------------------------------\n",
      "tensor(1347.8209)\n",
      "tensor(464.0963)\n",
      "tensor(1054.9730)\n",
      "tensor(27.1712)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 76.774292\n",
      "Epoch 9354\n",
      "-------------------------------\n",
      "tensor(1268.6725)\n",
      "tensor(427.2284)\n",
      "tensor(984.5842)\n",
      "tensor(24.9235)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.405457\n",
      "Epoch 9355\n",
      "-------------------------------\n",
      "tensor(570.9533)\n",
      "tensor(182.6207)\n",
      "tensor(428.2332)\n",
      "tensor(10.2935)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.099430\n",
      "Epoch 9356\n",
      "-------------------------------\n",
      "tensor(470.1558)\n",
      "tensor(169.2749)\n",
      "tensor(369.7590)\n",
      "tensor(9.7101)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.079388\n",
      "Epoch 9357\n",
      "-------------------------------\n",
      "tensor(594.2443)\n",
      "tensor(186.9917)\n",
      "tensor(445.0150)\n",
      "tensor(10.3636)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.717266\n",
      "Epoch 9358\n",
      "-------------------------------\n",
      "tensor(382.1151)\n",
      "tensor(124.9749)\n",
      "tensor(289.4225)\n",
      "tensor(7.0629)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.271732\n",
      "Epoch 9359\n",
      "-------------------------------\n",
      "tensor(169.8496)\n",
      "tensor(52.4403)\n",
      "tensor(125.5972)\n",
      "tensor(2.7478)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.686218\n",
      "Epoch 9360\n",
      "-------------------------------\n",
      "tensor(371.4295)\n",
      "tensor(117.2995)\n",
      "tensor(279.5337)\n",
      "tensor(6.3977)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.062847\n",
      "Epoch 9361\n",
      "-------------------------------\n",
      "tensor(319.5761)\n",
      "tensor(100.0702)\n",
      "tensor(239.6011)\n",
      "tensor(5.4240)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.255981\n",
      "Epoch 9362\n",
      "-------------------------------\n",
      "tensor(157.5995)\n",
      "tensor(47.6530)\n",
      "tensor(117.9010)\n",
      "tensor(2.5075)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.594208\n",
      "Epoch 9363\n",
      "-------------------------------\n",
      "tensor(93.3095)\n",
      "tensor(34.0980)\n",
      "tensor(70.8885)\n",
      "tensor(1.9863)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.245914\n",
      "Epoch 9364\n",
      "-------------------------------\n",
      "tensor(366.1823)\n",
      "tensor(120.8116)\n",
      "tensor(278.2028)\n",
      "tensor(6.8386)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.996998\n",
      "Epoch 9365\n",
      "-------------------------------\n",
      "tensor(343.2246)\n",
      "tensor(109.4151)\n",
      "tensor(259.9916)\n",
      "tensor(6.0913)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.645821\n",
      "Epoch 9366\n",
      "-------------------------------\n",
      "tensor(214.1318)\n",
      "tensor(80.9715)\n",
      "tensor(168.7771)\n",
      "tensor(4.6990)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.105080\n",
      "Epoch 9367\n",
      "-------------------------------\n",
      "tensor(433.5385)\n",
      "tensor(149.0503)\n",
      "tensor(334.8134)\n",
      "tensor(8.4836)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.145172\n",
      "Epoch 9368\n",
      "-------------------------------\n",
      "tensor(463.3674)\n",
      "tensor(160.0948)\n",
      "tensor(359.7835)\n",
      "tensor(9.3389)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.819473\n",
      "Epoch 9369\n",
      "-------------------------------\n",
      "tensor(55.2350)\n",
      "tensor(14.6175)\n",
      "tensor(18.8108)\n",
      "tensor(0.1491)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.260868\n",
      "Epoch 9370\n",
      "-------------------------------\n",
      "tensor(434.4790)\n",
      "tensor(141.9997)\n",
      "tensor(330.7636)\n",
      "tensor(8.6525)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.429237\n",
      "Epoch 9371\n",
      "-------------------------------\n",
      "tensor(797.1594)\n",
      "tensor(255.4678)\n",
      "tensor(605.6044)\n",
      "tensor(14.2732)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.201023\n",
      "Epoch 9372\n",
      "-------------------------------\n",
      "tensor(1100.4449)\n",
      "tensor(365.4844)\n",
      "tensor(845.1983)\n",
      "tensor(20.9592)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.134743\n",
      "Epoch 9373\n",
      "-------------------------------\n",
      "tensor(1334.3765)\n",
      "tensor(457.8029)\n",
      "tensor(1037.3274)\n",
      "tensor(26.4923)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.286057\n",
      "Epoch 9374\n",
      "-------------------------------\n",
      "tensor(1280.6364)\n",
      "tensor(429.2830)\n",
      "tensor(991.7361)\n",
      "tensor(24.8648)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.208199\n",
      "Epoch 9375\n",
      "-------------------------------\n",
      "tensor(595.0811)\n",
      "tensor(197.8265)\n",
      "tensor(456.8463)\n",
      "tensor(11.3041)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.086609\n",
      "Epoch 9376\n",
      "-------------------------------\n",
      "tensor(466.1491)\n",
      "tensor(154.3725)\n",
      "tensor(358.2816)\n",
      "tensor(8.9259)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.986912\n",
      "Epoch 9377\n",
      "-------------------------------\n",
      "tensor(598.4247)\n",
      "tensor(202.6499)\n",
      "tensor(461.5603)\n",
      "tensor(11.4365)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.148380\n",
      "Epoch 9378\n",
      "-------------------------------\n",
      "tensor(376.3647)\n",
      "tensor(128.3268)\n",
      "tensor(289.8417)\n",
      "tensor(7.2060)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.326145\n",
      "Epoch 9379\n",
      "-------------------------------\n",
      "tensor(177.1745)\n",
      "tensor(57.1197)\n",
      "tensor(132.2338)\n",
      "tensor(3.2552)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.807594\n",
      "Epoch 9380\n",
      "-------------------------------\n",
      "tensor(377.0679)\n",
      "tensor(124.3029)\n",
      "tensor(286.4797)\n",
      "tensor(7.0925)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.263668\n",
      "Epoch 9381\n",
      "-------------------------------\n",
      "tensor(320.4236)\n",
      "tensor(105.9985)\n",
      "tensor(243.3115)\n",
      "tensor(6.0618)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.390491\n",
      "Epoch 9382\n",
      "-------------------------------\n",
      "tensor(157.3246)\n",
      "tensor(52.1927)\n",
      "tensor(117.9931)\n",
      "tensor(3.0028)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.665573\n",
      "Epoch 9383\n",
      "-------------------------------\n",
      "tensor(101.2337)\n",
      "tensor(33.5374)\n",
      "tensor(74.5670)\n",
      "tensor(1.7246)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.337315\n",
      "Epoch 9384\n",
      "-------------------------------\n",
      "tensor(375.3011)\n",
      "tensor(124.6383)\n",
      "tensor(286.0743)\n",
      "tensor(6.9266)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.186367\n",
      "Epoch 9385\n",
      "-------------------------------\n",
      "tensor(342.8973)\n",
      "tensor(114.8189)\n",
      "tensor(266.1098)\n",
      "tensor(6.4596)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.740234\n",
      "Epoch 9386\n",
      "-------------------------------\n",
      "tensor(231.3193)\n",
      "tensor(77.2548)\n",
      "tensor(175.3421)\n",
      "tensor(4.4384)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.259552\n",
      "Epoch 9387\n",
      "-------------------------------\n",
      "tensor(440.1744)\n",
      "tensor(145.6234)\n",
      "tensor(335.8637)\n",
      "tensor(8.3727)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.330441\n",
      "Epoch 9388\n",
      "-------------------------------\n",
      "tensor(489.4184)\n",
      "tensor(156.8663)\n",
      "tensor(369.8899)\n",
      "tensor(9.1498)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.209454\n",
      "Epoch 9389\n",
      "-------------------------------\n",
      "tensor(72.9989)\n",
      "tensor(19.0443)\n",
      "tensor(37.6639)\n",
      "tensor(0.6881)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.246777\n",
      "Epoch 9390\n",
      "-------------------------------\n",
      "tensor(419.8797)\n",
      "tensor(132.5925)\n",
      "tensor(318.2731)\n",
      "tensor(7.4659)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.018192\n",
      "Epoch 9391\n",
      "-------------------------------\n",
      "tensor(776.9905)\n",
      "tensor(255.1651)\n",
      "tensor(595.5244)\n",
      "tensor(15.0219)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.489494\n",
      "Epoch 9392\n",
      "-------------------------------\n",
      "tensor(1082.6378)\n",
      "tensor(373.7080)\n",
      "tensor(842.9603)\n",
      "tensor(21.5134)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 61.542946\n",
      "Epoch 9393\n",
      "-------------------------------\n",
      "tensor(1332.5857)\n",
      "tensor(443.4481)\n",
      "tensor(1028.5946)\n",
      "tensor(25.5490)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.103561\n",
      "Epoch 9394\n",
      "-------------------------------\n",
      "tensor(1271.5641)\n",
      "tensor(426.8510)\n",
      "tensor(980.8351)\n",
      "tensor(24.5451)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.257690\n",
      "Epoch 9395\n",
      "-------------------------------\n",
      "tensor(586.5403)\n",
      "tensor(202.5166)\n",
      "tensor(454.7734)\n",
      "tensor(11.4065)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.978981\n",
      "Epoch 9396\n",
      "-------------------------------\n",
      "tensor(455.8387)\n",
      "tensor(161.8542)\n",
      "tensor(355.9238)\n",
      "tensor(9.1758)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.940487\n",
      "Epoch 9397\n",
      "-------------------------------\n",
      "tensor(600.9022)\n",
      "tensor(194.5405)\n",
      "tensor(457.2339)\n",
      "tensor(11.0871)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.043774\n",
      "Epoch 9398\n",
      "-------------------------------\n",
      "tensor(375.5280)\n",
      "tensor(123.9228)\n",
      "tensor(286.2784)\n",
      "tensor(7.1077)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.172188\n",
      "Epoch 9399\n",
      "-------------------------------\n",
      "tensor(178.0324)\n",
      "tensor(57.7552)\n",
      "tensor(133.6969)\n",
      "tensor(3.1116)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.764721\n",
      "Epoch 9400\n",
      "-------------------------------\n",
      "tensor(376.6942)\n",
      "tensor(122.9579)\n",
      "tensor(284.7189)\n",
      "tensor(6.8088)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.172337\n",
      "Epoch 9401\n",
      "-------------------------------\n",
      "tensor(320.5709)\n",
      "tensor(104.3719)\n",
      "tensor(241.2109)\n",
      "tensor(5.7690)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.270523\n",
      "Epoch 9402\n",
      "-------------------------------\n",
      "tensor(155.8941)\n",
      "tensor(50.3101)\n",
      "tensor(116.6052)\n",
      "tensor(2.7552)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.545521\n",
      "Epoch 9403\n",
      "-------------------------------\n",
      "tensor(101.5663)\n",
      "tensor(34.2183)\n",
      "tensor(75.3257)\n",
      "tensor(1.8980)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.259674\n",
      "Epoch 9404\n",
      "-------------------------------\n",
      "tensor(373.3619)\n",
      "tensor(123.5600)\n",
      "tensor(284.3291)\n",
      "tensor(6.9628)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.115234\n",
      "Epoch 9405\n",
      "-------------------------------\n",
      "tensor(344.5292)\n",
      "tensor(112.8859)\n",
      "tensor(262.2052)\n",
      "tensor(6.3579)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.683361\n",
      "Epoch 9406\n",
      "-------------------------------\n",
      "tensor(229.9484)\n",
      "tensor(79.1057)\n",
      "tensor(178.4604)\n",
      "tensor(4.5006)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.238331\n",
      "Epoch 9407\n",
      "-------------------------------\n",
      "tensor(449.0411)\n",
      "tensor(141.1106)\n",
      "tensor(336.7461)\n",
      "tensor(8.2346)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.294765\n",
      "Epoch 9408\n",
      "-------------------------------\n",
      "tensor(493.0861)\n",
      "tensor(161.1074)\n",
      "tensor(378.2948)\n",
      "tensor(9.3567)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.434410\n",
      "Epoch 9409\n",
      "-------------------------------\n",
      "tensor(55.7185)\n",
      "tensor(25.8504)\n",
      "tensor(33.3125)\n",
      "tensor(1.1439)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.175049\n",
      "Epoch 9410\n",
      "-------------------------------\n",
      "tensor(450.1250)\n",
      "tensor(145.3838)\n",
      "tensor(338.8243)\n",
      "tensor(8.6274)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.377495\n",
      "Epoch 9411\n",
      "-------------------------------\n",
      "tensor(841.8260)\n",
      "tensor(280.2612)\n",
      "tensor(645.0822)\n",
      "tensor(15.8328)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.781387\n",
      "Epoch 9412\n",
      "-------------------------------\n",
      "tensor(1197.9403)\n",
      "tensor(391.9976)\n",
      "tensor(916.5213)\n",
      "tensor(22.4375)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 66.860260\n",
      "Epoch 9413\n",
      "-------------------------------\n",
      "tensor(1470.8994)\n",
      "tensor(498.9168)\n",
      "tensor(1137.6230)\n",
      "tensor(28.5804)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 83.626762\n",
      "Epoch 9414\n",
      "-------------------------------\n",
      "tensor(1416.7543)\n",
      "tensor(476.0562)\n",
      "tensor(1098.2819)\n",
      "tensor(27.4695)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 80.546761\n",
      "Epoch 9415\n",
      "-------------------------------\n",
      "tensor(661.3157)\n",
      "tensor(220.5893)\n",
      "tensor(509.4995)\n",
      "tensor(12.8782)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.912014\n",
      "Epoch 9416\n",
      "-------------------------------\n",
      "tensor(511.3074)\n",
      "tensor(168.5336)\n",
      "tensor(396.9095)\n",
      "tensor(10.0509)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.124126\n",
      "Epoch 9417\n",
      "-------------------------------\n",
      "tensor(650.7397)\n",
      "tensor(226.8094)\n",
      "tensor(507.6436)\n",
      "tensor(12.8894)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.781776\n",
      "Epoch 9418\n",
      "-------------------------------\n",
      "tensor(412.7488)\n",
      "tensor(146.5885)\n",
      "tensor(325.8983)\n",
      "tensor(8.4609)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.956360\n",
      "Epoch 9419\n",
      "-------------------------------\n",
      "tensor(192.9725)\n",
      "tensor(60.3621)\n",
      "tensor(141.4881)\n",
      "tensor(3.3623)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.850876\n",
      "Epoch 9420\n",
      "-------------------------------\n",
      "tensor(410.3826)\n",
      "tensor(137.4100)\n",
      "tensor(313.5486)\n",
      "tensor(7.8952)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.799835\n",
      "Epoch 9421\n",
      "-------------------------------\n",
      "tensor(348.8746)\n",
      "tensor(119.4187)\n",
      "tensor(268.4870)\n",
      "tensor(6.9504)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.781143\n",
      "Epoch 9422\n",
      "-------------------------------\n",
      "tensor(171.0777)\n",
      "tensor(61.4983)\n",
      "tensor(132.5149)\n",
      "tensor(3.6832)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.715683\n",
      "Epoch 9423\n",
      "-------------------------------\n",
      "tensor(106.8428)\n",
      "tensor(32.0017)\n",
      "tensor(79.1809)\n",
      "tensor(1.4939)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.288319\n",
      "Epoch 9424\n",
      "-------------------------------\n",
      "tensor(408.2449)\n",
      "tensor(133.0869)\n",
      "tensor(313.0930)\n",
      "tensor(7.3194)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.722851\n",
      "Epoch 9425\n",
      "-------------------------------\n",
      "tensor(388.9277)\n",
      "tensor(129.3693)\n",
      "tensor(295.6284)\n",
      "tensor(7.0721)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.306541\n",
      "Epoch 9426\n",
      "-------------------------------\n",
      "tensor(256.5493)\n",
      "tensor(80.4128)\n",
      "tensor(191.4411)\n",
      "tensor(4.5560)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.425934\n",
      "Epoch 9427\n",
      "-------------------------------\n",
      "tensor(501.8617)\n",
      "tensor(157.2349)\n",
      "tensor(375.2230)\n",
      "tensor(8.7753)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.409847\n",
      "Epoch 9428\n",
      "-------------------------------\n",
      "tensor(544.8828)\n",
      "tensor(176.4822)\n",
      "tensor(410.5469)\n",
      "tensor(10.2361)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.465767\n",
      "Epoch 9429\n",
      "-------------------------------\n",
      "tensor(65.8535)\n",
      "tensor(21.0541)\n",
      "tensor(37.2112)\n",
      "tensor(1.0705)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.087986\n",
      "Epoch 9430\n",
      "-------------------------------\n",
      "tensor(480.8272)\n",
      "tensor(159.2392)\n",
      "tensor(368.3538)\n",
      "tensor(8.9871)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.129257\n",
      "Epoch 9431\n",
      "-------------------------------\n",
      "tensor(887.3118)\n",
      "tensor(294.7367)\n",
      "tensor(685.8830)\n",
      "tensor(17.3826)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.841492\n",
      "Epoch 9432\n",
      "-------------------------------\n",
      "tensor(1228.2706)\n",
      "tensor(421.9413)\n",
      "tensor(959.3427)\n",
      "tensor(24.4819)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.376999\n",
      "Epoch 9433\n",
      "-------------------------------\n",
      "tensor(1485.6450)\n",
      "tensor(505.6725)\n",
      "tensor(1156.8385)\n",
      "tensor(29.2736)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 85.382027\n",
      "Epoch 9434\n",
      "-------------------------------\n",
      "tensor(1400.4855)\n",
      "tensor(471.0791)\n",
      "tensor(1083.2727)\n",
      "tensor(27.1542)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.164162\n",
      "Epoch 9435\n",
      "-------------------------------\n",
      "tensor(632.6486)\n",
      "tensor(207.3866)\n",
      "tensor(481.2526)\n",
      "tensor(11.9468)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.071682\n",
      "Epoch 9436\n",
      "-------------------------------\n",
      "tensor(523.0426)\n",
      "tensor(176.1641)\n",
      "tensor(403.2686)\n",
      "tensor(10.5131)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.354542\n",
      "Epoch 9437\n",
      "-------------------------------\n",
      "tensor(654.4454)\n",
      "tensor(209.4309)\n",
      "tensor(498.4934)\n",
      "tensor(11.8055)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.685574\n",
      "Epoch 9438\n",
      "-------------------------------\n",
      "tensor(426.2378)\n",
      "tensor(137.0359)\n",
      "tensor(323.5511)\n",
      "tensor(7.7109)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.095695\n",
      "Epoch 9439\n",
      "-------------------------------\n",
      "tensor(183.8088)\n",
      "tensor(62.1192)\n",
      "tensor(138.6747)\n",
      "tensor(3.4546)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.785618\n",
      "Epoch 9440\n",
      "-------------------------------\n",
      "tensor(406.1386)\n",
      "tensor(133.9527)\n",
      "tensor(310.3851)\n",
      "tensor(7.5456)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.728695\n",
      "Epoch 9441\n",
      "-------------------------------\n",
      "tensor(352.0609)\n",
      "tensor(114.7029)\n",
      "tensor(267.3678)\n",
      "tensor(6.4147)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.792576\n",
      "Epoch 9442\n",
      "-------------------------------\n",
      "tensor(175.2388)\n",
      "tensor(55.6739)\n",
      "tensor(133.4180)\n",
      "tensor(3.0859)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.749786\n",
      "Epoch 9443\n",
      "-------------------------------\n",
      "tensor(99.1174)\n",
      "tensor(36.0198)\n",
      "tensor(76.0794)\n",
      "tensor(2.0806)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.291885\n",
      "Epoch 9444\n",
      "-------------------------------\n",
      "tensor(401.3560)\n",
      "tensor(135.7150)\n",
      "tensor(309.0052)\n",
      "tensor(7.7777)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.664219\n",
      "Epoch 9445\n",
      "-------------------------------\n",
      "tensor(383.3378)\n",
      "tensor(127.0593)\n",
      "tensor(293.5719)\n",
      "tensor(7.2540)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.317802\n",
      "Epoch 9446\n",
      "-------------------------------\n",
      "tensor(240.2337)\n",
      "tensor(84.6668)\n",
      "tensor(187.0912)\n",
      "tensor(4.8153)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.319420\n",
      "Epoch 9447\n",
      "-------------------------------\n",
      "tensor(486.0489)\n",
      "tensor(167.6804)\n",
      "tensor(374.7439)\n",
      "tensor(9.4600)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.266804\n",
      "Epoch 9448\n",
      "-------------------------------\n",
      "tensor(520.8533)\n",
      "tensor(171.0510)\n",
      "tensor(398.9139)\n",
      "tensor(9.8198)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.028835\n",
      "Epoch 9449\n",
      "-------------------------------\n",
      "tensor(47.2736)\n",
      "tensor(16.2902)\n",
      "tensor(17.3479)\n",
      "tensor(0.3161)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.026608\n",
      "Epoch 9450\n",
      "-------------------------------\n",
      "tensor(495.3703)\n",
      "tensor(155.6017)\n",
      "tensor(372.8074)\n",
      "tensor(9.2357)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.418289\n",
      "Epoch 9451\n",
      "-------------------------------\n",
      "tensor(892.0394)\n",
      "tensor(297.4736)\n",
      "tensor(686.2982)\n",
      "tensor(16.9143)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.871330\n",
      "Epoch 9452\n",
      "-------------------------------\n",
      "tensor(1231.7177)\n",
      "tensor(413.0381)\n",
      "tensor(954.2116)\n",
      "tensor(23.8604)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.461372\n",
      "Epoch 9453\n",
      "-------------------------------\n",
      "tensor(1486.3862)\n",
      "tensor(504.8375)\n",
      "tensor(1152.4421)\n",
      "tensor(29.3682)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 84.726349\n",
      "Epoch 9454\n",
      "-------------------------------\n",
      "tensor(1403.0250)\n",
      "tensor(474.8413)\n",
      "tensor(1086.3175)\n",
      "tensor(27.0541)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.715775\n",
      "Epoch 9455\n",
      "-------------------------------\n",
      "tensor(629.8845)\n",
      "tensor(200.7318)\n",
      "tensor(480.4058)\n",
      "tensor(11.3819)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.135895\n",
      "Epoch 9456\n",
      "-------------------------------\n",
      "tensor(547.6298)\n",
      "tensor(176.7058)\n",
      "tensor(415.8151)\n",
      "tensor(9.7805)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.854416\n",
      "Epoch 9457\n",
      "-------------------------------\n",
      "tensor(654.4681)\n",
      "tensor(214.6314)\n",
      "tensor(503.0286)\n",
      "tensor(12.3593)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.670444\n",
      "Epoch 9458\n",
      "-------------------------------\n",
      "tensor(441.2982)\n",
      "tensor(135.7052)\n",
      "tensor(331.9986)\n",
      "tensor(7.5941)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.333725\n",
      "Epoch 9459\n",
      "-------------------------------\n",
      "tensor(170.7563)\n",
      "tensor(70.7025)\n",
      "tensor(139.5173)\n",
      "tensor(4.1956)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.802414\n",
      "Epoch 9460\n",
      "-------------------------------\n",
      "tensor(404.3256)\n",
      "tensor(145.8265)\n",
      "tensor(317.8284)\n",
      "tensor(8.5066)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.838833\n",
      "Epoch 9461\n",
      "-------------------------------\n",
      "tensor(353.5500)\n",
      "tensor(125.4220)\n",
      "tensor(275.7073)\n",
      "tensor(7.2980)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.923737\n",
      "Epoch 9462\n",
      "-------------------------------\n",
      "tensor(181.0558)\n",
      "tensor(64.2766)\n",
      "tensor(139.5917)\n",
      "tensor(3.7523)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.821087\n",
      "Epoch 9463\n",
      "-------------------------------\n",
      "tensor(97.4302)\n",
      "tensor(33.3044)\n",
      "tensor(74.8628)\n",
      "tensor(1.7840)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.282368\n",
      "Epoch 9464\n",
      "-------------------------------\n",
      "tensor(404.3956)\n",
      "tensor(141.6535)\n",
      "tensor(316.5114)\n",
      "tensor(8.0496)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.752846\n",
      "Epoch 9465\n",
      "-------------------------------\n",
      "tensor(386.3668)\n",
      "tensor(141.4473)\n",
      "tensor(307.8942)\n",
      "tensor(8.0645)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.479156\n",
      "Epoch 9466\n",
      "-------------------------------\n",
      "tensor(251.0539)\n",
      "tensor(71.8263)\n",
      "tensor(180.8389)\n",
      "tensor(4.1325)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.318699\n",
      "Epoch 9467\n",
      "-------------------------------\n",
      "tensor(500.2496)\n",
      "tensor(161.3715)\n",
      "tensor(381.8394)\n",
      "tensor(9.6411)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.602734\n",
      "Epoch 9468\n",
      "-------------------------------\n",
      "tensor(520.3780)\n",
      "tensor(175.2366)\n",
      "tensor(396.6285)\n",
      "tensor(9.4446)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.176628\n",
      "Epoch 9469\n",
      "-------------------------------\n",
      "tensor(47.1862)\n",
      "tensor(14.8564)\n",
      "tensor(7.7314)\n",
      "tensor(0.1341)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.157066\n",
      "Epoch 9470\n",
      "-------------------------------\n",
      "tensor(519.2514)\n",
      "tensor(171.4994)\n",
      "tensor(393.9141)\n",
      "tensor(9.1762)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.002403\n",
      "Epoch 9471\n",
      "-------------------------------\n",
      "tensor(921.7381)\n",
      "tensor(293.9446)\n",
      "tensor(702.0425)\n",
      "tensor(17.4407)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.995411\n",
      "Epoch 9472\n",
      "-------------------------------\n",
      "tensor(1241.3965)\n",
      "tensor(444.2174)\n",
      "tensor(981.2361)\n",
      "tensor(25.8077)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 70.428345\n",
      "Epoch 9473\n",
      "-------------------------------\n",
      "tensor(1511.5299)\n",
      "tensor(513.2386)\n",
      "tensor(1173.4531)\n",
      "tensor(29.6476)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.855911\n",
      "Epoch 9474\n",
      "-------------------------------\n",
      "tensor(1434.3883)\n",
      "tensor(467.6078)\n",
      "tensor(1095.1864)\n",
      "tensor(26.4971)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 80.580856\n",
      "Epoch 9475\n",
      "-------------------------------\n",
      "tensor(641.7891)\n",
      "tensor(215.3825)\n",
      "tensor(493.6357)\n",
      "tensor(12.4586)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.437229\n",
      "Epoch 9476\n",
      "-------------------------------\n",
      "tensor(538.6134)\n",
      "tensor(173.9827)\n",
      "tensor(406.4702)\n",
      "tensor(9.8117)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.546150\n",
      "Epoch 9477\n",
      "-------------------------------\n",
      "tensor(664.1900)\n",
      "tensor(217.4972)\n",
      "tensor(511.3513)\n",
      "tensor(12.7049)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.012058\n",
      "Epoch 9478\n",
      "-------------------------------\n",
      "tensor(438.7589)\n",
      "tensor(138.4716)\n",
      "tensor(330.0987)\n",
      "tensor(7.8537)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.267887\n",
      "Epoch 9479\n",
      "-------------------------------\n",
      "tensor(183.2387)\n",
      "tensor(68.3616)\n",
      "tensor(145.0698)\n",
      "tensor(3.9489)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.808716\n",
      "Epoch 9480\n",
      "-------------------------------\n",
      "tensor(412.2355)\n",
      "tensor(142.7026)\n",
      "tensor(321.5886)\n",
      "tensor(8.2037)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.875828\n",
      "Epoch 9481\n",
      "-------------------------------\n",
      "tensor(358.1063)\n",
      "tensor(121.9477)\n",
      "tensor(277.1511)\n",
      "tensor(6.9598)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.875126\n",
      "Epoch 9482\n",
      "-------------------------------\n",
      "tensor(181.6212)\n",
      "tensor(60.4093)\n",
      "tensor(138.7984)\n",
      "tensor(3.3946)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.721909\n",
      "Epoch 9483\n",
      "-------------------------------\n",
      "tensor(102.6168)\n",
      "tensor(37.2100)\n",
      "tensor(78.0510)\n",
      "tensor(2.1407)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.246010\n",
      "Epoch 9484\n",
      "-------------------------------\n",
      "tensor(410.6277)\n",
      "tensor(143.3337)\n",
      "tensor(319.6229)\n",
      "tensor(8.3288)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.830887\n",
      "Epoch 9485\n",
      "-------------------------------\n",
      "tensor(389.1274)\n",
      "tensor(137.7446)\n",
      "tensor(305.5953)\n",
      "tensor(8.1024)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.482498\n",
      "Epoch 9486\n",
      "-------------------------------\n",
      "tensor(250.3186)\n",
      "tensor(80.2076)\n",
      "tensor(188.0997)\n",
      "tensor(4.3460)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.441975\n",
      "Epoch 9487\n",
      "-------------------------------\n",
      "tensor(500.2964)\n",
      "tensor(172.3205)\n",
      "tensor(386.8091)\n",
      "tensor(9.8139)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.701382\n",
      "Epoch 9488\n",
      "-------------------------------\n",
      "tensor(534.1895)\n",
      "tensor(168.8774)\n",
      "tensor(403.8335)\n",
      "tensor(9.4175)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.328079\n",
      "Epoch 9489\n",
      "-------------------------------\n",
      "tensor(46.2729)\n",
      "tensor(17.4205)\n",
      "tensor(9.4486)\n",
      "tensor(0.3438)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.094955\n",
      "Epoch 9490\n",
      "-------------------------------\n",
      "tensor(522.2709)\n",
      "tensor(160.5154)\n",
      "tensor(391.7300)\n",
      "tensor(9.3377)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.002697\n",
      "Epoch 9491\n",
      "-------------------------------\n",
      "tensor(922.2194)\n",
      "tensor(311.7530)\n",
      "tensor(713.0298)\n",
      "tensor(17.9295)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.211685\n",
      "Epoch 9492\n",
      "-------------------------------\n",
      "tensor(1266.2684)\n",
      "tensor(432.6312)\n",
      "tensor(988.5694)\n",
      "tensor(25.1551)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 71.753220\n",
      "Epoch 9493\n",
      "-------------------------------\n",
      "tensor(1528.2760)\n",
      "tensor(514.4666)\n",
      "tensor(1185.5500)\n",
      "tensor(29.9132)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 87.934204\n",
      "Epoch 9494\n",
      "-------------------------------\n",
      "tensor(1426.9243)\n",
      "tensor(485.4146)\n",
      "tensor(1110.9271)\n",
      "tensor(27.9005)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 81.444801\n",
      "Epoch 9495\n",
      "-------------------------------\n",
      "tensor(631.3287)\n",
      "tensor(210.7780)\n",
      "tensor(485.6154)\n",
      "tensor(12.2301)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.009995\n",
      "Epoch 9496\n",
      "-------------------------------\n",
      "tensor(549.3615)\n",
      "tensor(183.5289)\n",
      "tensor(422.2531)\n",
      "tensor(10.5989)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.813515\n",
      "Epoch 9497\n",
      "-------------------------------\n",
      "tensor(665.7376)\n",
      "tensor(217.6049)\n",
      "tensor(511.4562)\n",
      "tensor(12.7563)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.850166\n",
      "Epoch 9498\n",
      "-------------------------------\n",
      "tensor(433.6092)\n",
      "tensor(148.0501)\n",
      "tensor(336.6555)\n",
      "tensor(8.4168)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.245670\n",
      "Epoch 9499\n",
      "-------------------------------\n",
      "tensor(184.2580)\n",
      "tensor(58.5650)\n",
      "tensor(137.5766)\n",
      "tensor(3.3523)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.715855\n",
      "Epoch 9500\n",
      "-------------------------------\n",
      "tensor(415.3589)\n",
      "tensor(136.4846)\n",
      "tensor(316.9700)\n",
      "tensor(7.8484)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.784477\n",
      "Epoch 9501\n",
      "-------------------------------\n",
      "tensor(358.9217)\n",
      "tensor(118.2089)\n",
      "tensor(274.6906)\n",
      "tensor(6.8412)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.820045\n",
      "Epoch 9502\n",
      "-------------------------------\n",
      "tensor(183.2896)\n",
      "tensor(60.3028)\n",
      "tensor(138.5824)\n",
      "tensor(3.4894)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.682724\n",
      "Epoch 9503\n",
      "-------------------------------\n",
      "tensor(115.0888)\n",
      "tensor(39.1139)\n",
      "tensor(74.8266)\n",
      "tensor(1.7877)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.214375\n",
      "Epoch 9504\n",
      "-------------------------------\n",
      "tensor(408.7240)\n",
      "tensor(138.3381)\n",
      "tensor(312.7297)\n",
      "tensor(7.7577)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.718761\n",
      "Epoch 9505\n",
      "-------------------------------\n",
      "tensor(397.2029)\n",
      "tensor(134.6974)\n",
      "tensor(301.4870)\n",
      "tensor(7.6152)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.414219\n",
      "Epoch 9506\n",
      "-------------------------------\n",
      "tensor(251.7254)\n",
      "tensor(81.4475)\n",
      "tensor(188.7182)\n",
      "tensor(4.3662)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.314842\n",
      "Epoch 9507\n",
      "-------------------------------\n",
      "tensor(494.4155)\n",
      "tensor(168.3966)\n",
      "tensor(382.5491)\n",
      "tensor(9.4550)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.490841\n",
      "Epoch 9508\n",
      "-------------------------------\n",
      "tensor(534.9720)\n",
      "tensor(162.3620)\n",
      "tensor(399.4151)\n",
      "tensor(9.5449)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.257061\n",
      "Epoch 9509\n",
      "-------------------------------\n",
      "tensor(40.1559)\n",
      "tensor(15.9548)\n",
      "tensor(20.6448)\n",
      "tensor(0.8338)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.091373\n",
      "Epoch 9510\n",
      "-------------------------------\n",
      "tensor(501.1652)\n",
      "tensor(158.9963)\n",
      "tensor(377.7214)\n",
      "tensor(9.0476)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.474888\n",
      "Epoch 9511\n",
      "-------------------------------\n",
      "tensor(896.1880)\n",
      "tensor(305.6624)\n",
      "tensor(694.2673)\n",
      "tensor(17.5957)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 53.147667\n",
      "Epoch 9512\n",
      "-------------------------------\n",
      "tensor(1251.3026)\n",
      "tensor(412.9972)\n",
      "tensor(959.8608)\n",
      "tensor(23.7841)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 69.806198\n",
      "Epoch 9513\n",
      "-------------------------------\n",
      "tensor(1504.3300)\n",
      "tensor(505.3531)\n",
      "tensor(1166.1718)\n",
      "tensor(28.8923)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 86.506622\n",
      "Epoch 9514\n",
      "-------------------------------\n",
      "tensor(1394.8163)\n",
      "tensor(477.4109)\n",
      "tensor(1091.7791)\n",
      "tensor(27.8424)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 79.533966\n",
      "Epoch 9515\n",
      "-------------------------------\n",
      "tensor(596.7766)\n",
      "tensor(203.0314)\n",
      "tensor(461.5493)\n",
      "tensor(11.8188)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.108669\n",
      "Epoch 9516\n",
      "-------------------------------\n",
      "tensor(552.0797)\n",
      "tensor(190.2555)\n",
      "tensor(432.3589)\n",
      "tensor(11.3591)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.027462\n",
      "Epoch 9517\n",
      "-------------------------------\n",
      "tensor(635.6599)\n",
      "tensor(217.7024)\n",
      "tensor(492.6053)\n",
      "tensor(12.2933)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 44.172249\n",
      "Epoch 9518\n",
      "-------------------------------\n",
      "tensor(436.0203)\n",
      "tensor(157.3674)\n",
      "tensor(345.5850)\n",
      "tensor(9.2056)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.430798\n",
      "Epoch 9519\n",
      "-------------------------------\n",
      "tensor(172.1078)\n",
      "tensor(47.4044)\n",
      "tensor(118.3203)\n",
      "tensor(2.2355)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.644161\n",
      "Epoch 9520\n",
      "-------------------------------\n",
      "tensor(403.6642)\n",
      "tensor(128.4191)\n",
      "tensor(303.7454)\n",
      "tensor(7.0449)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.616714\n",
      "Epoch 9521\n",
      "-------------------------------\n",
      "tensor(354.6461)\n",
      "tensor(115.9418)\n",
      "tensor(271.6352)\n",
      "tensor(6.4704)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.866272\n",
      "Epoch 9522\n",
      "-------------------------------\n",
      "tensor(187.9607)\n",
      "tensor(63.1793)\n",
      "tensor(144.4753)\n",
      "tensor(3.5074)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.845860\n",
      "Epoch 9523\n",
      "-------------------------------\n",
      "tensor(90.1888)\n",
      "tensor(26.1486)\n",
      "tensor(62.0436)\n",
      "tensor(1.4113)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.205429\n",
      "Epoch 9524\n",
      "-------------------------------\n",
      "tensor(400.7375)\n",
      "tensor(126.6735)\n",
      "tensor(300.1617)\n",
      "tensor(7.1445)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.539547\n",
      "Epoch 9525\n",
      "-------------------------------\n",
      "tensor(399.9763)\n",
      "tensor(125.0015)\n",
      "tensor(299.0606)\n",
      "tensor(7.1210)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.513103\n",
      "Epoch 9526\n",
      "-------------------------------\n",
      "tensor(220.6842)\n",
      "tensor(80.4359)\n",
      "tensor(176.0201)\n",
      "tensor(4.3476)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.149551\n",
      "Epoch 9527\n",
      "-------------------------------\n",
      "tensor(498.9230)\n",
      "tensor(168.4480)\n",
      "tensor(385.1505)\n",
      "tensor(9.1260)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.579868\n",
      "Epoch 9528\n",
      "-------------------------------\n",
      "tensor(503.9791)\n",
      "tensor(166.4666)\n",
      "tensor(386.4443)\n",
      "tensor(9.8499)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.577980\n",
      "Epoch 9529\n",
      "-------------------------------\n",
      "tensor(51.6213)\n",
      "tensor(16.6519)\n",
      "tensor(6.3946)\n",
      "tensor(0.1757)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.165840\n",
      "Epoch 9530\n",
      "-------------------------------\n",
      "tensor(513.0135)\n",
      "tensor(172.3548)\n",
      "tensor(396.7375)\n",
      "tensor(10.6541)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.212132\n",
      "Epoch 9531\n",
      "-------------------------------\n",
      "tensor(883.1161)\n",
      "tensor(306.5974)\n",
      "tensor(694.7478)\n",
      "tensor(17.6351)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.935272\n",
      "Epoch 9532\n",
      "-------------------------------\n",
      "tensor(1196.6313)\n",
      "tensor(393.9607)\n",
      "tensor(918.7028)\n",
      "tensor(22.9166)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 67.329582\n",
      "Epoch 9533\n",
      "-------------------------------\n",
      "tensor(1391.9209)\n",
      "tensor(476.5492)\n",
      "tensor(1088.6534)\n",
      "tensor(27.2184)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 79.455963\n",
      "Epoch 9534\n",
      "-------------------------------\n",
      "tensor(1267.3734)\n",
      "tensor(428.9222)\n",
      "tensor(987.3757)\n",
      "tensor(25.3986)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.814255\n",
      "Epoch 9535\n",
      "-------------------------------\n",
      "tensor(515.7581)\n",
      "tensor(182.1884)\n",
      "tensor(404.3439)\n",
      "tensor(10.5844)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.194557\n",
      "Epoch 9536\n",
      "-------------------------------\n",
      "tensor(518.3015)\n",
      "tensor(181.9210)\n",
      "tensor(406.8672)\n",
      "tensor(10.8693)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.309322\n",
      "Epoch 9537\n",
      "-------------------------------\n",
      "tensor(576.6036)\n",
      "tensor(189.1532)\n",
      "tensor(441.4766)\n",
      "tensor(10.8446)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.529984\n",
      "Epoch 9538\n",
      "-------------------------------\n",
      "tensor(397.5150)\n",
      "tensor(148.8613)\n",
      "tensor(320.1602)\n",
      "tensor(8.4994)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.957657\n",
      "Epoch 9539\n",
      "-------------------------------\n",
      "tensor(150.8212)\n",
      "tensor(33.8026)\n",
      "tensor(99.3200)\n",
      "tensor(1.7672)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.672333\n",
      "Epoch 9540\n",
      "-------------------------------\n",
      "tensor(369.9274)\n",
      "tensor(108.9601)\n",
      "tensor(271.0697)\n",
      "tensor(6.1958)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.104412\n",
      "Epoch 9541\n",
      "-------------------------------\n",
      "tensor(331.3325)\n",
      "tensor(99.7161)\n",
      "tensor(244.5063)\n",
      "tensor(5.7617)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.534065\n",
      "Epoch 9542\n",
      "-------------------------------\n",
      "tensor(179.3091)\n",
      "tensor(52.6870)\n",
      "tensor(130.1510)\n",
      "tensor(3.1531)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.847504\n",
      "Epoch 9543\n",
      "-------------------------------\n",
      "tensor(78.0852)\n",
      "tensor(28.5372)\n",
      "tensor(57.6964)\n",
      "tensor(1.2247)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.280537\n",
      "Epoch 9544\n",
      "-------------------------------\n",
      "tensor(360.7386)\n",
      "tensor(117.7922)\n",
      "tensor(275.2242)\n",
      "tensor(6.3219)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.997185\n",
      "Epoch 9545\n",
      "-------------------------------\n",
      "tensor(366.6968)\n",
      "tensor(115.4515)\n",
      "tensor(276.8793)\n",
      "tensor(6.2135)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.025356\n",
      "Epoch 9546\n",
      "-------------------------------\n",
      "tensor(197.3716)\n",
      "tensor(72.9887)\n",
      "tensor(155.9662)\n",
      "tensor(4.2433)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.949680\n",
      "Epoch 9547\n",
      "-------------------------------\n",
      "tensor(463.3455)\n",
      "tensor(155.2052)\n",
      "tensor(353.9184)\n",
      "tensor(8.4990)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.766048\n",
      "Epoch 9548\n",
      "-------------------------------\n",
      "tensor(466.0586)\n",
      "tensor(148.1884)\n",
      "tensor(351.4919)\n",
      "tensor(9.1520)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.830574\n",
      "Epoch 9549\n",
      "-------------------------------\n",
      "tensor(61.8972)\n",
      "tensor(20.4256)\n",
      "tensor(1.8900)\n",
      "tensor(0.1289)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.258335\n",
      "Epoch 9550\n",
      "-------------------------------\n",
      "tensor(473.2901)\n",
      "tensor(159.9373)\n",
      "tensor(369.4448)\n",
      "tensor(9.2147)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.270218\n",
      "Epoch 9551\n",
      "-------------------------------\n",
      "tensor(820.7084)\n",
      "tensor(275.8260)\n",
      "tensor(636.6915)\n",
      "tensor(16.3718)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.231884\n",
      "Epoch 9552\n",
      "-------------------------------\n",
      "tensor(1118.0092)\n",
      "tensor(376.5077)\n",
      "tensor(863.7884)\n",
      "tensor(21.7253)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.092373\n",
      "Epoch 9553\n",
      "-------------------------------\n",
      "tensor(1341.7513)\n",
      "tensor(448.4649)\n",
      "tensor(1037.6260)\n",
      "tensor(25.4997)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 75.663681\n",
      "Epoch 9554\n",
      "-------------------------------\n",
      "tensor(1262.0116)\n",
      "tensor(420.9109)\n",
      "tensor(972.7347)\n",
      "tensor(24.4535)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 70.773918\n",
      "Epoch 9555\n",
      "-------------------------------\n",
      "tensor(558.0053)\n",
      "tensor(197.9284)\n",
      "tensor(440.0067)\n",
      "tensor(11.2417)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.342178\n",
      "Epoch 9556\n",
      "-------------------------------\n",
      "tensor(465.7024)\n",
      "tensor(165.9273)\n",
      "tensor(369.0004)\n",
      "tensor(9.5332)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.211658\n",
      "Epoch 9557\n",
      "-------------------------------\n",
      "tensor(582.1807)\n",
      "tensor(193.3014)\n",
      "tensor(447.8488)\n",
      "tensor(11.1600)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.741894\n",
      "Epoch 9558\n",
      "-------------------------------\n",
      "tensor(379.5002)\n",
      "tensor(130.5395)\n",
      "tensor(295.1280)\n",
      "tensor(7.6260)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.348705\n",
      "Epoch 9559\n",
      "-------------------------------\n",
      "tensor(164.9863)\n",
      "tensor(50.8142)\n",
      "tensor(119.9930)\n",
      "tensor(2.6579)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.657372\n",
      "Epoch 9560\n",
      "-------------------------------\n",
      "tensor(365.8011)\n",
      "tensor(118.8955)\n",
      "tensor(276.8318)\n",
      "tensor(6.6459)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.038059\n",
      "Epoch 9561\n",
      "-------------------------------\n",
      "tensor(318.0948)\n",
      "tensor(104.2684)\n",
      "tensor(240.5298)\n",
      "tensor(5.8506)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.300762\n",
      "Epoch 9562\n",
      "-------------------------------\n",
      "tensor(163.9352)\n",
      "tensor(53.6678)\n",
      "tensor(121.7973)\n",
      "tensor(3.0028)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.641144\n",
      "Epoch 9563\n",
      "-------------------------------\n",
      "tensor(91.8856)\n",
      "tensor(29.8207)\n",
      "tensor(65.9838)\n",
      "tensor(1.5507)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.237236\n",
      "Epoch 9564\n",
      "-------------------------------\n",
      "tensor(363.4716)\n",
      "tensor(119.9405)\n",
      "tensor(276.2539)\n",
      "tensor(6.7128)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.977489\n",
      "Epoch 9565\n",
      "-------------------------------\n",
      "tensor(347.5885)\n",
      "tensor(114.8692)\n",
      "tensor(264.5184)\n",
      "tensor(6.5019)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.713528\n",
      "Epoch 9566\n",
      "-------------------------------\n",
      "tensor(215.8100)\n",
      "tensor(72.3251)\n",
      "tensor(167.5621)\n",
      "tensor(3.9645)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.056667\n",
      "Epoch 9567\n",
      "-------------------------------\n",
      "tensor(450.0548)\n",
      "tensor(140.6520)\n",
      "tensor(339.6273)\n",
      "tensor(8.0692)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.305946\n",
      "Epoch 9568\n",
      "-------------------------------\n",
      "tensor(475.8259)\n",
      "tensor(151.7981)\n",
      "tensor(363.3252)\n",
      "tensor(8.9160)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.006859\n",
      "Epoch 9569\n",
      "-------------------------------\n",
      "tensor(55.1124)\n",
      "tensor(26.5495)\n",
      "tensor(18.2925)\n",
      "tensor(1.1052)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.102718\n",
      "Epoch 9570\n",
      "-------------------------------\n",
      "tensor(459.5058)\n",
      "tensor(151.3140)\n",
      "tensor(349.2977)\n",
      "tensor(9.1456)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.590843\n",
      "Epoch 9571\n",
      "-------------------------------\n",
      "tensor(823.7688)\n",
      "tensor(284.2449)\n",
      "tensor(641.4327)\n",
      "tensor(16.3124)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.332378\n",
      "Epoch 9572\n",
      "-------------------------------\n",
      "tensor(1151.9144)\n",
      "tensor(373.1711)\n",
      "tensor(881.7177)\n",
      "tensor(21.4208)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.850517\n",
      "Epoch 9573\n",
      "-------------------------------\n",
      "tensor(1369.5432)\n",
      "tensor(463.3427)\n",
      "tensor(1065.8611)\n",
      "tensor(26.7340)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.744919\n",
      "Epoch 9574\n",
      "-------------------------------\n",
      "tensor(1265.7610)\n",
      "tensor(435.4494)\n",
      "tensor(991.3779)\n",
      "tensor(25.5266)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 71.682243\n",
      "Epoch 9575\n",
      "-------------------------------\n",
      "tensor(553.5275)\n",
      "tensor(180.8897)\n",
      "tensor(421.9555)\n",
      "tensor(10.4770)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.881020\n",
      "Epoch 9576\n",
      "-------------------------------\n",
      "tensor(494.6612)\n",
      "tensor(169.6596)\n",
      "tensor(384.9422)\n",
      "tensor(10.2499)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.643585\n",
      "Epoch 9577\n",
      "-------------------------------\n",
      "tensor(582.7613)\n",
      "tensor(195.1482)\n",
      "tensor(450.2765)\n",
      "tensor(10.6826)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.714031\n",
      "Epoch 9578\n",
      "-------------------------------\n",
      "tensor(391.9243)\n",
      "tensor(137.7287)\n",
      "tensor(309.0086)\n",
      "tensor(7.6846)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.583801\n",
      "Epoch 9579\n",
      "-------------------------------\n",
      "tensor(158.8972)\n",
      "tensor(44.0483)\n",
      "tensor(112.3994)\n",
      "tensor(2.4968)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.562538\n",
      "Epoch 9580\n",
      "-------------------------------\n",
      "tensor(371.4729)\n",
      "tensor(115.5135)\n",
      "tensor(277.1999)\n",
      "tensor(6.5631)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.012924\n",
      "Epoch 9581\n",
      "-------------------------------\n",
      "tensor(326.8356)\n",
      "tensor(102.1499)\n",
      "tensor(244.0587)\n",
      "tensor(5.8232)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.321327\n",
      "Epoch 9582\n",
      "-------------------------------\n",
      "tensor(171.0662)\n",
      "tensor(52.2130)\n",
      "tensor(125.4849)\n",
      "tensor(3.0054)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.616199\n",
      "Epoch 9583\n",
      "-------------------------------\n",
      "tensor(88.6791)\n",
      "tensor(31.2707)\n",
      "tensor(63.7318)\n",
      "tensor(1.5137)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.138199\n",
      "Epoch 9584\n",
      "-------------------------------\n",
      "tensor(363.8176)\n",
      "tensor(120.1235)\n",
      "tensor(278.1574)\n",
      "tensor(6.6155)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.901279\n",
      "Epoch 9585\n",
      "-------------------------------\n",
      "tensor(363.0289)\n",
      "tensor(116.0758)\n",
      "tensor(271.4362)\n",
      "tensor(6.2884)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.807766\n",
      "Epoch 9586\n",
      "-------------------------------\n",
      "tensor(210.3541)\n",
      "tensor(75.1046)\n",
      "tensor(165.2153)\n",
      "tensor(4.3609)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.953968\n",
      "Epoch 9587\n",
      "-------------------------------\n",
      "tensor(457.7599)\n",
      "tensor(153.1901)\n",
      "tensor(351.6679)\n",
      "tensor(8.4680)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.623962\n",
      "Epoch 9588\n",
      "-------------------------------\n",
      "tensor(474.1237)\n",
      "tensor(151.8903)\n",
      "tensor(357.0938)\n",
      "tensor(9.3202)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.938473\n",
      "Epoch 9589\n",
      "-------------------------------\n",
      "tensor(42.8324)\n",
      "tensor(14.2240)\n",
      "tensor(9.5944)\n",
      "tensor(0.1245)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.042404\n",
      "Epoch 9590\n",
      "-------------------------------\n",
      "tensor(452.2115)\n",
      "tensor(159.1498)\n",
      "tensor(355.2650)\n",
      "tensor(9.2670)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.678440\n",
      "Epoch 9591\n",
      "-------------------------------\n",
      "tensor(819.5150)\n",
      "tensor(269.9443)\n",
      "tensor(627.9179)\n",
      "tensor(15.5865)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.876343\n",
      "Epoch 9592\n",
      "-------------------------------\n",
      "tensor(1139.5631)\n",
      "tensor(371.7896)\n",
      "tensor(869.6845)\n",
      "tensor(21.1307)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.755951\n",
      "Epoch 9593\n",
      "-------------------------------\n",
      "tensor(1368.2249)\n",
      "tensor(465.1032)\n",
      "tensor(1065.9495)\n",
      "tensor(26.7274)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.760750\n",
      "Epoch 9594\n",
      "-------------------------------\n",
      "tensor(1285.0706)\n",
      "tensor(439.9138)\n",
      "tensor(1003.4224)\n",
      "tensor(25.8948)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.569778\n",
      "Epoch 9595\n",
      "-------------------------------\n",
      "tensor(576.6759)\n",
      "tensor(186.9738)\n",
      "tensor(440.3719)\n",
      "tensor(10.8429)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.448830\n",
      "Epoch 9596\n",
      "-------------------------------\n",
      "tensor(477.1358)\n",
      "tensor(175.3202)\n",
      "tensor(380.7122)\n",
      "tensor(10.2921)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.475880\n",
      "Epoch 9597\n",
      "-------------------------------\n",
      "tensor(597.5363)\n",
      "tensor(190.0324)\n",
      "tensor(452.1775)\n",
      "tensor(10.6143)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.986271\n",
      "Epoch 9598\n",
      "-------------------------------\n",
      "tensor(392.8497)\n",
      "tensor(132.1071)\n",
      "tensor(301.9600)\n",
      "tensor(7.5727)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.489849\n",
      "Epoch 9599\n",
      "-------------------------------\n",
      "tensor(167.1051)\n",
      "tensor(49.8275)\n",
      "tensor(120.0644)\n",
      "tensor(2.4564)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.630817\n",
      "Epoch 9600\n",
      "-------------------------------\n",
      "tensor(374.0896)\n",
      "tensor(117.3932)\n",
      "tensor(281.1774)\n",
      "tensor(6.3678)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.107334\n",
      "Epoch 9601\n",
      "-------------------------------\n",
      "tensor(326.8080)\n",
      "tensor(102.4957)\n",
      "tensor(245.2728)\n",
      "tensor(5.5507)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.359871\n",
      "Epoch 9602\n",
      "-------------------------------\n",
      "tensor(165.1545)\n",
      "tensor(50.8333)\n",
      "tensor(125.0200)\n",
      "tensor(2.7101)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.626980\n",
      "Epoch 9603\n",
      "-------------------------------\n",
      "tensor(88.0017)\n",
      "tensor(31.3022)\n",
      "tensor(65.6234)\n",
      "tensor(1.8044)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.196259\n",
      "Epoch 9604\n",
      "-------------------------------\n",
      "tensor(369.0971)\n",
      "tensor(120.5099)\n",
      "tensor(279.6355)\n",
      "tensor(6.8153)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.017216\n",
      "Epoch 9605\n",
      "-------------------------------\n",
      "tensor(360.6387)\n",
      "tensor(112.7486)\n",
      "tensor(268.8977)\n",
      "tensor(6.2841)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.810005\n",
      "Epoch 9606\n",
      "-------------------------------\n",
      "tensor(214.8105)\n",
      "tensor(80.3676)\n",
      "tensor(170.5207)\n",
      "tensor(4.5402)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.051743\n",
      "Epoch 9607\n",
      "-------------------------------\n",
      "tensor(446.8020)\n",
      "tensor(153.5818)\n",
      "tensor(344.8300)\n",
      "tensor(8.4512)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.553711\n",
      "Epoch 9608\n",
      "-------------------------------\n",
      "tensor(463.9442)\n",
      "tensor(155.1922)\n",
      "tensor(360.4749)\n",
      "tensor(9.2387)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.919044\n",
      "Epoch 9609\n",
      "-------------------------------\n",
      "tensor(51.3912)\n",
      "tensor(16.9425)\n",
      "tensor(3.8637)\n",
      "tensor(0.2466)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.005665\n",
      "Epoch 9610\n",
      "-------------------------------\n",
      "tensor(460.2590)\n",
      "tensor(151.2902)\n",
      "tensor(355.7994)\n",
      "tensor(9.5395)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.764111\n",
      "Epoch 9611\n",
      "-------------------------------\n",
      "tensor(807.8396)\n",
      "tensor(280.8319)\n",
      "tensor(632.6223)\n",
      "tensor(16.1295)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.845192\n",
      "Epoch 9612\n",
      "-------------------------------\n",
      "tensor(1115.3108)\n",
      "tensor(364.7708)\n",
      "tensor(856.2512)\n",
      "tensor(20.9567)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 62.928226\n",
      "Epoch 9613\n",
      "-------------------------------\n",
      "tensor(1327.8961)\n",
      "tensor(446.7966)\n",
      "tensor(1027.7726)\n",
      "tensor(25.7742)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.721603\n",
      "Epoch 9614\n",
      "-------------------------------\n",
      "tensor(1236.0826)\n",
      "tensor(427.0693)\n",
      "tensor(965.0699)\n",
      "tensor(24.7107)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.921860\n",
      "Epoch 9615\n",
      "-------------------------------\n",
      "tensor(553.7959)\n",
      "tensor(177.6978)\n",
      "tensor(421.7994)\n",
      "tensor(10.1489)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.881012\n",
      "Epoch 9616\n",
      "-------------------------------\n",
      "tensor(472.5268)\n",
      "tensor(159.2523)\n",
      "tensor(367.3359)\n",
      "tensor(9.2260)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.115974\n",
      "Epoch 9617\n",
      "-------------------------------\n",
      "tensor(578.4872)\n",
      "tensor(181.6121)\n",
      "tensor(435.7818)\n",
      "tensor(10.6483)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.287693\n",
      "Epoch 9618\n",
      "-------------------------------\n",
      "tensor(375.0446)\n",
      "tensor(123.1347)\n",
      "tensor(288.3172)\n",
      "tensor(7.1061)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.185333\n",
      "Epoch 9619\n",
      "-------------------------------\n",
      "tensor(155.6227)\n",
      "tensor(53.4180)\n",
      "tensor(117.8826)\n",
      "tensor(2.9101)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.571712\n",
      "Epoch 9620\n",
      "-------------------------------\n",
      "tensor(356.4787)\n",
      "tensor(120.1890)\n",
      "tensor(273.0861)\n",
      "tensor(6.7633)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.863487\n",
      "Epoch 9621\n",
      "-------------------------------\n",
      "tensor(310.0280)\n",
      "tensor(104.6807)\n",
      "tensor(237.3436)\n",
      "tensor(5.9197)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.169788\n",
      "Epoch 9622\n",
      "-------------------------------\n",
      "tensor(158.4544)\n",
      "tensor(54.0720)\n",
      "tensor(120.1227)\n",
      "tensor(3.0668)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.577003\n",
      "Epoch 9623\n",
      "-------------------------------\n",
      "tensor(84.4007)\n",
      "tensor(27.3567)\n",
      "tensor(65.0178)\n",
      "tensor(1.4520)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.168682\n",
      "Epoch 9624\n",
      "-------------------------------\n",
      "tensor(359.0146)\n",
      "tensor(118.8611)\n",
      "tensor(272.8476)\n",
      "tensor(6.5197)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.838207\n",
      "Epoch 9625\n",
      "-------------------------------\n",
      "tensor(341.6571)\n",
      "tensor(115.4067)\n",
      "tensor(262.6431)\n",
      "tensor(6.3032)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.628433\n",
      "Epoch 9626\n",
      "-------------------------------\n",
      "tensor(215.7425)\n",
      "tensor(66.4711)\n",
      "tensor(160.5234)\n",
      "tensor(3.9570)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.992683\n",
      "Epoch 9627\n",
      "-------------------------------\n",
      "tensor(439.3495)\n",
      "tensor(136.9462)\n",
      "tensor(331.2132)\n",
      "tensor(7.9020)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.191902\n",
      "Epoch 9628\n",
      "-------------------------------\n",
      "tensor(444.4058)\n",
      "tensor(157.9973)\n",
      "tensor(349.3924)\n",
      "tensor(8.9866)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.448059\n",
      "Epoch 9629\n",
      "-------------------------------\n",
      "tensor(31.2864)\n",
      "tensor(12.4965)\n",
      "tensor(5.1974)\n",
      "tensor(0.2360)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 34.981377\n",
      "Epoch 9630\n",
      "-------------------------------\n",
      "tensor(437.9903)\n",
      "tensor(155.3915)\n",
      "tensor(345.5510)\n",
      "tensor(8.9983)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.441391\n",
      "Epoch 9631\n",
      "-------------------------------\n",
      "tensor(782.6021)\n",
      "tensor(255.1146)\n",
      "tensor(601.3532)\n",
      "tensor(15.2150)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.873760\n",
      "Epoch 9632\n",
      "-------------------------------\n",
      "tensor(1056.9216)\n",
      "tensor(359.7426)\n",
      "tensor(821.2643)\n",
      "tensor(20.5440)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 60.356705\n",
      "Epoch 9633\n",
      "-------------------------------\n",
      "tensor(1255.9349)\n",
      "tensor(421.4478)\n",
      "tensor(974.3766)\n",
      "tensor(24.3786)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.790428\n",
      "Epoch 9634\n",
      "-------------------------------\n",
      "tensor(1160.8468)\n",
      "tensor(392.8505)\n",
      "tensor(900.0987)\n",
      "tensor(22.9112)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.329590\n",
      "Epoch 9635\n",
      "-------------------------------\n",
      "tensor(507.6567)\n",
      "tensor(167.0449)\n",
      "tensor(386.8108)\n",
      "tensor(9.5572)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.722004\n",
      "Epoch 9636\n",
      "-------------------------------\n",
      "tensor(433.0871)\n",
      "tensor(163.2083)\n",
      "tensor(349.0093)\n",
      "tensor(9.3809)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.537842\n",
      "Epoch 9637\n",
      "-------------------------------\n",
      "tensor(530.5041)\n",
      "tensor(163.3807)\n",
      "tensor(398.5396)\n",
      "tensor(9.2811)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.173714\n",
      "Epoch 9638\n",
      "-------------------------------\n",
      "tensor(362.9392)\n",
      "tensor(115.9181)\n",
      "tensor(273.9299)\n",
      "tensor(6.6818)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.814503\n",
      "Epoch 9639\n",
      "-------------------------------\n",
      "tensor(145.8010)\n",
      "tensor(46.5938)\n",
      "tensor(104.0306)\n",
      "tensor(2.2847)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.371346\n",
      "Epoch 9640\n",
      "-------------------------------\n",
      "tensor(335.1146)\n",
      "tensor(107.3999)\n",
      "tensor(250.8452)\n",
      "tensor(5.8190)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.370197\n",
      "Epoch 9641\n",
      "-------------------------------\n",
      "tensor(294.1243)\n",
      "tensor(93.7477)\n",
      "tensor(220.2934)\n",
      "tensor(5.1123)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.797302\n",
      "Epoch 9642\n",
      "-------------------------------\n",
      "tensor(148.7584)\n",
      "tensor(46.4549)\n",
      "tensor(113.2528)\n",
      "tensor(2.5634)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.423416\n",
      "Epoch 9643\n",
      "-------------------------------\n",
      "tensor(76.8825)\n",
      "tensor(27.7002)\n",
      "tensor(57.1868)\n",
      "tensor(1.5187)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.067577\n",
      "Epoch 9644\n",
      "-------------------------------\n",
      "tensor(327.4039)\n",
      "tensor(109.3753)\n",
      "tensor(249.8667)\n",
      "tensor(6.1144)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.315186\n",
      "Epoch 9645\n",
      "-------------------------------\n",
      "tensor(319.7570)\n",
      "tensor(105.0952)\n",
      "tensor(243.3961)\n",
      "tensor(5.8311)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.224510\n",
      "Epoch 9646\n",
      "-------------------------------\n",
      "tensor(188.3843)\n",
      "tensor(64.8352)\n",
      "tensor(146.7057)\n",
      "tensor(3.7717)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.750782\n",
      "Epoch 9647\n",
      "-------------------------------\n",
      "tensor(410.9873)\n",
      "tensor(136.0359)\n",
      "tensor(310.6541)\n",
      "tensor(7.5233)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.608067\n",
      "Epoch 9648\n",
      "-------------------------------\n",
      "tensor(423.3316)\n",
      "tensor(141.9952)\n",
      "tensor(325.1836)\n",
      "tensor(8.1492)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.856293\n",
      "Epoch 9649\n",
      "-------------------------------\n",
      "tensor(54.0410)\n",
      "tensor(18.6164)\n",
      "tensor(7.8970)\n",
      "tensor(0.1862)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.030338\n",
      "Epoch 9650\n",
      "-------------------------------\n",
      "tensor(411.5027)\n",
      "tensor(132.9744)\n",
      "tensor(309.5098)\n",
      "tensor(8.0694)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.708569\n",
      "Epoch 9651\n",
      "-------------------------------\n",
      "tensor(735.6747)\n",
      "tensor(243.1545)\n",
      "tensor(565.7447)\n",
      "tensor(13.7339)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.089844\n",
      "Epoch 9652\n",
      "-------------------------------\n",
      "tensor(1016.3489)\n",
      "tensor(333.8607)\n",
      "tensor(780.8295)\n",
      "tensor(19.2672)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.019867\n",
      "Epoch 9653\n",
      "-------------------------------\n",
      "tensor(1224.0487)\n",
      "tensor(417.6452)\n",
      "tensor(950.3701)\n",
      "tensor(24.0871)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 68.860085\n",
      "Epoch 9654\n",
      "-------------------------------\n",
      "tensor(1166.4701)\n",
      "tensor(390.7710)\n",
      "tensor(904.9525)\n",
      "tensor(22.6051)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.717621\n",
      "Epoch 9655\n",
      "-------------------------------\n",
      "tensor(542.1043)\n",
      "tensor(177.7280)\n",
      "tensor(413.3928)\n",
      "tensor(10.3123)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.605289\n",
      "Epoch 9656\n",
      "-------------------------------\n",
      "tensor(426.2883)\n",
      "tensor(138.3920)\n",
      "tensor(323.9226)\n",
      "tensor(8.1889)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.085205\n",
      "Epoch 9657\n",
      "-------------------------------\n",
      "tensor(543.3736)\n",
      "tensor(187.3148)\n",
      "tensor(424.2773)\n",
      "tensor(10.3744)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.821011\n",
      "Epoch 9658\n",
      "-------------------------------\n",
      "tensor(343.2247)\n",
      "tensor(120.7210)\n",
      "tensor(267.4265)\n",
      "tensor(6.5897)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.745552\n",
      "Epoch 9659\n",
      "-------------------------------\n",
      "tensor(164.7721)\n",
      "tensor(48.9751)\n",
      "tensor(119.5127)\n",
      "tensor(2.8986)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.619751\n",
      "Epoch 9660\n",
      "-------------------------------\n",
      "tensor(344.4800)\n",
      "tensor(109.8046)\n",
      "tensor(260.1495)\n",
      "tensor(6.3596)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.610573\n",
      "Epoch 9661\n",
      "-------------------------------\n",
      "tensor(295.2257)\n",
      "tensor(94.0696)\n",
      "tensor(221.5001)\n",
      "tensor(5.4175)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.869282\n",
      "Epoch 9662\n",
      "-------------------------------\n",
      "tensor(147.2882)\n",
      "tensor(45.8698)\n",
      "tensor(107.8033)\n",
      "tensor(2.6494)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.430542\n",
      "Epoch 9663\n",
      "-------------------------------\n",
      "tensor(95.4299)\n",
      "tensor(32.9623)\n",
      "tensor(67.8779)\n",
      "tensor(1.6239)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.138477\n",
      "Epoch 9664\n",
      "-------------------------------\n",
      "tensor(340.2905)\n",
      "tensor(113.2275)\n",
      "tensor(260.5687)\n",
      "tensor(6.3020)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.513954\n",
      "Epoch 9665\n",
      "-------------------------------\n",
      "tensor(321.6046)\n",
      "tensor(104.4152)\n",
      "tensor(243.4310)\n",
      "tensor(5.8028)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.173660\n",
      "Epoch 9666\n",
      "-------------------------------\n",
      "tensor(208.3403)\n",
      "tensor(74.5651)\n",
      "tensor(162.4606)\n",
      "tensor(4.2049)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.917534\n",
      "Epoch 9667\n",
      "-------------------------------\n",
      "tensor(405.7348)\n",
      "tensor(139.2333)\n",
      "tensor(315.0271)\n",
      "tensor(7.7979)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.656071\n",
      "Epoch 9668\n",
      "-------------------------------\n",
      "tensor(444.1176)\n",
      "tensor(138.0382)\n",
      "tensor(333.5722)\n",
      "tensor(8.4388)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.260994\n",
      "Epoch 9669\n",
      "-------------------------------\n",
      "tensor(45.7183)\n",
      "tensor(14.6785)\n",
      "tensor(21.4022)\n",
      "tensor(0.4935)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 34.964512\n",
      "Epoch 9670\n",
      "-------------------------------\n",
      "tensor(407.2419)\n",
      "tensor(134.8797)\n",
      "tensor(307.6600)\n",
      "tensor(7.7717)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.583389\n",
      "Epoch 9671\n",
      "-------------------------------\n",
      "tensor(742.4191)\n",
      "tensor(247.9956)\n",
      "tensor(573.2347)\n",
      "tensor(14.2347)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.527687\n",
      "Epoch 9672\n",
      "-------------------------------\n",
      "tensor(1032.8896)\n",
      "tensor(336.9543)\n",
      "tensor(794.8818)\n",
      "tensor(19.5456)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.970619\n",
      "Epoch 9673\n",
      "-------------------------------\n",
      "tensor(1229.8854)\n",
      "tensor(424.0272)\n",
      "tensor(960.5594)\n",
      "tensor(24.6466)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.413055\n",
      "Epoch 9674\n",
      "-------------------------------\n",
      "tensor(1160.5365)\n",
      "tensor(394.0273)\n",
      "tensor(899.3121)\n",
      "tensor(22.4929)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.374580\n",
      "Epoch 9675\n",
      "-------------------------------\n",
      "tensor(538.7922)\n",
      "tensor(157.1998)\n",
      "tensor(397.8129)\n",
      "tensor(9.1479)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.310081\n",
      "Epoch 9676\n",
      "-------------------------------\n",
      "tensor(430.0713)\n",
      "tensor(136.6104)\n",
      "tensor(326.7360)\n",
      "tensor(7.8165)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.231033\n",
      "Epoch 9677\n",
      "-------------------------------\n",
      "tensor(538.4768)\n",
      "tensor(178.3810)\n",
      "tensor(410.9028)\n",
      "tensor(10.0131)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.474293\n",
      "Epoch 9678\n",
      "-------------------------------\n",
      "tensor(344.0588)\n",
      "tensor(110.9672)\n",
      "tensor(261.8835)\n",
      "tensor(6.3251)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.696907\n",
      "Epoch 9679\n",
      "-------------------------------\n",
      "tensor(151.3116)\n",
      "tensor(55.4647)\n",
      "tensor(115.5364)\n",
      "tensor(2.9386)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.544323\n",
      "Epoch 9680\n",
      "-------------------------------\n",
      "tensor(325.0578)\n",
      "tensor(113.4606)\n",
      "tensor(255.4647)\n",
      "tensor(6.3237)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.451637\n",
      "Epoch 9681\n",
      "-------------------------------\n",
      "tensor(280.5596)\n",
      "tensor(97.6566)\n",
      "tensor(220.3381)\n",
      "tensor(5.3832)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.823769\n",
      "Epoch 9682\n",
      "-------------------------------\n",
      "tensor(141.9008)\n",
      "tensor(50.4906)\n",
      "tensor(111.3549)\n",
      "tensor(2.6272)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.470730\n",
      "Epoch 9683\n",
      "-------------------------------\n",
      "tensor(83.6854)\n",
      "tensor(26.1058)\n",
      "tensor(59.6466)\n",
      "tensor(1.6555)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.156258\n",
      "Epoch 9684\n",
      "-------------------------------\n",
      "tensor(327.9465)\n",
      "tensor(108.3236)\n",
      "tensor(250.1537)\n",
      "tensor(6.4213)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.380314\n",
      "Epoch 9685\n",
      "-------------------------------\n",
      "tensor(311.6996)\n",
      "tensor(103.2794)\n",
      "tensor(239.5681)\n",
      "tensor(6.1532)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.197556\n",
      "Epoch 9686\n",
      "-------------------------------\n",
      "tensor(192.5711)\n",
      "tensor(67.1254)\n",
      "tensor(148.8335)\n",
      "tensor(3.6624)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.864971\n",
      "Epoch 9687\n",
      "-------------------------------\n",
      "tensor(395.9987)\n",
      "tensor(136.5918)\n",
      "tensor(307.4062)\n",
      "tensor(7.8054)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.489201\n",
      "Epoch 9688\n",
      "-------------------------------\n",
      "tensor(415.6048)\n",
      "tensor(138.5213)\n",
      "tensor(320.3186)\n",
      "tensor(7.7937)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.824978\n",
      "Epoch 9689\n",
      "-------------------------------\n",
      "tensor(45.9046)\n",
      "tensor(14.6130)\n",
      "tensor(2.4954)\n",
      "tensor(0.1606)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.016705\n",
      "Epoch 9690\n",
      "-------------------------------\n",
      "tensor(423.6287)\n",
      "tensor(127.3902)\n",
      "tensor(314.7408)\n",
      "tensor(7.7328)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.848305\n",
      "Epoch 9691\n",
      "-------------------------------\n",
      "tensor(741.1589)\n",
      "tensor(249.4156)\n",
      "tensor(571.9958)\n",
      "tensor(14.1798)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.280403\n",
      "Epoch 9692\n",
      "-------------------------------\n",
      "tensor(1020.1461)\n",
      "tensor(344.6760)\n",
      "tensor(793.3140)\n",
      "tensor(19.9598)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.588104\n",
      "Epoch 9693\n",
      "-------------------------------\n",
      "tensor(1237.4021)\n",
      "tensor(413.6076)\n",
      "tensor(956.3652)\n",
      "tensor(24.1444)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.303612\n",
      "Epoch 9694\n",
      "-------------------------------\n",
      "tensor(1168.6819)\n",
      "tensor(400.3575)\n",
      "tensor(908.3842)\n",
      "tensor(22.8651)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 66.038559\n",
      "Epoch 9695\n",
      "-------------------------------\n",
      "tensor(536.1981)\n",
      "tensor(169.5553)\n",
      "tensor(408.2922)\n",
      "tensor(9.5932)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.388081\n",
      "Epoch 9696\n",
      "-------------------------------\n",
      "tensor(448.5868)\n",
      "tensor(146.4400)\n",
      "tensor(341.2214)\n",
      "tensor(8.0454)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.443962\n",
      "Epoch 9697\n",
      "-------------------------------\n",
      "tensor(561.4702)\n",
      "tensor(172.6777)\n",
      "tensor(423.8711)\n",
      "tensor(10.3314)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.831959\n",
      "Epoch 9698\n",
      "-------------------------------\n",
      "tensor(364.2192)\n",
      "tensor(109.9441)\n",
      "tensor(269.4617)\n",
      "tensor(6.1274)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.803120\n",
      "Epoch 9699\n",
      "-------------------------------\n",
      "tensor(153.0771)\n",
      "tensor(61.5643)\n",
      "tensor(123.9508)\n",
      "tensor(3.5586)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.505226\n",
      "Epoch 9700\n",
      "-------------------------------\n",
      "tensor(342.3047)\n",
      "tensor(121.3751)\n",
      "tensor(267.8059)\n",
      "tensor(6.9766)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.608452\n",
      "Epoch 9701\n",
      "-------------------------------\n",
      "tensor(294.6917)\n",
      "tensor(102.4420)\n",
      "tensor(228.4493)\n",
      "tensor(5.8681)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.888699\n",
      "Epoch 9702\n",
      "-------------------------------\n",
      "tensor(147.5859)\n",
      "tensor(50.6644)\n",
      "tensor(112.2419)\n",
      "tensor(2.8767)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.402081\n",
      "Epoch 9703\n",
      "-------------------------------\n",
      "tensor(92.9874)\n",
      "tensor(32.2356)\n",
      "tensor(67.1663)\n",
      "tensor(1.6948)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.103474\n",
      "Epoch 9704\n",
      "-------------------------------\n",
      "tensor(343.0749)\n",
      "tensor(119.9563)\n",
      "tensor(265.5656)\n",
      "tensor(6.7668)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.552048\n",
      "Epoch 9705\n",
      "-------------------------------\n",
      "tensor(323.2990)\n",
      "tensor(116.9448)\n",
      "tensor(251.0611)\n",
      "tensor(6.5521)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.254032\n",
      "Epoch 9706\n",
      "-------------------------------\n",
      "tensor(221.3781)\n",
      "tensor(63.4217)\n",
      "tensor(160.4408)\n",
      "tensor(3.6063)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.949795\n",
      "Epoch 9707\n",
      "-------------------------------\n",
      "tensor(412.4762)\n",
      "tensor(132.2319)\n",
      "tensor(315.1404)\n",
      "tensor(7.7755)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.739670\n",
      "Epoch 9708\n",
      "-------------------------------\n",
      "tensor(444.2684)\n",
      "tensor(148.6744)\n",
      "tensor(338.0190)\n",
      "tensor(8.1567)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.236893\n",
      "Epoch 9709\n",
      "-------------------------------\n",
      "tensor(37.6578)\n",
      "tensor(14.7231)\n",
      "tensor(21.8764)\n",
      "tensor(0.4954)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.022175\n",
      "Epoch 9710\n",
      "-------------------------------\n",
      "tensor(395.4594)\n",
      "tensor(137.3765)\n",
      "tensor(306.1903)\n",
      "tensor(7.6442)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.595955\n",
      "Epoch 9711\n",
      "-------------------------------\n",
      "tensor(734.7297)\n",
      "tensor(233.2037)\n",
      "tensor(561.3163)\n",
      "tensor(13.7138)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.069771\n",
      "Epoch 9712\n",
      "-------------------------------\n",
      "tensor(1013.2380)\n",
      "tensor(345.8643)\n",
      "tensor(787.9473)\n",
      "tensor(20.0964)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.238239\n",
      "Epoch 9713\n",
      "-------------------------------\n",
      "tensor(1225.9877)\n",
      "tensor(421.5878)\n",
      "tensor(959.4948)\n",
      "tensor(24.3185)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.495445\n",
      "Epoch 9714\n",
      "-------------------------------\n",
      "tensor(1170.5637)\n",
      "tensor(382.2682)\n",
      "tensor(896.3721)\n",
      "tensor(22.1787)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 65.378258\n",
      "Epoch 9715\n",
      "-------------------------------\n",
      "tensor(529.3665)\n",
      "tensor(175.0927)\n",
      "tensor(403.2967)\n",
      "tensor(9.8759)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 41.468559\n",
      "Epoch 9716\n",
      "-------------------------------\n",
      "tensor(426.1102)\n",
      "tensor(152.9309)\n",
      "tensor(333.2762)\n",
      "tensor(8.5645)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.449318\n",
      "Epoch 9717\n",
      "-------------------------------\n",
      "tensor(535.1511)\n",
      "tensor(171.1633)\n",
      "tensor(409.3304)\n",
      "tensor(9.6654)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.458801\n",
      "Epoch 9718\n",
      "-------------------------------\n",
      "tensor(356.8056)\n",
      "tensor(116.4766)\n",
      "tensor(271.6264)\n",
      "tensor(6.3928)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.757061\n",
      "Epoch 9719\n",
      "-------------------------------\n",
      "tensor(144.8637)\n",
      "tensor(45.2647)\n",
      "tensor(109.1577)\n",
      "tensor(2.7306)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.379021\n",
      "Epoch 9720\n",
      "-------------------------------\n",
      "tensor(333.6944)\n",
      "tensor(104.8547)\n",
      "tensor(254.5804)\n",
      "tensor(6.0910)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.389740\n",
      "Epoch 9721\n",
      "-------------------------------\n",
      "tensor(293.5674)\n",
      "tensor(89.6589)\n",
      "tensor(221.7463)\n",
      "tensor(5.1372)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.817657\n",
      "Epoch 9722\n",
      "-------------------------------\n",
      "tensor(153.7940)\n",
      "tensor(42.5815)\n",
      "tensor(112.6751)\n",
      "tensor(2.3623)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.433735\n",
      "Epoch 9723\n",
      "-------------------------------\n",
      "tensor(73.2427)\n",
      "tensor(33.5655)\n",
      "tensor(60.4516)\n",
      "tensor(1.9357)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.057598\n",
      "Epoch 9724\n",
      "-------------------------------\n",
      "tensor(326.2775)\n",
      "tensor(115.2340)\n",
      "tensor(255.1165)\n",
      "tensor(6.6847)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.330589\n",
      "Epoch 9725\n",
      "-------------------------------\n",
      "tensor(321.2033)\n",
      "tensor(107.2136)\n",
      "tensor(247.4204)\n",
      "tensor(6.2526)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.238186\n",
      "Epoch 9726\n",
      "-------------------------------\n",
      "tensor(187.4143)\n",
      "tensor(72.2933)\n",
      "tensor(148.5673)\n",
      "tensor(3.9739)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.830864\n",
      "Epoch 9727\n",
      "-------------------------------\n",
      "tensor(402.1652)\n",
      "tensor(151.0152)\n",
      "tensor(317.9636)\n",
      "tensor(8.4500)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.823376\n",
      "Epoch 9728\n",
      "-------------------------------\n",
      "tensor(425.1243)\n",
      "tensor(129.2637)\n",
      "tensor(317.2685)\n",
      "tensor(7.4231)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.868805\n",
      "Epoch 9729\n",
      "-------------------------------\n",
      "tensor(59.8185)\n",
      "tensor(18.5870)\n",
      "tensor(6.2229)\n",
      "tensor(0.3166)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 34.992264\n",
      "Epoch 9730\n",
      "-------------------------------\n",
      "tensor(420.1942)\n",
      "tensor(127.3077)\n",
      "tensor(312.6595)\n",
      "tensor(7.7708)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.678841\n",
      "Epoch 9731\n",
      "-------------------------------\n",
      "tensor(738.2850)\n",
      "tensor(251.2584)\n",
      "tensor(569.8276)\n",
      "tensor(13.8395)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 47.252743\n",
      "Epoch 9732\n",
      "-------------------------------\n",
      "tensor(1034.6155)\n",
      "tensor(333.8100)\n",
      "tensor(790.2532)\n",
      "tensor(19.4167)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.731346\n",
      "Epoch 9733\n",
      "-------------------------------\n",
      "tensor(1250.1027)\n",
      "tensor(430.0389)\n",
      "tensor(974.0659)\n",
      "tensor(24.8307)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.412560\n",
      "Epoch 9734\n",
      "-------------------------------\n",
      "tensor(1197.7313)\n",
      "tensor(405.0428)\n",
      "tensor(931.6284)\n",
      "tensor(23.2989)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 67.608421\n",
      "Epoch 9735\n",
      "-------------------------------\n",
      "tensor(562.5911)\n",
      "tensor(175.8562)\n",
      "tensor(423.7302)\n",
      "tensor(10.2696)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.029858\n",
      "Epoch 9736\n",
      "-------------------------------\n",
      "tensor(442.7567)\n",
      "tensor(136.7016)\n",
      "tensor(333.6376)\n",
      "tensor(8.1750)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.428101\n",
      "Epoch 9737\n",
      "-------------------------------\n",
      "tensor(555.8475)\n",
      "tensor(194.9014)\n",
      "tensor(434.0250)\n",
      "tensor(10.7836)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 42.145302\n",
      "Epoch 9738\n",
      "-------------------------------\n",
      "tensor(355.0672)\n",
      "tensor(124.2756)\n",
      "tensor(274.8538)\n",
      "tensor(6.8366)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.885841\n",
      "Epoch 9739\n",
      "-------------------------------\n",
      "tensor(167.2514)\n",
      "tensor(52.5509)\n",
      "tensor(122.9548)\n",
      "tensor(3.0080)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.615372\n",
      "Epoch 9740\n",
      "-------------------------------\n",
      "tensor(351.9745)\n",
      "tensor(116.8021)\n",
      "tensor(268.0442)\n",
      "tensor(6.6680)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.761387\n",
      "Epoch 9741\n",
      "-------------------------------\n",
      "tensor(298.6357)\n",
      "tensor(100.6796)\n",
      "tensor(228.1680)\n",
      "tensor(5.7539)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.997021\n",
      "Epoch 9742\n",
      "-------------------------------\n",
      "tensor(146.5248)\n",
      "tensor(51.0941)\n",
      "tensor(111.2330)\n",
      "tensor(2.9271)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.482597\n",
      "Epoch 9743\n",
      "-------------------------------\n",
      "tensor(99.6613)\n",
      "tensor(29.9699)\n",
      "tensor(69.3808)\n",
      "tensor(1.4641)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.192272\n",
      "Epoch 9744\n",
      "-------------------------------\n",
      "tensor(352.5767)\n",
      "tensor(113.6689)\n",
      "tensor(267.1116)\n",
      "tensor(6.3089)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.695114\n",
      "Epoch 9745\n",
      "-------------------------------\n",
      "tensor(326.4283)\n",
      "tensor(106.0291)\n",
      "tensor(248.5325)\n",
      "tensor(5.9031)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.306450\n",
      "Epoch 9746\n",
      "-------------------------------\n",
      "tensor(218.1266)\n",
      "tensor(73.6231)\n",
      "tensor(166.2250)\n",
      "tensor(4.1677)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.021400\n",
      "Epoch 9747\n",
      "-------------------------------\n",
      "tensor(417.0607)\n",
      "tensor(136.3573)\n",
      "tensor(316.6978)\n",
      "tensor(7.6886)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.743748\n",
      "Epoch 9748\n",
      "-------------------------------\n",
      "tensor(461.8174)\n",
      "tensor(148.0377)\n",
      "tensor(348.9421)\n",
      "tensor(8.6750)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.557625\n",
      "Epoch 9749\n",
      "-------------------------------\n",
      "tensor(67.1768)\n",
      "tensor(17.8373)\n",
      "tensor(34.2866)\n",
      "tensor(0.7500)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.069401\n",
      "Epoch 9750\n",
      "-------------------------------\n",
      "tensor(397.7737)\n",
      "tensor(126.0257)\n",
      "tensor(302.3300)\n",
      "tensor(7.2034)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.410244\n",
      "Epoch 9751\n",
      "-------------------------------\n",
      "tensor(728.8465)\n",
      "tensor(241.9879)\n",
      "tensor(562.8601)\n",
      "tensor(14.2807)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.941826\n",
      "Epoch 9752\n",
      "-------------------------------\n",
      "tensor(1011.2695)\n",
      "tensor(345.9938)\n",
      "tensor(786.7639)\n",
      "tensor(19.9375)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 58.018711\n",
      "Epoch 9753\n",
      "-------------------------------\n",
      "tensor(1229.5569)\n",
      "tensor(409.2746)\n",
      "tensor(950.2150)\n",
      "tensor(23.5585)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 69.380234\n",
      "Epoch 9754\n",
      "-------------------------------\n",
      "tensor(1145.7306)\n",
      "tensor(385.5792)\n",
      "tensor(892.4580)\n",
      "tensor(22.2821)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.912827\n",
      "Epoch 9755\n",
      "-------------------------------\n",
      "tensor(500.9602)\n",
      "tensor(167.3629)\n",
      "tensor(384.8957)\n",
      "tensor(10.1089)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.600815\n",
      "Epoch 9756\n",
      "-------------------------------\n",
      "tensor(430.0568)\n",
      "tensor(154.2680)\n",
      "tensor(342.3051)\n",
      "tensor(9.0988)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.405884\n",
      "Epoch 9757\n",
      "-------------------------------\n",
      "tensor(518.6056)\n",
      "tensor(172.8132)\n",
      "tensor(400.0425)\n",
      "tensor(9.9737)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.146366\n",
      "Epoch 9758\n",
      "-------------------------------\n",
      "tensor(349.8420)\n",
      "tensor(121.5266)\n",
      "tensor(272.8403)\n",
      "tensor(7.2076)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.888168\n",
      "Epoch 9759\n",
      "-------------------------------\n",
      "tensor(143.6025)\n",
      "tensor(44.3928)\n",
      "tensor(104.6357)\n",
      "tensor(2.1224)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.560432\n",
      "Epoch 9760\n",
      "-------------------------------\n",
      "tensor(328.6960)\n",
      "tensor(108.2147)\n",
      "tensor(251.0513)\n",
      "tensor(5.9245)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.506622\n",
      "Epoch 9761\n",
      "-------------------------------\n",
      "tensor(286.5265)\n",
      "tensor(96.3980)\n",
      "tensor(220.6376)\n",
      "tensor(5.3462)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.933697\n",
      "Epoch 9762\n",
      "-------------------------------\n",
      "tensor(144.7113)\n",
      "tensor(51.2359)\n",
      "tensor(114.0608)\n",
      "tensor(2.8584)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.578205\n",
      "Epoch 9763\n",
      "-------------------------------\n",
      "tensor(81.2573)\n",
      "tensor(22.8783)\n",
      "tensor(55.2260)\n",
      "tensor(1.1817)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.222435\n",
      "Epoch 9764\n",
      "-------------------------------\n",
      "tensor(328.3224)\n",
      "tensor(104.5073)\n",
      "tensor(246.4580)\n",
      "tensor(5.8161)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.421211\n",
      "Epoch 9765\n",
      "-------------------------------\n",
      "tensor(318.1469)\n",
      "tensor(102.2995)\n",
      "tensor(239.8505)\n",
      "tensor(5.7266)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.251156\n",
      "Epoch 9766\n",
      "-------------------------------\n",
      "tensor(191.8317)\n",
      "tensor(63.6742)\n",
      "tensor(146.4132)\n",
      "tensor(3.4957)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.868294\n",
      "Epoch 9767\n",
      "-------------------------------\n",
      "tensor(402.7705)\n",
      "tensor(130.0954)\n",
      "tensor(305.4753)\n",
      "tensor(7.1039)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.608345\n",
      "Epoch 9768\n",
      "-------------------------------\n",
      "tensor(411.9903)\n",
      "tensor(139.4868)\n",
      "tensor(319.0430)\n",
      "tensor(8.1315)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.757755\n",
      "Epoch 9769\n",
      "-------------------------------\n",
      "tensor(64.8916)\n",
      "tensor(22.4431)\n",
      "tensor(5.6087)\n",
      "tensor(0.3733)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.175991\n",
      "Epoch 9770\n",
      "-------------------------------\n",
      "tensor(402.3338)\n",
      "tensor(134.5309)\n",
      "tensor(307.6981)\n",
      "tensor(8.4218)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.838669\n",
      "Epoch 9771\n",
      "-------------------------------\n",
      "tensor(703.4202)\n",
      "tensor(240.2156)\n",
      "tensor(551.3547)\n",
      "tensor(13.7845)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.496136\n",
      "Epoch 9772\n",
      "-------------------------------\n",
      "tensor(963.4434)\n",
      "tensor(311.5039)\n",
      "tensor(739.2365)\n",
      "tensor(18.1086)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.748310\n",
      "Epoch 9773\n",
      "-------------------------------\n",
      "tensor(1144.1866)\n",
      "tensor(391.8553)\n",
      "tensor(889.7096)\n",
      "tensor(22.6438)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 64.666153\n",
      "Epoch 9774\n",
      "-------------------------------\n",
      "tensor(1080.8613)\n",
      "tensor(364.0574)\n",
      "tensor(841.0829)\n",
      "tensor(20.9940)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 61.540283\n",
      "Epoch 9775\n",
      "-------------------------------\n",
      "tensor(502.1461)\n",
      "tensor(158.0529)\n",
      "tensor(377.5636)\n",
      "tensor(9.2205)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 40.761898\n",
      "Epoch 9776\n",
      "-------------------------------\n",
      "tensor(401.4562)\n",
      "tensor(125.3466)\n",
      "tensor(301.2171)\n",
      "tensor(7.4742)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.776363\n",
      "Epoch 9777\n",
      "-------------------------------\n",
      "tensor(500.4821)\n",
      "tensor(174.9720)\n",
      "tensor(393.7312)\n",
      "tensor(9.6500)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 40.947304\n",
      "Epoch 9778\n",
      "-------------------------------\n",
      "tensor(315.4328)\n",
      "tensor(110.8042)\n",
      "tensor(249.4603)\n",
      "tensor(6.0689)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.475872\n",
      "Epoch 9779\n",
      "-------------------------------\n",
      "tensor(148.6144)\n",
      "tensor(46.4540)\n",
      "tensor(109.1662)\n",
      "tensor(2.7998)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.636600\n",
      "Epoch 9780\n",
      "-------------------------------\n",
      "tensor(316.3595)\n",
      "tensor(103.5314)\n",
      "tensor(240.0698)\n",
      "tensor(6.0350)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.345791\n",
      "Epoch 9781\n",
      "-------------------------------\n",
      "tensor(269.1651)\n",
      "tensor(88.4829)\n",
      "tensor(204.6429)\n",
      "tensor(5.1584)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.726955\n",
      "Epoch 9782\n",
      "-------------------------------\n",
      "tensor(135.2010)\n",
      "tensor(44.0275)\n",
      "tensor(100.0949)\n",
      "tensor(2.5659)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.505478\n",
      "Epoch 9783\n",
      "-------------------------------\n",
      "tensor(82.0834)\n",
      "tensor(28.1284)\n",
      "tensor(61.9013)\n",
      "tensor(1.4498)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.232269\n",
      "Epoch 9784\n",
      "-------------------------------\n",
      "tensor(312.1363)\n",
      "tensor(104.5071)\n",
      "tensor(240.5508)\n",
      "tensor(5.8736)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.237331\n",
      "Epoch 9785\n",
      "-------------------------------\n",
      "tensor(292.8824)\n",
      "tensor(97.4296)\n",
      "tensor(225.9355)\n",
      "tensor(5.5174)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.964504\n",
      "Epoch 9786\n",
      "-------------------------------\n",
      "tensor(190.8489)\n",
      "tensor(65.2792)\n",
      "tensor(146.0029)\n",
      "tensor(3.7013)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 35.794109\n",
      "Epoch 9787\n",
      "-------------------------------\n",
      "tensor(372.2635)\n",
      "tensor(126.5578)\n",
      "tensor(288.2780)\n",
      "tensor(7.2354)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.025230\n",
      "Epoch 9788\n",
      "-------------------------------\n",
      "tensor(406.3618)\n",
      "tensor(125.7975)\n",
      "tensor(302.0294)\n",
      "tensor(7.4509)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 38.619331\n",
      "Epoch 9789\n",
      "-------------------------------\n",
      "tensor(63.9150)\n",
      "tensor(19.4188)\n",
      "tensor(20.3673)\n",
      "tensor(0.1271)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.334610\n",
      "Epoch 9790\n",
      "-------------------------------\n",
      "tensor(373.3871)\n",
      "tensor(117.1046)\n",
      "tensor(282.4615)\n",
      "tensor(6.4609)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.339664\n",
      "Epoch 9791\n",
      "-------------------------------\n",
      "tensor(678.4861)\n",
      "tensor(219.7962)\n",
      "tensor(516.2494)\n",
      "tensor(12.8681)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.271961\n",
      "Epoch 9792\n",
      "-------------------------------\n",
      "tensor(945.8910)\n",
      "tensor(325.0996)\n",
      "tensor(734.1136)\n",
      "tensor(18.5466)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 55.059399\n",
      "Epoch 9793\n",
      "-------------------------------\n",
      "tensor(1179.0774)\n",
      "tensor(389.5023)\n",
      "tensor(906.3957)\n",
      "tensor(22.2914)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 66.081444\n",
      "Epoch 9794\n",
      "-------------------------------\n",
      "tensor(1151.0479)\n",
      "tensor(383.1008)\n",
      "tensor(885.6970)\n",
      "tensor(21.8457)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 64.609550\n",
      "Epoch 9795\n",
      "-------------------------------\n",
      "tensor(562.0607)\n",
      "tensor(194.3911)\n",
      "tensor(438.8135)\n",
      "tensor(10.9985)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 42.206013\n",
      "Epoch 9796\n",
      "-------------------------------\n",
      "tensor(375.6378)\n",
      "tensor(131.6230)\n",
      "tensor(295.7029)\n",
      "tensor(7.3982)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.281143\n",
      "Epoch 9797\n",
      "-------------------------------\n",
      "tensor(556.1848)\n",
      "tensor(183.0089)\n",
      "tensor(424.3768)\n",
      "tensor(10.5515)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 41.881622\n",
      "Epoch 9798\n",
      "-------------------------------\n",
      "tensor(310.3271)\n",
      "tensor(103.1493)\n",
      "tensor(237.5259)\n",
      "tensor(6.0721)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.191139\n",
      "Epoch 9799\n",
      "-------------------------------\n",
      "tensor(182.6659)\n",
      "tensor(61.4927)\n",
      "tensor(139.6242)\n",
      "tensor(3.2500)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.790020\n",
      "Epoch 9800\n",
      "-------------------------------\n",
      "tensor(339.0323)\n",
      "tensor(114.5084)\n",
      "tensor(261.9477)\n",
      "tensor(6.3476)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 37.599014\n",
      "Epoch 9801\n",
      "-------------------------------\n",
      "tensor(274.2281)\n",
      "tensor(93.4431)\n",
      "tensor(212.4499)\n",
      "tensor(5.1967)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.690048\n",
      "Epoch 9802\n",
      "-------------------------------\n",
      "tensor(123.4244)\n",
      "tensor(43.3201)\n",
      "tensor(95.1270)\n",
      "tensor(2.3576)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.317005\n",
      "Epoch 9803\n",
      "-------------------------------\n",
      "tensor(109.3330)\n",
      "tensor(34.0116)\n",
      "tensor(77.8880)\n",
      "tensor(1.8619)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.208599\n",
      "Epoch 9804\n",
      "-------------------------------\n",
      "tensor(342.2360)\n",
      "tensor(111.8926)\n",
      "tensor(259.7273)\n",
      "tensor(6.3117)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 37.531208\n",
      "Epoch 9805\n",
      "-------------------------------\n",
      "tensor(295.8972)\n",
      "tensor(97.9624)\n",
      "tensor(226.2428)\n",
      "tensor(5.5342)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 36.897499\n",
      "Epoch 9806\n",
      "-------------------------------\n",
      "tensor(227.4917)\n",
      "tensor(74.5503)\n",
      "tensor(171.7643)\n",
      "tensor(4.1783)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.020107\n",
      "Epoch 9807\n",
      "-------------------------------\n",
      "tensor(379.1149)\n",
      "tensor(123.1393)\n",
      "tensor(287.2194)\n",
      "tensor(6.9336)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.038078\n",
      "Epoch 9808\n",
      "-------------------------------\n",
      "tensor(442.6491)\n",
      "tensor(151.4484)\n",
      "tensor(344.3685)\n",
      "tensor(8.5604)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 39.412266\n",
      "Epoch 9809\n",
      "-------------------------------\n",
      "tensor(80.2561)\n",
      "tensor(27.2374)\n",
      "tensor(46.9803)\n",
      "tensor(1.2999)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.166126\n",
      "Epoch 9810\n",
      "-------------------------------\n",
      "tensor(358.7003)\n",
      "tensor(111.0566)\n",
      "tensor(271.1172)\n",
      "tensor(6.9199)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.885395\n",
      "Epoch 9811\n",
      "-------------------------------\n",
      "tensor(693.0438)\n",
      "tensor(234.0719)\n",
      "tensor(535.1355)\n",
      "tensor(13.4445)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 45.846634\n",
      "Epoch 9812\n",
      "-------------------------------\n",
      "tensor(1001.1187)\n",
      "tensor(333.1539)\n",
      "tensor(771.8874)\n",
      "tensor(18.9035)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 57.410248\n",
      "Epoch 9813\n",
      "-------------------------------\n",
      "tensor(1265.2369)\n",
      "tensor(414.3636)\n",
      "tensor(966.0568)\n",
      "tensor(23.7721)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 70.377281\n",
      "Epoch 9814\n",
      "-------------------------------\n",
      "tensor(1244.5460)\n",
      "tensor(424.4799)\n",
      "tensor(967.2342)\n",
      "tensor(24.0869)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 69.973045\n",
      "Epoch 9815\n",
      "-------------------------------\n",
      "tensor(638.9723)\n",
      "tensor(208.4568)\n",
      "tensor(487.5713)\n",
      "tensor(12.1054)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.062862\n",
      "Epoch 9816\n",
      "-------------------------------\n",
      "tensor(396.4984)\n",
      "tensor(131.0468)\n",
      "tensor(304.4626)\n",
      "tensor(7.6305)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.564686\n",
      "Epoch 9817\n",
      "-------------------------------\n",
      "tensor(604.1713)\n",
      "tensor(202.7371)\n",
      "tensor(466.7288)\n",
      "tensor(11.5172)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.196205\n",
      "Epoch 9818\n",
      "-------------------------------\n",
      "tensor(330.6046)\n",
      "tensor(109.8920)\n",
      "tensor(252.8757)\n",
      "tensor(6.2895)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.389893\n",
      "Epoch 9819\n",
      "-------------------------------\n",
      "tensor(208.0869)\n",
      "tensor(70.9607)\n",
      "tensor(157.8381)\n",
      "tensor(3.8644)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.904324\n",
      "Epoch 9820\n",
      "-------------------------------\n",
      "tensor(372.6260)\n",
      "tensor(127.3534)\n",
      "tensor(286.0921)\n",
      "tensor(7.1208)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.022141\n",
      "Epoch 9821\n",
      "-------------------------------\n",
      "tensor(295.9058)\n",
      "tensor(102.6309)\n",
      "tensor(227.9293)\n",
      "tensor(5.7753)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.876945\n",
      "Epoch 9822\n",
      "-------------------------------\n",
      "tensor(126.9700)\n",
      "tensor(46.6725)\n",
      "tensor(97.7807)\n",
      "tensor(2.6378)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.282803\n",
      "Epoch 9823\n",
      "-------------------------------\n",
      "tensor(128.5927)\n",
      "tensor(38.5920)\n",
      "tensor(90.2530)\n",
      "tensor(1.9475)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.256401\n",
      "Epoch 9824\n",
      "-------------------------------\n",
      "tensor(374.6597)\n",
      "tensor(122.1376)\n",
      "tensor(281.9819)\n",
      "tensor(6.7024)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.007607\n",
      "Epoch 9825\n",
      "-------------------------------\n",
      "tensor(311.6237)\n",
      "tensor(105.6187)\n",
      "tensor(237.9942)\n",
      "tensor(5.8349)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.100662\n",
      "Epoch 9826\n",
      "-------------------------------\n",
      "tensor(260.9857)\n",
      "tensor(79.3739)\n",
      "tensor(193.2099)\n",
      "tensor(4.4400)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.359554\n",
      "Epoch 9827\n",
      "-------------------------------\n",
      "tensor(407.1105)\n",
      "tensor(126.5708)\n",
      "tensor(304.5750)\n",
      "tensor(7.0857)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 38.466068\n",
      "Epoch 9828\n",
      "-------------------------------\n",
      "tensor(496.7402)\n",
      "tensor(160.4147)\n",
      "tensor(377.4162)\n",
      "tensor(9.2912)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.244167\n",
      "Epoch 9829\n",
      "-------------------------------\n",
      "tensor(108.1330)\n",
      "tensor(36.3783)\n",
      "tensor(79.1420)\n",
      "tensor(2.1501)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.222126\n",
      "Epoch 9830\n",
      "-------------------------------\n",
      "tensor(347.4442)\n",
      "tensor(113.8539)\n",
      "tensor(264.1543)\n",
      "tensor(6.4361)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 37.626717\n",
      "Epoch 9831\n",
      "-------------------------------\n",
      "tensor(715.3721)\n",
      "tensor(234.2731)\n",
      "tensor(548.2739)\n",
      "tensor(13.7363)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 46.342079\n",
      "Epoch 9832\n",
      "-------------------------------\n",
      "tensor(1047.1229)\n",
      "tensor(352.5192)\n",
      "tensor(806.3541)\n",
      "tensor(20.0355)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 59.276711\n",
      "Epoch 9833\n",
      "-------------------------------\n",
      "tensor(1323.8557)\n",
      "tensor(439.7846)\n",
      "tensor(1018.8010)\n",
      "tensor(24.9129)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 74.284256\n",
      "Epoch 9834\n",
      "-------------------------------\n",
      "tensor(1302.6398)\n",
      "tensor(434.8747)\n",
      "tensor(1004.3912)\n",
      "tensor(24.9625)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 72.749374\n",
      "Epoch 9835\n",
      "-------------------------------\n",
      "tensor(639.7517)\n",
      "tensor(221.6543)\n",
      "tensor(496.9477)\n",
      "tensor(12.5103)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 44.384113\n",
      "Epoch 9836\n",
      "-------------------------------\n",
      "tensor(428.0385)\n",
      "tensor(151.8553)\n",
      "tensor(335.2029)\n",
      "tensor(8.6992)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.345016\n",
      "Epoch 9837\n",
      "-------------------------------\n",
      "tensor(625.3444)\n",
      "tensor(206.6878)\n",
      "tensor(481.7915)\n",
      "tensor(11.7126)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.813286\n",
      "Epoch 9838\n",
      "-------------------------------\n",
      "tensor(354.3692)\n",
      "tensor(121.7344)\n",
      "tensor(277.2265)\n",
      "tensor(6.9386)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.833111\n",
      "Epoch 9839\n",
      "-------------------------------\n",
      "tensor(203.4758)\n",
      "tensor(62.9023)\n",
      "tensor(148.3957)\n",
      "tensor(3.4959)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.796753\n",
      "Epoch 9840\n",
      "-------------------------------\n",
      "tensor(383.4286)\n",
      "tensor(123.0662)\n",
      "tensor(290.7124)\n",
      "tensor(7.0238)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.138123\n",
      "Epoch 9841\n",
      "-------------------------------\n",
      "tensor(315.6670)\n",
      "tensor(100.7253)\n",
      "tensor(238.4179)\n",
      "tensor(5.7637)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.077423\n",
      "Epoch 9842\n",
      "-------------------------------\n",
      "tensor(142.1011)\n",
      "tensor(43.5172)\n",
      "tensor(108.1071)\n",
      "tensor(2.5799)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.355610\n",
      "Epoch 9843\n",
      "-------------------------------\n",
      "tensor(113.7308)\n",
      "tensor(40.9893)\n",
      "tensor(85.9558)\n",
      "tensor(2.1915)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.207481\n",
      "Epoch 9844\n",
      "-------------------------------\n",
      "tensor(378.4072)\n",
      "tensor(128.7360)\n",
      "tensor(290.4434)\n",
      "tensor(7.2580)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.106926\n",
      "Epoch 9845\n",
      "-------------------------------\n",
      "tensor(332.1375)\n",
      "tensor(112.5140)\n",
      "tensor(254.6420)\n",
      "tensor(6.4571)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.374767\n",
      "Epoch 9846\n",
      "-------------------------------\n",
      "tensor(252.0313)\n",
      "tensor(85.6182)\n",
      "tensor(193.1496)\n",
      "tensor(4.5166)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.401978\n",
      "Epoch 9847\n",
      "-------------------------------\n",
      "tensor(420.9297)\n",
      "tensor(148.0820)\n",
      "tensor(327.9830)\n",
      "tensor(8.0772)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.008522\n",
      "Epoch 9848\n",
      "-------------------------------\n",
      "tensor(507.7636)\n",
      "tensor(155.6987)\n",
      "tensor(379.2003)\n",
      "tensor(8.9053)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.537155\n",
      "Epoch 9849\n",
      "-------------------------------\n",
      "tensor(81.3459)\n",
      "tensor(35.1256)\n",
      "tensor(61.9628)\n",
      "tensor(1.7902)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.038239\n",
      "Epoch 9850\n",
      "-------------------------------\n",
      "tensor(397.8936)\n",
      "tensor(121.3706)\n",
      "tensor(295.4438)\n",
      "tensor(7.1153)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.318741\n",
      "Epoch 9851\n",
      "-------------------------------\n",
      "tensor(774.6451)\n",
      "tensor(263.0842)\n",
      "tensor(597.2108)\n",
      "tensor(14.8296)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 48.211052\n",
      "Epoch 9852\n",
      "-------------------------------\n",
      "tensor(1119.8486)\n",
      "tensor(369.9073)\n",
      "tensor(860.3475)\n",
      "tensor(21.2940)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 63.388409\n",
      "Epoch 9853\n",
      "-------------------------------\n",
      "tensor(1381.3625)\n",
      "tensor(466.9203)\n",
      "tensor(1069.9446)\n",
      "tensor(26.6839)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 77.737755\n",
      "Epoch 9854\n",
      "-------------------------------\n",
      "tensor(1326.8588)\n",
      "tensor(447.7781)\n",
      "tensor(1027.2980)\n",
      "tensor(25.6150)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 75.303436\n",
      "Epoch 9855\n",
      "-------------------------------\n",
      "tensor(611.9191)\n",
      "tensor(202.4342)\n",
      "tensor(475.3204)\n",
      "tensor(11.7287)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 43.792137\n",
      "Epoch 9856\n",
      "-------------------------------\n",
      "tensor(497.6554)\n",
      "tensor(160.8681)\n",
      "tensor(380.6803)\n",
      "tensor(9.0335)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.696884\n",
      "Epoch 9857\n",
      "-------------------------------\n",
      "tensor(626.2178)\n",
      "tensor(213.0459)\n",
      "tensor(484.7509)\n",
      "tensor(12.2938)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 43.828365\n",
      "Epoch 9858\n",
      "-------------------------------\n",
      "tensor(395.7421)\n",
      "tensor(130.7674)\n",
      "tensor(304.9712)\n",
      "tensor(7.5821)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.480614\n",
      "Epoch 9859\n",
      "-------------------------------\n",
      "tensor(182.8261)\n",
      "tensor(66.8164)\n",
      "tensor(141.7827)\n",
      "tensor(3.7112)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 35.775085\n",
      "Epoch 9860\n",
      "-------------------------------\n",
      "tensor(388.8867)\n",
      "tensor(137.6052)\n",
      "tensor(302.9469)\n",
      "tensor(7.8422)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.450230\n",
      "Epoch 9861\n",
      "-------------------------------\n",
      "tensor(328.5980)\n",
      "tensor(117.5993)\n",
      "tensor(256.7619)\n",
      "tensor(6.7541)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.482887\n",
      "Epoch 9862\n",
      "-------------------------------\n",
      "tensor(158.3139)\n",
      "tensor(59.8683)\n",
      "tensor(124.5566)\n",
      "tensor(3.4830)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.605263\n",
      "Epoch 9863\n",
      "-------------------------------\n",
      "tensor(106.3502)\n",
      "tensor(31.4488)\n",
      "tensor(79.0534)\n",
      "tensor(1.6034)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.261230\n",
      "Epoch 9864\n",
      "-------------------------------\n",
      "tensor(390.1792)\n",
      "tensor(130.3512)\n",
      "tensor(301.8969)\n",
      "tensor(7.2797)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.397717\n",
      "Epoch 9865\n",
      "-------------------------------\n",
      "tensor(361.0066)\n",
      "tensor(126.8984)\n",
      "tensor(280.9697)\n",
      "tensor(7.0612)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.886032\n",
      "Epoch 9866\n",
      "-------------------------------\n",
      "tensor(258.7485)\n",
      "tensor(73.2132)\n",
      "tensor(186.0455)\n",
      "tensor(4.1221)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.334305\n",
      "Epoch 9867\n",
      "-------------------------------\n",
      "tensor(463.9426)\n",
      "tensor(142.7651)\n",
      "tensor(347.6913)\n",
      "tensor(8.2663)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.605293\n",
      "Epoch 9868\n",
      "-------------------------------\n",
      "tensor(517.4695)\n",
      "tensor(168.9085)\n",
      "tensor(392.5363)\n",
      "tensor(9.5235)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 40.769138\n",
      "Epoch 9869\n",
      "-------------------------------\n",
      "tensor(77.3580)\n",
      "tensor(21.1828)\n",
      "tensor(46.8159)\n",
      "tensor(1.1225)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.014820\n",
      "Epoch 9870\n",
      "-------------------------------\n",
      "tensor(430.1397)\n",
      "tensor(139.7147)\n",
      "tensor(329.0815)\n",
      "tensor(7.7269)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.105228\n",
      "Epoch 9871\n",
      "-------------------------------\n",
      "tensor(812.9861)\n",
      "tensor(263.9245)\n",
      "tensor(621.3936)\n",
      "tensor(15.6773)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 49.705639\n",
      "Epoch 9872\n",
      "-------------------------------\n",
      "tensor(1140.0696)\n",
      "tensor(399.1560)\n",
      "tensor(890.3519)\n",
      "tensor(22.6861)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 64.279419\n",
      "Epoch 9873\n",
      "-------------------------------\n",
      "tensor(1431.0334)\n",
      "tensor(469.7583)\n",
      "tensor(1098.1432)\n",
      "tensor(26.6792)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 80.647224\n",
      "Epoch 9874\n",
      "-------------------------------\n",
      "tensor(1396.2159)\n",
      "tensor(463.9886)\n",
      "tensor(1072.8143)\n",
      "tensor(26.5055)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 78.202759\n",
      "Epoch 9875\n",
      "-------------------------------\n",
      "tensor(682.2426)\n",
      "tensor(241.0649)\n",
      "tensor(534.7517)\n",
      "tensor(13.4836)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 45.519077\n",
      "Epoch 9876\n",
      "-------------------------------\n",
      "tensor(454.1717)\n",
      "tensor(166.1936)\n",
      "tensor(357.4183)\n",
      "tensor(9.1934)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.585579\n",
      "Epoch 9877\n",
      "-------------------------------\n",
      "tensor(683.9779)\n",
      "tensor(218.4881)\n",
      "tensor(517.2915)\n",
      "tensor(12.4951)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 45.059513\n",
      "Epoch 9878\n",
      "-------------------------------\n",
      "tensor(376.2225)\n",
      "tensor(123.0408)\n",
      "tensor(286.1917)\n",
      "tensor(7.1386)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 37.932514\n",
      "Epoch 9879\n",
      "-------------------------------\n",
      "tensor(232.1304)\n",
      "tensor(75.1544)\n",
      "tensor(172.4626)\n",
      "tensor(3.9596)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.003643\n",
      "Epoch 9880\n",
      "-------------------------------\n",
      "tensor(422.5526)\n",
      "tensor(137.5427)\n",
      "tensor(317.5593)\n",
      "tensor(7.5476)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 38.710041\n",
      "Epoch 9881\n",
      "-------------------------------\n",
      "tensor(338.1616)\n",
      "tensor(109.9969)\n",
      "tensor(254.2430)\n",
      "tensor(6.0886)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.311607\n",
      "Epoch 9882\n",
      "-------------------------------\n",
      "tensor(146.7129)\n",
      "tensor(47.2583)\n",
      "tensor(109.6696)\n",
      "tensor(2.6461)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.292984\n",
      "Epoch 9883\n",
      "-------------------------------\n",
      "tensor(135.1365)\n",
      "tensor(45.1951)\n",
      "tensor(100.7056)\n",
      "tensor(2.4033)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.200390\n",
      "Epoch 9884\n",
      "-------------------------------\n",
      "tensor(416.9481)\n",
      "tensor(138.5370)\n",
      "tensor(317.4472)\n",
      "tensor(7.6449)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 38.636742\n",
      "Epoch 9885\n",
      "-------------------------------\n",
      "tensor(351.1174)\n",
      "tensor(117.9822)\n",
      "tensor(268.5736)\n",
      "tensor(6.5027)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 37.536091\n",
      "Epoch 9886\n",
      "-------------------------------\n",
      "tensor(291.2484)\n",
      "tensor(93.5985)\n",
      "tensor(214.9797)\n",
      "tensor(5.1415)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.713482\n",
      "Epoch 9887\n",
      "-------------------------------\n",
      "tensor(459.4072)\n",
      "tensor(141.0473)\n",
      "tensor(340.0016)\n",
      "tensor(8.2016)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 39.374432\n",
      "Epoch 9888\n",
      "-------------------------------\n",
      "tensor(559.1996)\n",
      "tensor(182.0704)\n",
      "tensor(426.7591)\n",
      "tensor(10.2388)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 41.761265\n",
      "Epoch 9889\n",
      "-------------------------------\n",
      "tensor(106.3367)\n",
      "tensor(42.5028)\n",
      "tensor(85.4835)\n",
      "tensor(2.4248)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.152882\n",
      "Epoch 9890\n",
      "-------------------------------\n",
      "tensor(408.3958)\n",
      "tensor(136.0574)\n",
      "tensor(311.1542)\n",
      "tensor(7.4104)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 38.562798\n",
      "Epoch 9891\n",
      "-------------------------------\n",
      "tensor(833.2880)\n",
      "tensor(278.2085)\n",
      "tensor(638.1030)\n",
      "tensor(15.8761)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 50.339344\n",
      "Epoch 9892\n",
      "-------------------------------\n",
      "tensor(1215.5201)\n",
      "tensor(407.4156)\n",
      "tensor(940.3936)\n",
      "tensor(23.3627)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 68.120468\n",
      "Epoch 9893\n",
      "-------------------------------\n",
      "tensor(1536.1681)\n",
      "tensor(516.5614)\n",
      "tensor(1185.5096)\n",
      "tensor(29.5250)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 87.740631\n",
      "Epoch 9894\n",
      "-------------------------------\n",
      "tensor(1515.6436)\n",
      "tensor(508.4576)\n",
      "tensor(1172.2130)\n",
      "tensor(28.8384)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 86.756676\n",
      "Epoch 9895\n",
      "-------------------------------\n",
      "tensor(752.4538)\n",
      "tensor(252.7002)\n",
      "tensor(581.0974)\n",
      "tensor(14.6348)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.619900\n",
      "Epoch 9896\n",
      "-------------------------------\n",
      "tensor(503.6263)\n",
      "tensor(163.9871)\n",
      "tensor(385.4512)\n",
      "tensor(9.3928)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.593182\n",
      "Epoch 9897\n",
      "-------------------------------\n",
      "tensor(730.2887)\n",
      "tensor(244.6466)\n",
      "tensor(563.4404)\n",
      "tensor(14.3056)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 46.880657\n",
      "Epoch 9898\n",
      "-------------------------------\n",
      "tensor(408.2241)\n",
      "tensor(142.0414)\n",
      "tensor(316.5394)\n",
      "tensor(8.0986)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 38.711521\n",
      "Epoch 9899\n",
      "-------------------------------\n",
      "tensor(242.8921)\n",
      "tensor(78.6196)\n",
      "tensor(182.8275)\n",
      "tensor(4.3772)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.204929\n",
      "Epoch 9900\n",
      "-------------------------------\n",
      "tensor(449.2368)\n",
      "tensor(150.5089)\n",
      "tensor(345.3667)\n",
      "tensor(8.5555)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.387280\n",
      "Epoch 9901\n",
      "-------------------------------\n",
      "tensor(363.3384)\n",
      "tensor(123.4282)\n",
      "tensor(280.2505)\n",
      "tensor(7.0592)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.826920\n",
      "Epoch 9902\n",
      "-------------------------------\n",
      "tensor(162.8891)\n",
      "tensor(57.2554)\n",
      "tensor(125.2555)\n",
      "tensor(3.3042)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.467510\n",
      "Epoch 9903\n",
      "-------------------------------\n",
      "tensor(142.6632)\n",
      "tensor(44.2403)\n",
      "tensor(103.5019)\n",
      "tensor(2.2911)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.289387\n",
      "Epoch 9904\n",
      "-------------------------------\n",
      "tensor(450.0628)\n",
      "tensor(147.5427)\n",
      "tensor(342.7248)\n",
      "tensor(8.2185)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.294304\n",
      "Epoch 9905\n",
      "-------------------------------\n",
      "tensor(387.8586)\n",
      "tensor(129.5838)\n",
      "tensor(298.1949)\n",
      "tensor(7.2997)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.152229\n",
      "Epoch 9906\n",
      "-------------------------------\n",
      "tensor(305.9525)\n",
      "tensor(98.0080)\n",
      "tensor(229.4222)\n",
      "tensor(5.4464)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.875813\n",
      "Epoch 9907\n",
      "-------------------------------\n",
      "tensor(498.7897)\n",
      "tensor(162.9951)\n",
      "tensor(377.7587)\n",
      "tensor(9.1598)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.321922\n",
      "Epoch 9908\n",
      "-------------------------------\n",
      "tensor(598.7012)\n",
      "tensor(193.3115)\n",
      "tensor(453.7145)\n",
      "tensor(11.1547)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.850849\n",
      "Epoch 9909\n",
      "-------------------------------\n",
      "tensor(113.2685)\n",
      "tensor(35.4189)\n",
      "tensor(80.6239)\n",
      "tensor(1.9604)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.237556\n",
      "Epoch 9910\n",
      "-------------------------------\n",
      "tensor(447.3770)\n",
      "tensor(146.4664)\n",
      "tensor(340.7498)\n",
      "tensor(8.2591)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.346066\n",
      "Epoch 9911\n",
      "-------------------------------\n",
      "tensor(891.0948)\n",
      "tensor(292.3164)\n",
      "tensor(685.0696)\n",
      "tensor(16.8565)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 52.631413\n",
      "Epoch 9912\n",
      "-------------------------------\n",
      "tensor(1284.1958)\n",
      "tensor(433.1315)\n",
      "tensor(993.3358)\n",
      "tensor(24.8805)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 71.940468\n",
      "Epoch 9913\n",
      "-------------------------------\n",
      "tensor(1598.3687)\n",
      "tensor(541.5674)\n",
      "tensor(1241.6420)\n",
      "tensor(30.8651)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 92.818802\n",
      "Epoch 9914\n",
      "-------------------------------\n",
      "tensor(1565.3978)\n",
      "tensor(522.7454)\n",
      "tensor(1204.2614)\n",
      "tensor(29.9285)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 89.411819\n",
      "Epoch 9915\n",
      "-------------------------------\n",
      "tensor(770.6567)\n",
      "tensor(250.5018)\n",
      "tensor(586.7350)\n",
      "tensor(14.1225)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.262474\n",
      "Epoch 9916\n",
      "-------------------------------\n",
      "tensor(516.2444)\n",
      "tensor(181.0838)\n",
      "tensor(401.0644)\n",
      "tensor(10.1561)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.314522\n",
      "Epoch 9917\n",
      "-------------------------------\n",
      "tensor(747.0772)\n",
      "tensor(239.4160)\n",
      "tensor(570.7567)\n",
      "tensor(13.4956)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.549522\n",
      "Epoch 9918\n",
      "-------------------------------\n",
      "tensor(435.7325)\n",
      "tensor(139.3608)\n",
      "tensor(330.8138)\n",
      "tensor(7.7292)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.204056\n",
      "Epoch 9919\n",
      "-------------------------------\n",
      "tensor(233.1211)\n",
      "tensor(78.6478)\n",
      "tensor(178.7019)\n",
      "tensor(4.5043)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.204071\n",
      "Epoch 9920\n",
      "-------------------------------\n",
      "tensor(458.6143)\n",
      "tensor(149.5781)\n",
      "tensor(350.2623)\n",
      "tensor(8.4825)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.630676\n",
      "Epoch 9921\n",
      "-------------------------------\n",
      "tensor(379.0345)\n",
      "tensor(120.9494)\n",
      "tensor(287.9370)\n",
      "tensor(6.8146)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.133934\n",
      "Epoch 9922\n",
      "-------------------------------\n",
      "tensor(178.0880)\n",
      "tensor(53.1874)\n",
      "tensor(131.4478)\n",
      "tensor(2.8916)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.649624\n",
      "Epoch 9923\n",
      "-------------------------------\n",
      "tensor(130.2110)\n",
      "tensor(50.4270)\n",
      "tensor(102.4107)\n",
      "tensor(2.8884)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.348148\n",
      "Epoch 9924\n",
      "-------------------------------\n",
      "tensor(453.4969)\n",
      "tensor(156.9772)\n",
      "tensor(351.2277)\n",
      "tensor(8.9713)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.571312\n",
      "Epoch 9925\n",
      "-------------------------------\n",
      "tensor(405.3633)\n",
      "tensor(137.1961)\n",
      "tensor(311.2291)\n",
      "tensor(7.8476)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.599110\n",
      "Epoch 9926\n",
      "-------------------------------\n",
      "tensor(298.8566)\n",
      "tensor(103.3404)\n",
      "tensor(231.2606)\n",
      "tensor(5.7114)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.929756\n",
      "Epoch 9927\n",
      "-------------------------------\n",
      "tensor(508.9098)\n",
      "tensor(179.7442)\n",
      "tensor(399.2997)\n",
      "tensor(10.0894)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.850899\n",
      "Epoch 9928\n",
      "-------------------------------\n",
      "tensor(603.0733)\n",
      "tensor(190.7831)\n",
      "tensor(455.8160)\n",
      "tensor(10.9586)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 42.852741\n",
      "Epoch 9929\n",
      "-------------------------------\n",
      "tensor(81.8313)\n",
      "tensor(30.9323)\n",
      "tensor(60.7080)\n",
      "tensor(1.3902)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.164639\n",
      "Epoch 9930\n",
      "-------------------------------\n",
      "tensor(495.6894)\n",
      "tensor(158.0141)\n",
      "tensor(374.2839)\n",
      "tensor(9.1708)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.576969\n",
      "Epoch 9931\n",
      "-------------------------------\n",
      "tensor(942.9022)\n",
      "tensor(313.9809)\n",
      "tensor(729.2147)\n",
      "tensor(17.7449)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 55.082954\n",
      "Epoch 9932\n",
      "-------------------------------\n",
      "tensor(1324.6620)\n",
      "tensor(447.1503)\n",
      "tensor(1029.3986)\n",
      "tensor(26.1420)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 74.840462\n",
      "Epoch 9933\n",
      "-------------------------------\n",
      "tensor(1618.8730)\n",
      "tensor(562.5349)\n",
      "tensor(1265.0339)\n",
      "tensor(32.3131)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 94.228851\n",
      "Epoch 9934\n",
      "-------------------------------\n",
      "tensor(1570.4027)\n",
      "tensor(519.9979)\n",
      "tensor(1209.1799)\n",
      "tensor(29.4853)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 90.514610\n",
      "Epoch 9935\n",
      "-------------------------------\n",
      "tensor(740.0486)\n",
      "tensor(244.8017)\n",
      "tensor(569.8978)\n",
      "tensor(13.9835)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 47.433208\n",
      "Epoch 9936\n",
      "-------------------------------\n",
      "tensor(565.1463)\n",
      "tensor(175.7587)\n",
      "tensor(424.9741)\n",
      "tensor(9.7732)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 42.119728\n",
      "Epoch 9937\n",
      "-------------------------------\n",
      "tensor(742.5017)\n",
      "tensor(252.2701)\n",
      "tensor(576.4651)\n",
      "tensor(14.7658)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.440353\n",
      "Epoch 9938\n",
      "-------------------------------\n",
      "tensor(448.8400)\n",
      "tensor(153.1521)\n",
      "tensor(346.3408)\n",
      "tensor(8.5379)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.448189\n",
      "Epoch 9939\n",
      "-------------------------------\n",
      "tensor(229.2107)\n",
      "tensor(77.4236)\n",
      "tensor(175.9954)\n",
      "tensor(4.5829)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.113552\n",
      "Epoch 9940\n",
      "-------------------------------\n",
      "tensor(462.7125)\n",
      "tensor(156.7849)\n",
      "tensor(356.9221)\n",
      "tensor(9.1075)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.704716\n",
      "Epoch 9941\n",
      "-------------------------------\n",
      "tensor(385.0042)\n",
      "tensor(130.8135)\n",
      "tensor(296.8858)\n",
      "tensor(7.6134)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.230701\n",
      "Epoch 9942\n",
      "-------------------------------\n",
      "tensor(180.7457)\n",
      "tensor(61.9860)\n",
      "tensor(139.0425)\n",
      "tensor(3.6721)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.675976\n",
      "Epoch 9943\n",
      "-------------------------------\n",
      "tensor(133.9265)\n",
      "tensor(44.4155)\n",
      "tensor(98.9121)\n",
      "tensor(2.3110)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.327496\n",
      "Epoch 9944\n",
      "-------------------------------\n",
      "tensor(457.0100)\n",
      "tensor(155.4165)\n",
      "tensor(354.8346)\n",
      "tensor(8.8346)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.631573\n",
      "Epoch 9945\n",
      "-------------------------------\n",
      "tensor(413.8387)\n",
      "tensor(143.5796)\n",
      "tensor(322.3402)\n",
      "tensor(8.2688)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.760956\n",
      "Epoch 9946\n",
      "-------------------------------\n",
      "tensor(302.4779)\n",
      "tensor(95.5074)\n",
      "tensor(225.1517)\n",
      "tensor(5.1678)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.853222\n",
      "Epoch 9947\n",
      "-------------------------------\n",
      "tensor(525.7225)\n",
      "tensor(178.3520)\n",
      "tensor(406.8548)\n",
      "tensor(10.1627)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.108974\n",
      "Epoch 9948\n",
      "-------------------------------\n",
      "tensor(608.3684)\n",
      "tensor(188.1449)\n",
      "tensor(454.8311)\n",
      "tensor(10.6928)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.054955\n",
      "Epoch 9949\n",
      "-------------------------------\n",
      "tensor(81.6686)\n",
      "tensor(26.2087)\n",
      "tensor(55.5191)\n",
      "tensor(1.3959)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.220592\n",
      "Epoch 9950\n",
      "-------------------------------\n",
      "tensor(500.0273)\n",
      "tensor(157.1904)\n",
      "tensor(378.9872)\n",
      "tensor(8.7206)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 40.639477\n",
      "Epoch 9951\n",
      "-------------------------------\n",
      "tensor(932.8741)\n",
      "tensor(313.3783)\n",
      "tensor(722.3502)\n",
      "tensor(18.2783)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.613125\n",
      "Epoch 9952\n",
      "-------------------------------\n",
      "tensor(1316.7826)\n",
      "tensor(451.1206)\n",
      "tensor(1022.7962)\n",
      "tensor(26.0023)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 73.878609\n",
      "Epoch 9953\n",
      "-------------------------------\n",
      "tensor(1633.0599)\n",
      "tensor(544.6650)\n",
      "tensor(1261.0111)\n",
      "tensor(30.7934)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 94.829773\n",
      "Epoch 9954\n",
      "-------------------------------\n",
      "tensor(1587.7800)\n",
      "tensor(527.6677)\n",
      "tensor(1219.6537)\n",
      "tensor(30.0952)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 90.908279\n",
      "Epoch 9955\n",
      "-------------------------------\n",
      "tensor(772.9275)\n",
      "tensor(258.9051)\n",
      "tensor(594.7538)\n",
      "tensor(14.8848)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.248627\n",
      "Epoch 9956\n",
      "-------------------------------\n",
      "tensor(522.5270)\n",
      "tensor(184.5264)\n",
      "tensor(407.9693)\n",
      "tensor(10.3926)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 41.243847\n",
      "Epoch 9957\n",
      "-------------------------------\n",
      "tensor(758.7603)\n",
      "tensor(247.3920)\n",
      "tensor(578.9063)\n",
      "tensor(14.0751)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 47.648056\n",
      "Epoch 9958\n",
      "-------------------------------\n",
      "tensor(427.9854)\n",
      "tensor(143.0719)\n",
      "tensor(331.6234)\n",
      "tensor(8.2985)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.058407\n",
      "Epoch 9959\n",
      "-------------------------------\n",
      "tensor(247.0403)\n",
      "tensor(79.8857)\n",
      "tensor(181.9001)\n",
      "tensor(4.2744)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.182781\n",
      "Epoch 9960\n",
      "-------------------------------\n",
      "tensor(464.4669)\n",
      "tensor(152.6217)\n",
      "tensor(352.4229)\n",
      "tensor(8.5316)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 39.565140\n",
      "Epoch 9961\n",
      "-------------------------------\n",
      "tensor(374.6338)\n",
      "tensor(123.5314)\n",
      "tensor(287.7819)\n",
      "tensor(7.0159)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 37.984745\n",
      "Epoch 9962\n",
      "-------------------------------\n",
      "tensor(167.9540)\n",
      "tensor(54.9908)\n",
      "tensor(129.9361)\n",
      "tensor(3.1795)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.509148\n",
      "Epoch 9963\n",
      "-------------------------------\n",
      "tensor(139.1649)\n",
      "tensor(47.0793)\n",
      "tensor(103.9631)\n",
      "tensor(2.5549)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.308361\n",
      "Epoch 9964\n",
      "-------------------------------\n",
      "tensor(456.9959)\n",
      "tensor(153.5329)\n",
      "tensor(350.0187)\n",
      "tensor(8.6468)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 39.523129\n",
      "Epoch 9965\n",
      "-------------------------------\n",
      "tensor(397.7975)\n",
      "tensor(134.7895)\n",
      "tensor(306.7019)\n",
      "tensor(7.7100)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 38.431122\n",
      "Epoch 9966\n",
      "-------------------------------\n",
      "tensor(307.0712)\n",
      "tensor(100.4959)\n",
      "tensor(231.0341)\n",
      "tensor(5.4136)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.899250\n",
      "Epoch 9967\n",
      "-------------------------------\n",
      "tensor(515.5199)\n",
      "tensor(165.4115)\n",
      "tensor(389.2762)\n",
      "tensor(9.5203)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 40.606754\n",
      "Epoch 9968\n",
      "-------------------------------\n",
      "tensor(618.2381)\n",
      "tensor(193.9201)\n",
      "tensor(463.7097)\n",
      "tensor(10.8881)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.158386\n",
      "Epoch 9969\n",
      "-------------------------------\n",
      "tensor(107.9463)\n",
      "tensor(43.6840)\n",
      "tensor(84.5207)\n",
      "tensor(2.2585)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.203838\n",
      "Epoch 9970\n",
      "-------------------------------\n",
      "tensor(471.8809)\n",
      "tensor(147.2202)\n",
      "tensor(350.0539)\n",
      "tensor(8.2811)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 39.604134\n",
      "Epoch 9971\n",
      "-------------------------------\n",
      "tensor(942.5217)\n",
      "tensor(309.0461)\n",
      "tensor(719.1014)\n",
      "tensor(17.4202)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 54.482002\n",
      "Epoch 9972\n",
      "-------------------------------\n",
      "tensor(1359.0693)\n",
      "tensor(457.4561)\n",
      "tensor(1053.7635)\n",
      "tensor(26.3100)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 76.715599\n",
      "Epoch 9973\n",
      "-------------------------------\n",
      "tensor(1692.9791)\n",
      "tensor(578.7639)\n",
      "tensor(1315.2454)\n",
      "tensor(33.3007)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 99.305000\n",
      "Epoch 9974\n",
      "-------------------------------\n",
      "tensor(1646.9818)\n",
      "tensor(552.0993)\n",
      "tensor(1274.0884)\n",
      "tensor(31.3309)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 96.522781\n",
      "Epoch 9975\n",
      "-------------------------------\n",
      "tensor(776.5087)\n",
      "tensor(256.8879)\n",
      "tensor(600.7359)\n",
      "tensor(14.7505)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 48.749363\n",
      "Epoch 9976\n",
      "-------------------------------\n",
      "tensor(594.7524)\n",
      "tensor(190.8470)\n",
      "tensor(455.6866)\n",
      "tensor(10.6561)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 43.018738\n",
      "Epoch 9977\n",
      "-------------------------------\n",
      "tensor(777.3832)\n",
      "tensor(259.9837)\n",
      "tensor(599.2513)\n",
      "tensor(15.3746)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 48.433838\n",
      "Epoch 9978\n",
      "-------------------------------\n",
      "tensor(477.0443)\n",
      "tensor(159.1739)\n",
      "tensor(367.7124)\n",
      "tensor(9.1968)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 39.939342\n",
      "Epoch 9979\n",
      "-------------------------------\n",
      "tensor(231.6319)\n",
      "tensor(82.8704)\n",
      "tensor(180.0456)\n",
      "tensor(4.6746)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.086033\n",
      "Epoch 9980\n",
      "-------------------------------\n",
      "tensor(479.0823)\n",
      "tensor(168.0619)\n",
      "tensor(373.6520)\n",
      "tensor(9.6451)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.077091\n",
      "Epoch 9981\n",
      "-------------------------------\n",
      "tensor(401.1916)\n",
      "tensor(142.2633)\n",
      "tensor(313.9448)\n",
      "tensor(8.2294)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 38.540123\n",
      "Epoch 9982\n",
      "-------------------------------\n",
      "tensor(190.1005)\n",
      "tensor(70.7868)\n",
      "tensor(150.1526)\n",
      "tensor(4.1682)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 35.711472\n",
      "Epoch 9983\n",
      "-------------------------------\n",
      "tensor(136.2072)\n",
      "tensor(41.0822)\n",
      "tensor(99.6916)\n",
      "tensor(2.0848)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 35.273636\n",
      "Epoch 9984\n",
      "-------------------------------\n",
      "tensor(481.6101)\n",
      "tensor(161.1643)\n",
      "tensor(369.7747)\n",
      "tensor(8.9923)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 40.078373\n",
      "Epoch 9985\n",
      "-------------------------------\n",
      "tensor(435.1950)\n",
      "tensor(153.5236)\n",
      "tensor(340.6008)\n",
      "tensor(8.6972)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 39.227989\n",
      "Epoch 9986\n",
      "-------------------------------\n",
      "tensor(313.6175)\n",
      "tensor(90.6143)\n",
      "tensor(228.9833)\n",
      "tensor(4.9885)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 36.977184\n",
      "Epoch 9987\n",
      "-------------------------------\n",
      "tensor(571.6330)\n",
      "tensor(179.1763)\n",
      "tensor(429.0031)\n",
      "tensor(10.1595)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 41.922401\n",
      "Epoch 9988\n",
      "-------------------------------\n",
      "tensor(639.7938)\n",
      "tensor(205.0560)\n",
      "tensor(484.0655)\n",
      "tensor(11.4840)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 43.845829\n",
      "Epoch 9989\n",
      "-------------------------------\n",
      "tensor(85.5749)\n",
      "tensor(30.0642)\n",
      "tensor(57.7608)\n",
      "tensor(1.8398)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 35.066414\n",
      "Epoch 9990\n",
      "-------------------------------\n",
      "tensor(552.4431)\n",
      "tensor(174.8002)\n",
      "tensor(415.1787)\n",
      "tensor(9.4572)\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "loss: 41.519928\n",
      "Epoch 9991\n",
      "-------------------------------\n",
      "tensor(1031.1725)\n",
      "tensor(342.6992)\n",
      "tensor(797.9950)\n",
      "tensor(20.1477)\n",
      "Adjusting learning rate of group 0 to 9.7798e-05.\n",
      "loss: 58.824173\n",
      "Epoch 9992\n",
      "-------------------------------\n",
      "tensor(1447.1344)\n",
      "tensor(507.6788)\n",
      "tensor(1135.0242)\n",
      "tensor(29.3939)\n",
      "Adjusting learning rate of group 0 to 9.1406e-05.\n",
      "loss: 82.464615\n",
      "Epoch 9993\n",
      "-------------------------------\n",
      "tensor(1787.3789)\n",
      "tensor(599.4924)\n",
      "tensor(1384.7220)\n",
      "tensor(34.0021)\n",
      "Adjusting learning rate of group 0 to 8.1450e-05.\n",
      "loss: 107.588974\n",
      "Epoch 9994\n",
      "-------------------------------\n",
      "tensor(1707.8033)\n",
      "tensor(562.8097)\n",
      "tensor(1309.7333)\n",
      "tensor(32.0483)\n",
      "Adjusting learning rate of group 0 to 6.8906e-05.\n",
      "loss: 99.542168\n",
      "Epoch 9995\n",
      "-------------------------------\n",
      "tensor(773.5331)\n",
      "tensor(264.3795)\n",
      "tensor(598.1382)\n",
      "tensor(15.1469)\n",
      "Adjusting learning rate of group 0 to 5.5000e-05.\n",
      "loss: 49.054661\n",
      "Epoch 9996\n",
      "-------------------------------\n",
      "tensor(628.9462)\n",
      "tensor(212.9294)\n",
      "tensor(482.4277)\n",
      "tensor(12.1967)\n",
      "Adjusting learning rate of group 0 to 4.1094e-05.\n",
      "loss: 44.471352\n",
      "Epoch 9997\n",
      "-------------------------------\n",
      "tensor(792.9113)\n",
      "tensor(256.4366)\n",
      "tensor(611.3782)\n",
      "tensor(14.7291)\n",
      "Adjusting learning rate of group 0 to 2.8550e-05.\n",
      "loss: 49.602409\n",
      "Epoch 9998\n",
      "-------------------------------\n",
      "tensor(514.7891)\n",
      "tensor(166.3142)\n",
      "tensor(395.2425)\n",
      "tensor(9.3751)\n",
      "Adjusting learning rate of group 0 to 1.8594e-05.\n",
      "loss: 41.233578\n",
      "Epoch 9999\n",
      "-------------------------------\n",
      "tensor(224.9137)\n",
      "tensor(76.9510)\n",
      "tensor(167.3379)\n",
      "tensor(4.3897)\n",
      "Adjusting learning rate of group 0 to 1.2202e-05.\n",
      "loss: 36.293579\n",
      "Epoch 10000\n",
      "-------------------------------\n",
      "tensor(494.9904)\n",
      "tensor(163.9886)\n",
      "tensor(377.4449)\n",
      "tensor(9.4482)\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "loss: 40.497177\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "## Set up neural network.\n",
    "INPUT_SIZE = 1\n",
    "WIDTH = 100\n",
    "\n",
    "## Single hidden layer\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, WIDTH),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(WIDTH, 1)\n",
    "        )\n",
    "        # self.linear_stack = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear_relu_stack(x)\n",
    "        # output = self.linear_stack(x)\n",
    "        return output\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        # nn.init.normal_(m.weight,mean=0,std=1)\n",
    "        nn.init.normal_(m.bias,mean=0,std=1)\n",
    "\n",
    "## MSE Model\n",
    "model = NeuralNetwork().to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "## l2 loss with SGD optimizer using cosine annealing learning rate schedule\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer_MSE = torch.optim.SGD(model.parameters(), momentum=0.9, lr=1e-5)\n",
    "scheduler_MSE = lr_scheduler.CosineAnnealingLR(optimizer_MSE, T_max = 10, eta_min=1e-4,verbose=True)\n",
    "\n",
    "## Training loop\n",
    "def train(X, y, model, loss_fn, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        print(p.grad.norm())\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "train_MSE = True\n",
    "if train_MSE:\n",
    "    epochs = 10000\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(X_training, y, model, loss_fn, optimizer_MSE, scheduler_MSE)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch code to compute eNTK - unused at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s4531973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_functorch\\deprecated.py:97: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional', 'torch.func.functional_call')\n"
     ]
    }
   ],
   "source": [
    "# ## Code to find eNTK\n",
    "# fnet, params = make_functional(model)\n",
    "\n",
    "# def fnet_single(params, x):\n",
    "#     return fnet(params, x.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2):\n",
    "#     # Compute J(x1)\n",
    "#     jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x1)\n",
    "#     jac1 = [j.flatten(2) for j in jac1]\n",
    "    \n",
    "#     # Compute J(x2)\n",
    "#     jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n",
    "#     jac2 = [j.flatten(2) for j in jac2]\n",
    "    \n",
    "#     # Compute J(x1) @ J(x2).T\n",
    "#     result = torch.stack([torch.einsum('Naf,Mbf->NMab', j1, j2) for j1, j2 in zip(jac1, jac2)])\n",
    "#     result = result.sum(0)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual code to compute eNTK - definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find NTK\n",
    "def flatten_extend_gradient(parameters):\n",
    "    flat_list = []\n",
    "    for parameter in parameters:\n",
    "        flat_list.extend(parameter.grad.detach().numpy().flatten())\n",
    "    return flat_list\n",
    "\n",
    "def gradient_model(model,xi):\n",
    "    ## model needs to have parameters with requires_grad=true\n",
    "    optimizer_MSE.zero_grad()\n",
    "    model(xi).backward()\n",
    "    grad_vec = np.array(flatten_extend_gradient(list(model.parameters())))\n",
    "    return grad_vec\n",
    "\n",
    "def ntk_single(x1,x2,model):\n",
    "    j1 = gradient_model(model,x1)\n",
    "    j2 = gradient_model(model,x2)\n",
    "    return j1 @ j2.transpose()\n",
    "\n",
    "def ntk_matrix(X1,X2,model):\n",
    "# Xi must be a torch variable\n",
    "    Kappa = np.empty((X1.numpy().flatten().size,X2.numpy().flatten().size))\n",
    "    for i1,x1 in enumerate(X1):\n",
    "        for i2,x2 in enumerate(X2):\n",
    "            Kappa[i1,i2] = ntk_single(x1,x2,model)\n",
    "    return Kappa\n",
    "\n",
    "\n",
    "## For pseudo method\n",
    "def compute_J_X(X,model):\n",
    "    p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n = X.numpy().flatten().size\n",
    "    J_X = np.empty((n,p))\n",
    "    for i,x in enumerate(X):\n",
    "        J_X[i,:] = gradient_model(model,x)\n",
    "    return J_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual code to compute eNTK - Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition number of Kappa = 2110.7344005681593\n",
      "Eigenvalues of Kappa_XX are [1.89965056e+04 1.01175897e+04 2.77784231e+03 4.19777153e+02\n",
      " 3.31661086e+02 1.34401578e+02 8.33787849e+01 3.86961773e+01\n",
      " 3.43441672e+01 2.51028072e+01 1.90210694e+01 1.57609451e+01\n",
      " 1.27445493e+01 1.01976267e+01 9.10190596e+00 9.06438734e+00\n",
      " 9.02682053e+00 9.01375271e+00 8.99995073e+00 9.00008923e+00]\n",
      "Number of zero values for full rank: 0\n",
      "Number of negative values for full rank: 0\n",
      "Number of zero values for pseudo: 0\n",
      "Number of negative values for pseudo: 87\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=False)\n",
    "\n",
    "## Create UQ estimate\n",
    "uq_array_full_rank = np.empty((1,NUM_TEST_POINTS))\n",
    "uq_array_pseudo = np.empty((1,NUM_TEST_POINTS))\n",
    "\n",
    "epsilon = 9\n",
    "\n",
    "J_X = compute_J_X(X_training,model)\n",
    "J_X_pseudo = np.linalg.pinv(J_X,rcond=1e-15)\n",
    "J_X_matrix = J_X_pseudo @ J_X\n",
    "\n",
    "Kappa = ntk_matrix(X_training,X_training,model)\n",
    "Kappa = Kappa + epsilon*np.eye(NUM_TRAIN_POINTS)\n",
    "x_array = np.random.random((1,NUM_TRAIN_POINTS))\n",
    "print(\"condition number of Kappa = {}\".format(np.linalg.cond(Kappa)))\n",
    "print(\"Eigenvalues of Kappa_XX are {}\".format(np.linalg.eigvals(Kappa)))\n",
    "# print(np.linalg.eig(Kappa)[1])\n",
    "# print(\"x @ Kappa_XX @ x^T = {}\".format(x_array @ Kappa @ x_array.transpose()))\n",
    "\n",
    "compute_uq_array = True\n",
    "\n",
    "if compute_uq_array:\n",
    "    for i,x_single in enumerate(x_test):\n",
    "        x_single = x_single.reshape((1,1))\n",
    "        \n",
    "        kappa_xx = ntk_matrix(x_single, x_single,model)\n",
    "        # print(kappa_xx)\n",
    "        kappa_xX = ntk_matrix(x_single, X_training,model)\n",
    "\n",
    "        x_solve = np.linalg.solve(Kappa,kappa_xX.transpose())\n",
    "        resid_error = np.linalg.norm(Kappa @ x_solve  - kappa_xX.transpose()) / np.linalg.norm(kappa_xX.transpose())\n",
    "\n",
    "        uq_full_rank = kappa_xx - kappa_xX @ np.linalg.solve(Kappa,kappa_xX.transpose())\n",
    "\n",
    "        grad_f_x = gradient_model(model,x_single)\n",
    "        uq_pseudo = np.inner(grad_f_x,grad_f_x) - grad_f_x.transpose() @ J_X_matrix @ grad_f_x\n",
    "\n",
    "        uq_array_full_rank[0,i] = uq_full_rank\n",
    "        uq_array_pseudo[0,i] = uq_pseudo\n",
    "\n",
    "        # print(\"x_test = {}, uncertainty estimate is {}, kappa_xx is {}, mult is {}\".format(x_single.squeeze(0).squeeze(0),uq.squeeze(0).squeeze(0), kappa_xx.squeeze(0).squeeze(0), (kappa_xX @ np.linalg.solve(Kappa,kappa_xX.transpose())).squeeze(0).squeeze(0)))\n",
    "        # print(\"condition number of Kappa = {}\".format(np.linalg.cond(Kappa)))\n",
    "        # print(\"Residual error from solve = {}  \\n\".format(resid_error))\n",
    "\n",
    "print(\"Number of zero values for full rank: {}\".format(uq_array_full_rank[uq_array_full_rank==0].size))\n",
    "print(\"Number of negative values for full rank: {}\".format(uq_array_full_rank[uq_array_full_rank<0].size))\n",
    "print(\"Number of zero values for pseudo: {}\".format(uq_array_pseudo[uq_array_pseudo==0].size))\n",
    "print(\"Number of negative values for pseudo: {}\".format(uq_array_pseudo[uq_array_pseudo<0].size))\n",
    "if uq_array_pseudo[uq_array_pseudo<0].size != 0: uq_array_pseudo[uq_array_pseudo<0]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Deep Ensemble Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m     model_list\u001b[38;5;241m.\u001b[39mappend(EnsembleNetwork()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     66\u001b[0m     opt_list\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_list[i]\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m learning_rate))\n\u001b[1;32m---> 67\u001b[0m     sched_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241m.\u001b[39mPolynomialLR(opt_list[i], epochs, \u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_list[\u001b[38;5;241m0\u001b[39m](X_training[\u001b[38;5;241m1\u001b[39m])))\n\u001b[0;32m     72\u001b[0m NLL \u001b[38;5;241m=\u001b[39m CustomNLL()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "## Deep Ensemble\n",
    "def to_np(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "class EnsembleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(INPUT_SIZE,WIDTH)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_mu = nn.Linear(WIDTH,1)\n",
    "        self.linear_sig = nn.Linear(WIDTH,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        mu = self.linear_mu(x)\n",
    "        variance = self.linear_sig(x)\n",
    "        variance = F.softplus(variance) + 1e-6\n",
    "        return mu, variance\n",
    "\n",
    "class CustomNLL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNLL, self).__init__()\n",
    "\n",
    "    def forward(self, y, mean, var):\n",
    "        \n",
    "        loss = (0.5*torch.log(var) + 0.5*(y - mean).pow(2)/var).mean() + 1\n",
    "\n",
    "        if np.any(np.isnan(to_np(loss))):\n",
    "            print(torch.log(var))\n",
    "            print((y - mean).pow(2)/var)\n",
    "            raise ValueError('There is Nan in loss')\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def train_ensemble(X, y, model, loss_fn, optimizer, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    # Compute prediction error\n",
    "    mean, variance = model(X)\n",
    "    loss = loss_fn(y, mean, variance)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        print(p.grad.norm())\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "M = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 4000\n",
    "model_list = []\n",
    "opt_list = []\n",
    "sched_list = []\n",
    "\n",
    "for i in range(M):\n",
    "    model_list.append(EnsembleNetwork().to(device))\n",
    "    opt_list.append(torch.optim.Adam(model_list[i].parameters(), lr = learning_rate))\n",
    "    sched_list.append(lr_scheduler.PolynomialLR(opt_list[i], epochs, 0.5, verbose=True))\n",
    "\n",
    "\n",
    "print(\"Model is {}\".format(model_list[0](X_training[1])))\n",
    "\n",
    "NLL = CustomNLL()\n",
    "\n",
    "TRAIN_ENSEMBLES = True\n",
    "if TRAIN_ENSEMBLES:\n",
    "    for i in range(M):\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train_ensemble(X_training, y, model_list[i], NLL, opt_list[i], sched_list[i])\n",
    "            # train_ensemble(X_training, y, model_list[i], NLL, opt_list[i])\n",
    "        print(\"Done!\")\n",
    "\n",
    "mu_test_list = np.empty((M,NUM_TEST_POINTS))\n",
    "sigma_test_list = np.empty((M,NUM_TEST_POINTS))\n",
    "for i in range(M):\n",
    "    mu, sig = model_list[i](x_test)\n",
    "    mu_test_list[i,:] = np.reshape(to_np(mu), (NUM_TEST_POINTS))\n",
    "    sigma_test_list[i,:] = np.reshape(to_np(sig),(NUM_TEST_POINTS))\n",
    "mu_mean = np.mean(mu_test_list,axis=0)\n",
    "sigma_mean = np.mean(sigma_test_list, axis=0) + np.mean(np.square(mu_test_list), axis = 0) - np.square(mu_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot\n",
    "f, (ax1,ax2,ax3) = plt.subplots(3,1)\n",
    "\n",
    "f.set_figheight(9)\n",
    "f.set_figwidth(12)\n",
    "f.subplots_adjust(left=0.1, right=0.8, hspace=0.4)\n",
    "\n",
    "test_data_y = model(x_test)\n",
    "line1 = ax1.plot(x_test,test_data_y.detach().numpy(), markersize=12, label = \"Model prediction\")\n",
    "line2 = ax1.plot(x_test,y_test,'k', label = \"Ground truth\")\n",
    "line3 = ax1.scatter(train_x,train_y, s=100, c='m', marker='+', label = \"Training points\")\n",
    "ci = np.sqrt(uq_array_full_rank)*3\n",
    "y1 = test_data_y.detach().numpy()-ci.transpose()\n",
    "y2 = test_data_y.detach().numpy()+ci.transpose()\n",
    "# ax1.legend()\n",
    "ax1.fill_between(x_test.detach().numpy().squeeze(1), y1.squeeze(1), y2.squeeze(1) , color='g', alpha=.3)\n",
    "ax1.set_title(\"NTK Method - Full Rank\")\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "test_data_y = model(x_test)\n",
    "line1 = ax2.plot(x_test,test_data_y.detach().numpy(), markersize=12, label = \"Model prediction\")\n",
    "line2 = ax2.plot(x_test,y_test,'k', label = \"Ground truth\")\n",
    "line3 = ax2.scatter(train_x,train_y, s=100, c='m', marker='+', label = \"Training points\")\n",
    "ci = np.sqrt(uq_array_pseudo)*3\n",
    "y1 = test_data_y.detach().numpy()-ci.transpose()\n",
    "y2 = test_data_y.detach().numpy()+ci.transpose()\n",
    "# ax1.legend()\n",
    "ax2.fill_between(x_test.detach().numpy().squeeze(1), y1.squeeze(1), y2.squeeze(1) , color='g', alpha=.3)\n",
    "ax2.set_title(\"NTK Method - Pseudo\")\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "\n",
    "# test_data_y = model(x_test) * training_data_y_std + training_data_y_mean\n",
    "# print(test_data_y)\n",
    "ax3.plot(x_test,mu_mean,markersize=12, label=\"Model prediction $\\mu$\")\n",
    "ax3.plot(x_test,y_test, 'k',label=\"Ground truth\")\n",
    "ax3.scatter(train_x,train_y,s=100, c='m',marker='+',label=\"Training points\")\n",
    "ci = np.sqrt(sigma_mean)*3\n",
    "y1 = mu_mean-ci\n",
    "y2 = mu_mean+ci\n",
    "ax3.fill_between(x_test.detach().numpy().squeeze(1), y1, y2 , color='g', alpha=.3, label='$\\mu\\pm3\\sigma$')\n",
    "ax3.set_title(\"Deep Ensemble\")\n",
    "f.legend(*ax3.get_legend_handles_labels(),loc='right')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "f.suptitle(\"Uncertainty quantification - toy regression\")\n",
    "\n",
    "plt.savefig(\"./Plots/ToyRegression/UncertaintyComparison.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Code\n",
    ":: Outputs test error and calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading dataset Energy --- \n",
      "\n",
      "Number of data points = 768\n",
      "Number of coloumns = 10\n",
      "Number of features = 8\n"
     ]
    }
   ],
   "source": [
    "### --- Dependencies --- ###\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import solvers\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "### --- INPUT DATA HERE AS WELL AS DATASET NAME --- ###\n",
    "\n",
    "CSV = True\n",
    "dataset_str = \"Energy\"\n",
    "num_features = 8\n",
    "\n",
    "if CSV:\n",
    "    df = pd.read_excel('.\\data\\Energy\\ENB2012_data.xlsx')\n",
    "else:\n",
    "    data = np.loadtxt(fname='./data/Kin8mn/data.txt')\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# print(data.shape)\n",
    "print(\"--- Loading dataset {} --- \\n\".format(dataset_str))\n",
    "print(\"Number of data points = {}\".format(len(df)))\n",
    "print(\"Number of coloumns = {}\".format(len(df.columns)))\n",
    "print(\"Number of features = {}\".format(num_features))\n",
    "\n",
    "### --- --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has shape (691, 10) \n",
      "\n",
      "test set has shape (77, 10)\n",
      "\n",
      " Sample training point \n",
      " X: [ 0.535 -0.677 -0.006 -0.657  0.996  1.366 -1.012 -1.144], \n",
      " y: 0.018788065662671754 \n",
      "\n",
      "Sample test point \n",
      " X: [-1.368  1.558  1.12   0.977 -1.004  0.475  1.225  1.432], \n",
      " y: 16.48 \n",
      "\n",
      " Using cpu device\n",
      "Number of parameters in model = 1501\n",
      "\n",
      " Epoch 1 of 400\n",
      "train loss for batch 1 is 1.9877854181039658\n",
      "train loss for batch 2 is 36.329382954748944\n",
      "train loss for batch 3 is 4.42130997379035\n",
      "train loss for batch 4 is 12.511991351046527\n",
      "train loss for batch 5 is 7.732662048363449\n",
      "train loss for batch 6 is 7.509830119249516\n",
      "train loss for batch 7 is 5.854897900428676\n",
      "--- RMSE = 11.56 ---\n",
      "\n",
      " Epoch 2 of 400\n",
      "train loss for batch 1 is 1.2867170025564536\n",
      "train loss for batch 2 is 1.275097857116763\n",
      "train loss for batch 3 is 2.953477641419356\n",
      "train loss for batch 4 is 1.8860187879132277\n",
      "train loss for batch 5 is 1.4691375467615984\n",
      "train loss for batch 6 is 1.8168274177010793\n",
      "train loss for batch 7 is 1.6449679063900977\n",
      "--- RMSE = 5.89 ---\n",
      "\n",
      " Epoch 3 of 400\n",
      "train loss for batch 1 is 0.3639258364978002\n",
      "train loss for batch 2 is 0.8852895028639008\n",
      "train loss for batch 3 is 1.24686857199188\n",
      "train loss for batch 4 is 0.36039423045626434\n",
      "train loss for batch 5 is 0.4967099388298638\n",
      "train loss for batch 6 is 1.3123746861502392\n",
      "train loss for batch 7 is 0.2657405870533203\n",
      "--- RMSE = 8.30 ---\n",
      "\n",
      " Epoch 4 of 400\n",
      "train loss for batch 1 is 0.6253080214790689\n",
      "train loss for batch 2 is 0.6942254083727751\n",
      "train loss for batch 3 is 0.16697712573524412\n",
      "train loss for batch 4 is 0.444670290428609\n",
      "train loss for batch 5 is 0.45136153359013503\n",
      "train loss for batch 6 is 0.2943141642534369\n",
      "train loss for batch 7 is 0.34765729588309024\n",
      "--- RMSE = 7.30 ---\n",
      "\n",
      " Epoch 5 of 400\n",
      "train loss for batch 1 is 0.6295786251222385\n",
      "train loss for batch 2 is 0.1276832029157686\n",
      "train loss for batch 3 is 0.2295462966936468\n",
      "train loss for batch 4 is 0.406118412961176\n",
      "train loss for batch 5 is 0.22880778466086382\n",
      "train loss for batch 6 is 0.20539937999994073\n",
      "train loss for batch 7 is 0.24954701213630195\n",
      "--- RMSE = 4.08 ---\n",
      "\n",
      " Epoch 6 of 400\n",
      "train loss for batch 1 is 0.1725280707081746\n",
      "train loss for batch 2 is 0.1351014560704777\n",
      "train loss for batch 3 is 0.3009917208510528\n",
      "train loss for batch 4 is 0.12539439785243073\n",
      "train loss for batch 5 is 0.17663352332087165\n",
      "train loss for batch 6 is 0.20668181271839947\n",
      "train loss for batch 7 is 0.15133399399077585\n",
      "--- RMSE = 3.85 ---\n",
      "\n",
      " Epoch 7 of 400\n",
      "train loss for batch 1 is 0.12341373672877795\n",
      "train loss for batch 2 is 0.18726544964572664\n",
      "train loss for batch 3 is 0.08534838097562526\n",
      "train loss for batch 4 is 0.13042820507919234\n",
      "train loss for batch 5 is 0.13870321320489315\n",
      "train loss for batch 6 is 0.11370429376007522\n",
      "train loss for batch 7 is 0.0931234976979316\n",
      "--- RMSE = 3.53 ---\n",
      "\n",
      " Epoch 8 of 400\n",
      "train loss for batch 1 is 0.10256137499741216\n",
      "train loss for batch 2 is 0.09132290194962078\n",
      "train loss for batch 3 is 0.09216150804823048\n",
      "train loss for batch 4 is 0.0955769865682897\n",
      "train loss for batch 5 is 0.09118324275789118\n",
      "train loss for batch 6 is 0.08285369280305463\n",
      "train loss for batch 7 is 0.09886244265933422\n",
      "--- RMSE = 2.60 ---\n",
      "\n",
      " Epoch 9 of 400\n",
      "train loss for batch 1 is 0.05785802802355446\n",
      "train loss for batch 2 is 0.06737468540316999\n",
      "train loss for batch 3 is 0.08649000663161661\n",
      "train loss for batch 4 is 0.08179534553255569\n",
      "train loss for batch 5 is 0.08643144151905802\n",
      "train loss for batch 6 is 0.0756324226911017\n",
      "train loss for batch 7 is 0.06538835159023974\n",
      "--- RMSE = 2.52 ---\n",
      "\n",
      " Epoch 10 of 400\n",
      "train loss for batch 1 is 0.05885011008400781\n",
      "train loss for batch 2 is 0.07042900324430365\n",
      "train loss for batch 3 is 0.06237025125230271\n",
      "train loss for batch 4 is 0.05778373422766198\n",
      "train loss for batch 5 is 0.06773533909392233\n",
      "train loss for batch 6 is 0.07817380118137372\n",
      "train loss for batch 7 is 0.07093262258909214\n",
      "--- RMSE = 2.44 ---\n",
      "\n",
      " Epoch 11 of 400\n",
      "train loss for batch 1 is 0.03489357135877854\n",
      "train loss for batch 2 is 0.057233515191678415\n",
      "train loss for batch 3 is 0.05253971547876771\n",
      "train loss for batch 4 is 0.062484508633782666\n",
      "train loss for batch 5 is 0.059088736293765425\n",
      "train loss for batch 6 is 0.07134145586319572\n",
      "train loss for batch 7 is 0.0716643806869279\n",
      "--- RMSE = 2.32 ---\n",
      "\n",
      " Epoch 12 of 400\n",
      "train loss for batch 1 is 0.050244673871806526\n",
      "train loss for batch 2 is 0.045891851042479656\n",
      "train loss for batch 3 is 0.06598644962452328\n",
      "train loss for batch 4 is 0.05864430245902188\n",
      "train loss for batch 5 is 0.041772478871303045\n",
      "train loss for batch 6 is 0.05546380687754092\n",
      "train loss for batch 7 is 0.04313772991007455\n",
      "--- RMSE = 2.20 ---\n",
      "\n",
      " Epoch 13 of 400\n",
      "train loss for batch 1 is 0.05130379046865023\n",
      "train loss for batch 2 is 0.03668011159070161\n",
      "train loss for batch 3 is 0.05074410836688924\n",
      "train loss for batch 4 is 0.034946323858798256\n",
      "train loss for batch 5 is 0.05500794158001948\n",
      "train loss for batch 6 is 0.043969682338420905\n",
      "train loss for batch 7 is 0.047922466465646095\n",
      "--- RMSE = 2.07 ---\n",
      "\n",
      " Epoch 14 of 400\n",
      "train loss for batch 1 is 0.03407963016548395\n",
      "train loss for batch 2 is 0.05781034278476392\n",
      "train loss for batch 3 is 0.03578688447122926\n",
      "train loss for batch 4 is 0.048501623735341066\n",
      "train loss for batch 5 is 0.04012980593143678\n",
      "train loss for batch 6 is 0.044362146570409634\n",
      "train loss for batch 7 is 0.027107473727657285\n",
      "--- RMSE = 2.01 ---\n",
      "\n",
      " Epoch 15 of 400\n",
      "train loss for batch 1 is 0.024054963929766027\n",
      "train loss for batch 2 is 0.03086579900447842\n",
      "train loss for batch 3 is 0.032814368415455236\n",
      "train loss for batch 4 is 0.028740265453513932\n",
      "train loss for batch 5 is 0.036184480893488094\n",
      "train loss for batch 6 is 0.04839098031313666\n",
      "train loss for batch 7 is 0.050533723323691736\n",
      "--- RMSE = 1.89 ---\n",
      "\n",
      " Epoch 16 of 400\n",
      "train loss for batch 1 is 0.04833957922129557\n",
      "train loss for batch 2 is 0.03635769468734663\n",
      "train loss for batch 3 is 0.03100496575243958\n",
      "train loss for batch 4 is 0.027524568395539014\n",
      "train loss for batch 5 is 0.02754440382832584\n",
      "train loss for batch 6 is 0.031994941170541163\n",
      "train loss for batch 7 is 0.02168143936814937\n",
      "--- RMSE = 1.82 ---\n",
      "\n",
      " Epoch 17 of 400\n",
      "train loss for batch 1 is 0.03056280478242144\n",
      "train loss for batch 2 is 0.020613603311749707\n",
      "train loss for batch 3 is 0.026600309272102197\n",
      "train loss for batch 4 is 0.03179781453767267\n",
      "train loss for batch 5 is 0.02621714665583332\n",
      "train loss for batch 6 is 0.028244668311798237\n",
      "train loss for batch 7 is 0.023722809027209777\n",
      "--- RMSE = 1.71 ---\n",
      "\n",
      " Epoch 18 of 400\n",
      "train loss for batch 1 is 0.01963714493698261\n",
      "train loss for batch 2 is 0.0303641306944895\n",
      "train loss for batch 3 is 0.028752818272874025\n",
      "train loss for batch 4 is 0.014518511959272618\n",
      "train loss for batch 5 is 0.02382352972831927\n",
      "train loss for batch 6 is 0.01665125635118201\n",
      "train loss for batch 7 is 0.03214275034101553\n",
      "--- RMSE = 1.70 ---\n",
      "\n",
      " Epoch 19 of 400\n",
      "train loss for batch 1 is 0.01823661731917722\n",
      "train loss for batch 2 is 0.019340596439492007\n",
      "train loss for batch 3 is 0.025530333090038475\n",
      "train loss for batch 4 is 0.030185727688306158\n",
      "train loss for batch 5 is 0.014977105591891266\n",
      "train loss for batch 6 is 0.023054378171606552\n",
      "train loss for batch 7 is 0.02812888179499392\n",
      "--- RMSE = 1.70 ---\n",
      "\n",
      " Epoch 20 of 400\n",
      "train loss for batch 1 is 0.021989348536203556\n",
      "train loss for batch 2 is 0.025064838754124477\n",
      "train loss for batch 3 is 0.02098493097397836\n",
      "train loss for batch 4 is 0.020743579946390617\n",
      "train loss for batch 5 is 0.024613873623167103\n",
      "train loss for batch 6 is 0.015777565497081494\n",
      "train loss for batch 7 is 0.014346426402460814\n",
      "--- RMSE = 1.66 ---\n",
      "\n",
      " Epoch 21 of 400\n",
      "train loss for batch 1 is 0.015487630184055714\n",
      "train loss for batch 2 is 0.01903151914940814\n",
      "train loss for batch 3 is 0.018884684343338117\n",
      "train loss for batch 4 is 0.018664933466604285\n",
      "train loss for batch 5 is 0.0148042329996043\n",
      "train loss for batch 6 is 0.02759319280502015\n",
      "train loss for batch 7 is 0.028875863377663888\n",
      "--- RMSE = 1.49 ---\n",
      "\n",
      " Epoch 22 of 400\n",
      "train loss for batch 1 is 0.01142243953098522\n",
      "train loss for batch 2 is 0.01939692827082464\n",
      "train loss for batch 3 is 0.019089613502889295\n",
      "train loss for batch 4 is 0.02024507982675041\n",
      "train loss for batch 5 is 0.024769806213327895\n",
      "train loss for batch 6 is 0.01894306271216367\n",
      "train loss for batch 7 is 0.02170525326631257\n",
      "--- RMSE = 1.41 ---\n",
      "\n",
      " Epoch 23 of 400\n",
      "train loss for batch 1 is 0.018158800646697875\n",
      "train loss for batch 2 is 0.020217812335335088\n",
      "train loss for batch 3 is 0.01636842373424963\n",
      "train loss for batch 4 is 0.014149174972531278\n",
      "train loss for batch 5 is 0.020775439255424345\n",
      "train loss for batch 6 is 0.022659892882110642\n",
      "train loss for batch 7 is 0.0165489934566676\n",
      "--- RMSE = 1.47 ---\n",
      "\n",
      " Epoch 24 of 400\n",
      "train loss for batch 1 is 0.02133542039165742\n",
      "train loss for batch 2 is 0.01774141884608565\n",
      "train loss for batch 3 is 0.015913038618568368\n",
      "train loss for batch 4 is 0.021658766010875397\n",
      "train loss for batch 5 is 0.018816582510770914\n",
      "train loss for batch 6 is 0.014625712560227695\n",
      "train loss for batch 7 is 0.01643400722636243\n",
      "--- RMSE = 1.43 ---\n",
      "\n",
      " Epoch 25 of 400\n",
      "train loss for batch 1 is 0.02093172004071409\n",
      "train loss for batch 2 is 0.011695585267195267\n",
      "train loss for batch 3 is 0.017262901994527572\n",
      "train loss for batch 4 is 0.01719074035929903\n",
      "train loss for batch 5 is 0.011871238291091437\n",
      "train loss for batch 6 is 0.010475390200095554\n",
      "train loss for batch 7 is 0.0277268152763234\n",
      "--- RMSE = 1.19 ---\n",
      "\n",
      " Epoch 26 of 400\n",
      "train loss for batch 1 is 0.012705628594708689\n",
      "train loss for batch 2 is 0.0282785948330772\n",
      "train loss for batch 3 is 0.013794867374507991\n",
      "train loss for batch 4 is 0.009211339174080546\n",
      "train loss for batch 5 is 0.020261219934451437\n",
      "train loss for batch 6 is 0.013672062843464082\n",
      "train loss for batch 7 is 0.012653854083223061\n",
      "--- RMSE = 1.81 ---\n",
      "\n",
      " Epoch 27 of 400\n",
      "train loss for batch 1 is 0.028668844584840158\n",
      "train loss for batch 2 is 0.009549651805603087\n",
      "train loss for batch 3 is 0.014655592922191889\n",
      "train loss for batch 4 is 0.011855930996364552\n",
      "train loss for batch 5 is 0.010157137069854619\n",
      "train loss for batch 6 is 0.013169143153310477\n",
      "train loss for batch 7 is 0.01239180083871147\n",
      "--- RMSE = 1.06 ---\n",
      "\n",
      " Epoch 28 of 400\n",
      "train loss for batch 1 is 0.007896368924038819\n",
      "train loss for batch 2 is 0.012711701865050431\n",
      "train loss for batch 3 is 0.010402600251002732\n",
      "train loss for batch 4 is 0.009235808406878262\n",
      "train loss for batch 5 is 0.011298460052530511\n",
      "train loss for batch 6 is 0.011115451285513231\n",
      "train loss for batch 7 is 0.00556088524595429\n",
      "--- RMSE = 1.12 ---\n",
      "\n",
      " Epoch 29 of 400\n",
      "train loss for batch 1 is 0.012440532870189367\n",
      "train loss for batch 2 is 0.011565444341301568\n",
      "train loss for batch 3 is 0.009285925708876541\n",
      "train loss for batch 4 is 0.012354408980533799\n",
      "train loss for batch 5 is 0.013252461756847253\n",
      "train loss for batch 6 is 0.0126033410338697\n",
      "train loss for batch 7 is 0.023003337128050236\n",
      "--- RMSE = 1.12 ---\n",
      "\n",
      " Epoch 30 of 400\n",
      "train loss for batch 1 is 0.007963619363332246\n",
      "train loss for batch 2 is 0.015970586231083397\n",
      "train loss for batch 3 is 0.015366036170383464\n",
      "train loss for batch 4 is 0.010549278525833997\n",
      "train loss for batch 5 is 0.014568618599892953\n",
      "train loss for batch 6 is 0.02709692827594482\n",
      "train loss for batch 7 is 0.007855251519870456\n",
      "--- RMSE = 1.77 ---\n",
      "\n",
      " Epoch 31 of 400\n",
      "train loss for batch 1 is 0.025923274825692285\n",
      "train loss for batch 2 is 0.019138616170096597\n",
      "train loss for batch 3 is 0.0058927052402860634\n",
      "train loss for batch 4 is 0.011762017989525595\n",
      "train loss for batch 5 is 0.022142721412275367\n",
      "train loss for batch 6 is 0.011615076138629216\n",
      "train loss for batch 7 is 0.007789164739018473\n",
      "--- RMSE = 1.38 ---\n",
      "\n",
      " Epoch 32 of 400\n",
      "train loss for batch 1 is 0.013695025676913063\n",
      "train loss for batch 2 is 0.012583354205381316\n",
      "train loss for batch 3 is 0.00963458369937022\n",
      "train loss for batch 4 is 0.008227579104323498\n",
      "train loss for batch 5 is 0.014018788940465914\n",
      "train loss for batch 6 is 0.015067683685468701\n",
      "train loss for batch 7 is 0.007625016376155946\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 33 of 400\n",
      "train loss for batch 1 is 0.009881396011181334\n",
      "train loss for batch 2 is 0.010129932453701888\n",
      "train loss for batch 3 is 0.00856301817377882\n",
      "train loss for batch 4 is 0.011466696891748436\n",
      "train loss for batch 5 is 0.01220469552392739\n",
      "train loss for batch 6 is 0.012664217004389413\n",
      "train loss for batch 7 is 0.013851037248134959\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 34 of 400\n",
      "train loss for batch 1 is 0.009624169199308545\n",
      "train loss for batch 2 is 0.010563032975368331\n",
      "train loss for batch 3 is 0.010366618126413738\n",
      "train loss for batch 4 is 0.006047675492489571\n",
      "train loss for batch 5 is 0.0076555939290394726\n",
      "train loss for batch 6 is 0.006584018071146577\n",
      "train loss for batch 7 is 0.004954441644897139\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 35 of 400\n",
      "train loss for batch 1 is 0.013394089577299942\n",
      "train loss for batch 2 is 0.008672300188541223\n",
      "train loss for batch 3 is 0.007228895187878481\n",
      "train loss for batch 4 is 0.009380106531042747\n",
      "train loss for batch 5 is 0.004963997931959325\n",
      "train loss for batch 6 is 0.01041278441002546\n",
      "train loss for batch 7 is 0.008513653636741636\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 36 of 400\n",
      "train loss for batch 1 is 0.0043469467944387\n",
      "train loss for batch 2 is 0.00901686409504571\n",
      "train loss for batch 3 is 0.007636444290812514\n",
      "train loss for batch 4 is 0.00922833968147229\n",
      "train loss for batch 5 is 0.006816897964630681\n",
      "train loss for batch 6 is 0.011224469613851389\n",
      "train loss for batch 7 is 0.004158347712806901\n",
      "--- RMSE = 1.04 ---\n",
      "\n",
      " Epoch 37 of 400\n",
      "train loss for batch 1 is 0.014079912346043777\n",
      "train loss for batch 2 is 0.00885694841174274\n",
      "train loss for batch 3 is 0.009768412706955033\n",
      "train loss for batch 4 is 0.010099168363224055\n",
      "train loss for batch 5 is 0.006069712687608372\n",
      "train loss for batch 6 is 0.007311407071321103\n",
      "train loss for batch 7 is 0.008472928593157025\n",
      "--- RMSE = 1.08 ---\n",
      "\n",
      " Epoch 38 of 400\n",
      "train loss for batch 1 is 0.010024515369970177\n",
      "train loss for batch 2 is 0.007669650205263575\n",
      "train loss for batch 3 is 0.0063160402773011935\n",
      "train loss for batch 4 is 0.012629363142497203\n",
      "train loss for batch 5 is 0.0050132593919567595\n",
      "train loss for batch 6 is 0.00846908481092127\n",
      "train loss for batch 7 is 0.008048720852193951\n",
      "--- RMSE = 1.13 ---\n",
      "\n",
      " Epoch 39 of 400\n",
      "train loss for batch 1 is 0.012450228820850427\n",
      "train loss for batch 2 is 0.00918408616525606\n",
      "train loss for batch 3 is 0.013326056940700957\n",
      "train loss for batch 4 is 0.013382805752824236\n",
      "train loss for batch 5 is 0.008880892601529826\n",
      "train loss for batch 6 is 0.012971915166757907\n",
      "train loss for batch 7 is 0.011533203968191446\n",
      "--- RMSE = 1.11 ---\n",
      "\n",
      " Epoch 40 of 400\n",
      "train loss for batch 1 is 0.007729444501935539\n",
      "train loss for batch 2 is 0.035367111517138346\n",
      "train loss for batch 3 is 0.018713605088100136\n",
      "train loss for batch 4 is 0.02525262317650501\n",
      "train loss for batch 5 is 0.02504759040068484\n",
      "train loss for batch 6 is 0.013690056890173647\n",
      "train loss for batch 7 is 0.045554156875185416\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 41 of 400\n",
      "train loss for batch 1 is 0.00866582384486562\n",
      "train loss for batch 2 is 0.0382849152101457\n",
      "train loss for batch 3 is 0.017082003227629605\n",
      "train loss for batch 4 is 0.014097459305941059\n",
      "train loss for batch 5 is 0.017446652095524463\n",
      "train loss for batch 6 is 0.016216934491243506\n",
      "train loss for batch 7 is 0.014953568447562252\n",
      "--- RMSE = 1.29 ---\n",
      "\n",
      " Epoch 42 of 400\n",
      "train loss for batch 1 is 0.020864630239341184\n",
      "train loss for batch 2 is 0.02379107496307693\n",
      "train loss for batch 3 is 0.01330288835228419\n",
      "train loss for batch 4 is 0.02075213885918452\n",
      "train loss for batch 5 is 0.021919887291782413\n",
      "train loss for batch 6 is 0.009664467743703574\n",
      "train loss for batch 7 is 0.01905771140080382\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 43 of 400\n",
      "train loss for batch 1 is 0.00810522411409628\n",
      "train loss for batch 2 is 0.02190863426275296\n",
      "train loss for batch 3 is 0.011822723494172465\n",
      "train loss for batch 4 is 0.015244979945710566\n",
      "train loss for batch 5 is 0.015602946869615773\n",
      "train loss for batch 6 is 0.018653157257713292\n",
      "train loss for batch 7 is 0.01694213670176487\n",
      "--- RMSE = 1.15 ---\n",
      "\n",
      " Epoch 44 of 400\n",
      "train loss for batch 1 is 0.009106747155248158\n",
      "train loss for batch 2 is 0.022441937915774056\n",
      "train loss for batch 3 is 0.022549363369847766\n",
      "train loss for batch 4 is 0.0077068369592708805\n",
      "train loss for batch 5 is 0.017164643630015873\n",
      "train loss for batch 6 is 0.015391182804441548\n",
      "train loss for batch 7 is 0.011062225595480587\n",
      "--- RMSE = 1.43 ---\n",
      "\n",
      " Epoch 45 of 400\n",
      "train loss for batch 1 is 0.019228024005413707\n",
      "train loss for batch 2 is 0.013076898524009013\n",
      "train loss for batch 3 is 0.013458077866755836\n",
      "train loss for batch 4 is 0.00679188815572824\n",
      "train loss for batch 5 is 0.011827173606602731\n",
      "train loss for batch 6 is 0.016503359712180966\n",
      "train loss for batch 7 is 0.00874234728135164\n",
      "--- RMSE = 1.37 ---\n",
      "\n",
      " Epoch 46 of 400\n",
      "train loss for batch 1 is 0.020724152030224995\n",
      "train loss for batch 2 is 0.00896340446029538\n",
      "train loss for batch 3 is 0.008244534341516287\n",
      "train loss for batch 4 is 0.018198781024202703\n",
      "train loss for batch 5 is 0.006927181892711038\n",
      "train loss for batch 6 is 0.010752124611478458\n",
      "train loss for batch 7 is 0.010701101269356342\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 47 of 400\n",
      "train loss for batch 1 is 0.010408596633959459\n",
      "train loss for batch 2 is 0.011031617392237512\n",
      "train loss for batch 3 is 0.007668720594782916\n",
      "train loss for batch 4 is 0.011091381022821683\n",
      "train loss for batch 5 is 0.008008742828787474\n",
      "train loss for batch 6 is 0.004500018029456391\n",
      "train loss for batch 7 is 0.01238613818986994\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 48 of 400\n",
      "train loss for batch 1 is 0.011887079470553442\n",
      "train loss for batch 2 is 0.008542092187079325\n",
      "train loss for batch 3 is 0.011591218601234646\n",
      "train loss for batch 4 is 0.012367089480541826\n",
      "train loss for batch 5 is 0.012039348364917869\n",
      "train loss for batch 6 is 0.007398474607576263\n",
      "train loss for batch 7 is 0.010194485490111045\n",
      "--- RMSE = 1.20 ---\n",
      "\n",
      " Epoch 49 of 400\n",
      "train loss for batch 1 is 0.015484257782127414\n",
      "train loss for batch 2 is 0.007227797393546972\n",
      "train loss for batch 3 is 0.011116426040858718\n",
      "train loss for batch 4 is 0.008224756754405232\n",
      "train loss for batch 5 is 0.010450455132325492\n",
      "train loss for batch 6 is 0.015941078891733147\n",
      "train loss for batch 7 is 0.014779481771197986\n",
      "--- RMSE = 1.22 ---\n",
      "\n",
      " Epoch 50 of 400\n",
      "train loss for batch 1 is 0.007036948543306038\n",
      "train loss for batch 2 is 0.03228939727955504\n",
      "train loss for batch 3 is 0.015042629249557264\n",
      "train loss for batch 4 is 0.050292664964728964\n",
      "train loss for batch 5 is 0.022755481181214603\n",
      "train loss for batch 6 is 0.05719912809231164\n",
      "train loss for batch 7 is 0.02955248205765037\n",
      "--- RMSE = 2.17 ---\n",
      "\n",
      " Epoch 51 of 400\n",
      "train loss for batch 1 is 0.06540575651869782\n",
      "train loss for batch 2 is 0.02149588252510539\n",
      "train loss for batch 3 is 0.015111809605513083\n",
      "train loss for batch 4 is 0.04479043441250835\n",
      "train loss for batch 5 is 0.024232712843648842\n",
      "train loss for batch 6 is 0.03342446113014703\n",
      "train loss for batch 7 is 0.06755657558922472\n",
      "--- RMSE = 1.59 ---\n",
      "\n",
      " Epoch 52 of 400\n",
      "train loss for batch 1 is 0.020653150920985727\n",
      "train loss for batch 2 is 0.03953103596343276\n",
      "train loss for batch 3 is 0.04499183019149515\n",
      "train loss for batch 4 is 0.02628944250503318\n",
      "train loss for batch 5 is 0.035738578372141255\n",
      "train loss for batch 6 is 0.07070823075966542\n",
      "train loss for batch 7 is 0.030269975055812196\n",
      "--- RMSE = 2.43 ---\n",
      "\n",
      " Epoch 53 of 400\n",
      "train loss for batch 1 is 0.07229186277070608\n",
      "train loss for batch 2 is 0.06868012437598639\n",
      "train loss for batch 3 is 0.015806742935243583\n",
      "train loss for batch 4 is 0.06627840423140348\n",
      "train loss for batch 5 is 0.06695617624023066\n",
      "train loss for batch 6 is 0.02030668662633124\n",
      "train loss for batch 7 is 0.07036466929573153\n",
      "--- RMSE = 2.09 ---\n",
      "\n",
      " Epoch 54 of 400\n",
      "train loss for batch 1 is 0.04492636404360613\n",
      "train loss for batch 2 is 0.034036296644063165\n",
      "train loss for batch 3 is 0.047950656851640634\n",
      "train loss for batch 4 is 0.06249354581699993\n",
      "train loss for batch 5 is 0.01955276941098191\n",
      "train loss for batch 6 is 0.04349538273265789\n",
      "train loss for batch 7 is 0.0608830532232623\n",
      "--- RMSE = 1.40 ---\n",
      "\n",
      " Epoch 55 of 400\n",
      "train loss for batch 1 is 0.017183517721048626\n",
      "train loss for batch 2 is 0.0366309444320027\n",
      "train loss for batch 3 is 0.04583259735484209\n",
      "train loss for batch 4 is 0.04131701401555141\n",
      "train loss for batch 5 is 0.03533985547601349\n",
      "train loss for batch 6 is 0.11161421612595042\n",
      "train loss for batch 7 is 0.02971141178513667\n",
      "--- RMSE = 1.80 ---\n",
      "\n",
      " Epoch 56 of 400\n",
      "train loss for batch 1 is 0.040228242205782916\n",
      "train loss for batch 2 is 0.08388842752861991\n",
      "train loss for batch 3 is 0.03024098997252083\n",
      "train loss for batch 4 is 0.031750401770839515\n",
      "train loss for batch 5 is 0.04717665584330169\n",
      "train loss for batch 6 is 0.025059467219846344\n",
      "train loss for batch 7 is 0.03691119558421527\n",
      "--- RMSE = 1.81 ---\n",
      "\n",
      " Epoch 57 of 400\n",
      "train loss for batch 1 is 0.026912784538549256\n",
      "train loss for batch 2 is 0.02859773991519007\n",
      "train loss for batch 3 is 0.012199081739464719\n",
      "train loss for batch 4 is 0.03440761101779846\n",
      "train loss for batch 5 is 0.015456353192183456\n",
      "train loss for batch 6 is 0.01801564106287104\n",
      "train loss for batch 7 is 0.016264918047327343\n",
      "--- RMSE = 1.48 ---\n",
      "\n",
      " Epoch 58 of 400\n",
      "train loss for batch 1 is 0.01992716260040316\n",
      "train loss for batch 2 is 0.009794630067645315\n",
      "train loss for batch 3 is 0.018987007423146025\n",
      "train loss for batch 4 is 0.015533354008151957\n",
      "train loss for batch 5 is 0.022523450188736548\n",
      "train loss for batch 6 is 0.018208451236764903\n",
      "train loss for batch 7 is 0.027611458389831013\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 59 of 400\n",
      "train loss for batch 1 is 0.011405844735322796\n",
      "train loss for batch 2 is 0.020661478954814766\n",
      "train loss for batch 3 is 0.024926102186280642\n",
      "train loss for batch 4 is 0.026439757949720376\n",
      "train loss for batch 5 is 0.022807995142283856\n",
      "train loss for batch 6 is 0.01941069026669981\n",
      "train loss for batch 7 is 0.03600218728495915\n",
      "--- RMSE = 1.70 ---\n",
      "\n",
      " Epoch 60 of 400\n",
      "train loss for batch 1 is 0.024870905830688878\n",
      "train loss for batch 2 is 0.032991203589321366\n",
      "train loss for batch 3 is 0.03301938584603412\n",
      "train loss for batch 4 is 0.018873821934411665\n",
      "train loss for batch 5 is 0.03823211376954645\n",
      "train loss for batch 6 is 0.012632064371488925\n",
      "train loss for batch 7 is 0.02333057631000425\n",
      "--- RMSE = 2.00 ---\n",
      "\n",
      " Epoch 61 of 400\n",
      "train loss for batch 1 is 0.03747197798339971\n",
      "train loss for batch 2 is 0.014761074545300961\n",
      "train loss for batch 3 is 0.037432183766180536\n",
      "train loss for batch 4 is 0.015519251116905695\n",
      "train loss for batch 5 is 0.02348516842518551\n",
      "train loss for batch 6 is 0.03854989502762084\n",
      "train loss for batch 7 is 0.018381840864389723\n",
      "--- RMSE = 1.57 ---\n",
      "\n",
      " Epoch 62 of 400\n",
      "train loss for batch 1 is 0.02791803226326464\n",
      "train loss for batch 2 is 0.01905112459046689\n",
      "train loss for batch 3 is 0.0235783449106814\n",
      "train loss for batch 4 is 0.0426579493779863\n",
      "train loss for batch 5 is 0.017126847889298705\n",
      "train loss for batch 6 is 0.04167005827448216\n",
      "train loss for batch 7 is 0.01476617102227008\n",
      "--- RMSE = 1.94 ---\n",
      "\n",
      " Epoch 63 of 400\n",
      "train loss for batch 1 is 0.03131680376321455\n",
      "train loss for batch 2 is 0.02189446606822114\n",
      "train loss for batch 3 is 0.03221808965325502\n",
      "train loss for batch 4 is 0.04512577007420293\n",
      "train loss for batch 5 is 0.012177279391447589\n",
      "train loss for batch 6 is 0.04977360194776104\n",
      "train loss for batch 7 is 0.02129368873059639\n",
      "--- RMSE = 1.84 ---\n",
      "\n",
      " Epoch 64 of 400\n",
      "train loss for batch 1 is 0.04610812880276126\n",
      "train loss for batch 2 is 0.05408434620189187\n",
      "train loss for batch 3 is 0.01457803442960771\n",
      "train loss for batch 4 is 0.04097693398259064\n",
      "train loss for batch 5 is 0.013765577104261907\n",
      "train loss for batch 6 is 0.04119701352280491\n",
      "train loss for batch 7 is 0.038492859522162436\n",
      "--- RMSE = 1.20 ---\n",
      "\n",
      " Epoch 65 of 400\n",
      "train loss for batch 1 is 0.007584060207284123\n",
      "train loss for batch 2 is 0.030510448278107984\n",
      "train loss for batch 3 is 0.01066606615781296\n",
      "train loss for batch 4 is 0.0167195686706209\n",
      "train loss for batch 5 is 0.00882408955323819\n",
      "train loss for batch 6 is 0.011347029739888811\n",
      "train loss for batch 7 is 0.011441028199397225\n",
      "--- RMSE = 1.09 ---\n",
      "\n",
      " Epoch 66 of 400\n",
      "train loss for batch 1 is 0.010205788053174682\n",
      "train loss for batch 2 is 0.014477431177824343\n",
      "train loss for batch 3 is 0.010787020097148667\n",
      "train loss for batch 4 is 0.012262399326873513\n",
      "train loss for batch 5 is 0.015392481911855143\n",
      "train loss for batch 6 is 0.00954465617561001\n",
      "train loss for batch 7 is 0.009638785779604976\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 67 of 400\n",
      "train loss for batch 1 is 0.011271798058853456\n",
      "train loss for batch 2 is 0.006401135457050436\n",
      "train loss for batch 3 is 0.013513673516986999\n",
      "train loss for batch 4 is 0.009952083202248753\n",
      "train loss for batch 5 is 0.01391229536831325\n",
      "train loss for batch 6 is 0.008743346023589903\n",
      "train loss for batch 7 is 0.010107379882970266\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 68 of 400\n",
      "train loss for batch 1 is 0.008272146743861154\n",
      "train loss for batch 2 is 0.014169490621094603\n",
      "train loss for batch 3 is 0.01030499568521949\n",
      "train loss for batch 4 is 0.008776970467889493\n",
      "train loss for batch 5 is 0.008988403451612819\n",
      "train loss for batch 6 is 0.0030384632908567084\n",
      "train loss for batch 7 is 0.006736145841257665\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 69 of 400\n",
      "train loss for batch 1 is 0.0065940229408202765\n",
      "train loss for batch 2 is 0.005784991162106232\n",
      "train loss for batch 3 is 0.0058418902687061534\n",
      "train loss for batch 4 is 0.004316678388661036\n",
      "train loss for batch 5 is 0.004318357581886455\n",
      "train loss for batch 6 is 0.007916814584018897\n",
      "train loss for batch 7 is 0.005948782983921962\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 70 of 400\n",
      "train loss for batch 1 is 0.005170528557498436\n",
      "train loss for batch 2 is 0.007802720824261252\n",
      "train loss for batch 3 is 0.0029569550352918188\n",
      "train loss for batch 4 is 0.004412608114475502\n",
      "train loss for batch 5 is 0.007327992233823152\n",
      "train loss for batch 6 is 0.0044371313546285596\n",
      "train loss for batch 7 is 0.005684803291065634\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 71 of 400\n",
      "train loss for batch 1 is 0.005652880476054787\n",
      "train loss for batch 2 is 0.004713649971571492\n",
      "train loss for batch 3 is 0.006337859855392714\n",
      "train loss for batch 4 is 0.0028698868395531704\n",
      "train loss for batch 5 is 0.004544262691568956\n",
      "train loss for batch 6 is 0.005776964848585618\n",
      "train loss for batch 7 is 0.004004344987745723\n",
      "--- RMSE = 0.73 ---\n",
      "\n",
      " Epoch 72 of 400\n",
      "train loss for batch 1 is 0.0044927274791273985\n",
      "train loss for batch 2 is 0.007017041567749143\n",
      "train loss for batch 3 is 0.0031178383534631302\n",
      "train loss for batch 4 is 0.009655918256712422\n",
      "train loss for batch 5 is 0.0045642363293508705\n",
      "train loss for batch 6 is 0.0029431714982427215\n",
      "train loss for batch 7 is 0.009549668039544141\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 73 of 400\n",
      "train loss for batch 1 is 0.0030130651755741495\n",
      "train loss for batch 2 is 0.008180083323393782\n",
      "train loss for batch 3 is 0.003894434211614648\n",
      "train loss for batch 4 is 0.004340950245454432\n",
      "train loss for batch 5 is 0.0061899960306344825\n",
      "train loss for batch 6 is 0.003936100771856756\n",
      "train loss for batch 7 is 0.006854941395815537\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 74 of 400\n",
      "train loss for batch 1 is 0.005253916622265561\n",
      "train loss for batch 2 is 0.0026277109092529448\n",
      "train loss for batch 3 is 0.007149283266268065\n",
      "train loss for batch 4 is 0.0031257574761478545\n",
      "train loss for batch 5 is 0.006441873805876764\n",
      "train loss for batch 6 is 0.006554763996269907\n",
      "train loss for batch 7 is 0.0018131812964141784\n",
      "--- RMSE = 0.73 ---\n",
      "\n",
      " Epoch 75 of 400\n",
      "train loss for batch 1 is 0.006453336161139081\n",
      "train loss for batch 2 is 0.0075289016708011795\n",
      "train loss for batch 3 is 0.005355921038041706\n",
      "train loss for batch 4 is 0.00468049513458135\n",
      "train loss for batch 5 is 0.006885184358855661\n",
      "train loss for batch 6 is 0.009926114786400837\n",
      "train loss for batch 7 is 0.00896330946282892\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 76 of 400\n",
      "train loss for batch 1 is 0.010116521529776788\n",
      "train loss for batch 2 is 0.015666026940769205\n",
      "train loss for batch 3 is 0.0035010670858666238\n",
      "train loss for batch 4 is 0.00792058703568148\n",
      "train loss for batch 5 is 0.004553549095084247\n",
      "train loss for batch 6 is 0.01002099017878819\n",
      "train loss for batch 7 is 0.005120809051893248\n",
      "--- RMSE = 0.68 ---\n",
      "\n",
      " Epoch 77 of 400\n",
      "train loss for batch 1 is 0.0044624740251112925\n",
      "train loss for batch 2 is 0.005154618597472622\n",
      "train loss for batch 3 is 0.0043452865963861526\n",
      "train loss for batch 4 is 0.005544773874879704\n",
      "train loss for batch 5 is 0.005773008779608949\n",
      "train loss for batch 6 is 0.004937568910196302\n",
      "train loss for batch 7 is 0.004887594160645838\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 78 of 400\n",
      "train loss for batch 1 is 0.0037342063136198084\n",
      "train loss for batch 2 is 0.005444031779643078\n",
      "train loss for batch 3 is 0.005219498935409271\n",
      "train loss for batch 4 is 0.005887832459318427\n",
      "train loss for batch 5 is 0.007452148005759453\n",
      "train loss for batch 6 is 0.0025040979543661194\n",
      "train loss for batch 7 is 0.01187038027597191\n",
      "--- RMSE = 1.05 ---\n",
      "\n",
      " Epoch 79 of 400\n",
      "train loss for batch 1 is 0.010467992330787775\n",
      "train loss for batch 2 is 0.0028677453319848043\n",
      "train loss for batch 3 is 0.010736327708252195\n",
      "train loss for batch 4 is 0.0052505798093429625\n",
      "train loss for batch 5 is 0.0055246838399330015\n",
      "train loss for batch 6 is 0.008819709707026382\n",
      "train loss for batch 7 is 0.0023271159178800006\n",
      "--- RMSE = 0.74 ---\n",
      "\n",
      " Epoch 80 of 400\n",
      "train loss for batch 1 is 0.004578598005070972\n",
      "train loss for batch 2 is 0.0051449428551867725\n",
      "train loss for batch 3 is 0.003326517737165078\n",
      "train loss for batch 4 is 0.003378826595993548\n",
      "train loss for batch 5 is 0.00512362952015253\n",
      "train loss for batch 6 is 0.003235051808296062\n",
      "train loss for batch 7 is 0.0040249498243245495\n",
      "--- RMSE = 0.58 ---\n",
      "\n",
      " Epoch 81 of 400\n",
      "train loss for batch 1 is 0.0037615955105478405\n",
      "train loss for batch 2 is 0.0033102134058037936\n",
      "train loss for batch 3 is 0.003981831988274776\n",
      "train loss for batch 4 is 0.003630278849266429\n",
      "train loss for batch 5 is 0.0028824750532719717\n",
      "train loss for batch 6 is 0.006822086518936043\n",
      "train loss for batch 7 is 0.002218214451056894\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 82 of 400\n",
      "train loss for batch 1 is 0.0035944279640368593\n",
      "train loss for batch 2 is 0.0042468930307860055\n",
      "train loss for batch 3 is 0.004721097402638268\n",
      "train loss for batch 4 is 0.005045530206140339\n",
      "train loss for batch 5 is 0.004021768510977307\n",
      "train loss for batch 6 is 0.005848823057847021\n",
      "train loss for batch 7 is 0.005715373843677949\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 83 of 400\n",
      "train loss for batch 1 is 0.003272259958643622\n",
      "train loss for batch 2 is 0.004374353380931485\n",
      "train loss for batch 3 is 0.0034611899823248147\n",
      "train loss for batch 4 is 0.0032448133082332766\n",
      "train loss for batch 5 is 0.003142779046683996\n",
      "train loss for batch 6 is 0.0030672734453987154\n",
      "train loss for batch 7 is 0.0019363705990877637\n",
      "--- RMSE = 0.48 ---\n",
      "\n",
      " Epoch 84 of 400\n",
      "train loss for batch 1 is 0.00184181796592516\n",
      "train loss for batch 2 is 0.0031253370307723904\n",
      "train loss for batch 3 is 0.004366912177088213\n",
      "train loss for batch 4 is 0.003578406857110663\n",
      "train loss for batch 5 is 0.0031941322317547514\n",
      "train loss for batch 6 is 0.004311518113368976\n",
      "train loss for batch 7 is 0.007139537769795466\n",
      "--- RMSE = 0.76 ---\n",
      "\n",
      " Epoch 85 of 400\n",
      "train loss for batch 1 is 0.008112286341149945\n",
      "train loss for batch 2 is 0.00369204375647537\n",
      "train loss for batch 3 is 0.004165921462018324\n",
      "train loss for batch 4 is 0.0069130249000443275\n",
      "train loss for batch 5 is 0.004758655213878087\n",
      "train loss for batch 6 is 0.009000818590646376\n",
      "train loss for batch 7 is 0.007966405432778503\n",
      "--- RMSE = 0.66 ---\n",
      "\n",
      " Epoch 86 of 400\n",
      "train loss for batch 1 is 0.0042443979124534145\n",
      "train loss for batch 2 is 0.010444241318733448\n",
      "train loss for batch 3 is 0.010573515788255543\n",
      "train loss for batch 4 is 0.00531071229341844\n",
      "train loss for batch 5 is 0.004511315633154329\n",
      "train loss for batch 6 is 0.005990943114973405\n",
      "train loss for batch 7 is 0.00387993714790983\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 87 of 400\n",
      "train loss for batch 1 is 0.009214672733197206\n",
      "train loss for batch 2 is 0.006545219340240798\n",
      "train loss for batch 3 is 0.003136435946272591\n",
      "train loss for batch 4 is 0.00890035853751546\n",
      "train loss for batch 5 is 0.009126033850269968\n",
      "train loss for batch 6 is 0.0037089839920851895\n",
      "train loss for batch 7 is 0.009487103247307272\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 88 of 400\n",
      "train loss for batch 1 is 0.005705418885448604\n",
      "train loss for batch 2 is 0.004631130243441469\n",
      "train loss for batch 3 is 0.0060171338522789445\n",
      "train loss for batch 4 is 0.005027992642367854\n",
      "train loss for batch 5 is 0.00766162497983646\n",
      "train loss for batch 6 is 0.004263135611526883\n",
      "train loss for batch 7 is 0.006832224075759423\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 89 of 400\n",
      "train loss for batch 1 is 0.008577107959555902\n",
      "train loss for batch 2 is 0.008319257166547835\n",
      "train loss for batch 3 is 0.008427183455685572\n",
      "train loss for batch 4 is 0.006438118032160578\n",
      "train loss for batch 5 is 0.006255348454371073\n",
      "train loss for batch 6 is 0.004529447087673361\n",
      "train loss for batch 7 is 0.005211206546166688\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 90 of 400\n",
      "train loss for batch 1 is 0.003572780516700492\n",
      "train loss for batch 2 is 0.002718968668171724\n",
      "train loss for batch 3 is 0.005624018793977372\n",
      "train loss for batch 4 is 0.007084304333029496\n",
      "train loss for batch 5 is 0.002457231455240721\n",
      "train loss for batch 6 is 0.006199300087539673\n",
      "train loss for batch 7 is 0.0065513300748965215\n",
      "--- RMSE = 0.88 ---\n",
      "\n",
      " Epoch 91 of 400\n",
      "train loss for batch 1 is 0.008885569686837047\n",
      "train loss for batch 2 is 0.0069413092971877845\n",
      "train loss for batch 3 is 0.003566869970132297\n",
      "train loss for batch 4 is 0.012738758223382914\n",
      "train loss for batch 5 is 0.005994155797920933\n",
      "train loss for batch 6 is 0.005904236758061422\n",
      "train loss for batch 7 is 0.007125797471116636\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 92 of 400\n",
      "train loss for batch 1 is 0.00920199473143068\n",
      "train loss for batch 2 is 0.004576291825196312\n",
      "train loss for batch 3 is 0.005547382980021198\n",
      "train loss for batch 4 is 0.008333489393740227\n",
      "train loss for batch 5 is 0.005460463309786341\n",
      "train loss for batch 6 is 0.006581922822763029\n",
      "train loss for batch 7 is 0.0033489598634293717\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 93 of 400\n",
      "train loss for batch 1 is 0.005015913593042409\n",
      "train loss for batch 2 is 0.0068812989260664086\n",
      "train loss for batch 3 is 0.003132154262984759\n",
      "train loss for batch 4 is 0.006105882720697325\n",
      "train loss for batch 5 is 0.004696562215170342\n",
      "train loss for batch 6 is 0.004604724248508536\n",
      "train loss for batch 7 is 0.0067400690579104035\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 94 of 400\n",
      "train loss for batch 1 is 0.005083398209206006\n",
      "train loss for batch 2 is 0.004742758284876012\n",
      "train loss for batch 3 is 0.0038813884604915806\n",
      "train loss for batch 4 is 0.005611407303354156\n",
      "train loss for batch 5 is 0.008223727226035387\n",
      "train loss for batch 6 is 0.007373502651351988\n",
      "train loss for batch 7 is 0.004250516956321866\n",
      "--- RMSE = 1.04 ---\n",
      "\n",
      " Epoch 95 of 400\n",
      "train loss for batch 1 is 0.008938432645812766\n",
      "train loss for batch 2 is 0.004227952064634618\n",
      "train loss for batch 3 is 0.004498524480641995\n",
      "train loss for batch 4 is 0.007029394955234547\n",
      "train loss for batch 5 is 0.0036169875363524983\n",
      "train loss for batch 6 is 0.013303083447090098\n",
      "train loss for batch 7 is 0.010478288303551778\n",
      "--- RMSE = 0.91 ---\n",
      "\n",
      " Epoch 96 of 400\n",
      "train loss for batch 1 is 0.006545779888483573\n",
      "train loss for batch 2 is 0.01659369893240155\n",
      "train loss for batch 3 is 0.006502300233645572\n",
      "train loss for batch 4 is 0.012953338716935792\n",
      "train loss for batch 5 is 0.008777489385739738\n",
      "train loss for batch 6 is 0.011808366771359755\n",
      "train loss for batch 7 is 0.004924742738486448\n",
      "--- RMSE = 1.07 ---\n",
      "\n",
      " Epoch 97 of 400\n",
      "train loss for batch 1 is 0.011780794614241646\n",
      "train loss for batch 2 is 0.0063796141133782335\n",
      "train loss for batch 3 is 0.004667360971284523\n",
      "train loss for batch 4 is 0.009117678357058885\n",
      "train loss for batch 5 is 0.0050791697134763905\n",
      "train loss for batch 6 is 0.006626036019081385\n",
      "train loss for batch 7 is 0.0052251566853501645\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 98 of 400\n",
      "train loss for batch 1 is 0.00314001716823154\n",
      "train loss for batch 2 is 0.003330384662241694\n",
      "train loss for batch 3 is 0.0028597673397336132\n",
      "train loss for batch 4 is 0.004437713066823344\n",
      "train loss for batch 5 is 0.007990172422413564\n",
      "train loss for batch 6 is 0.003276954639850136\n",
      "train loss for batch 7 is 0.005339481588114154\n",
      "--- RMSE = 0.83 ---\n",
      "\n",
      " Epoch 99 of 400\n",
      "train loss for batch 1 is 0.0096377454261461\n",
      "train loss for batch 2 is 0.003655095024992332\n",
      "train loss for batch 3 is 0.004938224977187771\n",
      "train loss for batch 4 is 0.00809198280399326\n",
      "train loss for batch 5 is 0.009890047637149632\n",
      "train loss for batch 6 is 0.006643623551056889\n",
      "train loss for batch 7 is 0.012239200715819236\n",
      "--- RMSE = 1.12 ---\n",
      "\n",
      " Epoch 100 of 400\n",
      "train loss for batch 1 is 0.008392053591254181\n",
      "train loss for batch 2 is 0.004395022580676719\n",
      "train loss for batch 3 is 0.013196243915328231\n",
      "train loss for batch 4 is 0.004882981200596981\n",
      "train loss for batch 5 is 0.0041952741516388115\n",
      "train loss for batch 6 is 0.0057978397460462724\n",
      "train loss for batch 7 is 0.003294173358236761\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 101 of 400\n",
      "train loss for batch 1 is 0.003916813620416649\n",
      "train loss for batch 2 is 0.0047582890850300106\n",
      "train loss for batch 3 is 0.003716795143293073\n",
      "train loss for batch 4 is 0.005165224726458132\n",
      "train loss for batch 5 is 0.005191959157004367\n",
      "train loss for batch 6 is 0.006148513307508183\n",
      "train loss for batch 7 is 0.0034619651346027803\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 102 of 400\n",
      "train loss for batch 1 is 0.0028170325439799003\n",
      "train loss for batch 2 is 0.0042336452433425755\n",
      "train loss for batch 3 is 0.002888601789262172\n",
      "train loss for batch 4 is 0.0037622354672624085\n",
      "train loss for batch 5 is 0.006122774486391052\n",
      "train loss for batch 6 is 0.006215232949483043\n",
      "train loss for batch 7 is 0.003325169586886619\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 103 of 400\n",
      "train loss for batch 1 is 0.007150433688244972\n",
      "train loss for batch 2 is 0.004491799967560281\n",
      "train loss for batch 3 is 0.0047899928247022985\n",
      "train loss for batch 4 is 0.005912331876610087\n",
      "train loss for batch 5 is 0.0042717204460679704\n",
      "train loss for batch 6 is 0.008981963609791851\n",
      "train loss for batch 7 is 0.0048729978228891375\n",
      "--- RMSE = 1.02 ---\n",
      "\n",
      " Epoch 104 of 400\n",
      "train loss for batch 1 is 0.007913294984515895\n",
      "train loss for batch 2 is 0.00441960870064647\n",
      "train loss for batch 3 is 0.007020133246740795\n",
      "train loss for batch 4 is 0.010830187769878924\n",
      "train loss for batch 5 is 0.004518845968183038\n",
      "train loss for batch 6 is 0.003994995073799608\n",
      "train loss for batch 7 is 0.011980421804633089\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 105 of 400\n",
      "train loss for batch 1 is 0.005778305892679786\n",
      "train loss for batch 2 is 0.020190546111831277\n",
      "train loss for batch 3 is 0.0046354407687816445\n",
      "train loss for batch 4 is 0.025145036891747095\n",
      "train loss for batch 5 is 0.009533225997683028\n",
      "train loss for batch 6 is 0.016336570963667318\n",
      "train loss for batch 7 is 0.006737499678233686\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 106 of 400\n",
      "train loss for batch 1 is 0.010383075118800022\n",
      "train loss for batch 2 is 0.015423039538724606\n",
      "train loss for batch 3 is 0.011977928319281355\n",
      "train loss for batch 4 is 0.026998241485701117\n",
      "train loss for batch 5 is 0.008419967190814113\n",
      "train loss for batch 6 is 0.03169732718144329\n",
      "train loss for batch 7 is 0.024013637577962904\n",
      "--- RMSE = 2.11 ---\n",
      "\n",
      " Epoch 107 of 400\n",
      "train loss for batch 1 is 0.03983821743753864\n",
      "train loss for batch 2 is 0.012125454571976522\n",
      "train loss for batch 3 is 0.012548531884751767\n",
      "train loss for batch 4 is 0.03951829335599899\n",
      "train loss for batch 5 is 0.011417217683910762\n",
      "train loss for batch 6 is 0.06629289439439796\n",
      "train loss for batch 7 is 0.01413742187769946\n",
      "--- RMSE = 3.40 ---\n",
      "\n",
      " Epoch 108 of 400\n",
      "train loss for batch 1 is 0.12095205698940711\n",
      "train loss for batch 2 is 0.07547628676378478\n",
      "train loss for batch 3 is 0.04216330882604268\n",
      "train loss for batch 4 is 0.08944720425804477\n",
      "train loss for batch 5 is 0.08640583468675025\n",
      "train loss for batch 6 is 0.03863261358382768\n",
      "train loss for batch 7 is 0.11780740007944236\n",
      "--- RMSE = 2.41 ---\n",
      "\n",
      " Epoch 109 of 400\n",
      "train loss for batch 1 is 0.08572997015380622\n",
      "train loss for batch 2 is 0.06489910687227267\n",
      "train loss for batch 3 is 0.05392555910785729\n",
      "train loss for batch 4 is 0.06035940700186701\n",
      "train loss for batch 5 is 0.05807142600989442\n",
      "train loss for batch 6 is 0.06197632092124317\n",
      "train loss for batch 7 is 0.07275004692190135\n",
      "--- RMSE = 3.63 ---\n",
      "\n",
      " Epoch 110 of 400\n",
      "train loss for batch 1 is 0.13273033685684257\n",
      "train loss for batch 2 is 0.06855073117384908\n",
      "train loss for batch 3 is 0.08694998320496429\n",
      "train loss for batch 4 is 0.0722633000120093\n",
      "train loss for batch 5 is 0.06300164323291185\n",
      "train loss for batch 6 is 0.09433039867722239\n",
      "train loss for batch 7 is 0.039389811429176844\n",
      "--- RMSE = 2.71 ---\n",
      "\n",
      " Epoch 111 of 400\n",
      "train loss for batch 1 is 0.09288656556120534\n",
      "train loss for batch 2 is 0.10666246147094137\n",
      "train loss for batch 3 is 0.048562852322115685\n",
      "train loss for batch 4 is 0.07832742307846076\n",
      "train loss for batch 5 is 0.05957221508741615\n",
      "train loss for batch 6 is 0.02740021566641271\n",
      "train loss for batch 7 is 0.042789113913619066\n",
      "--- RMSE = 2.31 ---\n",
      "\n",
      " Epoch 112 of 400\n",
      "train loss for batch 1 is 0.04293724517276307\n",
      "train loss for batch 2 is 0.03759810528911255\n",
      "train loss for batch 3 is 0.023919572201742526\n",
      "train loss for batch 4 is 0.05078739859729623\n",
      "train loss for batch 5 is 0.033310903670530925\n",
      "train loss for batch 6 is 0.02677786355638248\n",
      "train loss for batch 7 is 0.04394507014743424\n",
      "--- RMSE = 1.26 ---\n",
      "\n",
      " Epoch 113 of 400\n",
      "train loss for batch 1 is 0.018931613980261722\n",
      "train loss for batch 2 is 0.045532230232546676\n",
      "train loss for batch 3 is 0.024724009702115697\n",
      "train loss for batch 4 is 0.024568532826998432\n",
      "train loss for batch 5 is 0.031143140712355456\n",
      "train loss for batch 6 is 0.017584574175066373\n",
      "train loss for batch 7 is 0.017618186767453962\n",
      "--- RMSE = 1.82 ---\n",
      "\n",
      " Epoch 114 of 400\n",
      "train loss for batch 1 is 0.03253749770091421\n",
      "train loss for batch 2 is 0.012101923543251784\n",
      "train loss for batch 3 is 0.02327676139938837\n",
      "train loss for batch 4 is 0.03686824624724504\n",
      "train loss for batch 5 is 0.018360094279328828\n",
      "train loss for batch 6 is 0.026208095812048752\n",
      "train loss for batch 7 is 0.025052012502420052\n",
      "--- RMSE = 1.00 ---\n",
      "\n",
      " Epoch 115 of 400\n",
      "train loss for batch 1 is 0.011967076329116812\n",
      "train loss for batch 2 is 0.02369539014310224\n",
      "train loss for batch 3 is 0.038790804450323145\n",
      "train loss for batch 4 is 0.02148047484636105\n",
      "train loss for batch 5 is 0.029674814732827556\n",
      "train loss for batch 6 is 0.03194820996248923\n",
      "train loss for batch 7 is 0.01184854223078389\n",
      "--- RMSE = 2.39 ---\n",
      "\n",
      " Epoch 116 of 400\n",
      "train loss for batch 1 is 0.03949270175268087\n",
      "train loss for batch 2 is 0.03574107673035732\n",
      "train loss for batch 3 is 0.03999897384331648\n",
      "train loss for batch 4 is 0.06781262077630744\n",
      "train loss for batch 5 is 0.051293259931838565\n",
      "train loss for batch 6 is 0.05670600849302269\n",
      "train loss for batch 7 is 0.10949392736791255\n",
      "--- RMSE = 1.50 ---\n",
      "\n",
      " Epoch 117 of 400\n",
      "train loss for batch 1 is 0.02505478515125287\n",
      "train loss for batch 2 is 0.07787398614649647\n",
      "train loss for batch 3 is 0.08790239688788254\n",
      "train loss for batch 4 is 0.021796257320268016\n",
      "train loss for batch 5 is 0.07862020472092601\n",
      "train loss for batch 6 is 0.13045086488846658\n",
      "train loss for batch 7 is 0.021016831681680236\n",
      "--- RMSE = 4.75 ---\n",
      "\n",
      " Epoch 118 of 400\n",
      "train loss for batch 1 is 0.27207946914854125\n",
      "train loss for batch 2 is 0.0798278723448819\n",
      "train loss for batch 3 is 0.14640296601915676\n",
      "train loss for batch 4 is 0.217543850521284\n",
      "train loss for batch 5 is 0.09530034399353689\n",
      "train loss for batch 6 is 0.07965670958056839\n",
      "train loss for batch 7 is 0.17144041369205687\n",
      "--- RMSE = 2.88 ---\n",
      "\n",
      " Epoch 119 of 400\n",
      "train loss for batch 1 is 0.11675614014788543\n",
      "train loss for batch 2 is 0.0371971855688051\n",
      "train loss for batch 3 is 0.12120017196517999\n",
      "train loss for batch 4 is 0.14422781939830526\n",
      "train loss for batch 5 is 0.0742726080677068\n",
      "train loss for batch 6 is 0.07124608603884373\n",
      "train loss for batch 7 is 0.12338901839263416\n",
      "--- RMSE = 2.39 ---\n",
      "\n",
      " Epoch 120 of 400\n",
      "train loss for batch 1 is 0.06588833016719488\n",
      "train loss for batch 2 is 0.046167129135176135\n",
      "train loss for batch 3 is 0.06443666600700093\n",
      "train loss for batch 4 is 0.040549736329887315\n",
      "train loss for batch 5 is 0.04444352313643275\n",
      "train loss for batch 6 is 0.038963211639790093\n",
      "train loss for batch 7 is 0.0413602082199675\n",
      "--- RMSE = 2.55 ---\n",
      "\n",
      " Epoch 121 of 400\n",
      "train loss for batch 1 is 0.06367661892983428\n",
      "train loss for batch 2 is 0.035186803857032024\n",
      "train loss for batch 3 is 0.04096336708784744\n",
      "train loss for batch 4 is 0.048914032581538675\n",
      "train loss for batch 5 is 0.0403061945141024\n",
      "train loss for batch 6 is 0.05071594440897759\n",
      "train loss for batch 7 is 0.031068233441469802\n",
      "--- RMSE = 2.07 ---\n",
      "\n",
      " Epoch 122 of 400\n",
      "train loss for batch 1 is 0.05489633322230488\n",
      "train loss for batch 2 is 0.05436323437806191\n",
      "train loss for batch 3 is 0.0296548636731203\n",
      "train loss for batch 4 is 0.024233453363829763\n",
      "train loss for batch 5 is 0.05776515562319587\n",
      "train loss for batch 6 is 0.056202696177169144\n",
      "train loss for batch 7 is 0.025723005750838898\n",
      "--- RMSE = 1.99 ---\n",
      "\n",
      " Epoch 123 of 400\n",
      "train loss for batch 1 is 0.03729703621197288\n",
      "train loss for batch 2 is 0.07225999668497086\n",
      "train loss for batch 3 is 0.019040642332869374\n",
      "train loss for batch 4 is 0.033053206943875306\n",
      "train loss for batch 5 is 0.04445640327776898\n",
      "train loss for batch 6 is 0.0414316136013509\n",
      "train loss for batch 7 is 0.014356505914878361\n",
      "--- RMSE = 1.68 ---\n",
      "\n",
      " Epoch 124 of 400\n",
      "train loss for batch 1 is 0.02584377278019999\n",
      "train loss for batch 2 is 0.042868528279103195\n",
      "train loss for batch 3 is 0.03531536674477318\n",
      "train loss for batch 4 is 0.018727053075176307\n",
      "train loss for batch 5 is 0.02869036438649026\n",
      "train loss for batch 6 is 0.04128028908175005\n",
      "train loss for batch 7 is 0.03563344188629222\n",
      "--- RMSE = 1.43 ---\n",
      "\n",
      " Epoch 125 of 400\n",
      "train loss for batch 1 is 0.02031344487546549\n",
      "train loss for batch 2 is 0.04316666804819383\n",
      "train loss for batch 3 is 0.02595832512890075\n",
      "train loss for batch 4 is 0.026630415554872947\n",
      "train loss for batch 5 is 0.024103111129046517\n",
      "train loss for batch 6 is 0.02265369748948175\n",
      "train loss for batch 7 is 0.019317011311785506\n",
      "--- RMSE = 1.02 ---\n",
      "\n",
      " Epoch 126 of 400\n",
      "train loss for batch 1 is 0.007868184601631382\n",
      "train loss for batch 2 is 0.018619136343675513\n",
      "train loss for batch 3 is 0.008425168375520932\n",
      "train loss for batch 4 is 0.018148874635973474\n",
      "train loss for batch 5 is 0.010183338446768095\n",
      "train loss for batch 6 is 0.012144108719356377\n",
      "train loss for batch 7 is 0.008167281615682347\n",
      "--- RMSE = 1.19 ---\n",
      "\n",
      " Epoch 127 of 400\n",
      "train loss for batch 1 is 0.012993390818692068\n",
      "train loss for batch 2 is 0.01283776401623336\n",
      "train loss for batch 3 is 0.007762739426085155\n",
      "train loss for batch 4 is 0.011495403482561785\n",
      "train loss for batch 5 is 0.006939929066510786\n",
      "train loss for batch 6 is 0.01579066005709264\n",
      "train loss for batch 7 is 0.007804349507529726\n",
      "--- RMSE = 1.30 ---\n",
      "\n",
      " Epoch 128 of 400\n",
      "train loss for batch 1 is 0.018747485485118873\n",
      "train loss for batch 2 is 0.011471547917361397\n",
      "train loss for batch 3 is 0.012344768894126452\n",
      "train loss for batch 4 is 0.009749094575978709\n",
      "train loss for batch 5 is 0.01563208776876282\n",
      "train loss for batch 6 is 0.007847848291165312\n",
      "train loss for batch 7 is 0.011802685200593237\n",
      "--- RMSE = 0.88 ---\n",
      "\n",
      " Epoch 129 of 400\n",
      "train loss for batch 1 is 0.007319676308030303\n",
      "train loss for batch 2 is 0.01206937865168114\n",
      "train loss for batch 3 is 0.006349235456184725\n",
      "train loss for batch 4 is 0.007321225256029525\n",
      "train loss for batch 5 is 0.004850339912887648\n",
      "train loss for batch 6 is 0.004618556941857568\n",
      "train loss for batch 7 is 0.007553502081874892\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 130 of 400\n",
      "train loss for batch 1 is 0.004206241107499048\n",
      "train loss for batch 2 is 0.0049223764594582475\n",
      "train loss for batch 3 is 0.009087792492857958\n",
      "train loss for batch 4 is 0.005858233386611325\n",
      "train loss for batch 5 is 0.004451503670946471\n",
      "train loss for batch 6 is 0.01000263395375532\n",
      "train loss for batch 7 is 0.005198062984007016\n",
      "--- RMSE = 0.77 ---\n",
      "\n",
      " Epoch 131 of 400\n",
      "train loss for batch 1 is 0.0053757601229446365\n",
      "train loss for batch 2 is 0.0069992066102261856\n",
      "train loss for batch 3 is 0.005444656066033821\n",
      "train loss for batch 4 is 0.0043131215844336435\n",
      "train loss for batch 5 is 0.008305702908632216\n",
      "train loss for batch 6 is 0.00518880800405282\n",
      "train loss for batch 7 is 0.0037925422363059183\n",
      "--- RMSE = 1.08 ---\n",
      "\n",
      " Epoch 132 of 400\n",
      "train loss for batch 1 is 0.006883149428534434\n",
      "train loss for batch 2 is 0.0065088091314889455\n",
      "train loss for batch 3 is 0.005458410092228752\n",
      "train loss for batch 4 is 0.003268777336621469\n",
      "train loss for batch 5 is 0.005238953186935312\n",
      "train loss for batch 6 is 0.0057312427022808725\n",
      "train loss for batch 7 is 0.004007804539208378\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 133 of 400\n",
      "train loss for batch 1 is 0.0064012232328693915\n",
      "train loss for batch 2 is 0.0036469838177816457\n",
      "train loss for batch 3 is 0.0037244313831057887\n",
      "train loss for batch 4 is 0.004384201709104109\n",
      "train loss for batch 5 is 0.006199605592711873\n",
      "train loss for batch 6 is 0.005154186866410239\n",
      "train loss for batch 7 is 0.0036162360718756895\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 134 of 400\n",
      "train loss for batch 1 is 0.004234754219268249\n",
      "train loss for batch 2 is 0.004856573078069931\n",
      "train loss for batch 3 is 0.004092139305181566\n",
      "train loss for batch 4 is 0.0024331944302738617\n",
      "train loss for batch 5 is 0.002731577908375812\n",
      "train loss for batch 6 is 0.003018824274188158\n",
      "train loss for batch 7 is 0.004285938497862223\n",
      "--- RMSE = 0.54 ---\n",
      "\n",
      " Epoch 135 of 400\n",
      "train loss for batch 1 is 0.0025393670168293325\n",
      "train loss for batch 2 is 0.0025681890499738014\n",
      "train loss for batch 3 is 0.003357724367024203\n",
      "train loss for batch 4 is 0.003028629433810768\n",
      "train loss for batch 5 is 0.0034616286315434035\n",
      "train loss for batch 6 is 0.0029894215467546575\n",
      "train loss for batch 7 is 0.0027176008755474866\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 136 of 400\n",
      "train loss for batch 1 is 0.00332209061590834\n",
      "train loss for batch 2 is 0.002828646609197554\n",
      "train loss for batch 3 is 0.0023910796567980014\n",
      "train loss for batch 4 is 0.0022577601007089877\n",
      "train loss for batch 5 is 0.002575283993205533\n",
      "train loss for batch 6 is 0.003638773420903247\n",
      "train loss for batch 7 is 0.003943055548657974\n",
      "--- RMSE = 0.54 ---\n",
      "\n",
      " Epoch 137 of 400\n",
      "train loss for batch 1 is 0.004283217993885848\n",
      "train loss for batch 2 is 0.0029383560366312783\n",
      "train loss for batch 3 is 0.0031510200249629194\n",
      "train loss for batch 4 is 0.0038868919039632506\n",
      "train loss for batch 5 is 0.0031986035904958267\n",
      "train loss for batch 6 is 0.002701775493405753\n",
      "train loss for batch 7 is 0.003648944810084122\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 138 of 400\n",
      "train loss for batch 1 is 0.0032537511384858686\n",
      "train loss for batch 2 is 0.003535693211619019\n",
      "train loss for batch 3 is 0.003149367977922357\n",
      "train loss for batch 4 is 0.00358627673129472\n",
      "train loss for batch 5 is 0.00245028593466388\n",
      "train loss for batch 6 is 0.0016824660330111466\n",
      "train loss for batch 7 is 0.0031436921521522743\n",
      "--- RMSE = 0.58 ---\n",
      "\n",
      " Epoch 139 of 400\n",
      "train loss for batch 1 is 0.0020805556750036367\n",
      "train loss for batch 2 is 0.00359440601133687\n",
      "train loss for batch 3 is 0.001563328147905826\n",
      "train loss for batch 4 is 0.0028206788796267274\n",
      "train loss for batch 5 is 0.003931998555661443\n",
      "train loss for batch 6 is 0.00182686005968871\n",
      "train loss for batch 7 is 0.0032978109966718895\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 140 of 400\n",
      "train loss for batch 1 is 0.0031385163252675933\n",
      "train loss for batch 2 is 0.003383203937767213\n",
      "train loss for batch 3 is 0.00374599758199034\n",
      "train loss for batch 4 is 0.0028437288755837837\n",
      "train loss for batch 5 is 0.0021826158004655365\n",
      "train loss for batch 6 is 0.0024910192424076184\n",
      "train loss for batch 7 is 0.002485134362320007\n",
      "--- RMSE = 0.48 ---\n",
      "\n",
      " Epoch 141 of 400\n",
      "train loss for batch 1 is 0.0033712733706451462\n",
      "train loss for batch 2 is 0.001960460335646738\n",
      "train loss for batch 3 is 0.0023206000635244096\n",
      "train loss for batch 4 is 0.001788772659655061\n",
      "train loss for batch 5 is 0.002034220458932453\n",
      "train loss for batch 6 is 0.0022788465285764775\n",
      "train loss for batch 7 is 0.0016961532696059887\n",
      "--- RMSE = 0.50 ---\n",
      "\n",
      " Epoch 142 of 400\n",
      "train loss for batch 1 is 0.002902580645302084\n",
      "train loss for batch 2 is 0.0017265549898651486\n",
      "train loss for batch 3 is 0.0020016606716689155\n",
      "train loss for batch 4 is 0.0020123624416687157\n",
      "train loss for batch 5 is 0.0021858353049589575\n",
      "train loss for batch 6 is 0.0021811387598192258\n",
      "train loss for batch 7 is 0.002643012128439978\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 143 of 400\n",
      "train loss for batch 1 is 0.0023911734879502196\n",
      "train loss for batch 2 is 0.002916130948454986\n",
      "train loss for batch 3 is 0.004481204275458594\n",
      "train loss for batch 4 is 0.0028575245693484718\n",
      "train loss for batch 5 is 0.006553660846209924\n",
      "train loss for batch 6 is 0.004764702025964591\n",
      "train loss for batch 7 is 0.004950402777482558\n",
      "--- RMSE = 0.89 ---\n",
      "\n",
      " Epoch 144 of 400\n",
      "train loss for batch 1 is 0.008651225743053487\n",
      "train loss for batch 2 is 0.005950122858812361\n",
      "train loss for batch 3 is 0.006946682593708119\n",
      "train loss for batch 4 is 0.009519852767173065\n",
      "train loss for batch 5 is 0.002987899883587429\n",
      "train loss for batch 6 is 0.0053873891591622695\n",
      "train loss for batch 7 is 0.00585186289424986\n",
      "--- RMSE = 0.77 ---\n",
      "\n",
      " Epoch 145 of 400\n",
      "train loss for batch 1 is 0.005442767134442806\n",
      "train loss for batch 2 is 0.00552599653059347\n",
      "train loss for batch 3 is 0.0027561697553095416\n",
      "train loss for batch 4 is 0.005846449827529161\n",
      "train loss for batch 5 is 0.004772283297301025\n",
      "train loss for batch 6 is 0.005149571113298501\n",
      "train loss for batch 7 is 0.003654348792671758\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 146 of 400\n",
      "train loss for batch 1 is 0.0047195339740535225\n",
      "train loss for batch 2 is 0.0026134793384468293\n",
      "train loss for batch 3 is 0.005435362345158996\n",
      "train loss for batch 4 is 0.003170882782563212\n",
      "train loss for batch 5 is 0.004698351115067191\n",
      "train loss for batch 6 is 0.002504685286723091\n",
      "train loss for batch 7 is 0.00505534913391037\n",
      "--- RMSE = 0.58 ---\n",
      "\n",
      " Epoch 147 of 400\n",
      "train loss for batch 1 is 0.0032856443471781242\n",
      "train loss for batch 2 is 0.004301970817744168\n",
      "train loss for batch 3 is 0.004132198255310643\n",
      "train loss for batch 4 is 0.004654046257564771\n",
      "train loss for batch 5 is 0.0036043679756935496\n",
      "train loss for batch 6 is 0.00538278740277067\n",
      "train loss for batch 7 is 0.004651125904946625\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 148 of 400\n",
      "train loss for batch 1 is 0.003218927622081388\n",
      "train loss for batch 2 is 0.00683208116559496\n",
      "train loss for batch 3 is 0.00246337714398564\n",
      "train loss for batch 4 is 0.006117115221791125\n",
      "train loss for batch 5 is 0.005053618475057984\n",
      "train loss for batch 6 is 0.0032255814123930276\n",
      "train loss for batch 7 is 0.003703618821723092\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 149 of 400\n",
      "train loss for batch 1 is 0.007390363482104586\n",
      "train loss for batch 2 is 0.0032698159534317976\n",
      "train loss for batch 3 is 0.003806923638519879\n",
      "train loss for batch 4 is 0.004366872590020452\n",
      "train loss for batch 5 is 0.003946728386527293\n",
      "train loss for batch 6 is 0.003949932096028824\n",
      "train loss for batch 7 is 0.007213124104465442\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 150 of 400\n",
      "train loss for batch 1 is 0.004205120251649354\n",
      "train loss for batch 2 is 0.004582768656430441\n",
      "train loss for batch 3 is 0.003227755588948278\n",
      "train loss for batch 4 is 0.005753668721561276\n",
      "train loss for batch 5 is 0.0026384334713271253\n",
      "train loss for batch 6 is 0.005822433573801868\n",
      "train loss for batch 7 is 0.001973027586265139\n",
      "--- RMSE = 0.73 ---\n",
      "\n",
      " Epoch 151 of 400\n",
      "train loss for batch 1 is 0.006283283271198431\n",
      "train loss for batch 2 is 0.003241003185250125\n",
      "train loss for batch 3 is 0.0061468722154689945\n",
      "train loss for batch 4 is 0.0037776003620899577\n",
      "train loss for batch 5 is 0.005829646602843203\n",
      "train loss for batch 6 is 0.0020219487005238247\n",
      "train loss for batch 7 is 0.00399574543622457\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 152 of 400\n",
      "train loss for batch 1 is 0.0030362375023764215\n",
      "train loss for batch 2 is 0.004651586782242969\n",
      "train loss for batch 3 is 0.0035919571091440113\n",
      "train loss for batch 4 is 0.004706086782990754\n",
      "train loss for batch 5 is 0.002357632674950193\n",
      "train loss for batch 6 is 0.00336729225290755\n",
      "train loss for batch 7 is 0.004125229382474912\n",
      "--- RMSE = 0.54 ---\n",
      "\n",
      " Epoch 153 of 400\n",
      "train loss for batch 1 is 0.0023006064326838594\n",
      "train loss for batch 2 is 0.0038596578524641156\n",
      "train loss for batch 3 is 0.002421735954596702\n",
      "train loss for batch 4 is 0.003909989278567018\n",
      "train loss for batch 5 is 0.0024125179683467815\n",
      "train loss for batch 6 is 0.006636857649509586\n",
      "train loss for batch 7 is 0.006382568589789961\n",
      "--- RMSE = 0.70 ---\n",
      "\n",
      " Epoch 154 of 400\n",
      "train loss for batch 1 is 0.004502382311799753\n",
      "train loss for batch 2 is 0.007273418767458899\n",
      "train loss for batch 3 is 0.0034078271669335867\n",
      "train loss for batch 4 is 0.009507054277633025\n",
      "train loss for batch 5 is 0.0025471090058796176\n",
      "train loss for batch 6 is 0.005785767741176538\n",
      "train loss for batch 7 is 0.003115112727143785\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 155 of 400\n",
      "train loss for batch 1 is 0.0041853572697878845\n",
      "train loss for batch 2 is 0.0033455990591033393\n",
      "train loss for batch 3 is 0.007387804223538823\n",
      "train loss for batch 4 is 0.004322668165712467\n",
      "train loss for batch 5 is 0.0075131248340304\n",
      "train loss for batch 6 is 0.0038683240045104803\n",
      "train loss for batch 7 is 0.005777495286019709\n",
      "--- RMSE = 0.56 ---\n",
      "\n",
      " Epoch 156 of 400\n",
      "train loss for batch 1 is 0.00374641422695317\n",
      "train loss for batch 2 is 0.00647987810060373\n",
      "train loss for batch 3 is 0.002909550160735549\n",
      "train loss for batch 4 is 0.007881877877700068\n",
      "train loss for batch 5 is 0.0027348864732854193\n",
      "train loss for batch 6 is 0.006844599261893387\n",
      "train loss for batch 7 is 0.002523689110513575\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 157 of 400\n",
      "train loss for batch 1 is 0.003942996286051722\n",
      "train loss for batch 2 is 0.006946230870486527\n",
      "train loss for batch 3 is 0.00836170814539188\n",
      "train loss for batch 4 is 0.008073668037966658\n",
      "train loss for batch 5 is 0.024035072193222477\n",
      "train loss for batch 6 is 0.009029183666409526\n",
      "train loss for batch 7 is 0.015995323967668222\n",
      "--- RMSE = 1.57 ---\n",
      "\n",
      " Epoch 158 of 400\n",
      "train loss for batch 1 is 0.023488840078867486\n",
      "train loss for batch 2 is 0.007941772257328518\n",
      "train loss for batch 3 is 0.017907930750748244\n",
      "train loss for batch 4 is 0.0274573666479061\n",
      "train loss for batch 5 is 0.007339371455322008\n",
      "train loss for batch 6 is 0.020395421399421267\n",
      "train loss for batch 7 is 0.016909449545750237\n",
      "--- RMSE = 0.84 ---\n",
      "\n",
      " Epoch 159 of 400\n",
      "train loss for batch 1 is 0.0075233195942177745\n",
      "train loss for batch 2 is 0.018278957527279503\n",
      "train loss for batch 3 is 0.01678555864937479\n",
      "train loss for batch 4 is 0.006388615242977546\n",
      "train loss for batch 5 is 0.023065817880275157\n",
      "train loss for batch 6 is 0.010891676435444024\n",
      "train loss for batch 7 is 0.01163802391560624\n",
      "--- RMSE = 1.59 ---\n",
      "\n",
      " Epoch 160 of 400\n",
      "train loss for batch 1 is 0.023511023284241168\n",
      "train loss for batch 2 is 0.017274120856420917\n",
      "train loss for batch 3 is 0.014646544467286105\n",
      "train loss for batch 4 is 0.028170121309980454\n",
      "train loss for batch 5 is 0.0027284153275536093\n",
      "train loss for batch 6 is 0.021833856030209094\n",
      "train loss for batch 7 is 0.012607267548233873\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 161 of 400\n",
      "train loss for batch 1 is 0.010142052495535354\n",
      "train loss for batch 2 is 0.009883823585425404\n",
      "train loss for batch 3 is 0.022999572467289245\n",
      "train loss for batch 4 is 0.004889310927886807\n",
      "train loss for batch 5 is 0.01766180642287034\n",
      "train loss for batch 6 is 0.007410885297568305\n",
      "train loss for batch 7 is 0.02187496049791308\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 162 of 400\n",
      "train loss for batch 1 is 0.007262186053252557\n",
      "train loss for batch 2 is 0.010011755019092133\n",
      "train loss for batch 3 is 0.004916845270646492\n",
      "train loss for batch 4 is 0.01563603848198352\n",
      "train loss for batch 5 is 0.008825205963259199\n",
      "train loss for batch 6 is 0.007891926258154797\n",
      "train loss for batch 7 is 0.0064850419912301374\n",
      "--- RMSE = 1.07 ---\n",
      "\n",
      " Epoch 163 of 400\n",
      "train loss for batch 1 is 0.012213478496502584\n",
      "train loss for batch 2 is 0.004313556905733847\n",
      "train loss for batch 3 is 0.008780385772078108\n",
      "train loss for batch 4 is 0.0052124234977077\n",
      "train loss for batch 5 is 0.011864570549264255\n",
      "train loss for batch 6 is 0.004629611356741317\n",
      "train loss for batch 7 is 0.009912763720613109\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 164 of 400\n",
      "train loss for batch 1 is 0.004069959175427643\n",
      "train loss for batch 2 is 0.010942826304354374\n",
      "train loss for batch 3 is 0.004875593712970616\n",
      "train loss for batch 4 is 0.0057138348154061516\n",
      "train loss for batch 5 is 0.004036069401488325\n",
      "train loss for batch 6 is 0.006697691496047852\n",
      "train loss for batch 7 is 0.007183391477355268\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 165 of 400\n",
      "train loss for batch 1 is 0.003964171993907373\n",
      "train loss for batch 2 is 0.0015720093849236603\n",
      "train loss for batch 3 is 0.004593355179047783\n",
      "train loss for batch 4 is 0.0036723462644692585\n",
      "train loss for batch 5 is 0.004725306527984614\n",
      "train loss for batch 6 is 0.00316521190751343\n",
      "train loss for batch 7 is 0.004345525970073684\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 166 of 400\n",
      "train loss for batch 1 is 0.0034222122868299374\n",
      "train loss for batch 2 is 0.004832183391276766\n",
      "train loss for batch 3 is 0.0028664798259550282\n",
      "train loss for batch 4 is 0.002728701911924214\n",
      "train loss for batch 5 is 0.003328059260355687\n",
      "train loss for batch 6 is 0.002645977828827413\n",
      "train loss for batch 7 is 0.0028391408324103013\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 167 of 400\n",
      "train loss for batch 1 is 0.0039130572122215036\n",
      "train loss for batch 2 is 0.0033092225019675224\n",
      "train loss for batch 3 is 0.0029447298617044635\n",
      "train loss for batch 4 is 0.002631008472689311\n",
      "train loss for batch 5 is 0.0022919359248510006\n",
      "train loss for batch 6 is 0.0018098097845873047\n",
      "train loss for batch 7 is 0.0022730073156866327\n",
      "--- RMSE = 0.43 ---\n",
      "\n",
      " Epoch 168 of 400\n",
      "train loss for batch 1 is 0.0016312582474998344\n",
      "train loss for batch 2 is 0.0025889905308180998\n",
      "train loss for batch 3 is 0.0022850485975071423\n",
      "train loss for batch 4 is 0.002776702231220866\n",
      "train loss for batch 5 is 0.002016624172017848\n",
      "train loss for batch 6 is 0.0033605012005551156\n",
      "train loss for batch 7 is 0.003392628066238349\n",
      "--- RMSE = 0.55 ---\n",
      "\n",
      " Epoch 169 of 400\n",
      "train loss for batch 1 is 0.0014691061349225642\n",
      "train loss for batch 2 is 0.0027559731064577386\n",
      "train loss for batch 3 is 0.0035260160570538073\n",
      "train loss for batch 4 is 0.002929948288699643\n",
      "train loss for batch 5 is 0.0037482152636394285\n",
      "train loss for batch 6 is 0.002841266165079409\n",
      "train loss for batch 7 is 0.0020492866813914315\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 170 of 400\n",
      "train loss for batch 1 is 0.003622409685063247\n",
      "train loss for batch 2 is 0.003528975819831958\n",
      "train loss for batch 3 is 0.002383956195717342\n",
      "train loss for batch 4 is 0.004509772951611361\n",
      "train loss for batch 5 is 0.00373049391393888\n",
      "train loss for batch 6 is 0.004201722413025366\n",
      "train loss for batch 7 is 0.003979493386799707\n",
      "--- RMSE = 0.53 ---\n",
      "\n",
      " Epoch 171 of 400\n",
      "train loss for batch 1 is 0.0021916795628133963\n",
      "train loss for batch 2 is 0.004797828972029156\n",
      "train loss for batch 3 is 0.0032569075448228746\n",
      "train loss for batch 4 is 0.0043315853125821865\n",
      "train loss for batch 5 is 0.0033133891605622223\n",
      "train loss for batch 6 is 0.005184284366170843\n",
      "train loss for batch 7 is 0.003566588056926267\n",
      "--- RMSE = 0.74 ---\n",
      "\n",
      " Epoch 172 of 400\n",
      "train loss for batch 1 is 0.00700492260711598\n",
      "train loss for batch 2 is 0.0029141493882397745\n",
      "train loss for batch 3 is 0.0053069884913355255\n",
      "train loss for batch 4 is 0.006383999497562619\n",
      "train loss for batch 5 is 0.005863581460948404\n",
      "train loss for batch 6 is 0.006411625166593278\n",
      "train loss for batch 7 is 0.006282747927673344\n",
      "--- RMSE = 0.86 ---\n",
      "\n",
      " Epoch 173 of 400\n",
      "train loss for batch 1 is 0.005908526890398707\n",
      "train loss for batch 2 is 0.0038338050212647933\n",
      "train loss for batch 3 is 0.00614723778325061\n",
      "train loss for batch 4 is 0.00479134045365348\n",
      "train loss for batch 5 is 0.00396810316718081\n",
      "train loss for batch 6 is 0.003962719203926152\n",
      "train loss for batch 7 is 0.005531556785004081\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 174 of 400\n",
      "train loss for batch 1 is 0.002811404540258748\n",
      "train loss for batch 2 is 0.009496747074502408\n",
      "train loss for batch 3 is 0.0027561289342631578\n",
      "train loss for batch 4 is 0.00778433861656775\n",
      "train loss for batch 5 is 0.003927963419820158\n",
      "train loss for batch 6 is 0.008826460017154609\n",
      "train loss for batch 7 is 0.007193555987750838\n",
      "--- RMSE = 0.87 ---\n",
      "\n",
      " Epoch 175 of 400\n",
      "train loss for batch 1 is 0.0075520230712838796\n",
      "train loss for batch 2 is 0.0048973509175608025\n",
      "train loss for batch 3 is 0.005369523185291349\n",
      "train loss for batch 4 is 0.00786459826505972\n",
      "train loss for batch 5 is 0.0023715007714347376\n",
      "train loss for batch 6 is 0.00950452124226855\n",
      "train loss for batch 7 is 0.0032773563929227234\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 176 of 400\n",
      "train loss for batch 1 is 0.0036538498076879727\n",
      "train loss for batch 2 is 0.015438668447431334\n",
      "train loss for batch 3 is 0.004626708522574565\n",
      "train loss for batch 4 is 0.01165744493937213\n",
      "train loss for batch 5 is 0.014841619501809195\n",
      "train loss for batch 6 is 0.004965714819712786\n",
      "train loss for batch 7 is 0.021202637344220385\n",
      "--- RMSE = 1.26 ---\n",
      "\n",
      " Epoch 177 of 400\n",
      "train loss for batch 1 is 0.012252558360572416\n",
      "train loss for batch 2 is 0.01186894761903559\n",
      "train loss for batch 3 is 0.03087419651801736\n",
      "train loss for batch 4 is 0.006307969684626451\n",
      "train loss for batch 5 is 0.02181463724807047\n",
      "train loss for batch 6 is 0.010354223932370812\n",
      "train loss for batch 7 is 0.009723409182931757\n",
      "--- RMSE = 1.05 ---\n",
      "\n",
      " Epoch 178 of 400\n",
      "train loss for batch 1 is 0.011914811643272061\n",
      "train loss for batch 2 is 0.015430517700904464\n",
      "train loss for batch 3 is 0.007139653609834328\n",
      "train loss for batch 4 is 0.017252771411778115\n",
      "train loss for batch 5 is 0.00486208077505732\n",
      "train loss for batch 6 is 0.006915964309312741\n",
      "train loss for batch 7 is 0.009436387371529704\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 179 of 400\n",
      "train loss for batch 1 is 0.007576525688650946\n",
      "train loss for batch 2 is 0.01114242448382551\n",
      "train loss for batch 3 is 0.009024252775042435\n",
      "train loss for batch 4 is 0.006731715019444688\n",
      "train loss for batch 5 is 0.006066266404088324\n",
      "train loss for batch 6 is 0.0076462407693424885\n",
      "train loss for batch 7 is 0.004715046146671843\n",
      "--- RMSE = 0.91 ---\n",
      "\n",
      " Epoch 180 of 400\n",
      "train loss for batch 1 is 0.007640922071536983\n",
      "train loss for batch 2 is 0.009620533763149686\n",
      "train loss for batch 3 is 0.005388136509054359\n",
      "train loss for batch 4 is 0.0051393345086731676\n",
      "train loss for batch 5 is 0.006555622926714557\n",
      "train loss for batch 6 is 0.0032345507133026193\n",
      "train loss for batch 7 is 0.006837119769917594\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 181 of 400\n",
      "train loss for batch 1 is 0.007144273515618975\n",
      "train loss for batch 2 is 0.004085581877317409\n",
      "train loss for batch 3 is 0.006754630905479323\n",
      "train loss for batch 4 is 0.005300577178859302\n",
      "train loss for batch 5 is 0.004040958015181834\n",
      "train loss for batch 6 is 0.005917855528349926\n",
      "train loss for batch 7 is 0.008017915616405165\n",
      "--- RMSE = 0.60 ---\n",
      "\n",
      " Epoch 182 of 400\n",
      "train loss for batch 1 is 0.0033269659490141566\n",
      "train loss for batch 2 is 0.007498989357262904\n",
      "train loss for batch 3 is 0.0033390854738658414\n",
      "train loss for batch 4 is 0.0046056266965353275\n",
      "train loss for batch 5 is 0.01254771696115819\n",
      "train loss for batch 6 is 0.00380556456064106\n",
      "train loss for batch 7 is 0.005288994746263753\n",
      "--- RMSE = 0.96 ---\n",
      "\n",
      " Epoch 183 of 400\n",
      "train loss for batch 1 is 0.013545197388188365\n",
      "train loss for batch 2 is 0.008438116100528487\n",
      "train loss for batch 3 is 0.012194156750437557\n",
      "train loss for batch 4 is 0.014683472858350534\n",
      "train loss for batch 5 is 0.00697760630087729\n",
      "train loss for batch 6 is 0.01763400503268665\n",
      "train loss for batch 7 is 0.008612181418440994\n",
      "--- RMSE = 1.00 ---\n",
      "\n",
      " Epoch 184 of 400\n",
      "train loss for batch 1 is 0.009287655312214992\n",
      "train loss for batch 2 is 0.011797267952277712\n",
      "train loss for batch 3 is 0.008557996470778645\n",
      "train loss for batch 4 is 0.011135302743942296\n",
      "train loss for batch 5 is 0.009751905419788933\n",
      "train loss for batch 6 is 0.00788578619877608\n",
      "train loss for batch 7 is 0.014545452779260918\n",
      "--- RMSE = 1.23 ---\n",
      "\n",
      " Epoch 185 of 400\n",
      "train loss for batch 1 is 0.010269099785944204\n",
      "train loss for batch 2 is 0.017627678402094782\n",
      "train loss for batch 3 is 0.015544323764428875\n",
      "train loss for batch 4 is 0.00691093367996795\n",
      "train loss for batch 5 is 0.010715550183116446\n",
      "train loss for batch 6 is 0.013697764382238766\n",
      "train loss for batch 7 is 0.010198247942757676\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 186 of 400\n",
      "train loss for batch 1 is 0.009048681518523725\n",
      "train loss for batch 2 is 0.007688045740121107\n",
      "train loss for batch 3 is 0.00992842399026051\n",
      "train loss for batch 4 is 0.008352496368018101\n",
      "train loss for batch 5 is 0.00984937462039469\n",
      "train loss for batch 6 is 0.008149516740329755\n",
      "train loss for batch 7 is 0.005876005085366075\n",
      "--- RMSE = 1.19 ---\n",
      "\n",
      " Epoch 187 of 400\n",
      "train loss for batch 1 is 0.014120179173210201\n",
      "train loss for batch 2 is 0.004328422528983439\n",
      "train loss for batch 3 is 0.009839590435997048\n",
      "train loss for batch 4 is 0.009367817522286248\n",
      "train loss for batch 5 is 0.00705183811784234\n",
      "train loss for batch 6 is 0.004494263508480018\n",
      "train loss for batch 7 is 0.012902596509543765\n",
      "--- RMSE = 0.86 ---\n",
      "\n",
      " Epoch 188 of 400\n",
      "train loss for batch 1 is 0.011008598743065106\n",
      "train loss for batch 2 is 0.006925106784570252\n",
      "train loss for batch 3 is 0.008766148510078297\n",
      "train loss for batch 4 is 0.010294483635260596\n",
      "train loss for batch 5 is 0.011436563698089772\n",
      "train loss for batch 6 is 0.005186022608290054\n",
      "train loss for batch 7 is 0.009794748283165481\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 189 of 400\n",
      "train loss for batch 1 is 0.009302475547145951\n",
      "train loss for batch 2 is 0.01134490561422275\n",
      "train loss for batch 3 is 0.007910831828684657\n",
      "train loss for batch 4 is 0.008924062859768798\n",
      "train loss for batch 5 is 0.004443346024142227\n",
      "train loss for batch 6 is 0.012907286083375859\n",
      "train loss for batch 7 is 0.007666932154742214\n",
      "--- RMSE = 1.15 ---\n",
      "\n",
      " Epoch 190 of 400\n",
      "train loss for batch 1 is 0.0114834863556525\n",
      "train loss for batch 2 is 0.006875610388807571\n",
      "train loss for batch 3 is 0.013568262422559653\n",
      "train loss for batch 4 is 0.006297585258887191\n",
      "train loss for batch 5 is 0.014235963306958166\n",
      "train loss for batch 6 is 0.008131557969089629\n",
      "train loss for batch 7 is 0.010577311575657411\n",
      "--- RMSE = 0.85 ---\n",
      "\n",
      " Epoch 191 of 400\n",
      "train loss for batch 1 is 0.006352391214283707\n",
      "train loss for batch 2 is 0.010942885367657052\n",
      "train loss for batch 3 is 0.01279478668473756\n",
      "train loss for batch 4 is 0.008956460608326144\n",
      "train loss for batch 5 is 0.005543047777109896\n",
      "train loss for batch 6 is 0.012947597089972708\n",
      "train loss for batch 7 is 0.00596709989515703\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 192 of 400\n",
      "train loss for batch 1 is 0.006911986493533473\n",
      "train loss for batch 2 is 0.008242460688526639\n",
      "train loss for batch 3 is 0.010964828546854316\n",
      "train loss for batch 4 is 0.005451755289142168\n",
      "train loss for batch 5 is 0.006818867971188136\n",
      "train loss for batch 6 is 0.006292326276152417\n",
      "train loss for batch 7 is 0.008056237365640135\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 193 of 400\n",
      "train loss for batch 1 is 0.006666503505441989\n",
      "train loss for batch 2 is 0.00555451496919149\n",
      "train loss for batch 3 is 0.00764020579095252\n",
      "train loss for batch 4 is 0.008337797696780913\n",
      "train loss for batch 5 is 0.003247786752141959\n",
      "train loss for batch 6 is 0.010329141403104185\n",
      "train loss for batch 7 is 0.003964317619165281\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 194 of 400\n",
      "train loss for batch 1 is 0.010177118668465318\n",
      "train loss for batch 2 is 0.0032218803539331565\n",
      "train loss for batch 3 is 0.013114604225821492\n",
      "train loss for batch 4 is 0.003409448059522625\n",
      "train loss for batch 5 is 0.007097193726857634\n",
      "train loss for batch 6 is 0.0025543133664368976\n",
      "train loss for batch 7 is 0.007412768126685148\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 195 of 400\n",
      "train loss for batch 1 is 0.003333492721563477\n",
      "train loss for batch 2 is 0.006060758551294407\n",
      "train loss for batch 3 is 0.0029569898762435306\n",
      "train loss for batch 4 is 0.003059708119922445\n",
      "train loss for batch 5 is 0.004598338518018327\n",
      "train loss for batch 6 is 0.0047437010018458224\n",
      "train loss for batch 7 is 0.005041312575278198\n",
      "--- RMSE = 0.48 ---\n",
      "\n",
      " Epoch 196 of 400\n",
      "train loss for batch 1 is 0.002026983485039782\n",
      "train loss for batch 2 is 0.003181762677713573\n",
      "train loss for batch 3 is 0.0031877253581654914\n",
      "train loss for batch 4 is 0.0025933205082821816\n",
      "train loss for batch 5 is 0.003518292240045857\n",
      "train loss for batch 6 is 0.006462613481874883\n",
      "train loss for batch 7 is 0.0038098411270287317\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 197 of 400\n",
      "train loss for batch 1 is 0.0056991246470156865\n",
      "train loss for batch 2 is 0.002829110416112146\n",
      "train loss for batch 3 is 0.004158644565047195\n",
      "train loss for batch 4 is 0.0034315742869338306\n",
      "train loss for batch 5 is 0.003114943206045606\n",
      "train loss for batch 6 is 0.003263906735000379\n",
      "train loss for batch 7 is 0.002857320446476728\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 198 of 400\n",
      "train loss for batch 1 is 0.0034626300406731193\n",
      "train loss for batch 2 is 0.005240352606650564\n",
      "train loss for batch 3 is 0.0022218590337033982\n",
      "train loss for batch 4 is 0.004171607452404522\n",
      "train loss for batch 5 is 0.004195331048710525\n",
      "train loss for batch 6 is 0.004035667316615892\n",
      "train loss for batch 7 is 0.0032973159896310567\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 199 of 400\n",
      "train loss for batch 1 is 0.00291521657410951\n",
      "train loss for batch 2 is 0.003490263708731349\n",
      "train loss for batch 3 is 0.0030755503287077947\n",
      "train loss for batch 4 is 0.0021820949611490504\n",
      "train loss for batch 5 is 0.0025691105888581024\n",
      "train loss for batch 6 is 0.0037996218947737802\n",
      "train loss for batch 7 is 0.0022579493290451314\n",
      "--- RMSE = 0.53 ---\n",
      "\n",
      " Epoch 200 of 400\n",
      "train loss for batch 1 is 0.003266308596205206\n",
      "train loss for batch 2 is 0.0028299861120569926\n",
      "train loss for batch 3 is 0.002703038405465111\n",
      "train loss for batch 4 is 0.0033024909584627865\n",
      "train loss for batch 5 is 0.0040549011670408\n",
      "train loss for batch 6 is 0.002801285433156047\n",
      "train loss for batch 7 is 0.0023129023853496\n",
      "--- RMSE = 0.37 ---\n",
      "\n",
      " Epoch 201 of 400\n",
      "train loss for batch 1 is 0.0016447085620875696\n",
      "train loss for batch 2 is 0.001842919556810864\n",
      "train loss for batch 3 is 0.003178245867859527\n",
      "train loss for batch 4 is 0.003371666034259393\n",
      "train loss for batch 5 is 0.002644176435829949\n",
      "train loss for batch 6 is 0.002163970493834452\n",
      "train loss for batch 7 is 0.002306540812149443\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 202 of 400\n",
      "train loss for batch 1 is 0.004292480538465935\n",
      "train loss for batch 2 is 0.003149970969831195\n",
      "train loss for batch 3 is 0.003113630564224467\n",
      "train loss for batch 4 is 0.002132284472726939\n",
      "train loss for batch 5 is 0.006550493032734488\n",
      "train loss for batch 6 is 0.004580484022193621\n",
      "train loss for batch 7 is 0.004169436934720799\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 203 of 400\n",
      "train loss for batch 1 is 0.006684202092703187\n",
      "train loss for batch 2 is 0.00603777417648106\n",
      "train loss for batch 3 is 0.010988908128930133\n",
      "train loss for batch 4 is 0.006990931041658346\n",
      "train loss for batch 5 is 0.008431096559152139\n",
      "train loss for batch 6 is 0.007814967161801783\n",
      "train loss for batch 7 is 0.002753580757064308\n",
      "--- RMSE = 1.02 ---\n",
      "\n",
      " Epoch 204 of 400\n",
      "train loss for batch 1 is 0.008889611710885254\n",
      "train loss for batch 2 is 0.0074083233926809\n",
      "train loss for batch 3 is 0.008564421534636857\n",
      "train loss for batch 4 is 0.012457776666433924\n",
      "train loss for batch 5 is 0.009425659548341975\n",
      "train loss for batch 6 is 0.023742541006479254\n",
      "train loss for batch 7 is 0.023346429971258086\n",
      "--- RMSE = 0.97 ---\n",
      "\n",
      " Epoch 205 of 400\n",
      "train loss for batch 1 is 0.014087804784473325\n",
      "train loss for batch 2 is 0.019419337476775055\n",
      "train loss for batch 3 is 0.009500340941650986\n",
      "train loss for batch 4 is 0.016578666121241228\n",
      "train loss for batch 5 is 0.011043505034418801\n",
      "train loss for batch 6 is 0.011713311055770266\n",
      "train loss for batch 7 is 0.030120912678620698\n",
      "--- RMSE = 0.83 ---\n",
      "\n",
      " Epoch 206 of 400\n",
      "train loss for batch 1 is 0.007625397829749833\n",
      "train loss for batch 2 is 0.013712038600593255\n",
      "train loss for batch 3 is 0.00789101557743702\n",
      "train loss for batch 4 is 0.009476019891227207\n",
      "train loss for batch 5 is 0.011735844459232658\n",
      "train loss for batch 6 is 0.007100135504192964\n",
      "train loss for batch 7 is 0.006251743046268732\n",
      "--- RMSE = 1.42 ---\n",
      "\n",
      " Epoch 207 of 400\n",
      "train loss for batch 1 is 0.017121451962539214\n",
      "train loss for batch 2 is 0.008048483763378314\n",
      "train loss for batch 3 is 0.011093544618272357\n",
      "train loss for batch 4 is 0.00797038955721487\n",
      "train loss for batch 5 is 0.004988199750502231\n",
      "train loss for batch 6 is 0.008951201231757046\n",
      "train loss for batch 7 is 0.005604089942940407\n",
      "--- RMSE = 0.77 ---\n",
      "\n",
      " Epoch 208 of 400\n",
      "train loss for batch 1 is 0.007136475474551714\n",
      "train loss for batch 2 is 0.0050716041797425945\n",
      "train loss for batch 3 is 0.005472517381497465\n",
      "train loss for batch 4 is 0.0027229061131977417\n",
      "train loss for batch 5 is 0.006940633061793556\n",
      "train loss for batch 6 is 0.002234723649066537\n",
      "train loss for batch 7 is 0.007983144927682278\n",
      "--- RMSE = 0.56 ---\n",
      "\n",
      " Epoch 209 of 400\n",
      "train loss for batch 1 is 0.0037402755272995126\n",
      "train loss for batch 2 is 0.007327615154953507\n",
      "train loss for batch 3 is 0.004689667421159302\n",
      "train loss for batch 4 is 0.004048469716537235\n",
      "train loss for batch 5 is 0.007054184022134134\n",
      "train loss for batch 6 is 0.003383628669654406\n",
      "train loss for batch 7 is 0.005111726185077189\n",
      "--- RMSE = 0.74 ---\n",
      "\n",
      " Epoch 210 of 400\n",
      "train loss for batch 1 is 0.004562436442865089\n",
      "train loss for batch 2 is 0.006472416899843664\n",
      "train loss for batch 3 is 0.0044625528650025885\n",
      "train loss for batch 4 is 0.004750819011997231\n",
      "train loss for batch 5 is 0.008130980653584087\n",
      "train loss for batch 6 is 0.00895386796382206\n",
      "train loss for batch 7 is 0.0038615615030251366\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 211 of 400\n",
      "train loss for batch 1 is 0.00681831698919562\n",
      "train loss for batch 2 is 0.0059418936825672095\n",
      "train loss for batch 3 is 0.008944491550632034\n",
      "train loss for batch 4 is 0.005637505074378778\n",
      "train loss for batch 5 is 0.011036141305007675\n",
      "train loss for batch 6 is 0.0036661682876600143\n",
      "train loss for batch 7 is 0.010560415968517416\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 212 of 400\n",
      "train loss for batch 1 is 0.0027520298587166915\n",
      "train loss for batch 2 is 0.00895586937017861\n",
      "train loss for batch 3 is 0.006001212528096731\n",
      "train loss for batch 4 is 0.004391479127231382\n",
      "train loss for batch 5 is 0.003320197075072917\n",
      "train loss for batch 6 is 0.003281332443032884\n",
      "train loss for batch 7 is 0.0024511050608367543\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 213 of 400\n",
      "train loss for batch 1 is 0.0028571126833315374\n",
      "train loss for batch 2 is 0.0043934593325535\n",
      "train loss for batch 3 is 0.004363433921102036\n",
      "train loss for batch 4 is 0.003899730345953968\n",
      "train loss for batch 5 is 0.004369872845845293\n",
      "train loss for batch 6 is 0.0067264467515406445\n",
      "train loss for batch 7 is 0.002910486534753646\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 214 of 400\n",
      "train loss for batch 1 is 0.005097163442071917\n",
      "train loss for batch 2 is 0.004594343928581422\n",
      "train loss for batch 3 is 0.004151326691526532\n",
      "train loss for batch 4 is 0.0036281070536299674\n",
      "train loss for batch 5 is 0.0020620344542224517\n",
      "train loss for batch 6 is 0.0031456360294733793\n",
      "train loss for batch 7 is 0.005048188958972954\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 215 of 400\n",
      "train loss for batch 1 is 0.004216174293049167\n",
      "train loss for batch 2 is 0.003931789759803008\n",
      "train loss for batch 3 is 0.005807739843130842\n",
      "train loss for batch 4 is 0.003955194867853673\n",
      "train loss for batch 5 is 0.004363102830797919\n",
      "train loss for batch 6 is 0.003332194782892795\n",
      "train loss for batch 7 is 0.0032176692371028385\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 216 of 400\n",
      "train loss for batch 1 is 0.00645239454754981\n",
      "train loss for batch 2 is 0.007412533474434887\n",
      "train loss for batch 3 is 0.003287018958147515\n",
      "train loss for batch 4 is 0.004288486091352427\n",
      "train loss for batch 5 is 0.002714228096226272\n",
      "train loss for batch 6 is 0.0038024202558177246\n",
      "train loss for batch 7 is 0.002470329920044482\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 217 of 400\n",
      "train loss for batch 1 is 0.006546084437182436\n",
      "train loss for batch 2 is 0.004297045339482198\n",
      "train loss for batch 3 is 0.0057162135945110345\n",
      "train loss for batch 4 is 0.0062965991772676545\n",
      "train loss for batch 5 is 0.0053519085446351485\n",
      "train loss for batch 6 is 0.007888241283628056\n",
      "train loss for batch 7 is 0.007198782774352998\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 218 of 400\n",
      "train loss for batch 1 is 0.0059241051854362955\n",
      "train loss for batch 2 is 0.006987809614637827\n",
      "train loss for batch 3 is 0.012501605721904467\n",
      "train loss for batch 4 is 0.0032952872624070007\n",
      "train loss for batch 5 is 0.0067238179588464475\n",
      "train loss for batch 6 is 0.007540005161259868\n",
      "train loss for batch 7 is 0.014114135677147142\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 219 of 400\n",
      "train loss for batch 1 is 0.008085988119337958\n",
      "train loss for batch 2 is 0.008756721509140263\n",
      "train loss for batch 3 is 0.014643016760844708\n",
      "train loss for batch 4 is 0.00607396390518004\n",
      "train loss for batch 5 is 0.006637667506413897\n",
      "train loss for batch 6 is 0.013005188591513532\n",
      "train loss for batch 7 is 0.007603934455885067\n",
      "--- RMSE = 0.97 ---\n",
      "\n",
      " Epoch 220 of 400\n",
      "train loss for batch 1 is 0.007056293947956323\n",
      "train loss for batch 2 is 0.009266027416286146\n",
      "train loss for batch 3 is 0.007540369695912723\n",
      "train loss for batch 4 is 0.007780474142181908\n",
      "train loss for batch 5 is 0.004058362474772676\n",
      "train loss for batch 6 is 0.0053873660463323566\n",
      "train loss for batch 7 is 0.01112401776502768\n",
      "--- RMSE = 0.77 ---\n",
      "\n",
      " Epoch 221 of 400\n",
      "train loss for batch 1 is 0.005471756870027741\n",
      "train loss for batch 2 is 0.0046683039234833275\n",
      "train loss for batch 3 is 0.005485237899885743\n",
      "train loss for batch 4 is 0.0089356943375314\n",
      "train loss for batch 5 is 0.00417863134585863\n",
      "train loss for batch 6 is 0.007182729887075256\n",
      "train loss for batch 7 is 0.004257353549165287\n",
      "--- RMSE = 0.96 ---\n",
      "\n",
      " Epoch 222 of 400\n",
      "train loss for batch 1 is 0.0096691971019159\n",
      "train loss for batch 2 is 0.004085038608928953\n",
      "train loss for batch 3 is 0.005298173536658655\n",
      "train loss for batch 4 is 0.008771427813720046\n",
      "train loss for batch 5 is 0.004946051385466413\n",
      "train loss for batch 6 is 0.0068997183117202494\n",
      "train loss for batch 7 is 0.004256763282696766\n",
      "--- RMSE = 0.76 ---\n",
      "\n",
      " Epoch 223 of 400\n",
      "train loss for batch 1 is 0.005004459354408357\n",
      "train loss for batch 2 is 0.008350660488613479\n",
      "train loss for batch 3 is 0.005095262514140796\n",
      "train loss for batch 4 is 0.007796422065285662\n",
      "train loss for batch 5 is 0.007620963551318639\n",
      "train loss for batch 6 is 0.009487897294140997\n",
      "train loss for batch 7 is 0.0034470869763012306\n",
      "--- RMSE = 0.81 ---\n",
      "\n",
      " Epoch 224 of 400\n",
      "train loss for batch 1 is 0.005076961605953267\n",
      "train loss for batch 2 is 0.008468419504429264\n",
      "train loss for batch 3 is 0.004659750991983709\n",
      "train loss for batch 4 is 0.007134375006561915\n",
      "train loss for batch 5 is 0.00800220425100753\n",
      "train loss for batch 6 is 0.009044348436525853\n",
      "train loss for batch 7 is 0.009519091165601538\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 225 of 400\n",
      "train loss for batch 1 is 0.009638073237019372\n",
      "train loss for batch 2 is 0.0068251731049378825\n",
      "train loss for batch 3 is 0.0033287031031894437\n",
      "train loss for batch 4 is 0.004570719653664339\n",
      "train loss for batch 5 is 0.008076973246596461\n",
      "train loss for batch 6 is 0.005116008323373935\n",
      "train loss for batch 7 is 0.009056779433150882\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 226 of 400\n",
      "train loss for batch 1 is 0.003112108785900335\n",
      "train loss for batch 2 is 0.006210909891858599\n",
      "train loss for batch 3 is 0.004207757169651717\n",
      "train loss for batch 4 is 0.004153504163672952\n",
      "train loss for batch 5 is 0.0049817054805118685\n",
      "train loss for batch 6 is 0.004068186826921212\n",
      "train loss for batch 7 is 0.004686141750717126\n",
      "--- RMSE = 0.94 ---\n",
      "\n",
      " Epoch 227 of 400\n",
      "train loss for batch 1 is 0.008167874738360819\n",
      "train loss for batch 2 is 0.005121757634070001\n",
      "train loss for batch 3 is 0.00609215433440776\n",
      "train loss for batch 4 is 0.004123145428892493\n",
      "train loss for batch 5 is 0.005886140966894242\n",
      "train loss for batch 6 is 0.0031172741316732324\n",
      "train loss for batch 7 is 0.007555159113566164\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 228 of 400\n",
      "train loss for batch 1 is 0.006220740891337384\n",
      "train loss for batch 2 is 0.003674724485945692\n",
      "train loss for batch 3 is 0.005115015809889846\n",
      "train loss for batch 4 is 0.0025920757651137025\n",
      "train loss for batch 5 is 0.0024279211314585276\n",
      "train loss for batch 6 is 0.0033548648764703637\n",
      "train loss for batch 7 is 0.002933375680128261\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 229 of 400\n",
      "train loss for batch 1 is 0.004429452173688819\n",
      "train loss for batch 2 is 0.002672004018782753\n",
      "train loss for batch 3 is 0.0020766635203878013\n",
      "train loss for batch 4 is 0.0022255111208553374\n",
      "train loss for batch 5 is 0.002531849156818108\n",
      "train loss for batch 6 is 0.0021592214139633252\n",
      "train loss for batch 7 is 0.0035205516955348257\n",
      "--- RMSE = 0.53 ---\n",
      "\n",
      " Epoch 230 of 400\n",
      "train loss for batch 1 is 0.0025456101813623034\n",
      "train loss for batch 2 is 0.004300470113057592\n",
      "train loss for batch 3 is 0.00288720943832242\n",
      "train loss for batch 4 is 0.003750617555625416\n",
      "train loss for batch 5 is 0.0028735237227089745\n",
      "train loss for batch 6 is 0.003245971333722998\n",
      "train loss for batch 7 is 0.0025911806773389473\n",
      "--- RMSE = 0.45 ---\n",
      "\n",
      " Epoch 231 of 400\n",
      "train loss for batch 1 is 0.0024434804119198104\n",
      "train loss for batch 2 is 0.002730317392369661\n",
      "train loss for batch 3 is 0.003139103454117167\n",
      "train loss for batch 4 is 0.002256105831440347\n",
      "train loss for batch 5 is 0.002310702282547507\n",
      "train loss for batch 6 is 0.0027252519662643005\n",
      "train loss for batch 7 is 0.004964684414094466\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 232 of 400\n",
      "train loss for batch 1 is 0.006154704415008666\n",
      "train loss for batch 2 is 0.003913511925900526\n",
      "train loss for batch 3 is 0.005879042606649148\n",
      "train loss for batch 4 is 0.004530998814980633\n",
      "train loss for batch 5 is 0.004433271526671643\n",
      "train loss for batch 6 is 0.004268468671060437\n",
      "train loss for batch 7 is 0.006679920941811389\n",
      "--- RMSE = 0.49 ---\n",
      "\n",
      " Epoch 233 of 400\n",
      "train loss for batch 1 is 0.0028240644769492386\n",
      "train loss for batch 2 is 0.005356022103503693\n",
      "train loss for batch 3 is 0.00482094559945214\n",
      "train loss for batch 4 is 0.004750464821196715\n",
      "train loss for batch 5 is 0.006011848383050772\n",
      "train loss for batch 6 is 0.005077350564092347\n",
      "train loss for batch 7 is 0.00880722053987748\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 234 of 400\n",
      "train loss for batch 1 is 0.0032196920034077603\n",
      "train loss for batch 2 is 0.005877743064956793\n",
      "train loss for batch 3 is 0.0065469547211805664\n",
      "train loss for batch 4 is 0.008458944266955391\n",
      "train loss for batch 5 is 0.007911346438213352\n",
      "train loss for batch 6 is 0.006287538102969185\n",
      "train loss for batch 7 is 0.010843116871320509\n",
      "--- RMSE = 0.76 ---\n",
      "\n",
      " Epoch 235 of 400\n",
      "train loss for batch 1 is 0.00380979922357204\n",
      "train loss for batch 2 is 0.003946941275865432\n",
      "train loss for batch 3 is 0.010654016112258144\n",
      "train loss for batch 4 is 0.008094348775476332\n",
      "train loss for batch 5 is 0.010009062751038982\n",
      "train loss for batch 6 is 0.012235291152457391\n",
      "train loss for batch 7 is 0.0060326729582207755\n",
      "--- RMSE = 1.47 ---\n",
      "\n",
      " Epoch 236 of 400\n",
      "train loss for batch 1 is 0.017310507060940946\n",
      "train loss for batch 2 is 0.006197186075962035\n",
      "train loss for batch 3 is 0.011012726077123326\n",
      "train loss for batch 4 is 0.004600130867448873\n",
      "train loss for batch 5 is 0.01668149499683692\n",
      "train loss for batch 6 is 0.012329446321653044\n",
      "train loss for batch 7 is 0.008539106301691307\n",
      "--- RMSE = 0.94 ---\n",
      "\n",
      " Epoch 237 of 400\n",
      "train loss for batch 1 is 0.010217274249214552\n",
      "train loss for batch 2 is 0.012686165731599983\n",
      "train loss for batch 3 is 0.01077858511403694\n",
      "train loss for batch 4 is 0.003495119503211699\n",
      "train loss for batch 5 is 0.008947424605808402\n",
      "train loss for batch 6 is 0.006910409358738348\n",
      "train loss for batch 7 is 0.01089894894107584\n",
      "--- RMSE = 0.85 ---\n",
      "\n",
      " Epoch 238 of 400\n",
      "train loss for batch 1 is 0.00827143010212767\n",
      "train loss for batch 2 is 0.0045360173442555605\n",
      "train loss for batch 3 is 0.007728688498236042\n",
      "train loss for batch 4 is 0.008433032133541062\n",
      "train loss for batch 5 is 0.008174798891029044\n",
      "train loss for batch 6 is 0.009873437920854291\n",
      "train loss for batch 7 is 0.009433505022299879\n",
      "--- RMSE = 1.00 ---\n",
      "\n",
      " Epoch 239 of 400\n",
      "train loss for batch 1 is 0.010380968790781115\n",
      "train loss for batch 2 is 0.009079751743906132\n",
      "train loss for batch 3 is 0.006351308154466645\n",
      "train loss for batch 4 is 0.006194669969902802\n",
      "train loss for batch 5 is 0.004584697487982476\n",
      "train loss for batch 6 is 0.0077543692821354475\n",
      "train loss for batch 7 is 0.00695957256468973\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 240 of 400\n",
      "train loss for batch 1 is 0.008000345619247253\n",
      "train loss for batch 2 is 0.01841812919856316\n",
      "train loss for batch 3 is 0.016833495404119016\n",
      "train loss for batch 4 is 0.022342480790336284\n",
      "train loss for batch 5 is 0.03273936583370727\n",
      "train loss for batch 6 is 0.02460683897729695\n",
      "train loss for batch 7 is 0.05559734067777278\n",
      "--- RMSE = 1.36 ---\n",
      "\n",
      " Epoch 241 of 400\n",
      "train loss for batch 1 is 0.020459690085727043\n",
      "train loss for batch 2 is 0.06820269168476542\n",
      "train loss for batch 3 is 0.009005774334870664\n",
      "train loss for batch 4 is 0.04450999127390567\n",
      "train loss for batch 5 is 0.016072107806604827\n",
      "train loss for batch 6 is 0.04493384941867677\n",
      "train loss for batch 7 is 0.03368625954113508\n",
      "--- RMSE = 2.15 ---\n",
      "\n",
      " Epoch 242 of 400\n",
      "train loss for batch 1 is 0.03215806916348419\n",
      "train loss for batch 2 is 0.04578948785626584\n",
      "train loss for batch 3 is 0.03370299963642159\n",
      "train loss for batch 4 is 0.021489793563463783\n",
      "train loss for batch 5 is 0.01634316955185773\n",
      "train loss for batch 6 is 0.022242106875910536\n",
      "train loss for batch 7 is 0.02244928265485042\n",
      "--- RMSE = 1.53 ---\n",
      "\n",
      " Epoch 243 of 400\n",
      "train loss for batch 1 is 0.02014022834619148\n",
      "train loss for batch 2 is 0.00959193228615935\n",
      "train loss for batch 3 is 0.011866097908576591\n",
      "train loss for batch 4 is 0.013818003918246528\n",
      "train loss for batch 5 is 0.013010618958742361\n",
      "train loss for batch 6 is 0.010462375205479556\n",
      "train loss for batch 7 is 0.009124422663904001\n",
      "--- RMSE = 1.09 ---\n",
      "\n",
      " Epoch 244 of 400\n",
      "train loss for batch 1 is 0.009863743881723855\n",
      "train loss for batch 2 is 0.009624250715404946\n",
      "train loss for batch 3 is 0.005770645141978944\n",
      "train loss for batch 4 is 0.007486987132495595\n",
      "train loss for batch 5 is 0.007295367261234686\n",
      "train loss for batch 6 is 0.006162207612587262\n",
      "train loss for batch 7 is 0.005551290671387119\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 245 of 400\n",
      "train loss for batch 1 is 0.006784146482185866\n",
      "train loss for batch 2 is 0.008357154598019159\n",
      "train loss for batch 3 is 0.00926697572829867\n",
      "train loss for batch 4 is 0.0041063725540550445\n",
      "train loss for batch 5 is 0.003486985817576552\n",
      "train loss for batch 6 is 0.005237513073400593\n",
      "train loss for batch 7 is 0.005811271335279867\n",
      "--- RMSE = 0.81 ---\n",
      "\n",
      " Epoch 246 of 400\n",
      "train loss for batch 1 is 0.005520805655657022\n",
      "train loss for batch 2 is 0.003266897137525999\n",
      "train loss for batch 3 is 0.0055461700127733835\n",
      "train loss for batch 4 is 0.004489192513075213\n",
      "train loss for batch 5 is 0.0053711453794473875\n",
      "train loss for batch 6 is 0.006223573702939605\n",
      "train loss for batch 7 is 0.0035045816206749833\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 247 of 400\n",
      "train loss for batch 1 is 0.005857599323430618\n",
      "train loss for batch 2 is 0.0036275515766711836\n",
      "train loss for batch 3 is 0.00518100453681653\n",
      "train loss for batch 4 is 0.0033975491847779444\n",
      "train loss for batch 5 is 0.003871814234200269\n",
      "train loss for batch 6 is 0.00425593020420021\n",
      "train loss for batch 7 is 0.002656568973554657\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 248 of 400\n",
      "train loss for batch 1 is 0.00362357524001849\n",
      "train loss for batch 2 is 0.0041651453359921944\n",
      "train loss for batch 3 is 0.0045887929317313956\n",
      "train loss for batch 4 is 0.00638361215508833\n",
      "train loss for batch 5 is 0.007969273987386071\n",
      "train loss for batch 6 is 0.006400978083307204\n",
      "train loss for batch 7 is 0.006007167885893048\n",
      "--- RMSE = 0.70 ---\n",
      "\n",
      " Epoch 249 of 400\n",
      "train loss for batch 1 is 0.0036747181194536423\n",
      "train loss for batch 2 is 0.0075144241895007255\n",
      "train loss for batch 3 is 0.0029733835147103165\n",
      "train loss for batch 4 is 0.00812102178265812\n",
      "train loss for batch 5 is 0.004985171464830448\n",
      "train loss for batch 6 is 0.0037819818370858527\n",
      "train loss for batch 7 is 0.004333820797831664\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 250 of 400\n",
      "train loss for batch 1 is 0.004139955794961822\n",
      "train loss for batch 2 is 0.0026854455410597077\n",
      "train loss for batch 3 is 0.005664217136897223\n",
      "train loss for batch 4 is 0.004687131705595971\n",
      "train loss for batch 5 is 0.006955380829739506\n",
      "train loss for batch 6 is 0.008225610612677712\n",
      "train loss for batch 7 is 0.0034046187277227366\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 251 of 400\n",
      "train loss for batch 1 is 0.012527016560264104\n",
      "train loss for batch 2 is 0.007989179796739935\n",
      "train loss for batch 3 is 0.008172213600654666\n",
      "train loss for batch 4 is 0.011184500638895754\n",
      "train loss for batch 5 is 0.0041284966687298395\n",
      "train loss for batch 6 is 0.01065288789294112\n",
      "train loss for batch 7 is 0.004898346212509972\n",
      "--- RMSE = 1.05 ---\n",
      "\n",
      " Epoch 252 of 400\n",
      "train loss for batch 1 is 0.007264601504519079\n",
      "train loss for batch 2 is 0.008959454419987876\n",
      "train loss for batch 3 is 0.007992733387973482\n",
      "train loss for batch 4 is 0.007126495246177707\n",
      "train loss for batch 5 is 0.009672354190011915\n",
      "train loss for batch 6 is 0.004169468954074834\n",
      "train loss for batch 7 is 0.00610975133553739\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 253 of 400\n",
      "train loss for batch 1 is 0.00284489650981439\n",
      "train loss for batch 2 is 0.0036595952047662776\n",
      "train loss for batch 3 is 0.01042942737839796\n",
      "train loss for batch 4 is 0.00867433465162314\n",
      "train loss for batch 5 is 0.007951809882823603\n",
      "train loss for batch 6 is 0.01486250823639371\n",
      "train loss for batch 7 is 0.0054298579759241026\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 254 of 400\n",
      "train loss for batch 1 is 0.0119151180993865\n",
      "train loss for batch 2 is 0.004285439462417228\n",
      "train loss for batch 3 is 0.016998573536452417\n",
      "train loss for batch 4 is 0.003915814606465181\n",
      "train loss for batch 5 is 0.013846678654381377\n",
      "train loss for batch 6 is 0.003252447249184781\n",
      "train loss for batch 7 is 0.008349392556736269\n",
      "--- RMSE = 0.49 ---\n",
      "\n",
      " Epoch 255 of 400\n",
      "train loss for batch 1 is 0.00297895719458906\n",
      "train loss for batch 2 is 0.009014125987108025\n",
      "train loss for batch 3 is 0.011203712338229577\n",
      "train loss for batch 4 is 0.014740058532461124\n",
      "train loss for batch 5 is 0.006709720198017751\n",
      "train loss for batch 6 is 0.028830240899909397\n",
      "train loss for batch 7 is 0.019249578182101837\n",
      "--- RMSE = 1.17 ---\n",
      "\n",
      " Epoch 256 of 400\n",
      "train loss for batch 1 is 0.017263855537823766\n",
      "train loss for batch 2 is 0.04560455504876457\n",
      "train loss for batch 3 is 0.00853523325412619\n",
      "train loss for batch 4 is 0.026719352669818827\n",
      "train loss for batch 5 is 0.010987976890429005\n",
      "train loss for batch 6 is 0.02666743153039456\n",
      "train loss for batch 7 is 0.008807985834164023\n",
      "--- RMSE = 1.77 ---\n",
      "\n",
      " Epoch 257 of 400\n",
      "train loss for batch 1 is 0.026754875185846308\n",
      "train loss for batch 2 is 0.01689211485666853\n",
      "train loss for batch 3 is 0.02109048852667905\n",
      "train loss for batch 4 is 0.008395330198983782\n",
      "train loss for batch 5 is 0.022622624110614095\n",
      "train loss for batch 6 is 0.009590122019995833\n",
      "train loss for batch 7 is 0.030153592517132437\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 258 of 400\n",
      "train loss for batch 1 is 0.0056644560047768786\n",
      "train loss for batch 2 is 0.02506502236029621\n",
      "train loss for batch 3 is 0.011894270292952536\n",
      "train loss for batch 4 is 0.042113204977629645\n",
      "train loss for batch 5 is 0.004732611081925979\n",
      "train loss for batch 6 is 0.022277348907592778\n",
      "train loss for batch 7 is 0.013659090142496721\n",
      "--- RMSE = 1.19 ---\n",
      "\n",
      " Epoch 259 of 400\n",
      "train loss for batch 1 is 0.016768684089068762\n",
      "train loss for batch 2 is 0.01877796000332332\n",
      "train loss for batch 3 is 0.014472778732124696\n",
      "train loss for batch 4 is 0.009492694906692021\n",
      "train loss for batch 5 is 0.02059852772171049\n",
      "train loss for batch 6 is 0.0035792246546850555\n",
      "train loss for batch 7 is 0.019648932517796217\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 260 of 400\n",
      "train loss for batch 1 is 0.004929936931304224\n",
      "train loss for batch 2 is 0.012852596362207678\n",
      "train loss for batch 3 is 0.009809184082789343\n",
      "train loss for batch 4 is 0.005751141112008936\n",
      "train loss for batch 5 is 0.00694504259937791\n",
      "train loss for batch 6 is 0.00801278591682246\n",
      "train loss for batch 7 is 0.006647308433603494\n",
      "--- RMSE = 0.94 ---\n",
      "\n",
      " Epoch 261 of 400\n",
      "train loss for batch 1 is 0.005247689892970064\n",
      "train loss for batch 2 is 0.0055490515982930845\n",
      "train loss for batch 3 is 0.00571125486275631\n",
      "train loss for batch 4 is 0.007386417737268278\n",
      "train loss for batch 5 is 0.005634285362167131\n",
      "train loss for batch 6 is 0.006010824193899025\n",
      "train loss for batch 7 is 0.0065796836161297655\n",
      "--- RMSE = 0.84 ---\n",
      "\n",
      " Epoch 262 of 400\n",
      "train loss for batch 1 is 0.0048813313402704716\n",
      "train loss for batch 2 is 0.010704480634725198\n",
      "train loss for batch 3 is 0.0033644227076009997\n",
      "train loss for batch 4 is 0.006801068954969427\n",
      "train loss for batch 5 is 0.006104431081480798\n",
      "train loss for batch 6 is 0.003473052922153046\n",
      "train loss for batch 7 is 0.008698809424899817\n",
      "--- RMSE = 0.85 ---\n",
      "\n",
      " Epoch 263 of 400\n",
      "train loss for batch 1 is 0.005241775002457657\n",
      "train loss for batch 2 is 0.00479276047987868\n",
      "train loss for batch 3 is 0.008495174746528375\n",
      "train loss for batch 4 is 0.005968555602178483\n",
      "train loss for batch 5 is 0.004652014436945448\n",
      "train loss for batch 6 is 0.005289113288361062\n",
      "train loss for batch 7 is 0.007607758196580187\n",
      "--- RMSE = 0.86 ---\n",
      "\n",
      " Epoch 264 of 400\n",
      "train loss for batch 1 is 0.006437933779824564\n",
      "train loss for batch 2 is 0.014925918137914102\n",
      "train loss for batch 3 is 0.014096931448920547\n",
      "train loss for batch 4 is 0.020587948684405097\n",
      "train loss for batch 5 is 0.036795839324232416\n",
      "train loss for batch 6 is 0.005582517807987083\n",
      "train loss for batch 7 is 0.028622394129433234\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 265 of 400\n",
      "train loss for batch 1 is 0.015031582424802781\n",
      "train loss for batch 2 is 0.020840023586421536\n",
      "train loss for batch 3 is 0.012027970046601073\n",
      "train loss for batch 4 is 0.012098864026096777\n",
      "train loss for batch 5 is 0.0061955698308665385\n",
      "train loss for batch 6 is 0.017450500431014676\n",
      "train loss for batch 7 is 0.010674902723525715\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 266 of 400\n",
      "train loss for batch 1 is 0.01132194802396364\n",
      "train loss for batch 2 is 0.0072848613478165535\n",
      "train loss for batch 3 is 0.013357945832685294\n",
      "train loss for batch 4 is 0.005484790446861354\n",
      "train loss for batch 5 is 0.009241276736286571\n",
      "train loss for batch 6 is 0.004342267127170422\n",
      "train loss for batch 7 is 0.00784215542102766\n",
      "--- RMSE = 0.94 ---\n",
      "\n",
      " Epoch 267 of 400\n",
      "train loss for batch 1 is 0.005674626964795942\n",
      "train loss for batch 2 is 0.005245572189380528\n",
      "train loss for batch 3 is 0.005249693463905205\n",
      "train loss for batch 4 is 0.006951463007362735\n",
      "train loss for batch 5 is 0.00429744762559124\n",
      "train loss for batch 6 is 0.009715677997186154\n",
      "train loss for batch 7 is 0.009370056907004532\n",
      "--- RMSE = 0.79 ---\n",
      "\n",
      " Epoch 268 of 400\n",
      "train loss for batch 1 is 0.006228209582259523\n",
      "train loss for batch 2 is 0.010194582645554118\n",
      "train loss for batch 3 is 0.0023485652786520332\n",
      "train loss for batch 4 is 0.0076594219569759954\n",
      "train loss for batch 5 is 0.004208766567693603\n",
      "train loss for batch 6 is 0.0050677760176780105\n",
      "train loss for batch 7 is 0.00577483372536143\n",
      "--- RMSE = 0.73 ---\n",
      "\n",
      " Epoch 269 of 400\n",
      "train loss for batch 1 is 0.005215860000738603\n",
      "train loss for batch 2 is 0.003324231471144432\n",
      "train loss for batch 3 is 0.006888744440957337\n",
      "train loss for batch 4 is 0.009283599904221925\n",
      "train loss for batch 5 is 0.006865806179745221\n",
      "train loss for batch 6 is 0.007774238582769671\n",
      "train loss for batch 7 is 0.003474692357415126\n",
      "--- RMSE = 1.06 ---\n",
      "\n",
      " Epoch 270 of 400\n",
      "train loss for batch 1 is 0.00800166818043629\n",
      "train loss for batch 2 is 0.007405416056213576\n",
      "train loss for batch 3 is 0.00865868486625413\n",
      "train loss for batch 4 is 0.007733550727676636\n",
      "train loss for batch 5 is 0.0041122683705410645\n",
      "train loss for batch 6 is 0.005585124618172844\n",
      "train loss for batch 7 is 0.005618782620070482\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 271 of 400\n",
      "train loss for batch 1 is 0.004680305272990531\n",
      "train loss for batch 2 is 0.004356874235597575\n",
      "train loss for batch 3 is 0.003627977510090002\n",
      "train loss for batch 4 is 0.003222001549688795\n",
      "train loss for batch 5 is 0.0033584105992085765\n",
      "train loss for batch 6 is 0.00284838268353694\n",
      "train loss for batch 7 is 0.006008380945160586\n",
      "--- RMSE = 0.53 ---\n",
      "\n",
      " Epoch 272 of 400\n",
      "train loss for batch 1 is 0.0034971408178422303\n",
      "train loss for batch 2 is 0.005009795630299486\n",
      "train loss for batch 3 is 0.007164385172956394\n",
      "train loss for batch 4 is 0.0038751413138157676\n",
      "train loss for batch 5 is 0.007546415187507808\n",
      "train loss for batch 6 is 0.005476461596984728\n",
      "train loss for batch 7 is 0.0038586209939361015\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 273 of 400\n",
      "train loss for batch 1 is 0.004249945203122766\n",
      "train loss for batch 2 is 0.006611322997004098\n",
      "train loss for batch 3 is 0.00485670623490916\n",
      "train loss for batch 4 is 0.005284226832947035\n",
      "train loss for batch 5 is 0.00793910639670405\n",
      "train loss for batch 6 is 0.003738774593827332\n",
      "train loss for batch 7 is 0.003415673030286954\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 274 of 400\n",
      "train loss for batch 1 is 0.0048799217106818915\n",
      "train loss for batch 2 is 0.0036555352465531044\n",
      "train loss for batch 3 is 0.006273191843617446\n",
      "train loss for batch 4 is 0.004010255367385401\n",
      "train loss for batch 5 is 0.005368033719991707\n",
      "train loss for batch 6 is 0.0039764407875642095\n",
      "train loss for batch 7 is 0.005078941314995396\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 275 of 400\n",
      "train loss for batch 1 is 0.0035662149033035993\n",
      "train loss for batch 2 is 0.003221219187716669\n",
      "train loss for batch 3 is 0.004592747852166471\n",
      "train loss for batch 4 is 0.0029547688389600325\n",
      "train loss for batch 5 is 0.002543849272064733\n",
      "train loss for batch 6 is 0.0017205670201264752\n",
      "train loss for batch 7 is 0.004067697775528389\n",
      "--- RMSE = 0.52 ---\n",
      "\n",
      " Epoch 276 of 400\n",
      "train loss for batch 1 is 0.003078821023900406\n",
      "train loss for batch 2 is 0.0026687962384530734\n",
      "train loss for batch 3 is 0.0021173720456277518\n",
      "train loss for batch 4 is 0.002676016944646816\n",
      "train loss for batch 5 is 0.0018248906651222767\n",
      "train loss for batch 6 is 0.0051256933957698125\n",
      "train loss for batch 7 is 0.0023386426770398536\n",
      "--- RMSE = 0.57 ---\n",
      "\n",
      " Epoch 277 of 400\n",
      "train loss for batch 1 is 0.0033358470909975557\n",
      "train loss for batch 2 is 0.0051844686985484945\n",
      "train loss for batch 3 is 0.00300295729220344\n",
      "train loss for batch 4 is 0.003856822246950758\n",
      "train loss for batch 5 is 0.004750503552232072\n",
      "train loss for batch 6 is 0.0020710261146196372\n",
      "train loss for batch 7 is 0.0029667749548837347\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 278 of 400\n",
      "train loss for batch 1 is 0.005128871484622315\n",
      "train loss for batch 2 is 0.0042883522717414874\n",
      "train loss for batch 3 is 0.004430786049554109\n",
      "train loss for batch 4 is 0.004076576012379066\n",
      "train loss for batch 5 is 0.0043417296354514275\n",
      "train loss for batch 6 is 0.006065456963946903\n",
      "train loss for batch 7 is 0.005108659549331074\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 279 of 400\n",
      "train loss for batch 1 is 0.004481301330085665\n",
      "train loss for batch 2 is 0.008996172183358832\n",
      "train loss for batch 3 is 0.00490153546397124\n",
      "train loss for batch 4 is 0.008015564820338418\n",
      "train loss for batch 5 is 0.0032217807128964004\n",
      "train loss for batch 6 is 0.009117626282879748\n",
      "train loss for batch 7 is 0.005839738380372946\n",
      "--- RMSE = 0.70 ---\n",
      "\n",
      " Epoch 280 of 400\n",
      "train loss for batch 1 is 0.00495942077992053\n",
      "train loss for batch 2 is 0.005494561414236241\n",
      "train loss for batch 3 is 0.008335646262191899\n",
      "train loss for batch 4 is 0.0024694462807092325\n",
      "train loss for batch 5 is 0.004365391746267569\n",
      "train loss for batch 6 is 0.008861673926315128\n",
      "train loss for batch 7 is 0.01134848331385713\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 281 of 400\n",
      "train loss for batch 1 is 0.003502332142896657\n",
      "train loss for batch 2 is 0.009619974254620965\n",
      "train loss for batch 3 is 0.006096434864181976\n",
      "train loss for batch 4 is 0.01091957972280146\n",
      "train loss for batch 5 is 0.006367638430730993\n",
      "train loss for batch 6 is 0.012904586144958203\n",
      "train loss for batch 7 is 0.004450715194257539\n",
      "--- RMSE = 1.17 ---\n",
      "\n",
      " Epoch 282 of 400\n",
      "train loss for batch 1 is 0.01098694800866443\n",
      "train loss for batch 2 is 0.006513319823203507\n",
      "train loss for batch 3 is 0.0056419816206745846\n",
      "train loss for batch 4 is 0.00400874923334695\n",
      "train loss for batch 5 is 0.007610308919692569\n",
      "train loss for batch 6 is 0.004999285475401711\n",
      "train loss for batch 7 is 0.005059603474807798\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 283 of 400\n",
      "train loss for batch 1 is 0.005393053409033215\n",
      "train loss for batch 2 is 0.0027757193603445613\n",
      "train loss for batch 3 is 0.010804214590636785\n",
      "train loss for batch 4 is 0.0051398545251945035\n",
      "train loss for batch 5 is 0.009656748651250163\n",
      "train loss for batch 6 is 0.008567858233026122\n",
      "train loss for batch 7 is 0.011505903610625248\n",
      "--- RMSE = 1.14 ---\n",
      "\n",
      " Epoch 284 of 400\n",
      "train loss for batch 1 is 0.01159943633296577\n",
      "train loss for batch 2 is 0.010158496624239224\n",
      "train loss for batch 3 is 0.015595259707520579\n",
      "train loss for batch 4 is 0.010342093362603959\n",
      "train loss for batch 5 is 0.018956044537350315\n",
      "train loss for batch 6 is 0.00798235654488375\n",
      "train loss for batch 7 is 0.013979908408019246\n",
      "--- RMSE = 1.08 ---\n",
      "\n",
      " Epoch 285 of 400\n",
      "train loss for batch 1 is 0.01005485193273426\n",
      "train loss for batch 2 is 0.011921522582768209\n",
      "train loss for batch 3 is 0.008578897418525104\n",
      "train loss for batch 4 is 0.00900895650070793\n",
      "train loss for batch 5 is 0.013157693091035663\n",
      "train loss for batch 6 is 0.008744640964462504\n",
      "train loss for batch 7 is 0.013050252434390813\n",
      "--- RMSE = 1.56 ---\n",
      "\n",
      " Epoch 286 of 400\n",
      "train loss for batch 1 is 0.028083498789940018\n",
      "train loss for batch 2 is 0.003757289522881454\n",
      "train loss for batch 3 is 0.023385618047495293\n",
      "train loss for batch 4 is 0.010817262235421933\n",
      "train loss for batch 5 is 0.019280396874600453\n",
      "train loss for batch 6 is 0.0053035102560934685\n",
      "train loss for batch 7 is 0.01646787295060906\n",
      "--- RMSE = 0.87 ---\n",
      "\n",
      " Epoch 287 of 400\n",
      "train loss for batch 1 is 0.006477041808612946\n",
      "train loss for batch 2 is 0.018234279338021235\n",
      "train loss for batch 3 is 0.011390570605933703\n",
      "train loss for batch 4 is 0.010531393033870953\n",
      "train loss for batch 5 is 0.011056004206520566\n",
      "train loss for batch 6 is 0.011804448911875012\n",
      "train loss for batch 7 is 0.008580980144364416\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 288 of 400\n",
      "train loss for batch 1 is 0.010207285958327492\n",
      "train loss for batch 2 is 0.012487833028657105\n",
      "train loss for batch 3 is 0.0036951405902291307\n",
      "train loss for batch 4 is 0.008666810339245115\n",
      "train loss for batch 5 is 0.0044807860505409\n",
      "train loss for batch 6 is 0.004997805758051886\n",
      "train loss for batch 7 is 0.011273351936975045\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 289 of 400\n",
      "train loss for batch 1 is 0.002586650102791196\n",
      "train loss for batch 2 is 0.006234229932587954\n",
      "train loss for batch 3 is 0.01019349715248533\n",
      "train loss for batch 4 is 0.011020220839035623\n",
      "train loss for batch 5 is 0.012663029606794133\n",
      "train loss for batch 6 is 0.014189653920199006\n",
      "train loss for batch 7 is 0.005371739094048716\n",
      "--- RMSE = 1.16 ---\n",
      "\n",
      " Epoch 290 of 400\n",
      "train loss for batch 1 is 0.012357626650090946\n",
      "train loss for batch 2 is 0.005917933290268549\n",
      "train loss for batch 3 is 0.009383201764136833\n",
      "train loss for batch 4 is 0.007441595468069968\n",
      "train loss for batch 5 is 0.007304114116503157\n",
      "train loss for batch 6 is 0.010563801540354639\n",
      "train loss for batch 7 is 0.014723790311815706\n",
      "--- RMSE = 1.19 ---\n",
      "\n",
      " Epoch 291 of 400\n",
      "train loss for batch 1 is 0.00907394188730793\n",
      "train loss for batch 2 is 0.012284534685108293\n",
      "train loss for batch 3 is 0.006698872188506788\n",
      "train loss for batch 4 is 0.012926731866015544\n",
      "train loss for batch 5 is 0.01809864313150876\n",
      "train loss for batch 6 is 0.011999944873943657\n",
      "train loss for batch 7 is 0.012107948592648129\n",
      "--- RMSE = 1.18 ---\n",
      "\n",
      " Epoch 292 of 400\n",
      "train loss for batch 1 is 0.018762838033285763\n",
      "train loss for batch 2 is 0.007836725944087089\n",
      "train loss for batch 3 is 0.01788451648250152\n",
      "train loss for batch 4 is 0.006254037609215254\n",
      "train loss for batch 5 is 0.023422929824521512\n",
      "train loss for batch 6 is 0.004049290001684374\n",
      "train loss for batch 7 is 0.016314166395775542\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 293 of 400\n",
      "train loss for batch 1 is 0.007737796812119728\n",
      "train loss for batch 2 is 0.017035376825053364\n",
      "train loss for batch 3 is 0.0038620127291787033\n",
      "train loss for batch 4 is 0.01895429088207967\n",
      "train loss for batch 5 is 0.00684334457559308\n",
      "train loss for batch 6 is 0.011140171489661786\n",
      "train loss for batch 7 is 0.018717259346398292\n",
      "--- RMSE = 1.82 ---\n",
      "\n",
      " Epoch 294 of 400\n",
      "train loss for batch 1 is 0.02895379724371984\n",
      "train loss for batch 2 is 0.008009764456830224\n",
      "train loss for batch 3 is 0.010576293915806598\n",
      "train loss for batch 4 is 0.009849033757284022\n",
      "train loss for batch 5 is 0.011985953002174081\n",
      "train loss for batch 6 is 0.009981198857974485\n",
      "train loss for batch 7 is 0.006627942151106721\n",
      "--- RMSE = 1.35 ---\n",
      "\n",
      " Epoch 295 of 400\n",
      "train loss for batch 1 is 0.008952709334414893\n",
      "train loss for batch 2 is 0.01498228973172383\n",
      "train loss for batch 3 is 0.01513121542339974\n",
      "train loss for batch 4 is 0.011193944213750591\n",
      "train loss for batch 5 is 0.019211762554659756\n",
      "train loss for batch 6 is 0.014454766035431115\n",
      "train loss for batch 7 is 0.020785114115969894\n",
      "--- RMSE = 1.16 ---\n",
      "\n",
      " Epoch 296 of 400\n",
      "train loss for batch 1 is 0.012808489904748355\n",
      "train loss for batch 2 is 0.018969482405117845\n",
      "train loss for batch 3 is 0.014269634052592538\n",
      "train loss for batch 4 is 0.01149886263395416\n",
      "train loss for batch 5 is 0.020672331426170067\n",
      "train loss for batch 6 is 0.011962458918016661\n",
      "train loss for batch 7 is 0.01417561452407639\n",
      "--- RMSE = 1.09 ---\n",
      "\n",
      " Epoch 297 of 400\n",
      "train loss for batch 1 is 0.013191108200466317\n",
      "train loss for batch 2 is 0.00947246244121514\n",
      "train loss for batch 3 is 0.016937738177570112\n",
      "train loss for batch 4 is 0.022301130657485516\n",
      "train loss for batch 5 is 0.015276402430283508\n",
      "train loss for batch 6 is 0.02963300913228947\n",
      "train loss for batch 7 is 0.007531430347480339\n",
      "--- RMSE = 1.64 ---\n",
      "\n",
      " Epoch 298 of 400\n",
      "train loss for batch 1 is 0.025651549104376656\n",
      "train loss for batch 2 is 0.01877622329894988\n",
      "train loss for batch 3 is 0.007301959761691238\n",
      "train loss for batch 4 is 0.02747417105795854\n",
      "train loss for batch 5 is 0.01001602783304417\n",
      "train loss for batch 6 is 0.016873423777266007\n",
      "train loss for batch 7 is 0.024330215590567077\n",
      "--- RMSE = 1.06 ---\n",
      "\n",
      " Epoch 299 of 400\n",
      "train loss for batch 1 is 0.014862172357979065\n",
      "train loss for batch 2 is 0.02535343611299074\n",
      "train loss for batch 3 is 0.0162291301333003\n",
      "train loss for batch 4 is 0.026390684724942104\n",
      "train loss for batch 5 is 0.02547482033698064\n",
      "train loss for batch 6 is 0.024535618311122914\n",
      "train loss for batch 7 is 0.0176107420196245\n",
      "--- RMSE = 1.93 ---\n",
      "\n",
      " Epoch 300 of 400\n",
      "train loss for batch 1 is 0.05564225758196922\n",
      "train loss for batch 2 is 0.03254681679521837\n",
      "train loss for batch 3 is 0.05410744640543683\n",
      "train loss for batch 4 is 0.019111900992429626\n",
      "train loss for batch 5 is 0.03181120590086108\n",
      "train loss for batch 6 is 0.032859974287608604\n",
      "train loss for batch 7 is 0.010549362801379586\n",
      "--- RMSE = 1.71 ---\n",
      "\n",
      " Epoch 301 of 400\n",
      "train loss for batch 1 is 0.031188288609988887\n",
      "train loss for batch 2 is 0.0195605718804562\n",
      "train loss for batch 3 is 0.01152735189223011\n",
      "train loss for batch 4 is 0.023125535550455663\n",
      "train loss for batch 5 is 0.01062599857848439\n",
      "train loss for batch 6 is 0.014639703259903042\n",
      "train loss for batch 7 is 0.016931785059189946\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 302 of 400\n",
      "train loss for batch 1 is 0.016449799820683637\n",
      "train loss for batch 2 is 0.015650451664008906\n",
      "train loss for batch 3 is 0.02805960378510739\n",
      "train loss for batch 4 is 0.009439862560674821\n",
      "train loss for batch 5 is 0.029992237552384306\n",
      "train loss for batch 6 is 0.012456180851224413\n",
      "train loss for batch 7 is 0.02623898012615527\n",
      "--- RMSE = 1.29 ---\n",
      "\n",
      " Epoch 303 of 400\n",
      "train loss for batch 1 is 0.019578647837994664\n",
      "train loss for batch 2 is 0.01145966311821474\n",
      "train loss for batch 3 is 0.01959136012331125\n",
      "train loss for batch 4 is 0.02408098314355982\n",
      "train loss for batch 5 is 0.008497040035628416\n",
      "train loss for batch 6 is 0.019436489374746287\n",
      "train loss for batch 7 is 0.011443062843233754\n",
      "--- RMSE = 0.87 ---\n",
      "\n",
      " Epoch 304 of 400\n",
      "train loss for batch 1 is 0.013442263038259438\n",
      "train loss for batch 2 is 0.019907848461493842\n",
      "train loss for batch 3 is 0.013245750944305372\n",
      "train loss for batch 4 is 0.018787142722538185\n",
      "train loss for batch 5 is 0.015190339541911768\n",
      "train loss for batch 6 is 0.008137191118752387\n",
      "train loss for batch 7 is 0.013935629595556373\n",
      "--- RMSE = 1.20 ---\n",
      "\n",
      " Epoch 305 of 400\n",
      "train loss for batch 1 is 0.014850166111152157\n",
      "train loss for batch 2 is 0.006151674099198574\n",
      "train loss for batch 3 is 0.012468747391226298\n",
      "train loss for batch 4 is 0.020481733992350256\n",
      "train loss for batch 5 is 0.0066977786816184185\n",
      "train loss for batch 6 is 0.017902378976241463\n",
      "train loss for batch 7 is 0.014015866395022322\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 306 of 400\n",
      "train loss for batch 1 is 0.00779248713133932\n",
      "train loss for batch 2 is 0.01924211729631418\n",
      "train loss for batch 3 is 0.008817902191725268\n",
      "train loss for batch 4 is 0.008172807066964615\n",
      "train loss for batch 5 is 0.01710503202758895\n",
      "train loss for batch 6 is 0.007223581116902349\n",
      "train loss for batch 7 is 0.015484179835985578\n",
      "--- RMSE = 1.37 ---\n",
      "\n",
      " Epoch 307 of 400\n",
      "train loss for batch 1 is 0.017786194196311157\n",
      "train loss for batch 2 is 0.004495967766853069\n",
      "train loss for batch 3 is 0.00898484300867345\n",
      "train loss for batch 4 is 0.018987027949888068\n",
      "train loss for batch 5 is 0.005252089785193997\n",
      "train loss for batch 6 is 0.018194318538604263\n",
      "train loss for batch 7 is 0.008685285060937744\n",
      "--- RMSE = 1.02 ---\n",
      "\n",
      " Epoch 308 of 400\n",
      "train loss for batch 1 is 0.00726913166017179\n",
      "train loss for batch 2 is 0.016329157857324098\n",
      "train loss for batch 3 is 0.010601143070780354\n",
      "train loss for batch 4 is 0.007989602556294845\n",
      "train loss for batch 5 is 0.00902054228614153\n",
      "train loss for batch 6 is 0.0068718120177927436\n",
      "train loss for batch 7 is 0.009435546372390193\n",
      "--- RMSE = 0.93 ---\n",
      "\n",
      " Epoch 309 of 400\n",
      "train loss for batch 1 is 0.009417040723836383\n",
      "train loss for batch 2 is 0.0036763780247432367\n",
      "train loss for batch 3 is 0.008528592422767258\n",
      "train loss for batch 4 is 0.007533453097127949\n",
      "train loss for batch 5 is 0.011494895300116097\n",
      "train loss for batch 6 is 0.013805207034395943\n",
      "train loss for batch 7 is 0.00470114144344674\n",
      "--- RMSE = 1.04 ---\n",
      "\n",
      " Epoch 310 of 400\n",
      "train loss for batch 1 is 0.009283797516251415\n",
      "train loss for batch 2 is 0.0056085167358437184\n",
      "train loss for batch 3 is 0.00526350618140192\n",
      "train loss for batch 4 is 0.00791674324463918\n",
      "train loss for batch 5 is 0.007622734822599728\n",
      "train loss for batch 6 is 0.008905777436206939\n",
      "train loss for batch 7 is 0.005543841107377472\n",
      "--- RMSE = 0.77 ---\n",
      "\n",
      " Epoch 311 of 400\n",
      "train loss for batch 1 is 0.0054712747610394415\n",
      "train loss for batch 2 is 0.00421669651132473\n",
      "train loss for batch 3 is 0.008079988727013307\n",
      "train loss for batch 4 is 0.0033194703697056126\n",
      "train loss for batch 5 is 0.005757019279096723\n",
      "train loss for batch 6 is 0.004235618534670916\n",
      "train loss for batch 7 is 0.004902283445950222\n",
      "--- RMSE = 0.68 ---\n",
      "\n",
      " Epoch 312 of 400\n",
      "train loss for batch 1 is 0.003568791443950449\n",
      "train loss for batch 2 is 0.006262660029322393\n",
      "train loss for batch 3 is 0.004007329696744191\n",
      "train loss for batch 4 is 0.00797835896330622\n",
      "train loss for batch 5 is 0.003791098735318482\n",
      "train loss for batch 6 is 0.005503217173543732\n",
      "train loss for batch 7 is 0.010353840907967706\n",
      "--- RMSE = 0.81 ---\n",
      "\n",
      " Epoch 313 of 400\n",
      "train loss for batch 1 is 0.009619759203309486\n",
      "train loss for batch 2 is 0.006355159015914286\n",
      "train loss for batch 3 is 0.00886939584411085\n",
      "train loss for batch 4 is 0.0066723681699063335\n",
      "train loss for batch 5 is 0.0071414416841739815\n",
      "train loss for batch 6 is 0.010947212304508576\n",
      "train loss for batch 7 is 0.006209102159661042\n",
      "--- RMSE = 1.08 ---\n",
      "\n",
      " Epoch 314 of 400\n",
      "train loss for batch 1 is 0.009224736885879627\n",
      "train loss for batch 2 is 0.003954795876005024\n",
      "train loss for batch 3 is 0.010875285317179555\n",
      "train loss for batch 4 is 0.005687650110134357\n",
      "train loss for batch 5 is 0.007114819800674115\n",
      "train loss for batch 6 is 0.003216805278390286\n",
      "train loss for batch 7 is 0.0042672359976682755\n",
      "--- RMSE = 0.59 ---\n",
      "\n",
      " Epoch 315 of 400\n",
      "train loss for batch 1 is 0.005134404905698471\n",
      "train loss for batch 2 is 0.0062166134462302335\n",
      "train loss for batch 3 is 0.004476384297455529\n",
      "train loss for batch 4 is 0.006005968333458478\n",
      "train loss for batch 5 is 0.006939545732888952\n",
      "train loss for batch 6 is 0.008426678344082096\n",
      "train loss for batch 7 is 0.018788280455340207\n",
      "--- RMSE = 1.33 ---\n",
      "\n",
      " Epoch 316 of 400\n",
      "train loss for batch 1 is 0.0219204742765947\n",
      "train loss for batch 2 is 0.01894538911240034\n",
      "train loss for batch 3 is 0.025754781510777095\n",
      "train loss for batch 4 is 0.02624630377042182\n",
      "train loss for batch 5 is 0.03367235282749536\n",
      "train loss for batch 6 is 0.02053872079385867\n",
      "train loss for batch 7 is 0.012417950439744465\n",
      "--- RMSE = 1.97 ---\n",
      "\n",
      " Epoch 317 of 400\n",
      "train loss for batch 1 is 0.03600534007970485\n",
      "train loss for batch 2 is 0.008943618078000246\n",
      "train loss for batch 3 is 0.017971725904718336\n",
      "train loss for batch 4 is 0.00975249081062116\n",
      "train loss for batch 5 is 0.047337917412246584\n",
      "train loss for batch 6 is 0.010573931614987445\n",
      "train loss for batch 7 is 0.031189854054421814\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 318 of 400\n",
      "train loss for batch 1 is 0.009159873043603741\n",
      "train loss for batch 2 is 0.0303801028763786\n",
      "train loss for batch 3 is 0.007124335388834489\n",
      "train loss for batch 4 is 0.03991021630406721\n",
      "train loss for batch 5 is 0.01105758718651992\n",
      "train loss for batch 6 is 0.038098118246037524\n",
      "train loss for batch 7 is 0.028725165681122846\n",
      "--- RMSE = 1.88 ---\n",
      "\n",
      " Epoch 319 of 400\n",
      "train loss for batch 1 is 0.04190814399726237\n",
      "train loss for batch 2 is 0.015408320237109923\n",
      "train loss for batch 3 is 0.03273422694543548\n",
      "train loss for batch 4 is 0.026114969288185293\n",
      "train loss for batch 5 is 0.04148321679238448\n",
      "train loss for batch 6 is 0.016647122031571523\n",
      "train loss for batch 7 is 0.030151783335728093\n",
      "--- RMSE = 1.76 ---\n",
      "\n",
      " Epoch 320 of 400\n",
      "train loss for batch 1 is 0.03603907645364404\n",
      "train loss for batch 2 is 0.017724312382950736\n",
      "train loss for batch 3 is 0.038895990615857315\n",
      "train loss for batch 4 is 0.01852218510353189\n",
      "train loss for batch 5 is 0.04355734154732426\n",
      "train loss for batch 6 is 0.010586200662622254\n",
      "train loss for batch 7 is 0.04924643860862522\n",
      "--- RMSE = 1.25 ---\n",
      "\n",
      " Epoch 321 of 400\n",
      "train loss for batch 1 is 0.01457227899062157\n",
      "train loss for batch 2 is 0.04641752393347767\n",
      "train loss for batch 3 is 0.011614521770991143\n",
      "train loss for batch 4 is 0.0418435085203743\n",
      "train loss for batch 5 is 0.01978231332274077\n",
      "train loss for batch 6 is 0.017159203980901407\n",
      "train loss for batch 7 is 0.02578165445996826\n",
      "--- RMSE = 1.63 ---\n",
      "\n",
      " Epoch 322 of 400\n",
      "train loss for batch 1 is 0.023891696873219067\n",
      "train loss for batch 2 is 0.03132577799973758\n",
      "train loss for batch 3 is 0.018337594037737825\n",
      "train loss for batch 4 is 0.02866223405385079\n",
      "train loss for batch 5 is 0.010755574367055494\n",
      "train loss for batch 6 is 0.03000053354991792\n",
      "train loss for batch 7 is 0.013510851048733038\n",
      "--- RMSE = 1.32 ---\n",
      "\n",
      " Epoch 323 of 400\n",
      "train loss for batch 1 is 0.01143685552712196\n",
      "train loss for batch 2 is 0.019877271495648663\n",
      "train loss for batch 3 is 0.011118463146559265\n",
      "train loss for batch 4 is 0.017084326786893776\n",
      "train loss for batch 5 is 0.012382178195528594\n",
      "train loss for batch 6 is 0.007184397659677104\n",
      "train loss for batch 7 is 0.01005074425565587\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 324 of 400\n",
      "train loss for batch 1 is 0.012585356554383476\n",
      "train loss for batch 2 is 0.0075862072111903215\n",
      "train loss for batch 3 is 0.006576273169566517\n",
      "train loss for batch 4 is 0.005768103154400675\n",
      "train loss for batch 5 is 0.004044673795963105\n",
      "train loss for batch 6 is 0.006278910103318114\n",
      "train loss for batch 7 is 0.00499928404677361\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 325 of 400\n",
      "train loss for batch 1 is 0.005087316415428847\n",
      "train loss for batch 2 is 0.0063857154031612025\n",
      "train loss for batch 3 is 0.0039498703699063714\n",
      "train loss for batch 4 is 0.008289409931799703\n",
      "train loss for batch 5 is 0.0042598298798254865\n",
      "train loss for batch 6 is 0.007209389729887164\n",
      "train loss for batch 7 is 0.0043938275703926805\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 326 of 400\n",
      "train loss for batch 1 is 0.0029038244909299783\n",
      "train loss for batch 2 is 0.0070885100990759845\n",
      "train loss for batch 3 is 0.005091204282274664\n",
      "train loss for batch 4 is 0.008088066051864575\n",
      "train loss for batch 5 is 0.006000150006476841\n",
      "train loss for batch 6 is 0.0077135815542936515\n",
      "train loss for batch 7 is 0.0068569361400463635\n",
      "--- RMSE = 1.02 ---\n",
      "\n",
      " Epoch 327 of 400\n",
      "train loss for batch 1 is 0.007470206724095543\n",
      "train loss for batch 2 is 0.003533196427126821\n",
      "train loss for batch 3 is 0.007423169502660642\n",
      "train loss for batch 4 is 0.006064478581905316\n",
      "train loss for batch 5 is 0.005262754483504099\n",
      "train loss for batch 6 is 0.0057094860896862555\n",
      "train loss for batch 7 is 0.004827078180478799\n",
      "--- RMSE = 0.71 ---\n",
      "\n",
      " Epoch 328 of 400\n",
      "train loss for batch 1 is 0.007003078205530427\n",
      "train loss for batch 2 is 0.006402115565827156\n",
      "train loss for batch 3 is 0.004665280373901068\n",
      "train loss for batch 4 is 0.0037611479826163167\n",
      "train loss for batch 5 is 0.007613285463526937\n",
      "train loss for batch 6 is 0.003062489851054575\n",
      "train loss for batch 7 is 0.004717919894944832\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 329 of 400\n",
      "train loss for batch 1 is 0.004138454404905539\n",
      "train loss for batch 2 is 0.0028111660115354474\n",
      "train loss for batch 3 is 0.005036090725286103\n",
      "train loss for batch 4 is 0.004036082088842582\n",
      "train loss for batch 5 is 0.005645865239603645\n",
      "train loss for batch 6 is 0.0040713378307859266\n",
      "train loss for batch 7 is 0.007750125404909186\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 330 of 400\n",
      "train loss for batch 1 is 0.004606527692386717\n",
      "train loss for batch 2 is 0.007275191019684418\n",
      "train loss for batch 3 is 0.00403123417967971\n",
      "train loss for batch 4 is 0.005541672024166451\n",
      "train loss for batch 5 is 0.0047380812742835576\n",
      "train loss for batch 6 is 0.0036422196479395357\n",
      "train loss for batch 7 is 0.00544645180868914\n",
      "--- RMSE = 0.66 ---\n",
      "\n",
      " Epoch 331 of 400\n",
      "train loss for batch 1 is 0.005528325365262211\n",
      "train loss for batch 2 is 0.004771556793019197\n",
      "train loss for batch 3 is 0.0029699015103350655\n",
      "train loss for batch 4 is 0.007678319804337654\n",
      "train loss for batch 5 is 0.004648131761676039\n",
      "train loss for batch 6 is 0.007205661432032444\n",
      "train loss for batch 7 is 0.004036330701162708\n",
      "--- RMSE = 0.79 ---\n",
      "\n",
      " Epoch 332 of 400\n",
      "train loss for batch 1 is 0.005721706593529451\n",
      "train loss for batch 2 is 0.004595733850531851\n",
      "train loss for batch 3 is 0.006682779922484735\n",
      "train loss for batch 4 is 0.0038674641777684967\n",
      "train loss for batch 5 is 0.006014894660548001\n",
      "train loss for batch 6 is 0.00643899919792859\n",
      "train loss for batch 7 is 0.005177697031008754\n",
      "--- RMSE = 0.75 ---\n",
      "\n",
      " Epoch 333 of 400\n",
      "train loss for batch 1 is 0.007516594364017609\n",
      "train loss for batch 2 is 0.007464187165018081\n",
      "train loss for batch 3 is 0.005904356226398906\n",
      "train loss for batch 4 is 0.003795990843377511\n",
      "train loss for batch 5 is 0.005972314276959811\n",
      "train loss for batch 6 is 0.003625884868686986\n",
      "train loss for batch 7 is 0.004306556631489147\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 334 of 400\n",
      "train loss for batch 1 is 0.005095868225809104\n",
      "train loss for batch 2 is 0.0034529168837023096\n",
      "train loss for batch 3 is 0.005844477163175441\n",
      "train loss for batch 4 is 0.0044443189043660595\n",
      "train loss for batch 5 is 0.006288053794752921\n",
      "train loss for batch 6 is 0.0032007648999963613\n",
      "train loss for batch 7 is 0.005966357859540979\n",
      "--- RMSE = 0.60 ---\n",
      "\n",
      " Epoch 335 of 400\n",
      "train loss for batch 1 is 0.0032279228768726156\n",
      "train loss for batch 2 is 0.004524720396960105\n",
      "train loss for batch 3 is 0.005193188255777009\n",
      "train loss for batch 4 is 0.004756473792357854\n",
      "train loss for batch 5 is 0.004167725694130132\n",
      "train loss for batch 6 is 0.0049501305339631095\n",
      "train loss for batch 7 is 0.004194980690709233\n",
      "--- RMSE = 0.74 ---\n",
      "\n",
      " Epoch 336 of 400\n",
      "train loss for batch 1 is 0.01471417893482394\n",
      "train loss for batch 2 is 0.008084895324304887\n",
      "train loss for batch 3 is 0.01093537176152808\n",
      "train loss for batch 4 is 0.01012652093827759\n",
      "train loss for batch 5 is 0.011376051132581005\n",
      "train loss for batch 6 is 0.01908991469054572\n",
      "train loss for batch 7 is 0.011283405533839533\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 337 of 400\n",
      "train loss for batch 1 is 0.008395725540155911\n",
      "train loss for batch 2 is 0.01687652515910267\n",
      "train loss for batch 3 is 0.011874634278147838\n",
      "train loss for batch 4 is 0.009457887713538438\n",
      "train loss for batch 5 is 0.013001165781346186\n",
      "train loss for batch 6 is 0.00836276849352023\n",
      "train loss for batch 7 is 0.011338200250854337\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 338 of 400\n",
      "train loss for batch 1 is 0.007108019961964025\n",
      "train loss for batch 2 is 0.00734803829015444\n",
      "train loss for batch 3 is 0.01234924593898717\n",
      "train loss for batch 4 is 0.013331780554510779\n",
      "train loss for batch 5 is 0.007955085043072227\n",
      "train loss for batch 6 is 0.006305143050795449\n",
      "train loss for batch 7 is 0.013841746008477469\n",
      "--- RMSE = 1.34 ---\n",
      "\n",
      " Epoch 339 of 400\n",
      "train loss for batch 1 is 0.01361077776292803\n",
      "train loss for batch 2 is 0.012244463271274075\n",
      "train loss for batch 3 is 0.009413799136002125\n",
      "train loss for batch 4 is 0.01246584525153046\n",
      "train loss for batch 5 is 0.015559699618058302\n",
      "train loss for batch 6 is 0.005864499592810031\n",
      "train loss for batch 7 is 0.013114260664307669\n",
      "--- RMSE = 1.10 ---\n",
      "\n",
      " Epoch 340 of 400\n",
      "train loss for batch 1 is 0.013192972004620409\n",
      "train loss for batch 2 is 0.006018508847448265\n",
      "train loss for batch 3 is 0.01564505058175369\n",
      "train loss for batch 4 is 0.007987539695672978\n",
      "train loss for batch 5 is 0.012638512693072817\n",
      "train loss for batch 6 is 0.0073122632724640846\n",
      "train loss for batch 7 is 0.009730358981828226\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 341 of 400\n",
      "train loss for batch 1 is 0.010713310309938552\n",
      "train loss for batch 2 is 0.015453615898351645\n",
      "train loss for batch 3 is 0.006116076701712594\n",
      "train loss for batch 4 is 0.010772347852261223\n",
      "train loss for batch 5 is 0.011461734160535125\n",
      "train loss for batch 6 is 0.009154303288468828\n",
      "train loss for batch 7 is 0.007165991673422804\n",
      "--- RMSE = 0.91 ---\n",
      "\n",
      " Epoch 342 of 400\n",
      "train loss for batch 1 is 0.006648679157724519\n",
      "train loss for batch 2 is 0.008039671083964606\n",
      "train loss for batch 3 is 0.005295548063904757\n",
      "train loss for batch 4 is 0.008773674018559895\n",
      "train loss for batch 5 is 0.0031607691327305166\n",
      "train loss for batch 6 is 0.005075084573527223\n",
      "train loss for batch 7 is 0.0053620912457809245\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 343 of 400\n",
      "train loss for batch 1 is 0.003365450697015497\n",
      "train loss for batch 2 is 0.005828946498749318\n",
      "train loss for batch 3 is 0.002719099363832475\n",
      "train loss for batch 4 is 0.004090788008356656\n",
      "train loss for batch 5 is 0.003372991416268062\n",
      "train loss for batch 6 is 0.008696767276680338\n",
      "train loss for batch 7 is 0.0041984219033599016\n",
      "--- RMSE = 0.95 ---\n",
      "\n",
      " Epoch 344 of 400\n",
      "train loss for batch 1 is 0.00897916053730258\n",
      "train loss for batch 2 is 0.002463651070370515\n",
      "train loss for batch 3 is 0.007252756856723326\n",
      "train loss for batch 4 is 0.004511454944237639\n",
      "train loss for batch 5 is 0.005577607540378713\n",
      "train loss for batch 6 is 0.005386946562247136\n",
      "train loss for batch 7 is 0.00400402399257371\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 345 of 400\n",
      "train loss for batch 1 is 0.005668481156460308\n",
      "train loss for batch 2 is 0.005567842199113322\n",
      "train loss for batch 3 is 0.006095055625956065\n",
      "train loss for batch 4 is 0.007467465856924871\n",
      "train loss for batch 5 is 0.007546210002641409\n",
      "train loss for batch 6 is 0.004703528465498417\n",
      "train loss for batch 7 is 0.019837069670214078\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 346 of 400\n",
      "train loss for batch 1 is 0.005836082267139629\n",
      "train loss for batch 2 is 0.00822151927654713\n",
      "train loss for batch 3 is 0.01184071783181085\n",
      "train loss for batch 4 is 0.007881486153053192\n",
      "train loss for batch 5 is 0.012540553554865221\n",
      "train loss for batch 6 is 0.009207676143045145\n",
      "train loss for batch 7 is 0.01624335143013886\n",
      "--- RMSE = 0.96 ---\n",
      "\n",
      " Epoch 347 of 400\n",
      "train loss for batch 1 is 0.00661696253490484\n",
      "train loss for batch 2 is 0.014504225092030783\n",
      "train loss for batch 3 is 0.013314939909919624\n",
      "train loss for batch 4 is 0.009972589003050144\n",
      "train loss for batch 5 is 0.024666671168435562\n",
      "train loss for batch 6 is 0.00274239899560752\n",
      "train loss for batch 7 is 0.022604074037790696\n",
      "--- RMSE = 1.10 ---\n",
      "\n",
      " Epoch 348 of 400\n",
      "train loss for batch 1 is 0.010616908913270344\n",
      "train loss for batch 2 is 0.01764004901097875\n",
      "train loss for batch 3 is 0.011544793436218044\n",
      "train loss for batch 4 is 0.01117292597537836\n",
      "train loss for batch 5 is 0.024106220254684715\n",
      "train loss for batch 6 is 0.012238409058367674\n",
      "train loss for batch 7 is 0.014776989476262391\n",
      "--- RMSE = 0.65 ---\n",
      "\n",
      " Epoch 349 of 400\n",
      "train loss for batch 1 is 0.004787267361808749\n",
      "train loss for batch 2 is 0.011610138276731217\n",
      "train loss for batch 3 is 0.013044320138174128\n",
      "train loss for batch 4 is 0.0124157078269464\n",
      "train loss for batch 5 is 0.015198441046884113\n",
      "train loss for batch 6 is 0.007822864652712676\n",
      "train loss for batch 7 is 0.02309497519166896\n",
      "--- RMSE = 1.16 ---\n",
      "\n",
      " Epoch 350 of 400\n",
      "train loss for batch 1 is 0.012318522187595504\n",
      "train loss for batch 2 is 0.026181343458753698\n",
      "train loss for batch 3 is 0.0126858044707628\n",
      "train loss for batch 4 is 0.02453392568881609\n",
      "train loss for batch 5 is 0.007531641822088999\n",
      "train loss for batch 6 is 0.018937514596965275\n",
      "train loss for batch 7 is 0.013256774871981108\n",
      "--- RMSE = 1.34 ---\n",
      "\n",
      " Epoch 351 of 400\n",
      "train loss for batch 1 is 0.01772259235456876\n",
      "train loss for batch 2 is 0.010443557693385407\n",
      "train loss for batch 3 is 0.015675812730718986\n",
      "train loss for batch 4 is 0.005969532683751992\n",
      "train loss for batch 5 is 0.0058488781692773335\n",
      "train loss for batch 6 is 0.0100203674853323\n",
      "train loss for batch 7 is 0.008069785512216113\n",
      "--- RMSE = 0.79 ---\n",
      "\n",
      " Epoch 352 of 400\n",
      "train loss for batch 1 is 0.010188139957340516\n",
      "train loss for batch 2 is 0.0037195264208815036\n",
      "train loss for batch 3 is 0.008560301212594401\n",
      "train loss for batch 4 is 0.008465702496903642\n",
      "train loss for batch 5 is 0.011634869293735656\n",
      "train loss for batch 6 is 0.003980209181506961\n",
      "train loss for batch 7 is 0.009191601982501062\n",
      "--- RMSE = 0.76 ---\n",
      "\n",
      " Epoch 353 of 400\n",
      "train loss for batch 1 is 0.012489844187401143\n",
      "train loss for batch 2 is 0.0050507101761989425\n",
      "train loss for batch 3 is 0.005383352551607424\n",
      "train loss for batch 4 is 0.006077939458374988\n",
      "train loss for batch 5 is 0.007678982973595432\n",
      "train loss for batch 6 is 0.006177180724962226\n",
      "train loss for batch 7 is 0.003817380077842744\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 354 of 400\n",
      "train loss for batch 1 is 0.0055067987016912115\n",
      "train loss for batch 2 is 0.006958648582290838\n",
      "train loss for batch 3 is 0.0061332187427301345\n",
      "train loss for batch 4 is 0.004657738322637855\n",
      "train loss for batch 5 is 0.004995468820098297\n",
      "train loss for batch 6 is 0.007779609176168675\n",
      "train loss for batch 7 is 0.004754965645568888\n",
      "--- RMSE = 0.67 ---\n",
      "\n",
      " Epoch 355 of 400\n",
      "train loss for batch 1 is 0.003619969267185322\n",
      "train loss for batch 2 is 0.010826435074153618\n",
      "train loss for batch 3 is 0.006134380940375201\n",
      "train loss for batch 4 is 0.0034587036236523837\n",
      "train loss for batch 5 is 0.004198885661388282\n",
      "train loss for batch 6 is 0.0054002714299145\n",
      "train loss for batch 7 is 0.0045844504604747715\n",
      "--- RMSE = 1.03 ---\n",
      "\n",
      " Epoch 356 of 400\n",
      "train loss for batch 1 is 0.010259758981131562\n",
      "train loss for batch 2 is 0.005410598614577857\n",
      "train loss for batch 3 is 0.013959659426619832\n",
      "train loss for batch 4 is 0.009429219837982266\n",
      "train loss for batch 5 is 0.015385226932614286\n",
      "train loss for batch 6 is 0.008676195006283485\n",
      "train loss for batch 7 is 0.02149965477357927\n",
      "--- RMSE = 0.85 ---\n",
      "\n",
      " Epoch 357 of 400\n",
      "train loss for batch 1 is 0.008472506528638098\n",
      "train loss for batch 2 is 0.006023772762640402\n",
      "train loss for batch 3 is 0.01813244949753785\n",
      "train loss for batch 4 is 0.0048461031131577905\n",
      "train loss for batch 5 is 0.01027807074077115\n",
      "train loss for batch 6 is 0.009653134242374609\n",
      "train loss for batch 7 is 0.011007471169229177\n",
      "--- RMSE = 1.05 ---\n",
      "\n",
      " Epoch 358 of 400\n",
      "train loss for batch 1 is 0.005648711241423417\n",
      "train loss for batch 2 is 0.007095492326235844\n",
      "train loss for batch 3 is 0.005719420187783232\n",
      "train loss for batch 4 is 0.005259704941319094\n",
      "train loss for batch 5 is 0.006163246516667167\n",
      "train loss for batch 6 is 0.010097900473505457\n",
      "train loss for batch 7 is 0.018626877606962765\n",
      "--- RMSE = 0.86 ---\n",
      "\n",
      " Epoch 359 of 400\n",
      "train loss for batch 1 is 0.010740975542725881\n",
      "train loss for batch 2 is 0.01272349886000164\n",
      "train loss for batch 3 is 0.009673812403238849\n",
      "train loss for batch 4 is 0.0073763058016757845\n",
      "train loss for batch 5 is 0.007489689927510967\n",
      "train loss for batch 6 is 0.01549359299226722\n",
      "train loss for batch 7 is 0.016997760624728096\n",
      "--- RMSE = 1.00 ---\n",
      "\n",
      " Epoch 360 of 400\n",
      "train loss for batch 1 is 0.007439547667107433\n",
      "train loss for batch 2 is 0.010619759313737842\n",
      "train loss for batch 3 is 0.01334811940384426\n",
      "train loss for batch 4 is 0.005852757651467994\n",
      "train loss for batch 5 is 0.011779602503992062\n",
      "train loss for batch 6 is 0.012485726449351302\n",
      "train loss for batch 7 is 0.011597990808804932\n",
      "--- RMSE = 0.79 ---\n",
      "\n",
      " Epoch 361 of 400\n",
      "train loss for batch 1 is 0.0054825971849043606\n",
      "train loss for batch 2 is 0.009665871447468822\n",
      "train loss for batch 3 is 0.006097442495562995\n",
      "train loss for batch 4 is 0.009914506706121588\n",
      "train loss for batch 5 is 0.008716607413731174\n",
      "train loss for batch 6 is 0.007301238479520415\n",
      "train loss for batch 7 is 0.011847119580769078\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 362 of 400\n",
      "train loss for batch 1 is 0.011227757089048978\n",
      "train loss for batch 2 is 0.011613485558588791\n",
      "train loss for batch 3 is 0.006936685378310063\n",
      "train loss for batch 4 is 0.014715924735135189\n",
      "train loss for batch 5 is 0.010449309964593116\n",
      "train loss for batch 6 is 0.0172018432872771\n",
      "train loss for batch 7 is 0.006965569724077845\n",
      "--- RMSE = 0.96 ---\n",
      "\n",
      " Epoch 363 of 400\n",
      "train loss for batch 1 is 0.010465752753435491\n",
      "train loss for batch 2 is 0.009216857169990404\n",
      "train loss for batch 3 is 0.009940207275762376\n",
      "train loss for batch 4 is 0.006499426490936757\n",
      "train loss for batch 5 is 0.012377585213453062\n",
      "train loss for batch 6 is 0.010350896268261758\n",
      "train loss for batch 7 is 0.020265077439636358\n",
      "--- RMSE = 0.68 ---\n",
      "\n",
      " Epoch 364 of 400\n",
      "train loss for batch 1 is 0.006001412905277834\n",
      "train loss for batch 2 is 0.016109160496701493\n",
      "train loss for batch 3 is 0.01546024118244949\n",
      "train loss for batch 4 is 0.009081196379962077\n",
      "train loss for batch 5 is 0.007967501546806699\n",
      "train loss for batch 6 is 0.012822023445655266\n",
      "train loss for batch 7 is 0.0060806683642238655\n",
      "--- RMSE = 0.69 ---\n",
      "\n",
      " Epoch 365 of 400\n",
      "train loss for batch 1 is 0.0051723294859062265\n",
      "train loss for batch 2 is 0.00506675659383993\n",
      "train loss for batch 3 is 0.008642579191612732\n",
      "train loss for batch 4 is 0.007697952437532292\n",
      "train loss for batch 5 is 0.007251156115275361\n",
      "train loss for batch 6 is 0.007475779253306022\n",
      "train loss for batch 7 is 0.006812358842619098\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 366 of 400\n",
      "train loss for batch 1 is 0.008604932265570395\n",
      "train loss for batch 2 is 0.004814635835595476\n",
      "train loss for batch 3 is 0.0055575921744410025\n",
      "train loss for batch 4 is 0.009525473379238938\n",
      "train loss for batch 5 is 0.010372320916819529\n",
      "train loss for batch 6 is 0.012212581054573455\n",
      "train loss for batch 7 is 0.011042871185918705\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 367 of 400\n",
      "train loss for batch 1 is 0.007800580548681419\n",
      "train loss for batch 2 is 0.010641845748606554\n",
      "train loss for batch 3 is 0.009127463306338884\n",
      "train loss for batch 4 is 0.00832567183801862\n",
      "train loss for batch 5 is 0.014006643515645546\n",
      "train loss for batch 6 is 0.01193413321106748\n",
      "train loss for batch 7 is 0.007735577886442767\n",
      "--- RMSE = 0.80 ---\n",
      "\n",
      " Epoch 368 of 400\n",
      "train loss for batch 1 is 0.006099073983825104\n",
      "train loss for batch 2 is 0.009976830827786009\n",
      "train loss for batch 3 is 0.008495916016689366\n",
      "train loss for batch 4 is 0.006318932018299489\n",
      "train loss for batch 5 is 0.00800142296579295\n",
      "train loss for batch 6 is 0.00897441356308932\n",
      "train loss for batch 7 is 0.004363425980980979\n",
      "--- RMSE = 0.79 ---\n",
      "\n",
      " Epoch 369 of 400\n",
      "train loss for batch 1 is 0.006161696796738397\n",
      "train loss for batch 2 is 0.0066962678223346185\n",
      "train loss for batch 3 is 0.005141322748466149\n",
      "train loss for batch 4 is 0.007915155307999482\n",
      "train loss for batch 5 is 0.006146799649387215\n",
      "train loss for batch 6 is 0.004050169765276572\n",
      "train loss for batch 7 is 0.004567194129965789\n",
      "--- RMSE = 0.64 ---\n",
      "\n",
      " Epoch 370 of 400\n",
      "train loss for batch 1 is 0.005247394464722115\n",
      "train loss for batch 2 is 0.004566899467622224\n",
      "train loss for batch 3 is 0.004842510975829\n",
      "train loss for batch 4 is 0.0066409818059918364\n",
      "train loss for batch 5 is 0.007943824002370288\n",
      "train loss for batch 6 is 0.005341281416328859\n",
      "train loss for batch 7 is 0.006282379434070878\n",
      "--- RMSE = 0.74 ---\n",
      "\n",
      " Epoch 371 of 400\n",
      "train loss for batch 1 is 0.006325651358336666\n",
      "train loss for batch 2 is 0.008504010483923057\n",
      "train loss for batch 3 is 0.0042321612368745685\n",
      "train loss for batch 4 is 0.004730886125700975\n",
      "train loss for batch 5 is 0.0052570865822322965\n",
      "train loss for batch 6 is 0.009906264684492537\n",
      "train loss for batch 7 is 0.00969532474994961\n",
      "--- RMSE = 0.66 ---\n",
      "\n",
      " Epoch 372 of 400\n",
      "train loss for batch 1 is 0.006885741655777648\n",
      "train loss for batch 2 is 0.007802332420620796\n",
      "train loss for batch 3 is 0.008554828760739716\n",
      "train loss for batch 4 is 0.007260104511878669\n",
      "train loss for batch 5 is 0.008005241462784385\n",
      "train loss for batch 6 is 0.002956139556898564\n",
      "train loss for batch 7 is 0.007024289828275679\n",
      "--- RMSE = 0.70 ---\n",
      "\n",
      " Epoch 373 of 400\n",
      "train loss for batch 1 is 0.0046468822777881395\n",
      "train loss for batch 2 is 0.004425474724711599\n",
      "train loss for batch 3 is 0.0064294212082363035\n",
      "train loss for batch 4 is 0.0032045532818997663\n",
      "train loss for batch 5 is 0.007413753995932271\n",
      "train loss for batch 6 is 0.004953660871836155\n",
      "train loss for batch 7 is 0.006313397580867882\n",
      "--- RMSE = 0.98 ---\n",
      "\n",
      " Epoch 374 of 400\n",
      "train loss for batch 1 is 0.00808573906854244\n",
      "train loss for batch 2 is 0.00812104178730288\n",
      "train loss for batch 3 is 0.005989925418448091\n",
      "train loss for batch 4 is 0.005337378349855219\n",
      "train loss for batch 5 is 0.0032157288412863376\n",
      "train loss for batch 6 is 0.006046722755716274\n",
      "train loss for batch 7 is 0.00559520910979513\n",
      "--- RMSE = 0.97 ---\n",
      "\n",
      " Epoch 375 of 400\n",
      "train loss for batch 1 is 0.010107659268546954\n",
      "train loss for batch 2 is 0.004117594193529457\n",
      "train loss for batch 3 is 0.007773890555279579\n",
      "train loss for batch 4 is 0.004571795202593607\n",
      "train loss for batch 5 is 0.010093005568692864\n",
      "train loss for batch 6 is 0.0029105403073464804\n",
      "train loss for batch 7 is 0.007361328758741151\n",
      "--- RMSE = 0.78 ---\n",
      "\n",
      " Epoch 376 of 400\n",
      "train loss for batch 1 is 0.007415194170893937\n",
      "train loss for batch 2 is 0.0075024942962248105\n",
      "train loss for batch 3 is 0.004503371095151419\n",
      "train loss for batch 4 is 0.00565745552195134\n",
      "train loss for batch 5 is 0.0036380331242360144\n",
      "train loss for batch 6 is 0.004769957545288889\n",
      "train loss for batch 7 is 0.004376771380046747\n",
      "--- RMSE = 0.73 ---\n",
      "\n",
      " Epoch 377 of 400\n",
      "train loss for batch 1 is 0.00618781709868899\n",
      "train loss for batch 2 is 0.0056841889889684685\n",
      "train loss for batch 3 is 0.003321991290293365\n",
      "train loss for batch 4 is 0.007641397100045293\n",
      "train loss for batch 5 is 0.005553999490768327\n",
      "train loss for batch 6 is 0.006625281502312707\n",
      "train loss for batch 7 is 0.008087821734567015\n",
      "--- RMSE = 0.66 ---\n",
      "\n",
      " Epoch 378 of 400\n",
      "train loss for batch 1 is 0.0036607769312459446\n",
      "train loss for batch 2 is 0.010429604251084561\n",
      "train loss for batch 3 is 0.003773506209363172\n",
      "train loss for batch 4 is 0.0038199826611900454\n",
      "train loss for batch 5 is 0.005583925254427762\n",
      "train loss for batch 6 is 0.004041572562925639\n",
      "train loss for batch 7 is 0.00398013162895643\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 379 of 400\n",
      "train loss for batch 1 is 0.006793737799265625\n",
      "train loss for batch 2 is 0.0025375143755545565\n",
      "train loss for batch 3 is 0.0022087212547146134\n",
      "train loss for batch 4 is 0.004305264560802659\n",
      "train loss for batch 5 is 0.0033020070260382202\n",
      "train loss for batch 6 is 0.003013901860289613\n",
      "train loss for batch 7 is 0.0036385991174647018\n",
      "--- RMSE = 0.55 ---\n",
      "\n",
      " Epoch 380 of 400\n",
      "train loss for batch 1 is 0.0029902662376973094\n",
      "train loss for batch 2 is 0.004676364991632077\n",
      "train loss for batch 3 is 0.0021960777823858656\n",
      "train loss for batch 4 is 0.002443189817273494\n",
      "train loss for batch 5 is 0.0027053002101343595\n",
      "train loss for batch 6 is 0.004360097487890609\n",
      "train loss for batch 7 is 0.0040410710622078285\n",
      "--- RMSE = 0.72 ---\n",
      "\n",
      " Epoch 381 of 400\n",
      "train loss for batch 1 is 0.006009051384498327\n",
      "train loss for batch 2 is 0.004228721127492781\n",
      "train loss for batch 3 is 0.0028586656307870405\n",
      "train loss for batch 4 is 0.003462482787645884\n",
      "train loss for batch 5 is 0.0035407057914435282\n",
      "train loss for batch 6 is 0.002906832437774178\n",
      "train loss for batch 7 is 0.0042764107746399195\n",
      "--- RMSE = 0.61 ---\n",
      "\n",
      " Epoch 382 of 400\n",
      "train loss for batch 1 is 0.0037513421533854\n",
      "train loss for batch 2 is 0.005448573818049608\n",
      "train loss for batch 3 is 0.003026495047695833\n",
      "train loss for batch 4 is 0.003944066527546178\n",
      "train loss for batch 5 is 0.0022021878864688783\n",
      "train loss for batch 6 is 0.004059007814863343\n",
      "train loss for batch 7 is 0.003255306063773647\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 383 of 400\n",
      "train loss for batch 1 is 0.005424039251848847\n",
      "train loss for batch 2 is 0.003223059446482133\n",
      "train loss for batch 3 is 0.0024072579029285313\n",
      "train loss for batch 4 is 0.004627122055716395\n",
      "train loss for batch 5 is 0.0032388505755848724\n",
      "train loss for batch 6 is 0.002198603526706032\n",
      "train loss for batch 7 is 0.0033562187033388297\n",
      "--- RMSE = 0.62 ---\n",
      "\n",
      " Epoch 384 of 400\n",
      "train loss for batch 1 is 0.0042551355734224494\n",
      "train loss for batch 2 is 0.004026399110595703\n",
      "train loss for batch 3 is 0.0040498896246614685\n",
      "train loss for batch 4 is 0.005432679730998214\n",
      "train loss for batch 5 is 0.004411178244650669\n",
      "train loss for batch 6 is 0.0033406010364501674\n",
      "train loss for batch 7 is 0.0025995265344829046\n",
      "--- RMSE = 0.83 ---\n",
      "\n",
      " Epoch 385 of 400\n",
      "train loss for batch 1 is 0.005285842146812616\n",
      "train loss for batch 2 is 0.003235539165419035\n",
      "train loss for batch 3 is 0.0037538492922004254\n",
      "train loss for batch 4 is 0.002215766115658078\n",
      "train loss for batch 5 is 0.003046070973092824\n",
      "train loss for batch 6 is 0.006277259888354088\n",
      "train loss for batch 7 is 0.003342826234820439\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 386 of 400\n",
      "train loss for batch 1 is 0.003893230148251952\n",
      "train loss for batch 2 is 0.0054621985793731976\n",
      "train loss for batch 3 is 0.002751858345807193\n",
      "train loss for batch 4 is 0.0039214002465008295\n",
      "train loss for batch 5 is 0.0044178921344602915\n",
      "train loss for batch 6 is 0.00463257762878136\n",
      "train loss for batch 7 is 0.007635745258955189\n",
      "--- RMSE = 0.48 ---\n",
      "\n",
      " Epoch 387 of 400\n",
      "train loss for batch 1 is 0.00206025126764951\n",
      "train loss for batch 2 is 0.006619790247862914\n",
      "train loss for batch 3 is 0.005186508294796532\n",
      "train loss for batch 4 is 0.005305240822138865\n",
      "train loss for batch 5 is 0.005728327909905925\n",
      "train loss for batch 6 is 0.0031992626092434114\n",
      "train loss for batch 7 is 0.006159851499753927\n",
      "--- RMSE = 0.81 ---\n",
      "\n",
      " Epoch 388 of 400\n",
      "train loss for batch 1 is 0.004471608128355927\n",
      "train loss for batch 2 is 0.006675274378148065\n",
      "train loss for batch 3 is 0.009578438831469437\n",
      "train loss for batch 4 is 0.0072359593712710436\n",
      "train loss for batch 5 is 0.008477580794262115\n",
      "train loss for batch 6 is 0.010465063066876524\n",
      "train loss for batch 7 is 0.010333361174556063\n",
      "--- RMSE = 1.10 ---\n",
      "\n",
      " Epoch 389 of 400\n",
      "train loss for batch 1 is 0.01314798226489069\n",
      "train loss for batch 2 is 0.007042444142087972\n",
      "train loss for batch 3 is 0.014365420642215332\n",
      "train loss for batch 4 is 0.007925809581951047\n",
      "train loss for batch 5 is 0.0185785741226881\n",
      "train loss for batch 6 is 0.007044320959400049\n",
      "train loss for batch 7 is 0.013577116758551144\n",
      "--- RMSE = 0.63 ---\n",
      "\n",
      " Epoch 390 of 400\n",
      "train loss for batch 1 is 0.005742600583175621\n",
      "train loss for batch 2 is 0.009907758826646611\n",
      "train loss for batch 3 is 0.004481990878701502\n",
      "train loss for batch 4 is 0.006771600934779048\n",
      "train loss for batch 5 is 0.004530352134543423\n",
      "train loss for batch 6 is 0.007840265770648166\n",
      "train loss for batch 7 is 0.0058383466159235824\n",
      "--- RMSE = 0.68 ---\n",
      "\n",
      " Epoch 391 of 400\n",
      "train loss for batch 1 is 0.0030681861119968225\n",
      "train loss for batch 2 is 0.00606456143274735\n",
      "train loss for batch 3 is 0.004134180174863994\n",
      "train loss for batch 4 is 0.003981705768481133\n",
      "train loss for batch 5 is 0.0030533278243257307\n",
      "train loss for batch 6 is 0.004354788680453851\n",
      "train loss for batch 7 is 0.0070003894493750225\n",
      "--- RMSE = 0.51 ---\n",
      "\n",
      " Epoch 392 of 400\n",
      "train loss for batch 1 is 0.0036426349442784073\n",
      "train loss for batch 2 is 0.004809352696823432\n",
      "train loss for batch 3 is 0.003338515425773786\n",
      "train loss for batch 4 is 0.0038662708142713133\n",
      "train loss for batch 5 is 0.004452107398648539\n",
      "train loss for batch 6 is 0.0038788061818834387\n",
      "train loss for batch 7 is 0.002415020466680207\n",
      "--- RMSE = 0.66 ---\n",
      "\n",
      " Epoch 393 of 400\n",
      "train loss for batch 1 is 0.0036667440127855122\n",
      "train loss for batch 2 is 0.0037598719323516995\n",
      "train loss for batch 3 is 0.004008577628254106\n",
      "train loss for batch 4 is 0.0037832655184664465\n",
      "train loss for batch 5 is 0.0021054725829511207\n",
      "train loss for batch 6 is 0.0033104985450900337\n",
      "train loss for batch 7 is 0.007491139190930572\n",
      "--- RMSE = 0.99 ---\n",
      "\n",
      " Epoch 394 of 400\n",
      "train loss for batch 1 is 0.009357314452387615\n",
      "train loss for batch 2 is 0.010934976885378455\n",
      "train loss for batch 3 is 0.012139745182743718\n",
      "train loss for batch 4 is 0.011791010354509707\n",
      "train loss for batch 5 is 0.011946063872502988\n",
      "train loss for batch 6 is 0.013665699800202746\n",
      "train loss for batch 7 is 0.00853495476108163\n",
      "--- RMSE = 1.42 ---\n",
      "\n",
      " Epoch 395 of 400\n",
      "train loss for batch 1 is 0.01971977873289292\n",
      "train loss for batch 2 is 0.004854245205053729\n",
      "train loss for batch 3 is 0.02371235182582099\n",
      "train loss for batch 4 is 0.0026157303193205905\n",
      "train loss for batch 5 is 0.021452039392650474\n",
      "train loss for batch 6 is 0.005360140796693016\n",
      "train loss for batch 7 is 0.018224278566030965\n",
      "--- RMSE = 0.97 ---\n",
      "\n",
      " Epoch 396 of 400\n",
      "train loss for batch 1 is 0.007973990643081188\n",
      "train loss for batch 2 is 0.026306797884626882\n",
      "train loss for batch 3 is 0.006679268780034216\n",
      "train loss for batch 4 is 0.023146315639409495\n",
      "train loss for batch 5 is 0.008895455688947933\n",
      "train loss for batch 6 is 0.03038894679136479\n",
      "train loss for batch 7 is 0.005848619320046413\n",
      "--- RMSE = 1.34 ---\n",
      "\n",
      " Epoch 397 of 400\n",
      "train loss for batch 1 is 0.0207014900432364\n",
      "train loss for batch 2 is 0.012166955044859202\n",
      "train loss for batch 3 is 0.022305174325114082\n",
      "train loss for batch 4 is 0.008266808852585752\n",
      "train loss for batch 5 is 0.016375878530527358\n",
      "train loss for batch 6 is 0.006695188236916643\n",
      "train loss for batch 7 is 0.014120780923638873\n",
      "--- RMSE = 1.01 ---\n",
      "\n",
      " Epoch 398 of 400\n",
      "train loss for batch 1 is 0.008570585710893783\n",
      "train loss for batch 2 is 0.015318853731817518\n",
      "train loss for batch 3 is 0.010985511204362254\n",
      "train loss for batch 4 is 0.011946872674192775\n",
      "train loss for batch 5 is 0.011540773179889679\n",
      "train loss for batch 6 is 0.024407654490001013\n",
      "train loss for batch 7 is 0.006020063336027869\n",
      "--- RMSE = 0.82 ---\n",
      "\n",
      " Epoch 399 of 400\n",
      "train loss for batch 1 is 0.006145162525558024\n",
      "train loss for batch 2 is 0.011668195630344057\n",
      "train loss for batch 3 is 0.01334334623008217\n",
      "train loss for batch 4 is 0.00681159203559274\n",
      "train loss for batch 5 is 0.009514180210853403\n",
      "train loss for batch 6 is 0.00919396518802765\n",
      "train loss for batch 7 is 0.007388635931711804\n",
      "--- RMSE = 1.06 ---\n",
      "\n",
      " Epoch 400 of 400\n",
      "train loss for batch 1 is 0.013343486094590164\n",
      "train loss for batch 2 is 0.006848329940285981\n",
      "train loss for batch 3 is 0.010072055657594363\n",
      "train loss for batch 4 is 0.010497534844844796\n",
      "train loss for batch 5 is 0.0052274663488459105\n",
      "train loss for batch 6 is 0.008114306082786425\n",
      "train loss for batch 7 is 0.007954193882665208\n",
      "--- RMSE = 0.76 ---\n",
      "\n",
      " Training process has finished.\n",
      "Final training MSE = 0.010\n",
      "Final test RMSE = 0.76\n",
      "\n",
      " --- Finding uncertainty estimates --- \n",
      "\n",
      "Time left = 3061.3s\n",
      "Finding values: 14.29%\n",
      "Time left = 2693.9s\n",
      "Finding values: 27.27%\n",
      "Time left = 2285.7s\n",
      "Finding values: 40.26%\n",
      "Time left = 1877.6s\n",
      "Finding values: 53.25%\n",
      "Time left = 1469.4s\n",
      "Finding values: 66.23%\n",
      "Time left = 1061.2s\n",
      "Finding values: 79.22%\n",
      "Time left = 653.1s\n",
      "Finding values: 92.21%\n",
      "Time left = 244.9s\n",
      "Finished! \n",
      "\n",
      "Number of zero values uncertainty array: 0\n",
      "Number of negative values for full rank: 0\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "--- Test NLL = 3.11 ---\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "--- Test NLL = 2.69 ---\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "--- Test NLL = 2.44 ---\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "--- Test NLL = 2.36 ---\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "--- Test NLL = 2.26 ---\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "--- Test NLL = 2.17 ---\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "--- Test NLL = 2.20 ---\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "--- Test NLL = 2.01 ---\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "--- Test NLL = 2.17 ---\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "--- Test NLL = 2.02 ---\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "--- Test NLL = 1.79 ---\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "--- Test NLL = 1.74 ---\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "--- Test NLL = 1.62 ---\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "--- Test NLL = 1.59 ---\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "--- Test NLL = 1.50 ---\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "--- Test NLL = 1.78 ---\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "--- Test NLL = 1.64 ---\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "--- Test NLL = 1.72 ---\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "--- Test NLL = 1.74 ---\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "--- Test NLL = 1.65 ---\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "--- Test NLL = 1.47 ---\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "--- Test NLL = 1.43 ---\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "--- Test NLL = 1.87 ---\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "--- Test NLL = 1.70 ---\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "--- Test NLL = 2.03 ---\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "--- Test NLL = 1.73 ---\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "--- Test NLL = 1.40 ---\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "--- Test NLL = 1.62 ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "--- Test NLL = 1.57 ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "--- Test NLL = 1.46 ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "--- Test NLL = 1.55 ---\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "--- Test NLL = 1.61 ---\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "--- Test NLL = 0.22 ---\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "--- Test NLL = 0.21 ---\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "--- Test NLL = 0.17 ---\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "--- Test NLL = 0.18 ---\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "--- Test NLL = 3.27 ---\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "--- Test NLL = 2.92 ---\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "--- Test NLL = 2.49 ---\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "--- Test NLL = 2.27 ---\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "--- Test NLL = 2.08 ---\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "--- Test NLL = 2.09 ---\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "--- Test NLL = 2.02 ---\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "--- Test NLL = 1.93 ---\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "--- Test NLL = 2.09 ---\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "--- Test NLL = 1.93 ---\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "--- Test NLL = 2.01 ---\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "--- Test NLL = 1.94 ---\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "--- Test NLL = 1.70 ---\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "--- Test NLL = 1.57 ---\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "--- Test NLL = 1.55 ---\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "--- Test NLL = 1.57 ---\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "--- Test NLL = 2.40 ---\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "--- Test NLL = 2.01 ---\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "--- Test NLL = 1.96 ---\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "--- Test NLL = 1.86 ---\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "--- Test NLL = 1.64 ---\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "--- Test NLL = 1.49 ---\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "--- Test NLL = 2.98 ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "--- Test NLL = 1.73 ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "--- Test NLL = 1.67 ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "--- Test NLL = 1.63 ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "--- Test NLL = 1.43 ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "--- Test NLL = 1.55 ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "--- Test NLL = 1.62 ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "--- Test NLL = 1.40 ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "--- Test NLL = 1.76 ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "--- Test NLL = 1.57 ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "--- Test NLL = 1.51 ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "--- Test NLL = 1.56 ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "--- Test NLL = 1.56 ---\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "--- Test NLL = 2.28 ---\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "--- Test NLL = 1.50 ---\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "--- Test NLL = 3.21 ---\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "--- Test NLL = 2.77 ---\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "--- Test NLL = 2.28 ---\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "--- Test NLL = 2.47 ---\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "--- Test NLL = 2.09 ---\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "--- Test NLL = 2.02 ---\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "--- Test NLL = 1.92 ---\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "--- Test NLL = 1.93 ---\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "--- Test NLL = 1.87 ---\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "--- Test NLL = 1.74 ---\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "--- Test NLL = 1.69 ---\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "--- Test NLL = 1.91 ---\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "--- Test NLL = 1.85 ---\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "--- Test NLL = 1.60 ---\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "--- Test NLL = 1.83 ---\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "--- Test NLL = 1.73 ---\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "--- Test NLL = 1.85 ---\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "--- Test NLL = 1.66 ---\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "--- Test NLL = 1.50 ---\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "--- Test NLL = 1.96 ---\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "--- Test NLL = 1.65 ---\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "--- Test NLL = 1.58 ---\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "--- Test NLL = 1.48 ---\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "--- Test NLL = 4.13 ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "--- Test NLL = 1.83 ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "--- Test NLL = 1.87 ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "--- Test NLL = 3.86 ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "--- Test NLL = 2.45 ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "--- Test NLL = 2.45 ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "--- Test NLL = 2.30 ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "--- Test NLL = 2.12 ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "--- Test NLL = 1.88 ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "--- Test NLL = 1.70 ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "--- Test NLL = 1.82 ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "--- Test NLL = 1.78 ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "--- Test NLL = 1.98 ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "--- Test NLL = 1.89 ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "--- Test NLL = 1.78 ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "--- Test NLL = 1.53 ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "--- Test NLL = 1.43 ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "--- Test NLL = 1.46 ---\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "--- Test NLL = 1.55 ---\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "--- Test NLL = 2.35 ---\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "--- Test NLL = 3.10 ---\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "--- Test NLL = 2.60 ---\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "--- Test NLL = 2.26 ---\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "--- Test NLL = 2.00 ---\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "--- Test NLL = 2.02 ---\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "--- Test NLL = 2.12 ---\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "--- Test NLL = 1.98 ---\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "--- Test NLL = 2.06 ---\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "--- Test NLL = 1.87 ---\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "--- Test NLL = 1.80 ---\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "--- Test NLL = 1.78 ---\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "--- Test NLL = 1.79 ---\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "--- Test NLL = 1.70 ---\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "--- Test NLL = 1.66 ---\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "--- Test NLL = 1.49 ---\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "--- Test NLL = 1.59 ---\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "--- Test NLL = 1.89 ---\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "--- Test NLL = 1.77 ---\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "--- Test NLL = 1.69 ---\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "--- Test NLL = 1.51 ---\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "--- Test NLL = 1.47 ---\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "--- Test NLL = 1.51 ---\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "--- Test NLL = 1.48 ---\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "--- Test NLL = 1.95 ---\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "--- Test NLL = 1.49 ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "--- Test NLL = 1.53 ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "--- Test NLL = 1.46 ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "--- Test NLL = 1.43 ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "--- Test NLL = 2.34 ---\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "--- Test NLL = 1.62 ---\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "--- Test NLL = 1.70 ---\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "--- Test NLL = 1.46 ---\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "--- Test NLL = 1.83 ---\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "--- Test NLL = 1.51 ---\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "--- Test NLL = 0.22 ---\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "--- Test NLL = 0.22 ---\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "--- Test NLL = 0.20 ---\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "--- Test NLL = 0.20 ---\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "--- Test NLL = 0.16 ---\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "--- Test NLL = 0.18 ---\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "--- Test NLL = 0.20 ---\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "--- Test NLL = 3.19 ---\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "--- Test NLL = 2.75 ---\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "--- Test NLL = 2.33 ---\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "--- Test NLL = 2.26 ---\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "--- Test NLL = 2.21 ---\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "--- Test NLL = 1.99 ---\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "--- Test NLL = 1.99 ---\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "--- Test NLL = 1.95 ---\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "--- Test NLL = 1.83 ---\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "--- Test NLL = 1.75 ---\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "--- Test NLL = 1.84 ---\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "--- Test NLL = 1.60 ---\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "--- Test NLL = 1.75 ---\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "--- Test NLL = 1.73 ---\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "--- Test NLL = 1.73 ---\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "--- Test NLL = 1.61 ---\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "--- Test NLL = 1.61 ---\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "--- Test NLL = 1.41 ---\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "--- Test NLL = 1.53 ---\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "--- Test NLL = 1.40 ---\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "--- Test NLL = 1.43 ---\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "--- Test NLL = 1.53 ---\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "--- Test NLL = 1.34 ---\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "--- Test NLL = 1.36 ---\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "--- Test NLL = 1.52 ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "--- Test NLL = 1.74 ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "--- Test NLL = 1.33 ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "--- Test NLL = 1.60 ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "--- Test NLL = 1.27 ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "--- Test NLL = 1.16 ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "--- Test NLL = 1.37 ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "--- Test NLL = 1.55 ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "--- Test NLL = 1.24 ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "--- Test NLL = 1.48 ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "--- Test NLL = 1.39 ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "--- Test NLL = 1.18 ---\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "--- Test NLL = 1.23 ---\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "--- Test NLL = 1.54 ---\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "--- Test NLL = 1.38 ---\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "--- Test NLL = 1.44 ---\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "--- Test NLL = 1.21 ---\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "--- Test NLL = 1.20 ---\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "--- Test NLL = 1.35 ---\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "--- Test NLL = 1.31 ---\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "--- Test NLL = 1.68 ---\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "--- Test NLL = 1.15 ---\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "--- Test NLL = 1.40 ---\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "--- Test NLL = 1.07 ---\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "--- Test NLL = 1.26 ---\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "--- Test NLL = 1.29 ---\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "--- Test NLL = 1.30 ---\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "--- Test NLL = 1.06 ---\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "--- Test NLL = 1.14 ---\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "--- Test NLL = 1.48 ---\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "--- Test NLL = 1.13 ---\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "--- Test NLL = 1.32 ---\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "--- Test NLL = 1.45 ---\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "--- Test NLL = 1.22 ---\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "--- Test NLL = 1.17 ---\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "--- Test NLL = 1.12 ---\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "--- Test NLL = 1.01 ---\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "--- Test NLL = 1.28 ---\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "--- Test NLL = 1.58 ---\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "--- Test NLL = 0.90 ---\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "--- Test NLL = 1.04 ---\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "--- Test NLL = 0.99 ---\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "--- Test NLL = 0.96 ---\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "--- Test NLL = 1.02 ---\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "--- Test NLL = 1.09 ---\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "--- Test NLL = 1.64 ---\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "--- Test NLL = 1.95 ---\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "--- Test NLL = 0.88 ---\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "--- Test NLL = 0.93 ---\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "--- Test NLL = 0.95 ---\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "--- Test NLL = 1.25 ---\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "--- Test NLL = 0.92 ---\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "--- Test NLL = 0.98 ---\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "--- Test NLL = 0.94 ---\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "--- Test NLL = 1.11 ---\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "--- Test NLL = 0.85 ---\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "--- Test NLL = 1.08 ---\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "--- Test NLL = 1.10 ---\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "--- Test NLL = 0.97 ---\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "--- Test NLL = 0.72 ---\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "--- Test NLL = 0.82 ---\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "--- Test NLL = 0.84 ---\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "--- Test NLL = 1.42 ---\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "--- Test NLL = 0.75 ---\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "--- Test NLL = 1.00 ---\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "--- Test NLL = 0.68 ---\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "--- Test NLL = 0.74 ---\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "--- Test NLL = 0.76 ---\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "--- Test NLL = 0.62 ---\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "--- Test NLL = 0.83 ---\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "--- Test NLL = 0.79 ---\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "--- Test NLL = 0.89 ---\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "--- Test NLL = 1.05 ---\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "--- Test NLL = 0.78 ---\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "--- Test NLL = 0.77 ---\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "--- Test NLL = 0.70 ---\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "--- Test NLL = 0.71 ---\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "--- Test NLL = 0.59 ---\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "--- Test NLL = 0.73 ---\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "--- Test NLL = 0.56 ---\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "--- Test NLL = 0.35 ---\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "--- Test NLL = 0.64 ---\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "--- Test NLL = 0.19 ---\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "--- Test NLL = 0.55 ---\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "--- Test NLL = 0.30 ---\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "--- Test NLL = 1.19 ---\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "--- Test NLL = 1.03 ---\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "--- Test NLL = 0.60 ---\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "--- Test NLL = 0.86 ---\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "--- Test NLL = 0.53 ---\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "--- Test NLL = 0.48 ---\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "--- Test NLL = 0.63 ---\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "--- Test NLL = 0.67 ---\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "--- Test NLL = 0.66 ---\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "--- Test NLL = 0.23 ---\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "--- Test NLL = 0.54 ---\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "--- Test NLL = 0.52 ---\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "--- Test NLL = 0.81 ---\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "--- Test NLL = 0.61 ---\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "--- Test NLL = 0.44 ---\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "--- Test NLL = 0.57 ---\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "--- Test NLL = 0.91 ---\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "--- Test NLL = 0.28 ---\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "--- Test NLL = 0.41 ---\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "--- Test NLL = 0.34 ---\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "--- Test NLL = 0.58 ---\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "--- Test NLL = 0.36 ---\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "--- Test NLL = 0.39 ---\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "--- Test NLL = 0.45 ---\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "--- Test NLL = 0.38 ---\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "--- Test NLL = 0.32 ---\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "--- Test NLL = 0.47 ---\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "--- Test NLL = 0.25 ---\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "--- Test NLL = 0.42 ---\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "--- Test NLL = 0.31 ---\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "--- Test NLL = 0.24 ---\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "--- Test NLL = 0.40 ---\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "--- Test NLL = 0.49 ---\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "--- Test NLL = 0.27 ---\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "--- Test NLL = 0.26 ---\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "--- Test NLL = 0.29 ---\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "--- Test NLL = 1.83 ---\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "--- Test NLL = 0.65 ---\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "--- Test NLL = 0.87 ---\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "--- Test NLL = 0.80 ---\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "--- Test NLL = 0.43 ---\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "--- Test NLL = 0.33 ---\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "--- Test NLL = 0.50 ---\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "--- Test NLL = 0.46 ---\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "--- Test NLL = 0.69 ---\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "--- Test NLL = 0.51 ---\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "--- Test NLL = 0.37 ---\n",
      "Done!\n",
      "[0.361 0.423 0.344 0.365 0.397]\n",
      "[0.652 0.396 0.313 0.43  0.373]\n",
      "\n",
      " --- Plotting Results --- \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHLCAYAAADSuXIVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABA3UlEQVR4nO3dd3QUZd/G8WsTSCGkEFoIhBaqNDUU6UioIkV6EUIT0YBAQIXHAggaigIWiqhPUARpgiA89GoBBSQgKFV6VYSEBEkgO+8fnOzrkkIIGzYTvp9z5sDeMzvzm9ndybUz98xaDMMwBAAAYEIuzi4AAAAgswgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgy2VjJkiXVu3dvZ5eR402ePFmlS5eWq6urHn30UWeXg/swZswYWSwW/fXXX3edls+XYz2s27N3794qWbKks8t4qBFkHpA5c+bIYrFo165dqY5v1KiRKleufN/L+d///qcxY8bc93weFuvWrdMrr7yiunXrKioqSu+8806a0/bu3VsWi8U25M2bV6VLl1bHjh319ddfy2q1PsDKMyf5D31aw4ULF5xdIpzA2fuNGTNmaM6cOU5b/t2cO3dOY8aMUXR0tLNLsTN//nxNmzbN2WU4XS5nF4C0HTp0SC4u95Y1//e//2n69OmEmQzatGmTXFxc9Nlnn8nNze2u07u7u+vTTz+VJP3zzz86efKkvv32W3Xs2FGNGjXS8uXL5ePjk9Vl37eZM2cqb968Kdr9/PwefDFwOkfsNzKzv0o2Y8YMFShQINse0Tl37pzGjh2rkiVLpjhq+8knnzjtS8z8+fO1f/9+DR061CnLzy4IMtmYu7u7s0u4Z/Hx8fLy8nJ2GRl26dIleXp6ZijESFKuXLn07LPP2rWNHz9eEyZM0KhRo/Tcc89p4cKFWVGqQ3Xs2FEFChRwdhlwMkd+Xs24v3KE3LlzO7uEhx6nlrKxO88537x5U2PHjlXZsmXl4eGh/Pnzq169elq/fr2k26c+pk+fLkl2pwuSxcfHa/jw4QoKCpK7u7vKly+vd999V3f+APo///yjl156SQUKFJC3t7fatGmjs2fPymKx2H1jSz5N8dtvv6l79+7Kly+f6tWrJ0nat2+fevfurdKlS8vDw0MBAQHq27evLl++bLes5HkcPnxYzz77rHx9fVWwYEG98cYbMgxDp0+fVtu2beXj46OAgAC99957Gdp2t27d0rhx4xQcHCx3d3eVLFlS//nPf5SQkGCbxmKxKCoqSvHx8bZtldnD2yNHjlSzZs20ePFiHT582G7c6tWrVb9+fXl5ecnb21utWrXSgQMHUszj4MGD6tixo/z9/eXh4aHq1atrxYoVdtMkn6Lctm2bnn/+eeXPn18+Pj7q1auXrly5kqnaU7NlyxZZLBYtWrRIb7/9tooVKyYPDw+Fhobq6NGjdtMeOXJEHTp0UEBAgDw8PFSsWDF17dpVMTExdtN9+eWXCgkJkaenp/z9/dW1a1edPn3abprkU6z79u1Tw4YNlSdPHpUpU0ZLliyRJG3dulW1atWSp6enypcvrw0bNqRa/19//aXOnTvLx8dH+fPn15AhQ3Tjxo27rvfVq1c1dOhQ22ekTJkymjhxYoa+cd/5+Uh25+c4+TX84YcfFBERoYIFC8rLy0vPPPOM/vzzzxTPX716tRo2bChvb2/5+PioRo0amj9/vt00P/30k1q0aCFfX1/lyZNHDRs21A8//GA3TVqf17vtN959913VqVNH+fPnl6enp0JCQmyvhyPWs2TJkjpw4IC2bt1qW3ajRo30xx9/yGKxaOrUqSmW9eOPP8piseirr75KMe7fEhISNHr0aJUpU0bu7u4KCgrSK6+8YrcfkKT169erXr168vPzU968eVW+fHn95z//kXT7s1CjRg1JUp8+fVLsK+7sI3PixAlZLBa9++67mj59ukqXLq08efKoWbNmOn36tAzD0Lhx41SsWDF5enqqbdu2+vvvv+3qWb58uVq1aqXAwEC5u7srODhY48aNU1JSkm2aRo0aadWqVTp58qStpn/X4Yh1NwuOyDxgMTExqXZEvHnz5l2fO2bMGEVGRqp///6qWbOmYmNjtWvXLv3yyy9q2rSpnn/+eZ07d07r16/X3Llz7Z5rGIbatGmjzZs3q1+/fnr00Ue1du1avfzyyzp79qzdzqJ3795atGiRevbsqSeeeEJbt25Vq1at0qyrU6dOKlu2rN555x1bKFq/fr3++OMP9enTRwEBATpw4IBmz56tAwcOaMeOHXY7Sknq0qWLKlasqAkTJmjVqlUaP368/P399fHHH6tx48aaOHGi5s2bpxEjRqhGjRpq0KBButuqf//++vzzz9WxY0cNHz5cP/30kyIjI/X7779r2bJlkqS5c+dq9uzZ+vnnn22ni+rUqXPX1yEtPXv21Lp167R+/XqVK1fOtoywsDA1b95cEydO1PXr1zVz5kzVq1dPe/bsse14Dhw4oLp166po0aIaOXKkvLy8tGjRIrVr105ff/21nnnmGbtlDRo0SH5+fhozZowOHTqkmTNn6uTJk7YAcjd37jil20eb7jy1NGHCBLm4uGjEiBGKiYnRpEmT1KNHD/3000+SpMTERDVv3lwJCQkaPHiwAgICdPbsWa1cuVJXr16Vr6+vJOntt9/WG2+8oc6dO6t///76888/9eGHH6pBgwbas2eP3XKvXLmip59+Wl27dlWnTp00c+ZMde3aVfPmzdPQoUM1cOBAde/eXZMnT1bHjh11+vRpeXt729XduXNnlSxZUpGRkdqxY4c++OADXblyRV988UWa2+T69etq2LChzp49q+eff17FixfXjz/+qFGjRun8+fMO74swePBg5cuXT6NHj9aJEyc0bdo0DRo0yO6I3pw5c9S3b19VqlRJo0aNkp+fn/bs2aM1a9aoe/fukm6fHm3ZsqVCQkI0evRoubi4KCoqSo0bN9Z3332nmjVr2i33zs/rY489luZ+Q5Lef/99tWnTRj169FBiYqIWLFigTp06aeXKlenuFzK6ntOmTdPgwYOVN29evfbaa5KkwoULq3Tp0qpbt67mzZunYcOG2c1z3rx58vb2Vtu2bdNcrtVqVZs2bfT9999rwIABqlixon799VdNnTpVhw8f1jfffCPp9mfv6aefVtWqVfXWW2/J3d1dR48etQXBihUr6q233tKbb76pAQMGqH79+pLuvq+YN2+eEhMTNXjwYP3999+aNGmSOnfurMaNG2vLli169dVXdfToUX344YcaMWKE/vvf/9qeO2fOHOXNm1cRERHKmzevNm3apDfffFOxsbGaPHmyJOm1115TTEyMzpw5Y9t/J58udtS6m4aBByIqKsqQlO5QqVIlu+eUKFHCCAsLsz2uVq2a0apVq3SXEx4ebqT2sn7zzTeGJGP8+PF27R07djQsFotx9OhRwzAMY/fu3YYkY+jQoXbT9e7d25BkjB492tY2evRoQ5LRrVu3FMu7fv16iravvvrKkGRs27YtxTwGDBhga7t165ZRrFgxw2KxGBMmTLC1X7lyxfD09LTbJqmJjo42JBn9+/e3ax8xYoQhydi0aZOtLSwszPDy8kp3fhmdds+ePYYkY9iwYYZhGMa1a9cMPz8/47nnnrOb7sKFC4avr69de2hoqFGlShXjxo0btjar1WrUqVPHKFu2rK0t+X0UEhJiJCYm2tonTZpkSDKWL1+e7jokb+/UhvLly9um27x5syHJqFixopGQkGBrf//99w1Jxq+//mq3zosXL05zmSdOnDBcXV2Nt99+2679119/NXLlymXX3rBhQ0OSMX/+fFvbwYMHDUmGi4uLsWPHDlv72rVrDUlGVFRUivVr06aN3bJefPFFQ5Kxd+9eW9udn69x48YZXl5exuHDh+2eO3LkSMPV1dU4depUmutoGEaKz0day0l+DZs0aWJYrVZb+7BhwwxXV1fj6tWrhmEYxtWrVw1vb2+jVq1axj///GM3z+TnWa1Wo2zZskbz5s3t5nX9+nWjVKlSRtOmTVNsm9Q+r2ntN5Ln9W+JiYlG5cqVjcaNGztkPQ3DMCpVqmQ0bNgwxbI//vhjQ5Lx+++/2y2/QIECd90PzJ0713BxcTG+++47u/ZZs2YZkowffvjBMAzDmDp1qiHJ+PPPP9Oc186dO1O815KFhYUZJUqUsD0+fvy4IckoWLCg3TqOGjXKkGRUq1bNuHnzpq29W7duhpubm91nP7X95/PPP2/kyZPHbrpWrVrZLTsr1t0MOLX0gE2fPl3r169PMVStWvWuz/Xz89OBAwd05MiRe17u//73P7m6uuqll16yax8+fLgMw9Dq1aslSWvWrJEkvfjii3bTDR48OM15Dxw4MEWbp6en7f83btzQX3/9pSeeeEKS9Msvv6SYvn///rb/u7q6qnr16jIMQ/369bO1+/n5qXz58vrjjz/SrEW6va6SFBERYdc+fPhwSdKqVavSfX5mJX8bunbtmqTbR6WuXr2qbt266a+//rINrq6uqlWrljZv3izp9tGRTZs2qXPnzrp27ZptusuXL6t58+Y6cuSIzp49a7esAQMG2J2bf+GFF5QrVy7but/N119/neI9GBUVlWK6Pn362PUfSv42mvwaJB9xWbt2ra5fv57qspYuXSqr1arOnTvbbYeAgACVLVvWth3+vR27du1qe1y+fHn5+fmpYsWKqlWrlq09+f+pvR/Cw8PtHie/f9PbPosXL1b9+vWVL18+uzqbNGmipKQkbdu2Lc3nZsaAAQPsjp7Vr19fSUlJOnnypKTb759r165p5MiR8vDwsHtu8vOio6N15MgRde/eXZcvX7bVHB8fr9DQUG3bti3FabHUPq/p+fdn+cqVK4qJiVH9+vVT/RxnZj3T07lzZ3l4eGjevHm2trVr1+qvv/5K0VftTosXL1bFihVVoUIFu9ezcePGkmR73yUfDVy+fLlDO+126tTJ9vmQ/v/9+uyzzypXrlx27YmJiXaf8X9v8+R9Qv369XX9+nUdPHjwrst29ro/aJxaesBq1qyp6tWrp2hP3nmm56233lLbtm1Vrlw5Va5cWS1atFDPnj0zFIJOnjypwMDAFIfgK1asaBuf/K+Li4tKlSplN12ZMmXSnPed00q3/ziPHTtWCxYs0KVLl+zG3dl3QpKKFy9u99jX11ceHh4pOqT6+vqm6Gdzp+R1uLPmgIAA+fn5ZWgHmhlxcXGSZNvGyYEzeedxp+Srm44ePSrDMPTGG2/ojTfeSHXaS5cuqWjRorbHZcuWtRufN29eFSlSRCdOnMhQrQ0aNMhQZ987X5d8+fJJkq0/TqlSpRQREaEpU6Zo3rx5ql+/vtq0aWPr7yTd3g6GYaSoOdmdnSWLFSuW4vSYr6+vgoKCUrT9u5Z/u3NZwcHBcnFxSXf7HDlyRPv27VPBggVTHX/n+/h+3W3bHjt2TJLSvS1D8nssLCwszWliYmJs85ZS/7ymZ+XKlRo/fryio6NT9DHLiLutZ3r8/PzUunVrzZ8/X+PGjZN0+5RN0aJF0/xcJTty5Ih+//33u76eXbp00aeffqr+/ftr5MiRCg0NVfv27dWxY8dMX4Ulpb5Pk5Sh9/GBAwf0+uuva9OmTYqNjbWbPrX9552cve4PGkHGRBo0aKBjx45p+fLlWrdunT799FNNnTpVs2bNsjui8aD9+9tDss6dO+vHH3/Uyy+/rEcffVR58+aV1WpVixYtUk3+rq6uGWqTlKJzcloyuqN1lP3790v6/9CXvJ5z585VQEBAiumTv5UlTzdixAg1b9481XmnFySzUkZeg/fee0+9e/e2vS9feuklW9+UYsWKyWq1ymKxaPXq1anO787LwNNa5v28HzLyXrBarWratKleeeWVVMcn93u6V//uoPlv9/v+lv7/vTN58uQ0b+Z45/ZN7fOalu+++05t2rRRgwYNNGPGDBUpUkS5c+dWVFRUig7Habnf9ezVq5cWL16sH3/8UVWqVNGKFSv04osv3vUPrdVqVZUqVTRlypRUxycHCk9PT23btk2bN2/WqlWrtGbNGi1cuFCNGzfWunXr0qz/bjL7Pr569aoaNmwoHx8fvfXWWwoODpaHh4d++eUXvfrqqxk6cuLsdX/QCDIm4+/vrz59+qhPnz6Ki4tTgwYNNGbMGFuQSWuHXaJECW3YsEHXrl2zOyqTfJiyRIkStn+tVquOHz9u9632zitV0nPlyhVt3LhRY8eO1Ztvvmlrz8wpscxIXocjR47YjjhJ0sWLF3X16lXbujra3LlzZbFY1LRpU0m3jwJIUqFChdSkSZM0n1e6dGlJt49MpDfdvx05ckRPPvmk7XFcXJzOnz+vp556KrPl35cqVaqoSpUqev311/Xjjz+qbt26mjVrlsaPH6/g4GAZhqFSpUplOgzcqyNHjtgdeTh69KisVmu6d2ANDg5WXFxchl+DO+XLl09Xr161a0tMTNT58+czNb/k98/+/fvTDLLJ0/j4+GS6bint/cbXX38tDw8PrV271u7y6tROQ96P9IJmixYtVLBgQc2bN0+1atXS9evX1bNnz7vOMzg4WHv37lVoaOhdg6yLi4tCQ0MVGhqqKVOm6J133tFrr72mzZs3q0mTJg/0S9GWLVt0+fJlLV261O6ihuPHj6eYNq26HLnuZmCeY0dIcUolb968KlOmjN3h3uR7Qty5Q33qqaeUlJSkjz76yK596tSpslgsatmypSTZjgjMmDHDbroPP/www3Ump/g7v3E9qDtQJv8xv3N5yd9OMnKlxb2aMGGC1q1bpy5dutgCYPPmzeXj46N33nkn1avSki9BLVSokBo1aqSPP/441T96qV2SO3v2bLt5zpw5U7du3bK9jg9KbGysbt26ZddWpUoVubi42N6X7du3l6urq8aOHZviPWEYxl1PFWZG8uXEyZLfv+ltn86dO2v79u1au3ZtinFXr15NsZ53Cg4OTtGPZvbs2WkekbmbZs2aydvbW5GRkSkuHU/ejiEhIQoODta7775rO7X5b6m9d1KT1n7D1dVVFovFbh1OnDhhu+rFUby8vFIsO1muXLnUrVs3LVq0SHPmzFGVKlUydDq9c+fOOnv2rD755JMU4/755x/Fx8dLSv0KvuSjW8nv4bS2T1ZIbf+ZmJiYYp+cXFdqp5ocue5mwBEZE3nkkUfUqFEjhYSEyN/fX7t27dKSJUs0aNAg2zQhISGSpJdeeknNmzeXq6urunbtqtatW+vJJ5/Ua6+9phMnTqhatWpat26dli9frqFDh9q+2YWEhKhDhw6aNm2aLl++bLv8OvneKBn5ZuLj46MGDRpo0qRJunnzpooWLap169al+o0iK1SrVk1hYWGaPXu27TDtzz//rM8//1zt2rWzO5Jxr27duqUvv/xS0u1OzCdPntSKFSu0b98+Pfnkk5o9e7ZtWh8fH82cOVM9e/bU448/rq5du6pgwYI6deqUVq1apbp169qC5fTp01WvXj1VqVJFzz33nEqXLq2LFy9q+/btOnPmjPbu3WtXR2JiokJDQ9W5c2cdOnRIM2bMUL169dSmTZsMrceSJUtSvbNv06ZNVbhw4Qxvj02bNmnQoEHq1KmTypUrp1u3bmnu3LlydXVVhw4dJN3+Az9+/HiNGjVKJ06cULt27eTt7a3jx49r2bJlGjBggEaMGJHhZWbE8ePH1aZNG7Vo0ULbt2/Xl19+qe7du6tatWppPufll1/WihUr9PTTT6t3794KCQlRfHy8fv31Vy1ZskQnTpxIt19R//79NXDgQHXo0EFNmzbV3r17tXbt2kzfeNDHx0dTp05V//79VaNGDdu9X/bu3avr16/r888/l4uLiz799FO1bNlSlSpVUp8+fVS0aFGdPXtWmzdvlo+Pj7799tu7Liut/UarVq00ZcoUtWjRQt27d9elS5c0ffp0lSlTRvv27cvUeqW1/JkzZ2r8+PEqU6aMChUqZNcHplevXvrggw+0efNmTZw4MUPz7NmzpxYtWqSBAwdq8+bNqlu3rpKSknTw4EEtWrRIa9euVfXq1fXWW29p27ZtatWqlUqUKKFLly5pxowZKlasmO2+WMHBwfLz89OsWbPk7e0tLy8v1apV6577G2VEnTp1lC9fPoWFhemll16SxWLR3LlzUz0VFxISooULFyoiIkI1atRQ3rx51bp1a4euuyk88OukHlLJlyLu3Lkz1fENGza86+XX48ePN2rWrGn4+fkZnp6eRoUKFYy3337b7jLcW7duGYMHDzYKFixoWCwWu0sqr127ZgwbNswIDAw0cufObZQtW9aYPHmy3aWRhmEY8fHxRnh4uOHv72/kzZvXaNeunXHo0CFDkt3l0MmXc6Z26d6ZM2eMZ555xvDz8zN8fX2NTp06GefOnUvzEu4755HWpc6pbafU3Lx50xg7dqxRqlQpI3fu3EZQUJAxatQou0sX01tOasLCwuwuV86TJ49RsmRJo0OHDsaSJUuMpKSkVJ+3efNmo3nz5oavr6/h4eFhBAcHG7179zZ27dplN92xY8eMXr16GQEBAUbu3LmNokWLGk8//bSxZMkS2zTJ76OtW7caAwYMMPLly2fkzZvX6NGjh3H58uW7rkN6l19LMjZv3myrWalcVp18aWnyZah//PGH0bdvXyM4ONjw8PAw/P39jSeffNLYsGFDimV//fXXRr169QwvLy/Dy8vLqFChghEeHm4cOnTINk1ar2+JEiVSvfWAJCM8PDzF+v32229Gx44dDW9vbyNfvnzGoEGDUlzCfOfnyzBuf0ZGjRpllClTxnBzczMKFChg1KlTx3j33XftPmepSUpKMl599VWjQIECRp48eYzmzZsbR48eTfOy5Dv3BcnbPPk1SLZixQqjTp06hqenp+Hj42PUrFnT+Oqrr+ym2bNnj9G+fXsjf/78hru7u1GiRAmjc+fOxsaNG1Nsm9Q+r+ntNz777DOjbNmyhru7u1GhQgUjKirKNq/0tue9rOeFCxeMVq1aGd7e3oakVC/FrlSpkuHi4mKcOXMmxbi0JCYmGhMnTjQqVapkuLu7G/ny5TNCQkKMsWPHGjExMYZhGMbGjRuNtm3bGoGBgYabm5sRGBhodOvWLcVl+MuXLzceeeQRI1euXHafgbQuv548eXKq633nZyq17fTDDz8YTzzxhOHp6WkEBgYar7zyiu12A//ebnFxcUb37t0NPz8/Q5JdHY5c9+zOYhj30LMMD63o6Gg99thj+vLLL9WjRw9nl/PQmjNnjvr06aOdO3emevUbkFM99thj8vf318aNG51dCrIZ+sgghX/++SdF27Rp0+Ti4nLXO+oCgKPt2rVL0dHR6tWrl7NLQTZEHxmkMGnSJO3evVtPPvmkcuXKpdWrV2v16tUaMGBAinsgAEBW2b9/v3bv3q333ntPRYoUUZcuXZxdErIhjsgghTp16ujvv//WuHHjNHz4cB0+fFhjxoxJcSUIAGSlJUuWqE+fPrp586a++uqrFHc4BiSJPjIAAMC0OCIDAABMiyADAABMK8d39rVarTp37py8vb0f+G/vAACAzDEMQ9euXVNgYGC6v62V44PMuXPnuNIGAACTOn36tIoVK5bm+BwfZJJ/IPH06dPy8fFxcjUAACAjYmNjFRQUZPdDx6nJ8UEm+XSSj48PQQYAAJO56y94P6A6AAAAHI4gAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATIsgAwAATCuXswsws5IjV6U57sSEVg+wEgAAHk4ckQEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKbl1CAzc+ZMVa1aVT4+PvLx8VHt2rW1evVq2/gbN24oPDxc+fPnV968edWhQwddvHjRiRUDAIDsxKlBplixYpowYYJ2796tXbt2qXHjxmrbtq0OHDggSRo2bJi+/fZbLV68WFu3btW5c+fUvn17Z5YMAACyEYthGIazi/g3f39/TZ48WR07dlTBggU1f/58dezYUZJ08OBBVaxYUdu3b9cTTzyRofnFxsbK19dXMTEx8vHxcWitJUeuSnPciQmtHLosAAAeJhn9+51t+sgkJSVpwYIFio+PV+3atbV7927dvHlTTZo0sU1ToUIFFS9eXNu3b09zPgkJCYqNjbUbAABAzuT0IPPrr78qb968cnd318CBA7Vs2TI98sgjunDhgtzc3OTn52c3feHChXXhwoU05xcZGSlfX1/bEBQUlMVrAAAAnMXpQaZ8+fKKjo7WTz/9pBdeeEFhYWH67bffMj2/UaNGKSYmxjacPn3agdUCAIDsJJezC3Bzc1OZMmUkSSEhIdq5c6fef/99denSRYmJibp69ardUZmLFy8qICAgzfm5u7vL3d09q8sGAADZgNOPyNzJarUqISFBISEhyp07tzZu3Ggbd+jQIZ06dUq1a9d2YoUAACC7cOoRmVGjRqlly5YqXry4rl27pvnz52vLli1au3atfH191a9fP0VERMjf318+Pj4aPHiwateuneErlgAAQM7m1CBz6dIl9erVS+fPn5evr6+qVq2qtWvXqmnTppKkqVOnysXFRR06dFBCQoKaN2+uGTNmOLNkAACQjWS7+8g4GveRAQDAfEx3HxkAAIB7RZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACm5dQgExkZqRo1asjb21uFChVSu3btdOjQIbtpGjVqJIvFYjcMHDjQSRUDAIDsxKlBZuvWrQoPD9eOHTu0fv163bx5U82aNVN8fLzddM8995zOnz9vGyZNmuSkigEAQHaSy5kLX7Nmjd3jOXPmqFChQtq9e7caNGhga8+TJ48CAgIedHkAACCby1Z9ZGJiYiRJ/v7+du3z5s1TgQIFVLlyZY0aNUrXr19Pcx4JCQmKjY21GwAAQM7k1CMy/2a1WjV06FDVrVtXlStXtrV3795dJUqUUGBgoPbt26dXX31Vhw4d0tKlS1OdT2RkpMaOHfugygYAAE5kMQzDcHYRkvTCCy9o9erV+v7771WsWLE0p9u0aZNCQ0N19OhRBQcHpxifkJCghIQE2+PY2FgFBQUpJiZGPj4+Dq255MhVaY47MaGVQ5cFAMDDJDY2Vr6+vnf9+50tjsgMGjRIK1eu1LZt29INMZJUq1YtSUozyLi7u8vd3T1L6gQAANmLU4OMYRgaPHiwli1bpi1btqhUqVJ3fU50dLQkqUiRIllcHQAAyO6cGmTCw8M1f/58LV++XN7e3rpw4YIkydfXV56enjp27Jjmz5+vp556Svnz59e+ffs0bNgwNWjQQFWrVnVm6QAAIBtwapCZOXOmpNs3vfu3qKgo9e7dW25ubtqwYYOmTZum+Ph4BQUFqUOHDnr99dedUC0AAMhunH5qKT1BQUHaunXrA6oGAACYTba6jwwAAMC9IMgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTIsgAAADTcmqQiYyMVI0aNeTt7a1ChQqpXbt2OnTokN00N27cUHh4uPLnz6+8efOqQ4cOunjxopMqBgAA2YlTg8zWrVsVHh6uHTt2aP369bp586aaNWum+Ph42zTDhg3Tt99+q8WLF2vr1q06d+6c2rdv78SqAQBAdpHLmQtfs2aN3eM5c+aoUKFC2r17txo0aKCYmBh99tlnmj9/vho3bixJioqKUsWKFbVjxw498cQTzigbAABkE9mqj0xMTIwkyd/fX5K0e/du3bx5U02aNLFNU6FCBRUvXlzbt29PdR4JCQmKjY21GwAAQM6UbYKM1WrV0KFDVbduXVWuXFmSdOHCBbm5ucnPz89u2sKFC+vChQupzicyMlK+vr62ISgoKKtLBwAATpJtgkx4eLj279+vBQsW3Nd8Ro0apZiYGNtw+vRpB1UIAACyG6f2kUk2aNAgrVy5Utu2bVOxYsVs7QEBAUpMTNTVq1ftjspcvHhRAQEBqc7L3d1d7u7uWV0yAADIBjJ1ROaPP/5wyMINw9CgQYO0bNkybdq0SaVKlbIbHxISoty5c2vjxo22tkOHDunUqVOqXbu2Q2oAAADmlakjMmXKlFHDhg3Vr18/dezYUR4eHplaeHh4uObPn6/ly5fL29vb1u/F19dXnp6e8vX1Vb9+/RQRESF/f3/5+Pho8ODBql27NlcsAQCAzB2R+eWXX1S1alVFREQoICBAzz//vH7++ed7ns/MmTMVExOjRo0aqUiRIrZh4cKFtmmmTp2qp59+Wh06dFCDBg0UEBCgpUuXZqZsAACQw1gMwzAy++Rbt25pxYoVmjNnjtasWaNy5cqpb9++6tmzpwoWLOjIOjMtNjZWvr6+iomJkY+Pj0PnXXLkqjTHnZjQyqHLAgDgYZLRv9/3ddVSrly51L59ey1evFgTJ07U0aNHNWLECAUFBalXr146f/78/cweAAAgXfcVZHbt2qUXX3xRRYoU0ZQpUzRixAgdO3ZM69ev17lz59S2bVtH1QkAAJBCpjr7TpkyRVFRUTp06JCeeuopffHFF3rqqafk4nI7F5UqVUpz5sxRyZIlHVkrAACAnUwFmZkzZ6pv377q3bu3ihQpkuo0hQoV0meffXZfxQEAAKTnvjr7moGzOvumh47AAACkL0s7+0ZFRWnx4sUp2hcvXqzPP/88M7MEAAC4Z5kKMpGRkSpQoECK9kKFCumdd96576IAAAAyIlNB5tSpUyl+TkCSSpQooVOnTt13UQAAABmRqSBTqFAh7du3L0X73r17lT9//vsuCgAAICMyFWS6deuml156SZs3b1ZSUpKSkpK0adMmDRkyRF27dnV0jQAAAKnK1OXX48aN04kTJxQaGqpcuW7Pwmq1qlevXvSRAQAAD0ymgoybm5sWLlyocePGae/evfL09FSVKlVUokQJR9cHAACQpkwFmWTlypVTuXLlHFULAADAPclUkElKStKcOXO0ceNGXbp0SVar1W78pk2bHFIcAABAejIVZIYMGaI5c+aoVatWqly5siwWi6PrAgAAuKtMBZkFCxZo0aJFeuqppxxdDwAAQIZl6vJrNzc3lSlTxtG1AAAA3JNMBZnhw4fr/fffVw7/vUkAAJDNZerU0vfff6/Nmzdr9erVqlSpknLnzm03funSpQ4pDgAAID2ZCjJ+fn565plnHF0LAADAPclUkImKinJ0HQAAAPcsU31kJOnWrVvasGGDPv74Y127dk2SdO7cOcXFxTmsOAAAgPRk6ojMyZMn1aJFC506dUoJCQlq2rSpvL29NXHiRCUkJGjWrFmOrhMAACCFTB2RGTJkiKpXr64rV67I09PT1v7MM89o48aNDisOAAAgPZk6IvPdd9/pxx9/lJubm117yZIldfbsWYcUBgAAcDeZOiJjtVqVlJSUov3MmTPy9va+76IAAAAyIlNBplmzZpo2bZrtscViUVxcnEaPHs3PFgAAgAcmU6eW3nvvPTVv3lyPPPKIbty4oe7du+vIkSMqUKCAvvrqK0fXCAAAkKpMBZlixYpp7969WrBggfbt26e4uDj169dPPXr0sOv8CwAAkJUyFWQkKVeuXHr22WcdWQsAAMA9yVSQ+eKLL9Id36tXr0wVAwAAcC8yFWSGDBli9/jmzZu6fv263NzclCdPHoIMAAB4IDJ11dKVK1fshri4OB06dEj16tWjsy8AAHhgMv1bS3cqW7asJkyYkOJoDQAAQFZxWJCRbncAPnfunCNnCQAAkKZM9ZFZsWKF3WPDMHT+/Hl99NFHqlu3rkMKAwAAuJtMBZl27drZPbZYLCpYsKAaN26s9957zxF1AQAA3FWmgozVanV0HQAAAPfMoX1kAAAAHqRMHZGJiIjI8LRTpkzJzCIAAADuKlNBZs+ePdqzZ49u3ryp8uXLS5IOHz4sV1dXPf7447bpLBaLY6oEAABIRaaCTOvWreXt7a3PP/9c+fLlk3T7Jnl9+vRR/fr1NXz4cIcWCQAAkJpM9ZF57733FBkZaQsxkpQvXz6NHz+eq5YAAMADk6kgExsbqz///DNF+59//qlr167dd1EAAAAZkakg88wzz6hPnz5aunSpzpw5ozNnzujrr79Wv3791L59+wzPZ9u2bWrdurUCAwNlsVj0zTff2I3v3bu3LBaL3dCiRYvMlAwAAHKgTPWRmTVrlkaMGKHu3bvr5s2bt2eUK5f69eunyZMnZ3g+8fHxqlatmvr27ZtmAGrRooWioqJsj93d3TNTMgAAyIEyFWTy5MmjGTNmaPLkyTp27JgkKTg4WF5eXvc0n5YtW6ply5bpTuPu7q6AgIDMlAkAAHK4+7oh3vnz53X+/HmVLVtWXl5eMgzDUXXZbNmyRYUKFVL58uX1wgsv6PLly+lOn5CQoNjYWLsBAADkTJkKMpcvX1ZoaKjKlSunp556SufPn5ck9evXz6GXXrdo0UJffPGFNm7cqIkTJ2rr1q1q2bKlkpKS0nxOZGSkfH19bUNQUJDD6gEAANlLpoLMsGHDlDt3bp06dUp58uSxtXfp0kVr1qxxWHFdu3ZVmzZtVKVKFbVr104rV67Uzp07tWXLljSfM2rUKMXExNiG06dPO6weAACQvWSqj8y6deu0du1aFStWzK69bNmyOnnypEMKS03p0qVVoEABHT16VKGhoalO4+7uTodgAAAeEpk6IhMfH293JCbZ33//naUh4syZM7p8+bKKFCmSZcsAAADmkakgU79+fX3xxRe2xxaLRVarVZMmTdKTTz6Z4fnExcUpOjpa0dHRkqTjx48rOjpap06dUlxcnF5++WXt2LFDJ06c0MaNG9W2bVuVKVNGzZs3z0zZAAAgh8nUqaVJkyYpNDRUu3btUmJiol555RUdOHBAf//9t3744YcMz2fXrl12wSf5V7XDwsI0c+ZM7du3T59//rmuXr2qwMBANWvWTOPGjePUEQAAkJTJIFO5cmUdPnxYH330kby9vRUXF6f27dsrPDz8nk77NGrUKN1LtteuXZuZ8gAAwEPinoPMzZs31aJFC82aNUuvvfZaVtQEAACQIffcRyZ37tzat29fVtQCAABwTzLV2ffZZ5/VZ5995uhaAAAA7kmm+sjcunVL//3vf7VhwwaFhISk+I2lKVOmOKQ4AACA9NxTkPnjjz9UsmRJ7d+/X48//rgk6fDhw3bTWCwWx1UHAACQjnsKMmXLltX58+e1efNmSbd/kuCDDz5Q4cKFs6Q4AACA9NxTH5k7L5VevXq14uPjHVoQAABARmWqs2+y9O4BAwAAkNXuKchYLJYUfWDoEwMAAJzlnvrIGIah3r17234i4MaNGxo4cGCKq5aWLl3quAoBAADScE9BJiwszO7xs88+69BiAAAA7sU9BZmoqKisqgMAAOCe3VdnXwAAAGciyAAAANMiyAAAANPK1G8t4f6UHLkqzXEnJrR6gJUAAGBuHJEBAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACm5dQgs23bNrVu3VqBgYGyWCz65ptv7MYbhqE333xTRYoUkaenp5o0aaIjR444p1gAAJDtODXIxMfHq1q1apo+fXqq4ydNmqQPPvhAs2bN0k8//SQvLy81b95cN27ceMCVAgCA7CiXMxfesmVLtWzZMtVxhmFo2rRpev3119W2bVtJ0hdffKHChQvrm2++UdeuXR9kqQAAIBvKtn1kjh8/rgsXLqhJkya2Nl9fX9WqVUvbt29P83kJCQmKjY21GwAAQM6UbYPMhQsXJEmFCxe2ay9cuLBtXGoiIyPl6+trG4KCgrK0TgAA4DzZNshk1qhRoxQTE2MbTp8+7eySAABAFsm2QSYgIECSdPHiRbv2ixcv2salxt3dXT4+PnYDAADImbJtkClVqpQCAgK0ceNGW1tsbKx++ukn1a5d24mVAQCA7MKpVy3FxcXp6NGjtsfHjx9XdHS0/P39Vbx4cQ0dOlTjx49X2bJlVapUKb3xxhsKDAxUu3btnFc0AADINpwaZHbt2qUnn3zS9jgiIkKSFBYWpjlz5uiVV15RfHy8BgwYoKtXr6pevXpas2aNPDw8nFUyAADIRiyGYRjOLiIrxcbGytfXVzExMQ7vL1Ny5CqHzk+STkxo5fB5AgBgNhn9+51t+8gAAADcDUEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYFkEGAACYVrYOMmPGjJHFYrEbKlSo4OyyAABANpHL2QXcTaVKlbRhwwbb41y5sn3JAADgAcn2qSBXrlwKCAhwdhkAACAbytanliTpyJEjCgwMVOnSpdWjRw+dOnUq3ekTEhIUGxtrNwAAgJzJYhiG4ewi0rJ69WrFxcWpfPnyOn/+vMaOHauzZ89q//798vb2TvU5Y8aM0dixY1O0x8TEyMfHx6H1lRy5yqHzu5sTE1o90OUBAOAssbGx8vX1vevf72wdZO509epVlShRQlOmTFG/fv1SnSYhIUEJCQm2x7GxsQoKCiLIAABgIhkNMtm+j8y/+fn5qVy5cjp69Gia07i7u8vd3f0BVgUAAJwl2/eR+be4uDgdO3ZMRYoUcXYpAAAgG8jWQWbEiBHaunWrTpw4oR9//FHPPPOMXF1d1a1bN2eXBgAAsoFsfWrpzJkz6tatmy5fvqyCBQuqXr162rFjhwoWLOjs0gAAQDaQrYPMggULnF0CAADIxrL1qSUAAID0EGQAAIBpEWQAAIBpEWQAAIBpZevOvrCX3p2EuesvAOBhxBEZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWgQZAABgWrmcXQAco+TIVam2n5jQ6gFXAgDAg8MRGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFoEGQAAYFr8RAGQAWn9BITEz0AAeDhk15/C4YgMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLVMEmenTp6tkyZLy8PBQrVq19PPPPzu7JAAAkA1k+yCzcOFCRUREaPTo0frll19UrVo1NW/eXJcuXXJ2aQAAwMmyfZCZMmWKnnvuOfXp00ePPPKIZs2apTx58ui///2vs0sDAABOlq3v7JuYmKjdu3dr1KhRtjYXFxc1adJE27dvT/U5CQkJSkhIsD2OiYmRJMXGxjq8PmvCdYfP09GyYr0fRum91mxjAA+DtPaDWbUPTJ6vYRjpTpetg8xff/2lpKQkFS5c2K69cOHCOnjwYKrPiYyM1NixY1O0BwUFZUmN2Z3vNGdXkPOxjQE8zLJ6H3jt2jX5+vqmOT5bB5nMGDVqlCIiImyPrVar/v77b+XPn18Wi8Uhy4iNjVVQUJBOnz4tHx8fh8wzO2I9c56HZV1Zz5yF9cxZMrqehmHo2rVrCgwMTHd+2TrIFChQQK6urrp48aJd+8WLFxUQEJDqc9zd3eXu7m7X5ufnlyX1+fj45Og3WzLWM+d5WNaV9cxZWM+cJSPrmd6RmGTZurOvm5ubQkJCtHHjRlub1WrVxo0bVbt2bSdWBgAAsoNsfURGkiIiIhQWFqbq1aurZs2amjZtmuLj49WnTx9nlwYAAJws2weZLl266M8//9Sbb76pCxcu6NFHH9WaNWtSdAB+kNzd3TV69OgUp7ByGtYz53lY1pX1zFlYz5zF0etpMe52XRMAAEA2la37yAAAAKSHIAMAAEyLIAMAAEyLIAMAAEyLIAMAAEyLIJMJ06dPV8mSJeXh4aFatWrp559/dnZJDrVt2za1bt1agYGBslgs+uabb5xdUpaIjIxUjRo15O3trUKFCqldu3Y6dOiQs8tyuJkzZ6pq1aq2u2jWrl1bq1evdnZZWW7ChAmyWCwaOnSos0txuDFjxshisdgNFSpUcHZZWeLs2bN69tlnlT9/fnl6eqpKlSratWuXs8tyqJIlS6Z4PS0Wi8LDw51dmkMlJSXpjTfeUKlSpeTp6ang4GCNGzfurj8KeTcEmXu0cOFCRUREaPTo0frll19UrVo1NW/eXJcuXXJ2aQ4THx+vatWqafr06c4uJUtt3bpV4eHh2rFjh9avX6+bN2+qWbNmio+Pd3ZpDlWsWDFNmDBBu3fv1q5du9S4cWO1bdtWBw4ccHZpWWbnzp36+OOPVbVqVWeXkmUqVaqk8+fP24bvv//e2SU53JUrV1S3bl3lzp1bq1ev1m+//ab33ntP+fLlc3ZpDrVz506713L9+vWSpE6dOjm5MseaOHGiZs6cqY8++ki///67Jk6cqEmTJunDDz+8vxkbuCc1a9Y0wsPDbY+TkpKMwMBAIzIy0olVZR1JxrJly5xdxgNx6dIlQ5KxdetWZ5eS5fLly2d8+umnzi4jS1y7ds0oW7assX79eqNhw4bGkCFDnF2Sw40ePdqoVq2as8vIcq+++qpRr149Z5fxwA0ZMsQIDg42rFars0txqFatWhl9+/a1a2vfvr3Ro0eP+5ovR2TuQWJionbv3q0mTZrY2lxcXNSkSRNt377diZXBEWJiYiRJ/v7+Tq4k6yQlJWnBggWKj4/Psb9XFh4erlatWtl9TnOiI0eOKDAwUKVLl1aPHj106tQpZ5fkcCtWrFD16tXVqVMnFSpUSI899pg++eQTZ5eVpRITE/Xll1+qb9++slgszi7HoerUqaONGzfq8OHDkqS9e/fq+++/V8uWLe9rvtn+Jwqyk7/++ktJSUkpfh6hcOHCOnjwoJOqgiNYrVYNHTpUdevWVeXKlZ1djsP9+uuvql27tm7cuKG8efNq2bJleuSRR5xdlsMtWLBAv/zyi3bu3OnsUrJUrVq1NGfOHJUvX17nz5/X2LFjVb9+fe3fv1/e3t7OLs9h/vjjD82cOVMRERH6z3/+o507d+qll16Sm5ubwsLCnF1elvjmm2909epV9e7d29mlONzIkSMVGxurChUqyNXVVUlJSXr77bfVo0eP+5ovQQbQ7W/x+/fvz5H9DCSpfPnyio6OVkxMjJYsWaKwsDBt3bo1R4WZ06dPa8iQIVq/fr08PDycXU6W+vc32KpVq6pWrVoqUaKEFi1apH79+jmxMseyWq2qXr263nnnHUnSY489pv3792vWrFk5Nsh89tlnatmypQIDA51disMtWrRI8+bN0/z581WpUiVFR0dr6NChCgwMvK/XkyBzDwoUKCBXV1ddvHjRrv3ixYsKCAhwUlW4X4MGDdLKlSu1bds2FStWzNnlZAk3NzeVKVNGkhQSEqKdO3fq/fff18cff+zkyhxn9+7dunTpkh5//HFbW1JSkrZt26aPPvpICQkJcnV1dWKFWcfPz0/lypXT0aNHnV2KQxUpUiRF2K5YsaK+/vprJ1WUtU6ePKkNGzZo6dKlzi4lS7z88ssaOXKkunbtKkmqUqWKTp48qcjIyPsKMvSRuQdubm4KCQnRxo0bbW1Wq1UbN27Msf0NcjLDMDRo0CAtW7ZMmzZtUqlSpZxd0gNjtVqVkJDg7DIcKjQ0VL/++quio6NtQ/Xq1dWjRw9FR0fn2BAjSXFxcTp27JiKFCni7FIcqm7duiluiXD48GGVKFHCSRVlraioKBUqVEitWrVydilZ4vr163JxsY8drq6uslqt9zVfjsjco4iICIWFhal69eqqWbOmpk2bpvj4ePXp08fZpTlMXFyc3Te748ePKzo6Wv7+/ipevLgTK3Os8PBwzZ8/X8uXL5e3t7cuXLggSfL19ZWnp6eTq3OcUaNGqWXLlipevLiuXbum+fPna8uWLVq7dq2zS3Mob2/vFP2bvLy8lD9//hzX72nEiBFq3bq1SpQooXPnzmn06NFydXVVt27dnF2aQw0bNkx16tTRO++8o86dO+vnn3/W7NmzNXv2bGeX5nBWq1VRUVEKCwtTrlw5809z69at9fbbb6t48eKqVKmS9uzZoylTpqhv3773N+P7uubpIfXhhx8axYsXN9zc3IyaNWsaO3bscHZJDrV582ZDUoohLCzM2aU5VGrrKMmIiopydmkO1bdvX6NEiRKGm5ubUbBgQSM0NNRYt26ds8t6IHLq5dddunQxihQpYri5uRlFixY1unTpYhw9etTZZWWJb7/91qhcubLh7u5uVKhQwZg9e7azS8oSa9euNSQZhw4dcnYpWSY2NtYYMmSIUbx4ccPDw8MoXbq08dprrxkJCQn3NV+LYdznLfUAAACchD4yAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAADAtAgyAHKE06dPq1GjRnrkkUdUtWpVLV682NklAXgA+IkCADnC+fPndfHiRT366KO6cOGCQkJCdPjwYXl5eTm7NABZKGf+xCaAh06RIkVUpEgRSVJAQIAKFCigv//+myAD5HCcWgKQ4+zevVtJSUkKCgpydikAshhHZADkKH///bd69eqlTz75xNmlAHgAOCIDwDR27Nih0NBQ5c+fXxaLxW6IjY1VQkKC2rVrp5EjR6pOnTrOLhfAA0CQAWAKe/fuVaNGjfTYY4/pu+++05o1a+Tv76/Q0FAtXLhQ3t7e6t27txo3bqyePXs6u1wADwhXLQEwhYYNG6po0aKaP3++rW3QoEHavXu3tm/fru+//14NGjRQ1apVbePnzp2rKlWqOKNcAA8IfWQAZHsXL17U999/r61bt9q1e3l5yWKxSJLq1asnq9XqjPIAOBGnlgBke7t375bValW1atVStFevXt1JVQHIDggyALK95CMt8fHxtrZ9+/Zp27Zt6t69u7PKApANEGQAZHu1atWSp6enXn75ZR08eFCrVq1SmzZtFB4erieeeMLZ5QFwIjr7AjCFlStXavjw4Tp+/LiKFy+ugQMHKiIiQi4ufB8DHmYEGQAAYFp8lQEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKZFkAEAAKb1f0w7W8H/XTXOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- RESULTS --- \n",
      "\n",
      "NTK RMSE = 0.76\n",
      "Deep Ensemble RMSE: mean = 0.38, std = 0.03\n",
      "Deep Ensemble NLL: mean = 0.43, std 0.12\n"
     ]
    }
   ],
   "source": [
    "### --- Setup dataset --- ###\n",
    "class RegressionDataset(Dataset):\n",
    "    '''\n",
    "  Prepare dataset for regression.\n",
    "  Input the number of features.\n",
    "\n",
    "  Input:\n",
    "   - dataset: numpy array\n",
    "\n",
    "   Returns:\n",
    "    - Tuple (X,y) - X is a numpy array, y is a double value.\n",
    "  '''\n",
    "    def __init__(self, dataset, input_dim, mX=0, sX=1, my=0, sy=1):\n",
    "        self.X, self.y = dataset[:,:input_dim], dataset[:,input_dim]\n",
    "        self.X, self.y = (self.X - mX)/sX, (self.y - my)/sy\n",
    "        self.len_data = self.X.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "BATCH_SIZE = 100\n",
    "NORMALIZE_X = True\n",
    "NORMALIZE_Y = True\n",
    "\n",
    "## Num of points and dimension of data\n",
    "num_points = len(df)\n",
    "dimension = len(df.columns)-1\n",
    "\n",
    "train_size = int(num_points*TRAIN_RATIO)\n",
    "test_size = num_points - train_size\n",
    "dataset_numpy = df.values\n",
    "np.random.shuffle(dataset_numpy)\n",
    "training_set, test_set = dataset_numpy[:train_size,:], dataset_numpy[train_size:,:]\n",
    "print(\"training set has shape {} \\n\".format(training_set.shape))\n",
    "print(\"test set has shape {}\".format(test_set.shape))\n",
    "\n",
    "if NORMALIZE_X:\n",
    "    train_mX = training_set[:,:num_features].mean(axis=0)\n",
    "    train_sX = training_set[:,:num_features].std(axis=0)\n",
    "    train_sX[train_sX==0]=1\n",
    "else:\n",
    "    train_mX = 0\n",
    "    train_sX = 1\n",
    "\n",
    "if NORMALIZE_Y:\n",
    "    train_my = training_set[:,num_features].mean(axis=0)\n",
    "    train_sy = training_set[:,num_features].std(axis=0)\n",
    "    if train_sy==0:\n",
    "        train_sy=1\n",
    "else:\n",
    "    train_my = 0\n",
    "    train_sy = 1\n",
    "\n",
    "train_dataset = RegressionDataset(training_set, input_dim=num_features, mX=train_mX, sX=train_sX, my=train_my, sy=train_sy)\n",
    "test_dataset = RegressionDataset(test_set, input_dim=num_features, mX=train_mX, sX=train_sX)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_size, shuffle=False)\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "print(\"\\n Sample training point \\n X: {}, \\n y: {} \\n\".format(train_dataset.__getitem__(0)[0], train_dataset.__getitem__(0)[1]))\n",
    "print(\"Sample test point \\n X: {}, \\n y: {} \".format(test_dataset.__getitem__(0)[0], test_dataset.__getitem__(0)[1]))\n",
    "\n",
    "\n",
    "### --- Setup NTK Neural Network --- ###\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"\\n Using {device} device\")\n",
    "\n",
    "INPUT_DIM = 8\n",
    "LAYER_WIDTH = 150\n",
    "  \n",
    "class NeuralNetwork(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(INPUT_DIM, LAYER_WIDTH),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(LAYER_WIDTH, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)\n",
    "  \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        # nn.init.normal_(m.weight,mean=0,std=1)\n",
    "        nn.init.normal_(m.bias,mean=0,std=1)\n",
    "\n",
    "## MSE Model\n",
    "model = NeuralNetwork().to(device=device, dtype=torch.float64)\n",
    "model.apply(weights_init)\n",
    "print(\"Number of parameters in model = {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "\n",
    "### --- Train NTK Neural Network --- ###\n",
    "\n",
    "NTK_LR = 1e-1\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=NTK_LR)\n",
    "\n",
    "def initial_eval(trainloader,testloader,model,loss_function,my,sy):\n",
    "    for i, (X,y) in enumerate(trainloader):\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "        if i==0:\n",
    "            print(\"X has shape {}\".format(X.shape))\n",
    "            print(\"y has shape {}\".format(y.shape))\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred,y)\n",
    "        print(\"Initial train loss for batch {} is {}\".format(i+1, loss))\n",
    "    for i, (X,y) in enumerate(testloader):\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "        if i==0:\n",
    "            print(\"X has shape {}\".format(X.shape))\n",
    "            print(\"y has shape {}\".format(y.shape))\n",
    "        pred = model(X)\n",
    "        pred = pred*sy + my\n",
    "        loss = loss_function(pred,y)\n",
    "        print(\"Initial test loss for batch {} is {}\".format(i+1, loss))\n",
    "\n",
    "def training_loop(dataloader, model, optimizer, loss_function, verbose=False):\n",
    "    model.train()\n",
    "    error = []\n",
    "    train_loss_total = 0\n",
    "    for i, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        # Get and prepare inputs\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(pred, y)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        error.append(torch.square(y-pred))\n",
    "        train_loss_total += train_loss\n",
    "        # Print statistics\n",
    "        if verbose:\n",
    "            print(\"train loss for batch {} is {}\".format(i+1, train_loss))\n",
    "\n",
    "    # train_loss /= num_batches\n",
    "    # print(\"Average train loss is {}\".format(train_loss))\n",
    "    train_loss_mean = train_loss_total / i\n",
    "    return error, train_loss_mean\n",
    "\n",
    "def test_loop(dataloader, model, my, sy, loss_function):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.reshape((y.shape[0],1))\n",
    "            pred = model(X)\n",
    "            pred = pred * sy + my\n",
    "            test_loss = (loss_function(pred, y).item())**0.5\n",
    "            rel_error = torch.square(y-pred)\n",
    "            print(\"--- RMSE = {:.2f} ---\".format(test_loss))\n",
    "    return test_loss, rel_error\n",
    "\n",
    "TRAINING_RUN = True\n",
    "INITIAL_EVAL = False\n",
    "NTK_EPOCHS = 400\n",
    "\n",
    "if INITIAL_EVAL:\n",
    "    initial_eval(train_loader, test_loader, model=model, loss_function=mse_loss, my=train_my, sy=train_sy)\n",
    "\n",
    "if TRAINING_RUN:\n",
    "    # Run the training loop\n",
    "    for epoch in range(NTK_EPOCHS):\n",
    "        print(\"\\n Epoch {} of {}\".format(epoch+1, NTK_EPOCHS))\n",
    "        final_ntk_error, train_loss_mean  = training_loop(train_loader, model, optimizer=adam_optimizer, loss_function=mse_loss,verbose=True)\n",
    "        final_ntk_RMSE, final_ntk_rel_error = test_loop(test_loader, model, my=train_my, sy=train_sy, loss_function=mse_loss)\n",
    "    # Process is complete.\n",
    "    print('\\n Training process has finished.')\n",
    "    print(\"Final training MSE = {:.3f}\".format(train_loss_mean))\n",
    "    print(\"Final test RMSE = {:.2f}\".format(final_ntk_RMSE))\n",
    "\n",
    "\n",
    "\n",
    "### --- Compute uncertainty estimate - NTK --- ###\n",
    "    \n",
    "## Find NTK\n",
    "def flatten_extend_gradient(parameters):\n",
    "    flat_list = []\n",
    "    for parameter in parameters:\n",
    "        flat_list.extend(parameter.grad.detach().numpy().flatten())\n",
    "    return flat_list\n",
    "\n",
    "def gradient_model(model,optimizer,xi):\n",
    "    ## model needs to have parameters with requires_grad=true\n",
    "    optimizer.zero_grad()\n",
    "    model(xi).backward()\n",
    "    grad_vec = np.array(flatten_extend_gradient(list(model.parameters())))\n",
    "    return grad_vec\n",
    "\n",
    "def ntk_single(x1,x2,model):\n",
    "    j1 = gradient_model(model=model,optimizer=adam_optimizer,xi=x1)\n",
    "    j2 = gradient_model(model=model,optimizer=adam_optimizer,xi=x2)\n",
    "    return j1 @ j2.transpose()\n",
    "\n",
    "def ntk_matrix(X1,X2,model):\n",
    "    # Xi must be a torch variable\n",
    "    Kappa = np.empty((len(X1),len(X2)))\n",
    "    for i1,x1 in enumerate(X1):\n",
    "        if len(X1)>1 and len(X2)>1:\n",
    "            if i1==0:\n",
    "                start = time.time()\n",
    "                end = 0\n",
    "            if i1==1:\n",
    "                end = time.time()\n",
    "                it_time = end-start\n",
    "                print(\"Time left = {:.1f}s\".format(it_time*(len(X1)-(i1+1))))\n",
    "            if i1 % 10 == 0 and i1 > 0:\n",
    "                print(\"Computing rows: {:.2f}%\".format(100*(i1+1)/len(X1)))\n",
    "                print(\"Time left = {:.1f}s\".format(it_time*(len(X1)-(i1+1))))\n",
    "        if type(x1) is tuple:\n",
    "            x1,_ = x1\n",
    "        x1 = torch.from_numpy(x1)\n",
    "        for i2,x2 in enumerate(X2):\n",
    "            if type(x2) is tuple:\n",
    "                x2,_ = x2\n",
    "            x2 = torch.from_numpy(x2)\n",
    "            Kappa[i1,i2] = ntk_single(x1,x2,model)\n",
    "    return Kappa\n",
    "\n",
    "def MVP_JTX(v,model,X_training):\n",
    "    p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    mvp = np.zeros((p,1))\n",
    "    for i,(xi,_) in enumerate(X_training):\n",
    "        xi = torch.from_numpy(xi)\n",
    "        g = gradient_model(model=model,optimizer=adam_optimizer,xi=xi).reshape((p,1))\n",
    "        mvp += v[i]*g\n",
    "    return mvp\n",
    "\n",
    "def MVP_JX(v,model,X_training):\n",
    "    p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n = len(train_dataset)\n",
    "    mvp = np.zeros((n,1))\n",
    "    v = v.reshape((p,1))\n",
    "    for i,(xi,_) in enumerate(X_training):\n",
    "        xi = torch.from_numpy(xi)\n",
    "        g = gradient_model(model=model,optimizer=adam_optimizer,xi=xi).reshape((p,1))\n",
    "        mvp[i,0] = g.transpose() @ v\n",
    "    return mvp\n",
    "\n",
    "def MVP_JJT(v,model,X_training):\n",
    "    x1 = MVP_JTX(v,model,X_training)\n",
    "    x2 = MVP_JX(x1,model,X_training)\n",
    "    return x2\n",
    "\n",
    "np.set_printoptions(suppress=False, precision=3)\n",
    "\n",
    "FIND_KAPPA = False\n",
    "REPORT_KAPPA = False\n",
    "FIND_UNCERTAINTY = False\n",
    "FIND_UNCERTAINTY_ITERATIVE = True\n",
    "\n",
    "if FIND_UNCERTAINTY_ITERATIVE:\n",
    "    mvp = lambda v : MVP_JJT(v=v,model=model,X_training=train_dataset)\n",
    "    A = LinearOperator((len(train_dataset),len(train_dataset)), matvec=mvp)\n",
    "    print(\"\\n --- Finding uncertainty estimates --- \\n\")\n",
    "    uncertainty_array = np.empty((1,len(test_dataset)))\n",
    "    for i,(x,_) in enumerate(test_dataset):\n",
    "        if i==0:\n",
    "            start = time.time()\n",
    "            end = 0\n",
    "        if i==1:\n",
    "            end = time.time()\n",
    "            it_time = end-start\n",
    "            print(\"Time left = {:.1f}s\".format(it_time*(len(test_dataset)-(i+1))))\n",
    "            \n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(\"Finding values: {:.2f}%\".format(100*(i+1)/len(test_dataset)))\n",
    "            print(\"Time left = {:.1f}s\".format(it_time*(len(test_dataset)-(i+1))))\n",
    "        x = x.reshape((1,8))\n",
    "        \n",
    "        kappa_xx = ntk_matrix(x, x, model)\n",
    "        kappa_xX = ntk_matrix(train_dataset, x, model)\n",
    "        b = kappa_xX\n",
    "\n",
    "        if i==0:\n",
    "            init = False\n",
    "            x0 = 0\n",
    "        else:\n",
    "            init = True\n",
    "            x0 = x_solve\n",
    "\n",
    "        x_solve = solvers.CR(A,b,rtol=1e-9,init=False, x0=0, maxit=100, VERBOSE=False)\n",
    "\n",
    "\n",
    "        uncertainty_estimate = kappa_xx - b.transpose() @ x_solve\n",
    "\n",
    "        uncertainty_array[0,i] = uncertainty_estimate\n",
    "    \n",
    "    print(\"Finished! \\n\")\n",
    "    print(\"Number of zero values uncertainty array: {}\".format(uncertainty_array[uncertainty_array==0].size))\n",
    "    print(\"Number of negative values for full rank: {}\".format(uncertainty_array[uncertainty_array<0].size))\n",
    "\n",
    "\n",
    "if FIND_KAPPA:\n",
    "    ntk_error = torch.cat(final_ntk_error).detach().numpy()\n",
    "    std_ntk_error = np.std(ntk_error-np.mean(ntk_error))\n",
    "    print(std_ntk_error**2)\n",
    "    EPSILON = std_ntk_error**2\n",
    "    EPSILON = 0\n",
    "    if not NORMALIZE_X and not NORMALIZE_Y:\n",
    "        EPSILON = max(std_ntk_error**2,10)\n",
    "    print(\"\\n --- Finding Kappa --- \\n\")\n",
    "    Kappa = ntk_matrix(train_dataset,train_dataset,model)\n",
    "    Kappa = Kappa + EPSILON*np.eye(len(train_dataset))\n",
    "\n",
    "if REPORT_KAPPA:\n",
    "    x_array = np.random.random((1,len(train_dataset)))\n",
    "    print(\"\\n--- Kappa Summary --- \\n\")\n",
    "    print(\"Regularising constant = {}\".format(EPSILON))\n",
    "    print(\"Condition number of Kappa = {:.2f}\".format(np.linalg.cond(Kappa)))\n",
    "    eigvals = np.linalg.eigvals(Kappa)\n",
    "    print(\"Number of negative eigenvalues of Kappa = {}\".format(eigvals[eigvals<0].size))\n",
    "    print(\"Number of zero eigenvalues of Kappa = {}\".format(eigvals[eigvals==0].size))\n",
    "    print(\"Smallest eigenvalue is = {}\".format(sorted(eigvals)[0]))\n",
    "\n",
    "if FIND_UNCERTAINTY:\n",
    "    print(\"\\n --- Finding uncertainty estimates --- \\n\")\n",
    "    uncertainty_array = np.empty((1,len(test_dataset)))\n",
    "    for i,(x,_) in enumerate(test_dataset):\n",
    "        if i==0:\n",
    "            start = time.time()\n",
    "            end = 0\n",
    "        if i==1:\n",
    "            end = time.time()\n",
    "            it_time = end-start\n",
    "            print(\"Time left = {:.1f}s\".format(it_time*(len(test_dataset)-(i+1))))\n",
    "            \n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(\"Finding values: {:.2f}%\".format(100*(i+1)/len(test_dataset)))\n",
    "            print(\"Time left = {:.1f}s\".format(it_time*(len(test_dataset)-(i+1))))\n",
    "        x = x.reshape((1,8))\n",
    "        \n",
    "        kappa_xx = ntk_matrix(x, x, model)\n",
    "        kappa_xX = ntk_matrix(x, train_dataset, model)\n",
    "\n",
    "        x_solve = np.linalg.solve(Kappa, kappa_xX.transpose())\n",
    "        resid_error = np.linalg.norm(Kappa @ x_solve  - kappa_xX.transpose()) / np.linalg.norm(kappa_xX.transpose())\n",
    "\n",
    "        uncertainty_estimate = kappa_xx - kappa_xX @ np.linalg.solve(Kappa,kappa_xX.transpose())\n",
    "\n",
    "        uncertainty_array[0,i] = uncertainty_estimate\n",
    "    \n",
    "    print(\"Finished! \\n\")\n",
    "    print(\"Number of zero values uncertainty array: {}\".format(uncertainty_array[uncertainty_array==0].size))\n",
    "    print(\"Number of negative values for full rank: {}\".format(uncertainty_array[uncertainty_array<0].size))\n",
    "\n",
    "### --- Setup, train and find uncertainty estimate for Deep Ensemble --- ###\n",
    "\n",
    "def to_np(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "INPUT_DIM = 8\n",
    "LAYER_WIDTH = 50\n",
    "\n",
    "class EnsembleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(INPUT_DIM,LAYER_WIDTH)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_mu = nn.Linear(LAYER_WIDTH,1)\n",
    "        self.linear_sig = nn.Linear(LAYER_WIDTH,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        mu = self.linear_mu(x)\n",
    "        variance = self.linear_sig(x)\n",
    "        variance = F.softplus(variance) + 1e-6\n",
    "        return mu, variance\n",
    "\n",
    "class CustomNLL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNLL, self).__init__()\n",
    "\n",
    "    def forward(self, y, mean, var):\n",
    "        \n",
    "        loss = (0.5*torch.log(var) + 0.5*(y - mean).pow(2)/var).mean() + 1\n",
    "\n",
    "        if np.any(np.isnan(to_np(loss))):\n",
    "            print(torch.log(var))\n",
    "            print((y - mean).pow(2)/var)\n",
    "            raise ValueError('There is Nan in loss')\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def training_loop(dataloader, model, optimizer, loss_function, verbose=False):\n",
    "    model.train()\n",
    "    for i, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        # Get and prepare inputs\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        mean, variance = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(y, mean, variance)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Print statistics\n",
    "        if verbose:\n",
    "            print(\"train loss for batch {} is {}\".format(i+1, train_loss))\n",
    "\n",
    "    # train_loss /= num_batches\n",
    "    # print(\"Average train loss is {}\".format(train_loss))\n",
    "\n",
    "def test_loop(dataloader, model, my, sy, loss_function):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.reshape((y.shape[0],1))\n",
    "            mean, variance = model(X)\n",
    "            mean = mean * sy + my\n",
    "            variance = variance * (sy**2)\n",
    "            test_loss = loss_function(y, mean, variance)\n",
    "            rel_error = torch.square(y-mean)\n",
    "            print(\"--- Test NLL = {:.2f} ---\".format(test_loss))\n",
    "    MSE = mse_loss(y,mean).item()\n",
    "    RMSE = MSE**0.5\n",
    "    return RMSE, test_loss, rel_error\n",
    "\n",
    "M = 5\n",
    "ensemble_learning_rate = 1e-2\n",
    "ENSEMBLE_EPOCHS = 1000\n",
    "\n",
    "model_list = []\n",
    "opt_list = []\n",
    "RMSE_list = np.empty(M)\n",
    "NLL_list = np.empty(M)\n",
    "Ensemble_rel_error_arr = np.empty((M,test_size))\n",
    "\n",
    "for i in range(M):\n",
    "    model_list.append(EnsembleNetwork().to(device=device,dtype=torch.float64))\n",
    "    opt_list.append(torch.optim.Adam(model_list[i].parameters(), lr = ensemble_learning_rate))\n",
    "\n",
    "NLL = CustomNLL()\n",
    "\n",
    "TRAIN_ENSEMBLES = True\n",
    "if TRAIN_ENSEMBLES:\n",
    "    for i in range(M):\n",
    "        for t in range(ENSEMBLE_EPOCHS):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            training_loop(dataloader=train_loader, model=model_list[i], optimizer=opt_list[i], loss_function=NLL, verbose=False)\n",
    "            Ensemble_final_RMSE, Ensemble_final_NLL, Ensemble_rel_error = test_loop(test_loader, model=model_list[i], my=train_my, sy=train_sy, loss_function=NLL)\n",
    "            RMSE_list[i] = Ensemble_final_RMSE\n",
    "            NLL_list[i] = Ensemble_final_NLL\n",
    "            Ensemble_rel_error_arr[i,:] = Ensemble_rel_error.detach().numpy().squeeze(1)\n",
    "            # train_ensemble(X_training, y, model_list[i], NLL, opt_list[i])\n",
    "        print(\"Done!\")\n",
    "\n",
    "print(RMSE_list)\n",
    "print(NLL_list)\n",
    "\n",
    "mu_test_list = np.empty((M,test_size))\n",
    "sigma_test_list = np.empty((M,test_size))\n",
    "for i in range(M):\n",
    "    for X,y in test_loader:\n",
    "        y = y.reshape((y.shape[0],1))\n",
    "        mu, sig = model_list[i](X)\n",
    "        mu = mu * train_sy + train_my\n",
    "        sig = sig * (train_sy**2)\n",
    "        mu_test_list[i,:] = np.reshape(to_np(mu), (test_size))\n",
    "        sigma_test_list[i,:] = np.reshape(to_np(sig),(test_size))\n",
    "mu_mean = np.mean(mu_test_list,axis=0)\n",
    "sigma_mean = np.mean(sigma_test_list, axis=0) + np.mean(np.square(mu_test_list), axis = 0) - np.square(mu_mean)\n",
    "Ensemble_rel_error_mean = np.mean(Ensemble_rel_error_arr,axis=0)\n",
    "\n",
    "### --- Plot results --- ###\n",
    "print(\"\\n --- Plotting Results --- \\n\")\n",
    "\n",
    "if NORMALIZE_X and NORMALIZE_Y:\n",
    "    plot_dir = \"./Plots/{}/Norm/\".format(dataset_str)\n",
    "elif NORMALIZE_X and not NORMALIZE_Y:\n",
    "    plot_dir = \"./Plots/{}/Norm_X/\".format(dataset_str)\n",
    "elif not NORMALIZE_X and NORMALIZE_Y:\n",
    "    plot_dir = \"./Plots/{}/Norm_Y/\".format(dataset_str)\n",
    "else:\n",
    "    plot_dir = \"./Plots/{}/Un_Norm/\".format(dataset_str)\n",
    "\n",
    "if not os.path.isdir(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "## NTK Scatter\n",
    "plot_name = \"NTK_scatter.pdf\"\n",
    "plt.scatter(final_ntk_rel_error, uncertainty_array*(train_sy**2))\n",
    "plt.xlabel(\"Relative squared error\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "plt.title(\"NTK Method\")\n",
    "plt.savefig(plot_dir + plot_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "## Deep Ensemble scatter\n",
    "plot_name = \"Deep_ensemble_scatter.pdf\"\n",
    "plt.scatter(Ensemble_rel_error_mean, sigma_mean)\n",
    "plt.xlabel(\"Relative squared error\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "plt.title(\"Deep Ensemble\")\n",
    "plt.savefig(plot_dir + plot_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "## NTK Histogram\n",
    "plot_name = \"NTK_hist.pdf\"\n",
    "plt.hist(sorted((train_sy**2)*uncertainty_array.squeeze(0)), bins='auto')\n",
    "plt.xlabel(\"$\\sigma^2$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of NTK uncertainty estimates\")\n",
    "plt.savefig(plot_dir + plot_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "## Deep Ensemble Histogram\n",
    "plot_name = \"Deep_ensemble_hist.pdf\"\n",
    "plt.hist(sorted(sigma_mean), bins='auto')\n",
    "plt.xlabel(\"$\\sigma^2$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Deep Ensemble uncertainty estimates\")\n",
    "plt.savefig(plot_dir + plot_name.format(dataset_str), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "### --- Plot calibration curve --- ###\n",
    "\n",
    "def calibration_curve_ntk(testloader, uncertainties, model, num_c,my,sy):\n",
    "    c = np.linspace(0,1,num_c)\n",
    "    observed_true = np.empty(num_c)\n",
    "    total = uncertainties.size\n",
    "    for i, (X,y) in enumerate(testloader):\n",
    "         Yhat_pre = model(X)\n",
    "         Yhat = Yhat_pre.detach().numpy()*sy + my\n",
    "         y = y.detach().numpy()\n",
    "    for i,ci in enumerate(c):\n",
    "        z = scipy.stats.norm.ppf((1+ci)/2)\n",
    "        ci_c = z * np.sqrt(uncertainties*(sy**2))\n",
    "        left_ci = y >= (Yhat - ci_c.reshape(-1,1)).squeeze(1)\n",
    "        right_ci = y <= (Yhat + ci_c.reshape(-1,1)).squeeze(1)\n",
    "        observed_true_c = np.logical_and(left_ci,right_ci)\n",
    "        num_true = observed_true_c[observed_true_c==True].size\n",
    "        observed_true[i] = num_true/total\n",
    "        # print(num_true)\n",
    "    return observed_true\n",
    "\n",
    "def calibration_curve_ensemble(testloader, mu, sigma2, num_c):\n",
    "    c = np.linspace(0,1,num_c)\n",
    "    observed_true = np.empty(num_c)\n",
    "    total = mu.size\n",
    "    for i, (_,y) in enumerate(testloader):\n",
    "         y = y.detach().numpy()\n",
    "    for i,ci in enumerate(c):\n",
    "        z = scipy.stats.norm.ppf((1+ci)/2)\n",
    "        ci_c = z * np.sqrt(sigma2)\n",
    "        left_ci = y >= (mu - ci_c)\n",
    "        right_ci = y <= (mu + ci_c)\n",
    "        observed_true_c = np.logical_and(left_ci,right_ci)\n",
    "        num_true = observed_true_c[observed_true_c==True].size\n",
    "        observed_true[i] = num_true/total\n",
    "        # print(num_true)\n",
    "    return observed_true\n",
    "\n",
    "plot_name = \"calibration_curve.pdf\"\n",
    "def plot_calibration(observed_true_ntk, observed_true_ensemble, dataset_str):\n",
    "    num_c = observed_true_ntk.size\n",
    "    c = c = np.linspace(0,1,num_c)\n",
    "    plt.plot(c,c)\n",
    "    plt.plot(c,observed_true_ntk, label='NTK')\n",
    "    plt.plot(c,observed_true_ensemble, label='Deep Ensemble')\n",
    "    plt.xlabel(\"Expected accuracy\")\n",
    "    plt.ylabel(\"Observed accuracy\")\n",
    "    plt.title(\"Calibration curve - {} Dataset\".format(dataset_str))\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_dir + plot_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "observed_true_ntk = calibration_curve_ntk(testloader=test_loader, uncertainties=uncertainty_array, model=model, num_c=11, my=train_my, sy=train_sy)\n",
    "observed_true_ensemble = calibration_curve_ensemble(testloader=test_loader, mu=mu_mean, sigma2=sigma_mean, num_c=11)\n",
    "plot_calibration(observed_true_ntk=observed_true_ntk, observed_true_ensemble=observed_true_ensemble, dataset_str=dataset_str)\n",
    "\n",
    "results_name = \"results.txt\"\n",
    "with open(plot_dir+results_name,'w') as results:\n",
    "    results.write(\"--- Results --- \\n\")\n",
    "    results.write(\"NTK RMSE = {:.2f} \\n\".format(final_ntk_RMSE))\n",
    "    results.write(\"Deep Ensemble RMSE: mean = {:.2f}, std = {:.2f} \\n\".format(np.mean(RMSE_list),np.std(RMSE_list)))\n",
    "    results.write(\"Deep Ensemble NLL: mean = {:.2f}, std {:.2f} \\n\".format(np.mean(NLL_list),np.std(NLL_list)))\n",
    "    results.write(\"\\n --- Training Details --- \\n\")\n",
    "    results.write(\"NTK Learning Rate = {} \\n\".format(NTK_LR))\n",
    "    results.write(\"NTK Epochs = {} \\n\".format(NTK_EPOCHS))\n",
    "    results.write(\"Deep Ensemble Learning Rate = {} \\n\".format(ensemble_learning_rate))\n",
    "    results.write(\"Deep Ensemble Epochs = {} \\n\".format(ENSEMBLE_EPOCHS))\n",
    "    results.write(\"Number of ensembles = {} \\n\".format(M))\n",
    "    if FIND_UNCERTAINTY_ITERATIVE:\n",
    "        results.write(\"\\n --- NTK Method Details --- \\n\")\n",
    "        results.write(\"\\n Using CR.\")\n",
    "        results.write(\"RTOL = 1e-9\")\n",
    "        results.write(\"MaxIt = 100\")\n",
    "    else:\n",
    "        results.write(\"\\n --- NTK Method Details --- \\n\")\n",
    "        results.write(\"Regularising constant = {:.2f} \\n\".format(EPSILON))\n",
    "        results.write(\"Condition number of Kappa = {:.2f} \\n\".format(np.linalg.cond(Kappa)))\n",
    "        results.write(\"Number of negative eigenvalues of Kappa = {} \\n\".format(eigvals[eigvals<0].size))\n",
    "        results.write(\"Number of zero eigenvalues of Kappa = {} \\n\".format(eigvals[eigvals==0].size))\n",
    "        results.write(\"Number of negative values in NTK uncertainties array: {} \\n\".format(uncertainty_array[uncertainty_array<0].size))\n",
    "        results.write(\"Number of zero values in NTK uncertainties array: {} \\n\".format(uncertainty_array[uncertainty_array==0].size))\n",
    "\n",
    "print(\"\\n --- RESULTS --- \\n\")\n",
    "print(\"NTK RMSE = {:.2f}\".format(final_ntk_RMSE))\n",
    "print(\"Deep Ensemble RMSE: mean = {:.2f}, std = {:.2f}\".format(np.mean(RMSE_list),np.std(RMSE_list)))\n",
    "print(\"Deep Ensemble NLL: mean = {:.2f}, std {:.2f}\".format(np.mean(NLL_list),np.std(NLL_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.005, 0.002, 0.003, 0.01 , 0.001, 0.006, 0.002, 0.001,\n",
       "        0.002, 0.001, 0.004, 0.001, 0.013, 0.001, 0.002, 0.002, 0.013,\n",
       "        0.002, 0.002, 0.007, 0.001, 0.008, 0.003, 0.001, 0.001, 0.001,\n",
       "        0.004, 0.001, 0.007, 0.002, 0.004, 0.001, 0.001, 0.015, 0.001,\n",
       "        0.001, 0.004, 0.001, 0.002, 0.001, 0.002, 0.001, 0.003, 0.003,\n",
       "        0.02 , 0.003, 0.001, 0.01 , 0.003, 0.006, 0.001, 0.001, 0.002,\n",
       "        0.003, 0.005, 0.002, 0.004, 0.018, 0.006, 0.008, 0.005, 0.015,\n",
       "        0.002, 0.003, 0.007, 0.001, 0.001, 0.002, 0.024, 0.004, 0.002,\n",
       "        0.001, 0.002, 0.002, 0.012, 0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
